<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Preparing Data for Modeling | Uncovering Data Science with R</title>
<meta name="author" content="Reza Mohammadi">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Chapter 6 Preparing Data for Modeling | Uncovering Data Science with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://uncovering-data-science.netlify.app/chapter-modeling.html">
<meta property="og:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 Preparing Data for Modeling | Uncovering Data Science with R">
<meta name="twitter:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="Before we can build reliable machine learning models, we must ensure our data is well-prepared. The previous chapters established a foundation by addressing key steps in the Data Science Workflow...">
<meta property="og:description" content="Before we can build reliable machine learning models, we must ensure our data is well-prepared. The previous chapters established a foundation by addressing key steps in the Data Science Workflow...">
<meta name="twitter:description" content="Before we can build reliable machine learning models, we must ensure our data is well-prepared. The previous chapters established a foundation by addressing key steps in the Data Science Workflow...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Uncovering Data Science with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="chapter-into-R.html"><span class="header-section-number">1</span> The Basics for R</a></li>
<li><a class="" href="chapter-intro-DS.html"><span class="header-section-number">2</span> Introduction to Data Science</a></li>
<li><a class="" href="chapter-data-prep.html"><span class="header-section-number">3</span> Data Preparation</a></li>
<li><a class="" href="chapter-EDA.html"><span class="header-section-number">4</span> Exploratory Data Analysis</a></li>
<li><a class="" href="chapter-statistics.html"><span class="header-section-number">5</span> Statistical Inference and Hypothesis Testing</a></li>
<li><a class="active" href="chapter-modeling.html"><span class="header-section-number">6</span> Preparing Data for Modeling</a></li>
<li><a class="" href="chapter-knn.html"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></li>
<li><a class="" href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></li>
<li><a class="" href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></li>
<li><a class="" href="chapter-regression.html"><span class="header-section-number">10</span> Regression Analysis: Foundations and Applications</a></li>
<li><a class="" href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></li>
<li><a class="" href="chapter-nn.html"><span class="header-section-number">12</span> Neural Networks: The Building Blocks of Artificial Intelligence</a></li>
<li><a class="" href="chapter-cluster.html"><span class="header-section-number">13</span> Clustering</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/RezaMoammadi/Book-Data-Science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-modeling" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Preparing Data for Modeling<a class="anchor" aria-label="anchor" href="#chapter-modeling"><i class="fas fa-link"></i></a>
</h1>
<p>Before we can build reliable machine learning models, we must ensure our data is well-prepared. The previous chapters established a foundation by addressing key steps in the Data Science Workflow (Figure <a href="chapter-intro-DS.html#fig:CRISP-DM">2.3</a>). Now, we focus on transitioning from data exploration to model building.</p>
<p>In Chapter <a href="chapter-intro-DS.html#problem-understanding">2.4</a>, we discussed defining the problem and aligning objectives with data-driven strategies. Chapter <a href="chapter-data-prep.html#chapter-data-prep">3</a> addressed handling missing values, outliers, and data transformations to create a clean dataset. In Chapter <a href="chapter-EDA.html#chapter-EDA">4</a>, we visualized data to uncover patterns, while Chapter <a href="chapter-statistics.html#chapter-statistics">5</a> introduced statistical inference, including hypothesis testing and feature selection—tools that will help us validate our data partitioning.</p>
<p>Before diving into machine learning, we must complete the <em>Setup Phase</em>, which ensures that our dataset is structured for robust model development. This phase involves three essential steps:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Partitioning the Data</strong>: Splitting the dataset into training and testing sets to create a clear separation between model learning and evaluation.<br>
</li>
<li>
<strong>Validating the Partition</strong>: Ensuring that the split is representative and unbiased so that insights from training generalize to new data.<br>
</li>
<li>
<strong>Balancing the Training Dataset</strong>: Addressing potential class imbalances in categorical targets to prevent biased models.</li>
</ol>
<p>Although often overlooked, these steps are critical in ensuring that the modeling process is rigorous, fair, and effective. Students often ask, <em>“Why is it necessary to partition the data?”</em> or <em>“Why do we need to follow these specific steps?”</em> These are important questions, and we will address them throughout this chapter. But before we do, it’s useful to briefly examine how the data science process aligns with and diverges from statistical inference. Understanding these similarities and differences helps bridge traditional statistics with the practical demands of modern machine learning.</p>
<div id="statistical-inference-in-the-context-of-data-science" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Statistical Inference in the Context of Data Science<a class="anchor" aria-label="anchor" href="#statistical-inference-in-the-context-of-data-science"><i class="fas fa-link"></i></a>
</h2>
<p>Although statistical inference remains a fundamental tool in data science, its role shifts when preparing data for modeling, as the goals and applications differ. While traditional inference focuses on drawing conclusions about populations from sample data, machine learning prioritizes predictive accuracy and generalization.</p>
<p>Statistical inference and data science diverge in two key ways when applied to modeling tasks:</p>
<ol style="list-style-type: decimal">
<li><p><em>From Significance to Practicality</em>: With large datasets containing thousands or even millions of observations, nearly any detected relationship can become statistically significant. However, statistical significance does not always translate into practical importance. A machine learning model might identify a minute effect size that is statistically valid but has negligible impact on decision-making. In modeling, the focus shifts from statistical significance to assessing whether an effect is strong enough to meaningfully influence predictions.</p></li>
<li><p><em>Exploration vs. Hypothesis Testing</em>: Traditional statistical inference begins with a predefined hypothesis, such as testing whether a new treatment improves outcomes compared to a control. In contrast, data science often adopts an exploratory approach, using data to uncover patterns, relationships, and predictive features without rigid hypotheses. Rather than testing predefined relationships, machine learning practitioners iteratively refine datasets and evaluate which features contribute most to predictive accuracy.</p></li>
</ol>
<p>Despite these differences, statistical inference remains crucial in key stages of data preparation, particularly in:</p>
<ul>
<li>
<em>Partition Validation</em>: When dividing data into training and testing sets, statistical tests help ensure the subsets are representative of the original dataset.</li>
<li>
<em>Feature Selection</em>: Hypothesis testing can aid in selecting features that have strong relationships with the target variable, enhancing model performance.</li>
</ul>
<p>By understanding these differences and applying statistical inference strategically, we can ensure that data preparation supports building robust, interpretable, and generalizable models. Throughout this chapter, we will see how inference techniques continue to play a role in refining datasets for machine learning.</p>
</div>
<div id="why-is-it-necessary-to-partition-the-data" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Why Is It Necessary to Partition the Data?<a class="anchor" aria-label="anchor" href="#why-is-it-necessary-to-partition-the-data"><i class="fas fa-link"></i></a>
</h2>
<p>Partitioning the dataset is a crucial step in preparing data for modeling. A common question students ask is, <em>Why do we need to partition the data?</em> The answer lies in <em>generalization</em>—the ability of a model to perform well on unseen data. Without proper partitioning, models may fit the training data exceptionally well but fail to make accurate predictions in real-world scenarios. Partitioning ensures that performance is evaluated on data the model has not seen before, providing an unbiased measure of its ability to generalize effectively.</p>
<p>Partitioning involves dividing the dataset into two subsets: the <em>training set</em>, used to build the model, and the <em>testing set</em>, used to evaluate performance. This separation simulates real-world conditions, where the model must make predictions on new data. It helps detect and address two common pitfalls in machine learning: <em>overfitting</em> and <em>underfitting</em>. These trade-offs are illustrated in Figure <a href="chapter-modeling.html#fig:model-complexity">6.1</a>, which highlights the balance between model complexity and performance on training and testing datasets.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:model-complexity"></span>
<img src="images/model_complexity.png" alt="The trade-off between model complexity and accuracy on the training and test sets. It highlights the optimal model complexity (sweet spot), where the test set accuracy reaches its highest value for unseen data." width="65%"><p class="caption">
Figure 6.1: The trade-off between model complexity and accuracy on the training and test sets. It highlights the optimal model complexity (sweet spot), where the test set accuracy reaches its highest value for unseen data.
</p>
</div>
<p><strong>Overfitting</strong> occurs when a model memorizes the training data, including noise and random fluctuations, instead of capturing general patterns. Such models achieve high accuracy on the training set but perform poorly on unseen data. For instance, a churn prediction model might memorize specific customer IDs rather than recognizing broader behavioral trends, making it ineffective for new customers.</p>
<p><strong>Underfitting</strong>, in contrast, occurs when a model is too simplistic to capture underlying patterns. This might happen if the model lacks complexity or if preprocessing removes too much useful information. An underfitted churn model, for example, might predict a constant churn rate for all customers without considering individual differences, leading to poor performance.</p>
<p>Partitioning mitigates these risks by allowing us to evaluate performance on unseen data. Comparing accuracy on the training and testing sets helps determine whether the model is overfitting (high training accuracy but low testing accuracy) or underfitting (low accuracy on both). This evaluation enables iterative refinements to strike the right balance between complexity and generalization.</p>
<p>Partitioning also prevents <em>data leakage</em>, a critical issue where information from the testing set inadvertently influences training. Data leakage inflates performance metrics, creating a false sense of confidence in the model’s ability to generalize. Strictly separating the testing set from the training process ensures a more realistic assessment of model performance.</p>
<p>Beyond a simple train-test split, <em>cross-validation</em> further enhances robustness. In cross-validation, the dataset is divided into multiple subsets (<em>folds</em>). The model is trained on one subset and tested on another, repeating this process across all folds. The results are averaged to provide a more reliable estimate of model performance. Cross-validation is particularly useful when working with small datasets or tuning hyperparameters, as it minimizes bias introduced by a single train-test split.</p>
<p>Partitioning isn’t just a technical step—it’s fundamental to building models that generalize well. By addressing overfitting, underfitting, and data leakage, and by leveraging techniques like cross-validation, we ensure that models are both accurate and reliable in real-world applications.</p>
<p>To summarize, the general strategy for supervised machine learning consists of three key steps, illustrated in Figure <a href="chapter-modeling.html#fig:modeling">6.2</a>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Partitioning</strong> the dataset into training and testing sets, followed by validating the partition.</li>
<li>
<strong>Building</strong> machine learning models on the training data.</li>
<li>
<strong>Evaluating</strong> the performance of models on the testing data to select the most effective approach.</li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:modeling"></span>
<img src="images/partitioning.png" alt="A general predictive machine learning process for building and evaluating models. The 80-20 split ratio is an example and may vary based on the dataset and task." width="80%"><p class="caption">
Figure 6.2: A general predictive machine learning process for building and evaluating models. The 80-20 split ratio is an example and may vary based on the dataset and task.
</p>
</div>
<p>By following this structured process, we build models that are both robust and capable of making accurate predictions on unseen data. This chapter focuses on the first step: partitioning the data effectively, validating the partition, and preparing a balanced training dataset—key steps for developing reliable and interpretable machine learning models.</p>
</div>
<div id="sec-partitioning" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Partitioning the Data<a class="anchor" aria-label="anchor" href="#sec-partitioning"><i class="fas fa-link"></i></a>
</h2>
<p>Partitioning the dataset is a fundamental step in preparing data for machine learning. The most common approach is the <em>train-test split</em>, also known as the holdout method, where the dataset is divided into two subsets: a <em>training set</em> used to build the model and a <em>testing set</em> reserved for evaluating its performance. This separation ensures that the model is assessed on unseen data, providing an unbiased estimate of how well it generalizes.</p>
<p>A typical train-test split ratio is 70-30, 80-20, or 90-10, depending on the dataset size and modeling needs. The training set contains all available features, including the target variable, which is used to teach the model patterns in the data. The testing set, however, has the target variable temporarily hidden to simulate real-world conditions. The trained model is then applied to the testing set to predict these hidden values, and its predictions are compared to the actual target values to assess performance.</p>
<div id="example-train-test-split-in-r" class="section level3 unnumbered">
<h3>Example: Train-Test Split in R<a class="anchor" aria-label="anchor" href="#example-train-test-split-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>To illustrate, we revisit the <em>churn</em> dataset from Section <a href="chapter-EDA.html#EDA-sec-churn">4.3</a>, where the goal is to predict customer churn. We first load the dataset:</p>
<div class="sourceCode" id="cb126"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.uva.nl/profile/a.mohammadi">liver</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span> </span></code></pre></div>
<p>We then split the <em>churn</em> dataset into training and testing subsets by using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function from the <strong>liver</strong> package. The following code demonstrates how to create these subsets in R:</p>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">43</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/partition.html">partition</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">actual_test</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">churn</span></span></code></pre></div>
<p>The code begins by setting a seed using <code>set.seed(43)</code>, which ensures reproducibility by making the random split consistent across different runs. This is particularly important when sharing results or collaborating on a project. Next, the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function is used to divide the dataset into two subsets: 80% of the data is assigned to <code>train_set</code> for model training, while the remaining 20% is stored in <code>test_set</code> for evaluation. Finally, <code>actual_test</code> is created to store the true target values from the test set, which will be used later to assess the model’s predictive accuracy.</p>
<p>Reproducibility is essential in machine learning. Setting a seed guarantees that results remain consistent, allowing others to replicate findings exactly. While the seed value itself is arbitrary, using one ensures that the partitioning process is stable across different runs.</p>
</div>
<div id="why-partitioning-matters" class="section level3 unnumbered">
<h3>Why Partitioning Matters<a class="anchor" aria-label="anchor" href="#why-partitioning-matters"><i class="fas fa-link"></i></a>
</h3>
<p>The primary reason for partitioning is to prevent <em>data leakage</em>—a situation where information from the testing set influences training, leading to overly optimistic performance estimates. By strictly separating these sets, we ensure that performance metrics reflect the model’s ability to generalize to new data rather than just memorizing training patterns.</p>
<p>Beyond a simple train-test split, <em>cross-validation</em> can further enhance robustness by training and testing the model on multiple subsets of the data. This method is particularly useful when working with small datasets or tuning hyperparameters.</p>
<p>Partitioning lays the groundwork for reliable machine learning models. However, a well-executed split alone does not guarantee that the training and testing sets are representative of the original dataset. In the next section, we will validate the partition to confirm that both subsets retain key statistical properties, ensuring fair and unbiased model evaluation.</p>
</div>
</div>
<div id="sec-validate-partition" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Validating the Partition<a class="anchor" aria-label="anchor" href="#sec-validate-partition"><i class="fas fa-link"></i></a>
</h2>
<p>The success of the entire modeling process depends on the quality of the data partition. Validating the partition ensures that both the training and testing sets are representative of the original dataset, enabling the model to learn from diverse examples and generalize effectively to unseen data. Without validation, the modeling process risks bias—either the model fails to generalize because the training set isn’t representative, or the testing set doesn’t provide an accurate evaluation of real-world performance.</p>
<p>Validation involves comparing the training and testing sets to confirm that their distributions are statistically similar, particularly for key variables. Since datasets often include many variables, this step typically focuses on a small set of randomly selected features or features of particular importance, such as the target variable. The choice of statistical test depends on the type of variable being compared, as shown in Table <a href="chapter-modeling.html#tab:partition-test">6.1</a>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:partition-test">Table 6.1: </span> Suggested hypothesis tests for validating partitions, based on the type of target variable.</caption>
<colgroup>
<col width="43%">
<col width="56%">
</colgroup>
<thead><tr>
<th>Type of Variable</th>
<th>Suggested Test (from Chapter <a href="chapter-statistics.html#chapter-statistics">5</a>)</th>
</tr></thead>
<tbody>
<tr>
<td>Numerical variable</td>
<td>Two-sample t-test</td>
</tr>
<tr>
<td>Binary/Flag variable</td>
<td>Two-sample Z-test</td>
</tr>
<tr>
<td>Categorical variable (with &gt; 2 categories)</td>
<td>Chi-square test</td>
</tr>
</tbody>
</table></div>
<p>Validating the partition is more than a procedural step—it is a safeguard against biased modeling. If the training and testing sets differ significantly, the model’s performance could be compromised. If the training set is not representative of the original dataset, the model may fail to generalize effectively. Conversely, if the testing set does not reflect the population, model evaluation could be misleading. Ensuring that the split retains the characteristics of the original dataset allows for fair and reliable model assessment.</p>
<div id="example-validating-the-target-variable-churn" class="section level3 unnumbered">
<h3>Example: Validating the Target Variable <em>churn</em><a class="anchor" aria-label="anchor" href="#example-validating-the-target-variable-churn"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s consider the <em>churn</em> dataset introduced in the previous section. The target variable, <em>churn</em> (whether a customer has churned or not), is binary. According to Table <a href="chapter-modeling.html#tab:partition-test">6.1</a>, the appropriate statistical test to validate the partition for this variable is a <em>Two-Sample Z-Test</em>, which compares the proportion of churned customers in the training and testing sets. Thus, the hypotheses for the test are:<br><span class="math display">\[
\begin{cases}
H_0:  \pi_{\text{churn, train}} = \pi_{\text{churn, test}} \quad \text{(Proportions are equal)} \\
H_a:  \pi_{\text{churn, train}} \neq \pi_{\text{churn, test}} \quad \text{(Proportions are not equal)}
\end{cases}
\]</span></p>
<p>Here’s how it can be implemented in <strong>R</strong>:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="chapter-modeling.html#cb128-1" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(train_set<span class="sc">$</span>churn <span class="sc">==</span> <span class="st">"yes"</span>)</span>
<span id="cb128-2"><a href="chapter-modeling.html#cb128-2" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(test_set<span class="sc">$</span>churn <span class="sc">==</span> <span class="st">"yes"</span>)</span>
<span id="cb128-3"><a href="chapter-modeling.html#cb128-3" tabindex="-1"></a></span>
<span id="cb128-4"><a href="chapter-modeling.html#cb128-4" tabindex="-1"></a>n1 <span class="ot">&lt;-</span> <span class="fu">nrow</span>(train_set)</span>
<span id="cb128-5"><a href="chapter-modeling.html#cb128-5" tabindex="-1"></a>n2 <span class="ot">&lt;-</span> <span class="fu">nrow</span>(test_set)</span>
<span id="cb128-6"><a href="chapter-modeling.html#cb128-6" tabindex="-1"></a></span>
<span id="cb128-7"><a href="chapter-modeling.html#cb128-7" tabindex="-1"></a>test_churn <span class="ot">&lt;-</span> <span class="fu">prop.test</span>(<span class="at">x =</span> <span class="fu">c</span>(x1, x2), <span class="at">n =</span> <span class="fu">c</span>(n1, n2))</span>
<span id="cb128-8"><a href="chapter-modeling.html#cb128-8" tabindex="-1"></a>test_churn</span>
<span id="cb128-9"><a href="chapter-modeling.html#cb128-9" tabindex="-1"></a>   </span>
<span id="cb128-10"><a href="chapter-modeling.html#cb128-10" tabindex="-1"></a>    <span class="dv">2</span><span class="sc">-</span>sample test <span class="cf">for</span> equality of proportions with continuity correction</span>
<span id="cb128-11"><a href="chapter-modeling.html#cb128-11" tabindex="-1"></a>   </span>
<span id="cb128-12"><a href="chapter-modeling.html#cb128-12" tabindex="-1"></a>   data<span class="sc">:</span>  <span class="fu">c</span>(x1, x2) out of <span class="fu">c</span>(n1, n2)</span>
<span id="cb128-13"><a href="chapter-modeling.html#cb128-13" tabindex="-1"></a>   X<span class="sc">-</span>squared <span class="ot">=</span> <span class="fl">0.1566</span>, df <span class="ot">=</span> <span class="dv">1</span>, p<span class="sc">-</span>value <span class="ot">=</span> <span class="fl">0.6923</span></span>
<span id="cb128-14"><a href="chapter-modeling.html#cb128-14" tabindex="-1"></a>   alternative hypothesis<span class="sc">:</span> two.sided</span>
<span id="cb128-15"><a href="chapter-modeling.html#cb128-15" tabindex="-1"></a>   <span class="dv">95</span> percent confidence interval<span class="sc">:</span></span>
<span id="cb128-16"><a href="chapter-modeling.html#cb128-16" tabindex="-1"></a>    <span class="sc">-</span><span class="fl">0.0190317</span>  <span class="fl">0.0300317</span></span>
<span id="cb128-17"><a href="chapter-modeling.html#cb128-17" tabindex="-1"></a>   sample estimates<span class="sc">:</span></span>
<span id="cb128-18"><a href="chapter-modeling.html#cb128-18" tabindex="-1"></a>   prop <span class="dv">1</span> prop <span class="dv">2</span> </span>
<span id="cb128-19"><a href="chapter-modeling.html#cb128-19" tabindex="-1"></a>   <span class="fl">0.1425</span> <span class="fl">0.1370</span></span></code></pre></div>
<p>Here, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> represent the number of churned customers in the training and testing sets, respectively, while <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> denote the total number of observations in each set. The <code><a href="https://rdrr.io/r/stats/prop.test.html">prop.test()</a></code> function is used to compare the proportions of churned customers between the two subsets.</p>
<p>The test result provides a <em>p</em>-value = 0.69. Since the <em>p</em>-value is greater than the significance level (<span class="math inline">\(\alpha = 0.05\)</span>), we fail to reject the null hypothesis (<span class="math inline">\(H_0\)</span>). This indicates no statistically significant difference in the proportions of churned customers between the training and testing sets. By failing to reject <span class="math inline">\(H_0\)</span>, we confirm that the partition is valid with respect to the target variable <em>churn</em>. The proportions of churned customers are consistent across both subsets, ensuring that the model will be trained and tested on representative data.</p>
<p>While validating the target variable is crucial, extending this process to key predictors such as <code>customer.calls</code> or <code>day.mins</code> ensures that both subsets remain representative across all important features. For example, numerical features can be validated using a two-sample t-test, while categorical features with multiple levels can be assessed using a Chi-square test. This broader validation ensures that important variables retain their statistical properties across training and testing sets.</p>
</div>
<div id="what-if-the-partition-is-invalid" class="section level3 unnumbered">
<h3>What If the Partition Is Invalid?<a class="anchor" aria-label="anchor" href="#what-if-the-partition-is-invalid"><i class="fas fa-link"></i></a>
</h3>
<p>If statistical tests reveal significant differences between the training and testing sets, adjustments are necessary to ensure the partition remains representative. Several strategies can be applied:</p>
<ul>
<li>
<em>Revisiting the partitioning process</em>: Changing the random seed or adjusting the split ratio can sometimes lead to a more balanced split.</li>
<li>
<em>Stratified sampling</em>: Ensuring that key categorical variables, such as the target variable, are proportionally represented in both subsets.</li>
<li>
<em>Cross-validation</em>: Using k-fold cross-validation instead of a single train-test split to provide a more robust evaluation of model performance.</li>
</ul>
<p>Additionally, if the dataset is small or highly variable, minor differences between training and testing sets might be inevitable. In such cases, alternative approaches like bootstrapping can help validate model performance more effectively.</p>
<p>Validating the partition is a critical step in the data preparation process. It ensures that the modeling process is fair, reliable, and capable of producing generalizable results. By addressing potential discrepancies early, we set the stage for robust machine learning models that perform effectively on real-world, unseen data.</p>
</div>
</div>
<div id="balancing-the-training-dataset" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Balancing the Training Dataset<a class="anchor" aria-label="anchor" href="#balancing-the-training-dataset"><i class="fas fa-link"></i></a>
</h2>
<p>In many real-world classification problems, one class of the target variable is significantly underrepresented. This imbalance can lead to biased models that perform well for the majority class but fail to predict the minority class accurately. For example, in fraud detection, fraudulent transactions are rare compared to legitimate ones, and in churn prediction, the majority of customers may not churn. Without addressing this issue, models may appear to perform well based on accuracy alone but fail to identify rare yet important events.</p>
<p>Imbalanced datasets pose a challenge because most machine learning algorithms optimize for overall accuracy, which can favor the majority class. A churn prediction model trained on an imbalanced dataset, for example, might classify nearly all customers as non-churners, leading to high accuracy but failing to detect actual churners. This is problematic when the minority class (e.g., fraud cases, churners, or patients with a rare disease) is the key focus of the analysis.</p>
<div id="techniques-for-addressing-class-imbalance" class="section level3 unnumbered">
<h3>Techniques for Addressing Class Imbalance<a class="anchor" aria-label="anchor" href="#techniques-for-addressing-class-imbalance"><i class="fas fa-link"></i></a>
</h3>
<p>Balancing the training dataset ensures that both classes are adequately represented during model training, improving the model’s ability to generalize. Several techniques can be used to address class imbalance:</p>
<ul>
<li>
<em>Oversampling</em>: Increasing the number of minority class examples by duplicating existing observations or generating synthetic samples. The <em>Synthetic Minority Over-sampling Technique (SMOTE)</em> is a popular approach that generates synthetic examples instead of simple duplication.</li>
<li>
<em>Undersampling</em>: Reducing the number of majority class examples by randomly removing observations.</li>
<li>
<em>Hybrid Methods</em>: Combining oversampling and undersampling to achieve a balanced dataset.</li>
<li>
<em>Class Weights</em>: Modifying the algorithm to penalize misclassifications of the minority class more heavily.</li>
</ul>
<p>The choice of technique depends on factors such as dataset size, the severity of imbalance, and the specific machine learning algorithm used.</p>
</div>
<div id="example-balancing-the-churn-dataset" class="section level3 unnumbered">
<h3>Example: Balancing the <em>Churn</em> Dataset<a class="anchor" aria-label="anchor" href="#example-balancing-the-churn-dataset"><i class="fas fa-link"></i></a>
</h3>
<p>First, we examine the distribution of the target variable (<em>churn</em>) in the training dataset:</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="chapter-modeling.html#cb129-1" tabindex="-1"></a><span class="co"># Check the class distribution</span></span>
<span id="cb129-2"><a href="chapter-modeling.html#cb129-2" tabindex="-1"></a><span class="fu">table</span>(train_set<span class="sc">$</span>churn)</span>
<span id="cb129-3"><a href="chapter-modeling.html#cb129-3" tabindex="-1"></a>   </span>
<span id="cb129-4"><a href="chapter-modeling.html#cb129-4" tabindex="-1"></a>    yes   no </span>
<span id="cb129-5"><a href="chapter-modeling.html#cb129-5" tabindex="-1"></a>    <span class="dv">570</span> <span class="dv">3430</span></span>
<span id="cb129-6"><a href="chapter-modeling.html#cb129-6" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(train_set<span class="sc">$</span>churn))</span>
<span id="cb129-7"><a href="chapter-modeling.html#cb129-7" tabindex="-1"></a>   </span>
<span id="cb129-8"><a href="chapter-modeling.html#cb129-8" tabindex="-1"></a>      yes     no </span>
<span id="cb129-9"><a href="chapter-modeling.html#cb129-9" tabindex="-1"></a>   <span class="fl">0.1425</span> <span class="fl">0.8575</span></span></code></pre></div>
<p>Suppose the output shows that churners (<code>churn = "yes"</code>) constitute only 0.14, while non-churners (<code>churn = "no"</code>) make up 0.86. This significant imbalance suggests that balancing may be necessary, particularly if churn prediction is a business priority.</p>
<p>To address this, we use the <strong>ROSE</strong> package in R to oversample the minority class (<code>churn = "yes"</code>) so that it constitutes 30% of the training dataset:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="chapter-modeling.html#cb130-1" tabindex="-1"></a><span class="co"># Load the ROSE package</span></span>
<span id="cb130-2"><a href="chapter-modeling.html#cb130-2" tabindex="-1"></a><span class="fu">library</span>(ROSE)</span>
<span id="cb130-3"><a href="chapter-modeling.html#cb130-3" tabindex="-1"></a></span>
<span id="cb130-4"><a href="chapter-modeling.html#cb130-4" tabindex="-1"></a><span class="co"># Oversample the training set to balance the classes with 30% churners</span></span>
<span id="cb130-5"><a href="chapter-modeling.html#cb130-5" tabindex="-1"></a>balanced_train_set <span class="ot">&lt;-</span> <span class="fu">ovun.sample</span>(churn <span class="sc">~</span> ., <span class="at">data =</span> train_set, <span class="at">method =</span> <span class="st">"over"</span>, <span class="at">p =</span> <span class="fl">0.3</span>)<span class="sc">$</span>data</span>
<span id="cb130-6"><a href="chapter-modeling.html#cb130-6" tabindex="-1"></a></span>
<span id="cb130-7"><a href="chapter-modeling.html#cb130-7" tabindex="-1"></a><span class="co"># Check the new class distribution</span></span>
<span id="cb130-8"><a href="chapter-modeling.html#cb130-8" tabindex="-1"></a><span class="fu">table</span>(balanced_train_set<span class="sc">$</span>churn)</span>
<span id="cb130-9"><a href="chapter-modeling.html#cb130-9" tabindex="-1"></a>   </span>
<span id="cb130-10"><a href="chapter-modeling.html#cb130-10" tabindex="-1"></a>     no  yes </span>
<span id="cb130-11"><a href="chapter-modeling.html#cb130-11" tabindex="-1"></a>   <span class="dv">3430</span> <span class="dv">1444</span></span>
<span id="cb130-12"><a href="chapter-modeling.html#cb130-12" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(balanced_train_set<span class="sc">$</span>churn))</span>
<span id="cb130-13"><a href="chapter-modeling.html#cb130-13" tabindex="-1"></a>   </span>
<span id="cb130-14"><a href="chapter-modeling.html#cb130-14" tabindex="-1"></a>          no       yes </span>
<span id="cb130-15"><a href="chapter-modeling.html#cb130-15" tabindex="-1"></a>   <span class="fl">0.7037341</span> <span class="fl">0.2962659</span></span></code></pre></div>
<p>In this example, the <code>ovun.sample()</code> function increases the proportion of churners to 30% of the training dataset. The formula notation <code>churn ~ .</code> specifies that the balancing is applied based on the target variable (<em>churn</em>). After oversampling, the new class distribution is checked to ensure the desired balance.</p>
</div>
<div id="key-considerations-for-balancing" class="section level3 unnumbered">
<h3>Key Considerations for Balancing<a class="anchor" aria-label="anchor" href="#key-considerations-for-balancing"><i class="fas fa-link"></i></a>
</h3>
<p>Balancing should be performed <em>only on the training dataset</em>, not the test dataset. The test dataset should remain representative of the original class distribution to provide an unbiased evaluation of model performance. Modifying the test set would introduce bias and make the model’s performance appear artificially better than it would be in real-world scenarios.</p>
<p>Furthermore, balancing must be applied <em>after partitioning</em> the dataset. If balancing is done before splitting, information from the test set may influence the training process (<em>data leakage</em>), leading to misleadingly high performance.</p>
<p>That said, balancing is <em>not always necessary</em>. Many modern machine learning algorithms, such as random forests and gradient boosting, incorporate class weighting or ensemble learning to handle imbalanced datasets effectively. Additionally, alternative evaluation metrics such as <em>precision, recall, F1-score, and AUC-ROC</em> can provide better insights into model performance when dealing with imbalanced classes.</p>
<p>In summary, balancing the training dataset can improve model performance, especially when the minority class is the primary focus. However, it is not always required and should be used selectively. If balancing is necessary, it must be applied <em>only after partitioning</em> to maintain the validity of model evaluation. By ensuring that both classes are adequately represented during training, we help machine learning models make more accurate and fair predictions.</p>
</div>
</div>
<div id="exercises-4" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-4"><i class="fas fa-link"></i></a>
</h2>
<div id="conceptual-questions-2" class="section level3 unnumbered">
<h3>Conceptual Questions<a class="anchor" aria-label="anchor" href="#conceptual-questions-2"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p>Why is partitioning the dataset crucial before training a machine learning model? Explain its role in ensuring generalization.<br></p></li>
<li><p>What is the main risk of training a model without separating the dataset into training and testing subsets? Provide an example where this could lead to misleading results.</p></li>
<li><p>Explain the difference between <em>overfitting</em> and <em>underfitting</em>. How does proper partitioning help address these issues?</p></li>
<li><p>Describe the role of the <em>training set</em> and the <em>testing set</em> in machine learning. Why should the test set remain unseen during model training?</p></li>
<li><p>What is <em>data leakage</em>, and how can it occur during data partitioning? Provide an example of a scenario where data leakage could lead to overly optimistic model performance.</p></li>
<li><p>Compare and contrast <em>random partitioning</em> and <em>stratified partitioning</em>. When would stratified partitioning be preferred?</p></li>
<li><p>Why is it necessary to validate the partition after splitting the dataset? What could go wrong if the training and test sets are significantly different?</p></li>
<li><p>How would you validate that numerical variables, such as <code>customer.calls</code> in the <em>churn</em> dataset, have similar distributions in both the training and testing sets?</p></li>
<li><p>If a dataset is highly imbalanced, why might a model trained on it fail to generalize well? Provide an example from a real-world domain where class imbalance is a serious issue.</p></li>
<li><p>Compare <em>oversampling</em>, <em>undersampling</em>, and <em>hybrid methods</em> for handling imbalanced datasets. What are the advantages and disadvantages of each?</p></li>
<li><p>Why should balancing techniques be applied <em>only</em> to the training dataset and <em>not</em> to the test dataset?</p></li>
<li><p>Some machine learning algorithms are robust to class imbalance, while others require explicit handling of imbalance. Which types of models typically require class balancing, and which can handle imbalance naturally?</p></li>
<li><p>When dealing with class imbalance, why is <em>accuracy</em> not always the best metric to evaluate model performance? Which alternative metrics should be considered?</p></li>
<li><p>Suppose a dataset has a rare but critical class (e.g., fraud detection). What steps should be taken in the <em>data partitioning and balancing phase</em> to ensure an effective model?</p></li>
</ol>
</div>
<div id="hands-on-practice" class="section level3 unnumbered">
<h3>Hands-On Practice<a class="anchor" aria-label="anchor" href="#hands-on-practice"><i class="fas fa-link"></i></a>
</h3>
<p>For the following exercises, use the <em>churn</em>, <em>bank</em>, and <em>risk</em> datasets available in the <strong>liver</strong> package. We have previously used the <em>churn</em> and <em>bank</em> datasets in this and earlier chapters. In Chapter <a href="chapter-bayes.html#chapter-bayes">9</a>, we will introduce the <em>risk</em> dataset. Load the datasets using:</p>
<div class="sourceCode" id="cb131"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.uva.nl/profile/a.mohammadi">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load datasets</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">bank</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">risk</span><span class="op">)</span></span></code></pre></div>
<div id="partitioning-the-data" class="section level4 unnumbered">
<h4>Partitioning the Data<a class="anchor" aria-label="anchor" href="#partitioning-the-data"><i class="fas fa-link"></i></a>
</h4>
<ol start="15" style="list-style-type: decimal">
<li><p>Using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function, split the <em>churn</em> dataset into 75% training and 25% testing. Ensure reproducibility by setting a seed value before partitioning.</p></li>
<li><p>Perform a <em>90-10 train-test split</em> on the <em>bank</em> dataset. Report the number of observations in each subset.</p></li>
<li><p>Apply <em>stratified sampling</em> to partition the <em>churn</em> dataset, ensuring that the proportion of churners (<code>churn == "yes"</code>) remains the same in both training and test sets.</p></li>
<li><p>In the <em>risk</em> dataset, partition the data using a <em>60-40</em> split and store the training and test sets as <code>train_risk</code> and <code>test_risk</code>.</p></li>
<li><p>Compare the distribution of <code>income</code> in the training and test sets of the <em>bank</em> dataset using <em>density plots</em>. Do they appear similar?</p></li>
</ol>
</div>
<div id="validating-the-partition" class="section level4 unnumbered">
<h4>Validating the Partition<a class="anchor" aria-label="anchor" href="#validating-the-partition"><i class="fas fa-link"></i></a>
</h4>
<ol start="20" style="list-style-type: decimal">
<li><p>In the <em>churn</em> dataset, test whether the proportion of churners is <em>statistically different</em> between the training and test sets. Use a <em>two-sample Z-test</em>.</p></li>
<li><p>In the <em>bank</em> dataset, test whether the <em>average age</em> of customers differs significantly between the training and test sets using a <em>two-sample t-test</em>.</p></li>
<li><p>Perform a <em>Chi-square test</em> to validate whether the distribution of marital status (<code>marital</code>) in the <em>bank</em> dataset is similar between the training and test sets.</p></li>
<li><p>Suppose the <em>churn</em> dataset was partitioned incorrectly, resulting in the training set having <em>30% churners</em> and the test set having <em>15% churners</em>. What statistical test could confirm this issue, and how could it be corrected?</p></li>
<li><p>Select three numerical variables from the <em>risk</em> dataset and validate whether their distributions differ between the training and test sets using appropriate statistical tests.</p></li>
</ol>
</div>
<div id="balancing-the-training-dataset-1" class="section level4 unnumbered">
<h4>Balancing the Training Dataset<a class="anchor" aria-label="anchor" href="#balancing-the-training-dataset-1"><i class="fas fa-link"></i></a>
</h4>
<ol start="25" style="list-style-type: decimal">
<li><p>In the <em>churn</em> dataset, check whether churners (<code>churn = "yes"</code>) are underrepresented in the training dataset. Report the class proportions.</p></li>
<li><p>Use <em>random oversampling</em> to increase the number of churners (<code>churn = "yes"</code>) in the training set to <em>40%</em> of the dataset using the <em>ROSE</em> package.</p></li>
<li><p>Apply <em>undersampling</em> in the <em>bank</em> dataset so that the proportion of customers with <code>deposit = "yes"</code> and <code>deposit = "no"</code> is <em>equal</em> in the training set.</p></li>
<li><p>Compare the class distributions <em>before and after balancing</em> the <em>churn</em> dataset. Use <em>bar plots</em> to visualize the change.</p></li>
</ol>
</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="chapter-statistics.html"><span class="header-section-number">5</span> Statistical Inference and Hypothesis Testing</a></div>
<div class="next"><a href="chapter-knn.html"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-modeling"><span class="header-section-number">6</span> Preparing Data for Modeling</a></li>
<li><a class="nav-link" href="#statistical-inference-in-the-context-of-data-science"><span class="header-section-number">6.1</span> Statistical Inference in the Context of Data Science</a></li>
<li><a class="nav-link" href="#why-is-it-necessary-to-partition-the-data"><span class="header-section-number">6.2</span> Why Is It Necessary to Partition the Data?</a></li>
<li>
<a class="nav-link" href="#sec-partitioning"><span class="header-section-number">6.3</span> Partitioning the Data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-train-test-split-in-r">Example: Train-Test Split in R</a></li>
<li><a class="nav-link" href="#why-partitioning-matters">Why Partitioning Matters</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-validate-partition"><span class="header-section-number">6.4</span> Validating the Partition</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-validating-the-target-variable-churn">Example: Validating the Target Variable churn</a></li>
<li><a class="nav-link" href="#what-if-the-partition-is-invalid">What If the Partition Is Invalid?</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#balancing-the-training-dataset"><span class="header-section-number">6.5</span> Balancing the Training Dataset</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#techniques-for-addressing-class-imbalance">Techniques for Addressing Class Imbalance</a></li>
<li><a class="nav-link" href="#example-balancing-the-churn-dataset">Example: Balancing the Churn Dataset</a></li>
<li><a class="nav-link" href="#key-considerations-for-balancing">Key Considerations for Balancing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#exercises-4"><span class="header-section-number">6.6</span> Exercises</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#conceptual-questions-2">Conceptual Questions</a></li>
<li><a class="nav-link" href="#hands-on-practice">Hands-On Practice</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/RezaMoammadi/Book-Data-Science/blob/master/modeling.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/RezaMoammadi/Book-Data-Science/edit/master/modeling.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Uncovering Data Science with R</strong>" was written by Reza Mohammadi. It was last built on 2025-02-25.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
