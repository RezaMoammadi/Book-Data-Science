<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Naive Bayes Classifier | Uncovering Data Science with R</title>
<meta name="author" content="Reza Mohammadi">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Chapter 9 Naive Bayes Classifier | Uncovering Data Science with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://uncovering-data-science.netlify.app/chapter-bayes.html">
<meta property="og:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Naive Bayes Classifier | Uncovering Data Science with R">
<meta name="twitter:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="How can we make highly accurate predictions with minimal data and computation? Imagine a bank deciding whether to approve a loan based on some factors—such as a customer’s income, age, and...">
<meta property="og:description" content="How can we make highly accurate predictions with minimal data and computation? Imagine a bank deciding whether to approve a loan based on some factors—such as a customer’s income, age, and...">
<meta name="twitter:description" content="How can we make highly accurate predictions with minimal data and computation? Imagine a bank deciding whether to approve a loan based on some factors—such as a customer’s income, age, and...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Uncovering Data Science with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="chapter-into-R.html"><span class="header-section-number">1</span> The Basics for R</a></li>
<li><a class="" href="chapter-intro-DS.html"><span class="header-section-number">2</span> Introduction to Data Science</a></li>
<li><a class="" href="chapter-data-prep.html"><span class="header-section-number">3</span> Data Preparation</a></li>
<li><a class="" href="chapter-EDA.html"><span class="header-section-number">4</span> Exploratory Data Analysis</a></li>
<li><a class="" href="chapter-statistics.html"><span class="header-section-number">5</span> Statistical Inference and Hypothesis Testing</a></li>
<li><a class="" href="chapter-modeling.html"><span class="header-section-number">6</span> Preparing Data for Modeling</a></li>
<li><a class="" href="chapter-knn.html"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></li>
<li><a class="" href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></li>
<li><a class="active" href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></li>
<li><a class="" href="chapter-regression.html"><span class="header-section-number">10</span> Regression Modeling: From Basics to Advanced Techniques</a></li>
<li><a class="" href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></li>
<li><a class="" href="chapter-nn.html"><span class="header-section-number">12</span> Neural Networks: The Building Blocks of Artificial Intelligence</a></li>
<li><a class="" href="chapter-cluster.html"><span class="header-section-number">13</span> Clustering</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/RezaMoammadi/Book-Data-Science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-bayes" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Naive Bayes Classifier<a class="anchor" aria-label="anchor" href="#chapter-bayes"><i class="fas fa-link"></i></a>
</h1>
<p>How can we make highly accurate predictions with minimal data and computation? Imagine a bank deciding whether to approve a loan based on some factors—such as a customer’s income, age, and mortgage status. The Naive Bayes classifier offers a remarkably simple yet effective approach to such problems, relying on probability theory to make rapid, informed decisions.</p>
<p>Naive Bayes is a probabilistic classification algorithm that balances simplicity with effectiveness, making it a widely used approach in machine learning. It belongs to a family of classifiers based on Bayes’ theorem and operates under a key simplifying assumption: all features are conditionally independent given the target class. While this assumption is rarely true in real-world data, it allows for fast computation and efficient probability estimation, making the algorithm highly scalable and practical.</p>
<p>Despite its simplicity, Naive Bayes delivers strong performance in a variety of applications, particularly in text classification, spam detection, sentiment analysis, and financial risk assessment. In these domains, feature dependencies are often weak enough that the independence assumption does not significantly impact accuracy.</p>
<p>Beyond its theoretical foundations, Naive Bayes is computationally efficient, making it well-suited for large-scale datasets with high-dimensional feature spaces. For instance, in risk prediction, where multiple financial indicators must be analyzed, Naive Bayes can assess a customer’s likelihood of default in milliseconds. Its intuitive probabilistic reasoning and ease of implementation make it a valuable tool for both beginners and experienced practitioners.</p>
<p>The power of Naive Bayes comes from its foundation in Bayesian probability theory, specifically <em>Bayes’ Theorem</em>, introduced by the 18th-century mathematician Thomas Bayes.<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Thomas Bayes, &lt;em&gt;Essay Toward Solving a Problem in the Doctrine of Chances&lt;/em&gt; (Biometrika Office, 1958).&lt;/p&gt;"><sup>6</sup></a></span> This theorem provides a mathematical framework for updating probability estimates as new data becomes available. By combining prior knowledge with new evidence, Bayes’ theorem serves as the basis for many <em>Bayesian methods</em> in statistics and machine learning.</p>
<div id="strengths-and-limitations" class="section level3 unnumbered">
<h3>Strengths and Limitations<a class="anchor" aria-label="anchor" href="#strengths-and-limitations"><i class="fas fa-link"></i></a>
</h3>
<p>The Naive Bayes classifier is widely valued for its simplicity and efficiency. It offers several advantages:</p>
<ul>
<li>It performs well on high-dimensional datasets, such as text classification problems with thousands of features.<br>
</li>
<li>It is computationally efficient, making it ideal for real-time applications like spam filtering and risk prediction.<br>
</li>
<li>It remains effective even when the independence assumption is violated, as long as feature dependencies are not too strong.</li>
</ul>
<p>However, Naive Bayes also has limitations:</p>
<ul>
<li>The assumption that features are conditionally independent is rarely true in real-world datasets, especially when features exhibit strong correlations.<br>
</li>
<li>It struggles with continuous data unless a Gaussian distribution is assumed, which may not always be appropriate.<br>
</li>
<li>More complex models, such as decision trees or gradient boosting, often outperform Naive Bayes on datasets with intricate relationships between features.</li>
</ul>
<p>Despite these limitations, Naive Bayes remains an essential tool in machine learning. Its ease of implementation, interpretability, and strong baseline performance make it a valuable first-choice model for many classification tasks.</p>
</div>
<div id="what-this-chapter-covers" class="section level3 unnumbered">
<h3>What This Chapter Covers<a class="anchor" aria-label="anchor" href="#what-this-chapter-covers"><i class="fas fa-link"></i></a>
</h3>
<p>This chapter provides a comprehensive exploration of the Naive Bayes classifier. Specifically, we will:</p>
<ol style="list-style-type: decimal">
<li>Explain the mathematical foundations of Naive Bayes, focusing on Bayes’ theorem and its role in probabilistic classification.<br>
</li>
<li>Walk through the mechanics of Naive Bayes with step-by-step examples.<br>
</li>
<li>Introduce different variants of the algorithm—Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes—and discuss their appropriate use cases.<br>
</li>
<li>Examine practical considerations, including strengths, limitations, and real-world applications.<br>
</li>
<li>Implement Naive Bayes in R using the <em>risk</em> dataset from the <strong>liver</strong> package to demonstrate its effectiveness.</li>
</ol>
<p>By the end of this chapter, you will have a thorough understanding of the Naive Bayes classifier, equipping you to apply it confidently in real-world classification problems.</p>
</div>
<div id="bayes-theorem-and-probabilistic-foundations" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Bayes’ Theorem and Probabilistic Foundations<a class="anchor" aria-label="anchor" href="#bayes-theorem-and-probabilistic-foundations"><i class="fas fa-link"></i></a>
</h2>
<p>When evaluating financial risk, how do we update our beliefs about a borrower’s likelihood of defaulting as new information—such as income, debt, or mortgage status—becomes available? The ability to quantify uncertainty and refine predictions as new evidence arises is essential in decision-making, and this is precisely what Bayes’ Theorem provides.</p>
<p>This theorem forms the foundation of probabilistic learning, helping us make data-driven decisions across diverse fields, including finance, medicine, and machine learning. When determining whether a loan applicant poses a financial risk, we often start with general expectations based on population statistics (<em>prior knowledge</em>). However, as additional details—such as mortgage status or outstanding loans—become available, this new evidence refines our estimate (<em>posterior probability</em>), leading to more informed decisions.</p>
<p>The foundation for this method was laid by <em>Thomas Bayes</em>, an 18th-century Presbyterian minister and self-taught mathematician. His pioneering work introduced a systematic approach to updating probabilities as new data emerges, forming the basis of what is now known as <em>Bayesian inference</em>. Those interested in exploring this concept further may find the book <a href="https://www.goodreads.com/book/show/199798096-everything-is-predictable">“Everything Is Predictable: How Bayesian Statistics Explain Our World”</a> insightful. The author argues that Bayesian statistics not only help predict the future but also shape rational decision-making in everyday life.</p>
<div id="the-essence-of-bayes-theorem" class="section level3 unnumbered">
<h3>The Essence of Bayes’ Theorem<a class="anchor" aria-label="anchor" href="#the-essence-of-bayes-theorem"><i class="fas fa-link"></i></a>
</h3>
<p>Bayes’ Theorem provides a systematic way to update probabilities in light of new evidence. It answers the question: <em>Given what we already know, how should our belief in a hypothesis change when we observe new data?</em></p>
<p>Mathematically, Bayes’ Theorem is expressed as:</p>
<p><span class="math display" id="eq:bayes-theorem">\[\begin{equation}
P(A|B) = P(A) \cdot \frac{P(B|A)}{P(B)}
\tag{9.1}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(P(A|B)\)</span> is the posterior probability, representing the probability of event <span class="math inline">\(A\)</span> (hypothesis) given that event <span class="math inline">\(B\)</span> (evidence) has occurred.<br>
</li>
<li>
<span class="math inline">\(P(A)\)</span> is the prior probability, which reflects our initial belief about <span class="math inline">\(A\)</span> before considering <span class="math inline">\(B\)</span>.<br>
</li>
<li>
<span class="math inline">\(P(B|A)\)</span> is the likelihood, representing the probability of observing <span class="math inline">\(B\)</span> assuming <span class="math inline">\(A\)</span> is true.<br>
</li>
<li>
<span class="math inline">\(P(B)\)</span> is the evidence, which accounts for the total probability of observing <span class="math inline">\(B\)</span>.</li>
</ul>
<p>Bayes’ Theorem provides a structured way to refine our understanding of uncertainty by combining prior knowledge with new observations. This principle underpins many probabilistic learning techniques, including the Naive Bayes classifier.</p>
<p>To illustrate its application, consider a financial risk assessment scenario from the <code>risk</code> dataset in the <strong>liver</strong> package. Suppose we want to estimate the probability that a customer has a good risk profile (<span class="math inline">\(A\)</span>) given that they have a mortgage (<span class="math inline">\(B\)</span>). Financial institutions often use such risk models to assess creditworthiness based on various factors, including mortgage status.</p>
<div class="example">
<p><span id="exm:ex-bayes-risk" class="example"><strong>Example 9.1  </strong></span>Let’s use the <code>risk</code> dataset to calculate the probability of a customer being classified as good risk, given that they have a mortgage. We start by loading the dataset and inspecting the relevant data:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="chapter-bayes.html#cb151-1" tabindex="-1"></a><span class="fu">library</span>(liver)         </span>
<span id="cb151-2"><a href="chapter-bayes.html#cb151-2" tabindex="-1"></a></span>
<span id="cb151-3"><a href="chapter-bayes.html#cb151-3" tabindex="-1"></a><span class="fu">data</span>(risk)</span>
<span id="cb151-4"><a href="chapter-bayes.html#cb151-4" tabindex="-1"></a></span>
<span id="cb151-5"><a href="chapter-bayes.html#cb151-5" tabindex="-1"></a><span class="fu">xtabs</span>(<span class="sc">~</span> risk <span class="sc">+</span> mortgage, <span class="at">data =</span> risk)</span>
<span id="cb151-6"><a href="chapter-bayes.html#cb151-6" tabindex="-1"></a>              mortgage</span>
<span id="cb151-7"><a href="chapter-bayes.html#cb151-7" tabindex="-1"></a>   risk        yes no</span>
<span id="cb151-8"><a href="chapter-bayes.html#cb151-8" tabindex="-1"></a>     good risk  <span class="dv">81</span> <span class="dv">42</span></span>
<span id="cb151-9"><a href="chapter-bayes.html#cb151-9" tabindex="-1"></a>     bad risk   <span class="dv">94</span> <span class="dv">29</span></span></code></pre></div>
<p>To improve readability, we add row and column totals to the contingency table:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="chapter-bayes.html#cb152-1" tabindex="-1"></a><span class="fu">addmargins</span>(<span class="fu">xtabs</span>(<span class="sc">~</span> risk <span class="sc">+</span> mortgage, <span class="at">data =</span> risk))</span>
<span id="cb152-2"><a href="chapter-bayes.html#cb152-2" tabindex="-1"></a>              mortgage</span>
<span id="cb152-3"><a href="chapter-bayes.html#cb152-3" tabindex="-1"></a>   risk        yes  no Sum</span>
<span id="cb152-4"><a href="chapter-bayes.html#cb152-4" tabindex="-1"></a>     good risk  <span class="dv">81</span>  <span class="dv">42</span> <span class="dv">123</span></span>
<span id="cb152-5"><a href="chapter-bayes.html#cb152-5" tabindex="-1"></a>     bad risk   <span class="dv">94</span>  <span class="dv">29</span> <span class="dv">123</span></span>
<span id="cb152-6"><a href="chapter-bayes.html#cb152-6" tabindex="-1"></a>     Sum       <span class="dv">175</span>  <span class="dv">71</span> <span class="dv">246</span></span></code></pre></div>
<p>Now, we define the relevant events:</p>
<ul>
<li>
<span class="math inline">\(A\)</span>: The customer has a <em>good risk</em> profile.<br>
</li>
<li>
<span class="math inline">\(B\)</span>: The customer has a mortgage (<code>mortgage = yes</code>).</li>
</ul>
<p>The prior probability of a customer having good risk is given by:</p>
<p><span class="math display">\[
P(A) = \frac{\text{Total Good Risk Cases}}{\text{Total Cases}} = \frac{123}{246} = 0.5
\]</span></p>
<p>Using Bayes’ Theorem, we compute the probability of a customer being classified as good risk given that they have a mortgage:</p>
<p><span class="math display">\[\begin{equation}
\label{eq1}
\begin{split}
P(\text{Good Risk} | \text{Mortgage = Yes}) &amp; = \frac{P(\text{Good Risk} \cap \text{Mortgage = Yes})}{P(\text{Mortgage = Yes})} \\
&amp; = \frac{\text{Good Risk with Mortgage Cases}}{\text{Total Mortgage Cases}} \\
&amp; = \frac{81}{175} \\
&amp; = 0.463
\end{split}
\end{equation}\]</span></p>
<p>This result indicates that among customers with mortgages, the probability of having a good risk profile is lower than in the general population. Such insights help financial institutions refine credit risk models by incorporating new evidence systematically.</p>
</div>
</div>
<div id="how-does-bayes-theorem-work" class="section level3 unnumbered">
<h3>How Does Bayes’ Theorem Work?<a class="anchor" aria-label="anchor" href="#how-does-bayes-theorem-work"><i class="fas fa-link"></i></a>
</h3>
<p>Bayes’ Theorem provides a structured way to update our understanding of uncertainty based on new information. In many real-world scenarios, we start with an initial belief about an event’s likelihood, and as we gather more data, we refine this belief to make better-informed decisions.</p>
<p>For instance, in financial risk assessment, banks initially estimate a borrower’s risk level based on general population statistics. However, as they collect more details—such as income, credit history, and mortgage status—Bayes’ Theorem allows them to update the probability of the borrower being classified as high or low risk. This enables more precise lending decisions.</p>
<p>Beyond finance, Bayes’ Theorem is widely applied in other domains:</p>
<ul>
<li>In medical diagnostics, it helps estimate the probability of a disease (<span class="math inline">\(A\)</span>) given a positive test result (<span class="math inline">\(B\)</span>), incorporating both the test’s reliability and the disease’s prevalence.<br>
</li>
<li>In spam detection, it computes the probability that an email is spam (<span class="math inline">\(A\)</span>) based on the presence of certain keywords (<span class="math inline">\(B\)</span>), refining predictions as new messages are analyzed.</li>
</ul>
<p>Probability theory provides a rigorous mathematical structure for reasoning under uncertainty. Bayes’ Theorem extends this by enabling a systematic approach to <strong>learning from data</strong> and improving decision-making in fields ranging from healthcare to finance and beyond.</p>
</div>
<div id="a-gateway-to-naive-bayes" class="section level3 unnumbered">
<h3>A Gateway to Naive Bayes<a class="anchor" aria-label="anchor" href="#a-gateway-to-naive-bayes"><i class="fas fa-link"></i></a>
</h3>
<p>Bayes’ Theorem provides a mathematical foundation for updating probabilities as new evidence emerges. However, in practical classification tasks, computing these probabilities directly can be computationally expensive, particularly for datasets with many features. This is where the <em>Naive Bayes Classifier</em> comes in.</p>
<p>Naive Bayes builds directly on Bayes’ Theorem by introducing a key simplification: it assumes that features are <em>conditionally independent</em> given the target class. While this assumption is rarely true in real-world data, it drastically reduces computational complexity, making the algorithm highly efficient for large-scale problems.</p>
<p>Despite this simplification, Naive Bayes performs remarkably well in many applications. For example, in financial risk prediction, a bank may assess a borrower’s creditworthiness using features like income, loan history, and mortgage status. While these factors may be correlated, Naive Bayes assumes they are independent given the borrower’s risk category, allowing for rapid probability estimation and classification.</p>
<p>This efficiency makes Naive Bayes particularly effective in domains such as text classification, spam filtering, and sentiment analysis, where feature independence is a reasonable approximation. In the following sections, we will explore how this assumption enables <em>fast, interpretable, and scalable classification</em> while maintaining competitive performance.</p>
</div>
</div>
<div id="why-is-it-called-naive" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Why is it Called “Naive”?<a class="anchor" aria-label="anchor" href="#why-is-it-called-naive"><i class="fas fa-link"></i></a>
</h2>
<p>Imagine assessing a borrower’s financial risk based on their income, mortgage status, and number of loans. Intuitively, these factors are related—individuals with higher income may have better loan repayment histories, and those with more loans might have a higher probability of financial distress. However, Naive Bayes assumes that all these features are independent once we know the risk category (good risk or bad risk).</p>
<p>This assumption is what makes the algorithm “naive.” In reality, features are often correlated, such as income and age, but by treating them as independent, Naive Bayes significantly simplifies probability calculations, making it both efficient and scalable.</p>
<p>To illustrate, consider the <code>risk</code> dataset from the <strong>liver</strong> package:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="chapter-bayes.html#cb153-1" tabindex="-1"></a><span class="fu">str</span>(risk)</span>
<span id="cb153-2"><a href="chapter-bayes.html#cb153-2" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">246</span> obs. of  <span class="dv">6</span> variables<span class="sc">:</span></span>
<span id="cb153-3"><a href="chapter-bayes.html#cb153-3" tabindex="-1"></a>    <span class="er">$</span> age     <span class="sc">:</span> int  <span class="dv">34</span> <span class="dv">37</span> <span class="dv">29</span> <span class="dv">33</span> <span class="dv">39</span> <span class="dv">28</span> <span class="dv">28</span> <span class="dv">25</span> <span class="dv">41</span> <span class="dv">26</span> ...</span>
<span id="cb153-4"><a href="chapter-bayes.html#cb153-4" tabindex="-1"></a>    <span class="sc">$</span> marital <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"single"</span>,<span class="st">"married"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> ...</span>
<span id="cb153-5"><a href="chapter-bayes.html#cb153-5" tabindex="-1"></a>    <span class="sc">$</span> income  <span class="sc">:</span> num  <span class="dv">28061</span> <span class="dv">28009</span> <span class="dv">27615</span> <span class="dv">27287</span> <span class="dv">26954</span> ...</span>
<span id="cb153-6"><a href="chapter-bayes.html#cb153-6" tabindex="-1"></a>    <span class="sc">$</span> mortgage<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb153-7"><a href="chapter-bayes.html#cb153-7" tabindex="-1"></a>    <span class="sc">$</span> nr.loans<span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb153-8"><a href="chapter-bayes.html#cb153-8" tabindex="-1"></a>    <span class="sc">$</span> risk    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"good risk"</span>,<span class="st">"bad risk"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span></code></pre></div>
<p>This dataset includes financial indicators such as age, income, marital status, mortgage, and number of loans. Naive Bayes assumes that given a person’s risk classification (<code>good risk</code> or <code>bad risk</code>), these features do not influence one another. Mathematically, the probability of a customer being in the <code>good risk</code> category given their attributes is expressed as:</p>
<p><span class="math display">\[
P(Y = y_1 | X_1, X_2, \dots, X_5) = \frac{P(Y = y_1) \cdot P(X_1, X_2, \dots, X_5 | Y = y_1)}{P(X_1, X_2, \dots, X_5)}
\]</span></p>
<p>However, directly computing <span class="math inline">\(P(X_1, X_2, \dots, X_5 | Y = y_1)\)</span> is computationally expensive, especially as the number of features grows. For instance, in datasets with hundreds or thousands of features, storing and calculating joint probabilities for all possible feature combinations becomes impractical.</p>
<p>The naive assumption of conditional independence simplifies this problem by expressing the joint probability as the product of individual probabilities:</p>
<p><span class="math display">\[
P(X_1, X_2, \dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \cdot P(X_2 | Y = y_1) \cdots P(X_5 | Y = y_1)
\]</span></p>
<p>This transformation eliminates the need to compute complex joint probabilities, making the algorithm scalable even for high-dimensional data. Instead of handling an exponential number of feature combinations, Naive Bayes only requires computing simple conditional probabilities for each feature given the class label.</p>
<p>In practice, this independence assumption is rarely true—features often exhibit some degree of correlation. However, Naive Bayes frequently performs well despite this limitation. It remains widely used in domains where:</p>
<ul>
<li>Feature dependencies are weak enough that the assumption does not significantly impact accuracy.</li>
<li>The focus is on speed and interpretability rather than capturing complex relationships.</li>
<li>Slight violations of the independence assumption do not severely affect predictive performance.</li>
</ul>
<p>For example, in risk prediction, while income and mortgage status are likely correlated, treating them as independent still allows Naive Bayes to classify borrowers effectively. Similarly, in spam detection or text classification, where features (such as words in an email) are often independent enough, the algorithm delivers fast and accurate predictions.</p>
<p>By balancing computational efficiency with predictive power, Naive Bayes remains a foundational algorithm in machine learning, particularly for applications that demand scalability and interpretability.</p>
</div>
<div id="the-laplace-smoothing-technique" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> The Laplace Smoothing Technique<a class="anchor" aria-label="anchor" href="#the-laplace-smoothing-technique"><i class="fas fa-link"></i></a>
</h2>
<p>One of the challenges in Naive Bayes classification is handling feature categories that appear in the test data but are absent in the training data. Suppose we train a model on a dataset where no borrowers classified as “bad risk” are married. If we later encounter a married borrower in the test set, Naive Bayes would compute <span class="math inline">\(P(\text{bad risk} | \text{married})\)</span> as zero. Because the algorithm multiplies probabilities when making predictions, even a single zero probability results in an overall probability of zero for that class, making it impossible for the model to predict that class.</p>
<p>This issue arises because Naive Bayes estimates probabilities from frequency counts in the training data. If a feature value never appears in a given class, its estimated probability is zero, which can lead to misclassification errors. To address this, <em>Laplace smoothing</em> (also known as <em>add-one smoothing</em>) is used. Named after the mathematician <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace</a>, this technique ensures that every feature-category combination has a small, non-zero probability, even if it is missing in the training data.</p>
<p>To illustrate, consider the <code>marital</code> variable in the <code>risk</code> dataset. Suppose the category <code>married</code> is entirely absent for customers labeled as <code>bad risk</code>. This scenario can be visualized as follows:</p>
<pre><code>            risk
   marital   good risk bad risk
     single         21       11
     married        51        0
     other           8       10</code></pre>
<p>Without smoothing, the probability of <code>bad risk</code> given <code>married</code> is:</p>
<p><span class="math display">\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married})}{\text{count}(\text{married})} = \frac{0}{\text{count}(\text{married})} = 0
\]</span></p>
<p>This means that any married borrower will always be classified as <code>good risk</code>, regardless of their other characteristics.</p>
<p>Laplace smoothing resolves this by modifying the probability calculation. Instead of assigning a strict zero probability, a small constant <span class="math inline">\(k\)</span> (usually <span class="math inline">\(k = 1\)</span>) is added to each count in the frequency table. The adjusted probability is given by:</p>
<p><span class="math display">\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married}) + k}{\text{count}(\text{bad risk}) + k \times \text{number of categories in } \text{marital}}
\]</span></p>
<p>This adjustment ensures that:
- Every category receives a small positive count, avoiding zero probabilities.
- The total probability distribution remains valid.</p>
<p>In R, Laplace smoothing can be applied using the <code>laplace</code> argument in the <strong>naivebayes</strong> package. By default, <code>laplace = 0</code>, meaning no smoothing is applied. To apply smoothing, simply set <code>laplace = 1</code>:</p>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/majkamichal/naivebayes">naivebayes</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit Naive Bayes with Laplace smoothing</span></span>
<span><span class="va">formula_nb</span> <span class="op">=</span> <span class="va">risk</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">marital</span> <span class="op">+</span> <span class="va">mortgage</span> <span class="op">+</span> <span class="va">nr.loans</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula_nb</span>, data <span class="op">=</span> <span class="va">risk</span>, laplace <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>This ensures that no category is assigned a probability of zero, improving the model’s robustness—particularly in cases where the training data is limited or imbalanced.</p>
<p>Laplace smoothing is a simple yet effective technique that prevents Naive Bayes from being overly sensitive to missing categories in training data. While <span class="math inline">\(k = 1\)</span> is the most common approach, the value of <span class="math inline">\(k\)</span> can be adjusted based on specific domain knowledge. By ensuring that probabilities remain well-defined, Laplace smoothing enhances the reliability of Naive Bayes classifiers in real-world applications.</p>
</div>
<div id="types-of-naive-bayes-classifiers" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Types of Naive Bayes Classifiers<a class="anchor" aria-label="anchor" href="#types-of-naive-bayes-classifiers"><i class="fas fa-link"></i></a>
</h2>
<p>Naive Bayes is a versatile algorithm with different variants designed for specific data types and distributions. The choice of which variant to use depends on the nature of the features and the assumptions made about their underlying distribution. The three most common types are:</p>
<ul>
<li><p><strong>Multinomial Naive Bayes</strong>: Best suited for categorical or count-based features, such as word frequencies in text data. This variant is commonly used in text classification, where features represent discrete counts (e.g., the number of times a word appears in a document). In the <em>risk</em> dataset, the <code>marital</code> variable, which takes categorical values such as <code>single</code>, <code>married</code>, and <code>other</code>, aligns well with this variant.</p></li>
<li><p><strong>Bernoulli Naive Bayes</strong>: Designed for binary features, where each variable represents the presence or absence of a characteristic. This variant is particularly useful in applications where data is represented as a set of binary indicators, such as whether an email contains a specific keyword in spam detection. In the <em>risk</em> dataset, the <code>mortgage</code> variable, which has two possible values (<code>yes</code> or <code>no</code>), is an example of a binary feature suitable for this approach.</p></li>
<li><p><strong>Gaussian Naive Bayes</strong>: Applied to continuous data where features are assumed to follow a normal (Gaussian) distribution. This variant estimates the likelihood of each feature using a normal distribution, making it ideal for datasets with numerical attributes such as age, income, or credit scores. In the <em>risk</em> dataset, variables like <code>age</code> and <code>income</code> are continuous and thus well suited for this variant.</p></li>
</ul>
<p>Each of these Naive Bayes classifiers is optimized for different data types, making it essential to select the one that best fits the dataset’s characteristics. Understanding these distinctions allows for better model selection and improved performance. In the following sections, we will explore each variant in greater detail, examining their assumptions, strengths, and use cases.</p>
</div>
<div id="case-study-predicting-financial-risk-with-naive-bayes" class="section level2" number="9.5">
<h2>
<span class="header-section-number">9.5</span> Case Study: Predicting Financial Risk with Naive Bayes<a class="anchor" aria-label="anchor" href="#case-study-predicting-financial-risk-with-naive-bayes"><i class="fas fa-link"></i></a>
</h2>
<p>Financial institutions must assess loan applicants carefully to balance profitability with risk management. Lending decisions rely on estimating the likelihood of default, which depends on various financial and demographic factors. A robust risk classification model helps institutions make informed decisions, reducing financial losses while ensuring fair lending practices.</p>
<p>In this case study, we apply the Naive Bayes classifier to predict whether a customer is a <em>good risk</em> or <em>bad risk</em> based on financial and demographic attributes. Using the <code>risk</code> dataset from the <a href="https://CRAN.R-project.org/package=liver"><strong>liver</strong></a> package in R, we train and evaluate a probabilistic classification model. This case study demonstrates how Naive Bayes can be leveraged in financial decision-making, providing insights into customer risk profiles and supporting more effective credit evaluation.</p>
<div id="problem-understanding-2" class="section level3 unnumbered">
<h3>Problem Understanding<a class="anchor" aria-label="anchor" href="#problem-understanding-2"><i class="fas fa-link"></i></a>
</h3>
<p>A key challenge in financial risk assessment is distinguishing between customers who are likely to repay loans and those at higher risk of default. Predictive modeling enables financial institutions to anticipate risk, optimize credit policies, and reduce non-performing loans. Key business questions include:</p>
<ul>
<li>What financial and demographic factors contribute to a customer’s risk profile?<br>
</li>
<li>How can we predict whether a customer is a good or bad risk before approving a loan?<br>
</li>
<li>What insights can be gained to refine lending policies and mitigate financial losses?</li>
</ul>
<p>By analyzing the <code>risk</code> dataset, we aim to develop a model that classifies customers based on risk level. This will allow lenders to make <em>data-driven</em> decisions, improve credit scoring, and enhance loan approval strategies.</p>
</div>
<div id="data-understanding-1" class="section level3 unnumbered">
<h3>Data Understanding<a class="anchor" aria-label="anchor" href="#data-understanding-1"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>risk</code> dataset contains financial and demographic attributes that help assess a customer’s likelihood of being classified as either a <em>good risk</em> or <em>bad risk</em>. This dataset, included in the <strong>liver</strong> package, consists of 246 observations and 6 variables. It provides a structured way to analyze customer characteristics and predict financial risk levels.</p>
<p>The dataset includes 5 predictors and a binary target variable, <code>risk</code>, which distinguishes between customers who are more or less likely to default. The key variables are:</p>
<ul>
<li>
<code>age</code>: Customer’s age in years.<br>
</li>
<li>
<code>marital</code>: Marital status (<code>single</code>, <code>married</code>, <code>other</code>).<br>
</li>
<li>
<code>income</code>: Annual income.<br>
</li>
<li>
<code>mortgage</code>: Indicates whether the customer has a mortgage (<code>yes</code>, <code>no</code>).<br>
</li>
<li>
<code>nr_loans</code>: Number of loans held by the customer.<br>
</li>
<li>
<code>risk</code>: The target variable (<code>good risk</code>, <code>bad risk</code>).</li>
</ul>
<p>For additional details about the dataset, refer to its <a href="https://search.r-project.org/CRAN/refmans/liver/html/risk.html">documentation</a>.</p>
<p>To begin the analysis, we load the dataset and examine its structure to understand its variables and data types:</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="chapter-bayes.html#cb156-1" tabindex="-1"></a><span class="fu">data</span>(risk)</span>
<span id="cb156-2"><a href="chapter-bayes.html#cb156-2" tabindex="-1"></a></span>
<span id="cb156-3"><a href="chapter-bayes.html#cb156-3" tabindex="-1"></a><span class="fu">str</span>(risk)</span>
<span id="cb156-4"><a href="chapter-bayes.html#cb156-4" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">246</span> obs. of  <span class="dv">6</span> variables<span class="sc">:</span></span>
<span id="cb156-5"><a href="chapter-bayes.html#cb156-5" tabindex="-1"></a>    <span class="er">$</span> age     <span class="sc">:</span> int  <span class="dv">34</span> <span class="dv">37</span> <span class="dv">29</span> <span class="dv">33</span> <span class="dv">39</span> <span class="dv">28</span> <span class="dv">28</span> <span class="dv">25</span> <span class="dv">41</span> <span class="dv">26</span> ...</span>
<span id="cb156-6"><a href="chapter-bayes.html#cb156-6" tabindex="-1"></a>    <span class="sc">$</span> marital <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"single"</span>,<span class="st">"married"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> ...</span>
<span id="cb156-7"><a href="chapter-bayes.html#cb156-7" tabindex="-1"></a>    <span class="sc">$</span> income  <span class="sc">:</span> num  <span class="dv">28061</span> <span class="dv">28009</span> <span class="dv">27615</span> <span class="dv">27287</span> <span class="dv">26954</span> ...</span>
<span id="cb156-8"><a href="chapter-bayes.html#cb156-8" tabindex="-1"></a>    <span class="sc">$</span> mortgage<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb156-9"><a href="chapter-bayes.html#cb156-9" tabindex="-1"></a>    <span class="sc">$</span> nr.loans<span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb156-10"><a href="chapter-bayes.html#cb156-10" tabindex="-1"></a>    <span class="sc">$</span> risk    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"good risk"</span>,<span class="st">"bad risk"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span></code></pre></div>
<p>To gain further insights, we summarize the dataset’s key statistics:</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="chapter-bayes.html#cb157-1" tabindex="-1"></a><span class="fu">summary</span>(risk)</span>
<span id="cb157-2"><a href="chapter-bayes.html#cb157-2" tabindex="-1"></a>         age           marital        income      mortgage     nr.loans    </span>
<span id="cb157-3"><a href="chapter-bayes.html#cb157-3" tabindex="-1"></a>    Min.   <span class="sc">:</span><span class="fl">17.00</span>   single <span class="sc">:</span><span class="dv">111</span>   Min.   <span class="sc">:</span><span class="dv">15301</span>   yes<span class="sc">:</span><span class="dv">175</span>   Min.   <span class="sc">:</span><span class="fl">0.000</span>  </span>
<span id="cb157-4"><a href="chapter-bayes.html#cb157-4" tabindex="-1"></a>    <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">32.00</span>   married<span class="sc">:</span> <span class="dv">78</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="dv">26882</span>   no <span class="sc">:</span> <span class="dv">71</span>   <span class="dv">1</span>st Qu.<span class="sc">:</span><span class="fl">1.000</span>  </span>
<span id="cb157-5"><a href="chapter-bayes.html#cb157-5" tabindex="-1"></a>    Median <span class="sc">:</span><span class="fl">41.00</span>   other  <span class="sc">:</span> <span class="dv">57</span>   Median <span class="sc">:</span><span class="dv">37662</span>             Median <span class="sc">:</span><span class="fl">1.000</span>  </span>
<span id="cb157-6"><a href="chapter-bayes.html#cb157-6" tabindex="-1"></a>    Mean   <span class="sc">:</span><span class="fl">40.64</span>                 Mean   <span class="sc">:</span><span class="dv">38790</span>             Mean   <span class="sc">:</span><span class="fl">1.309</span>  </span>
<span id="cb157-7"><a href="chapter-bayes.html#cb157-7" tabindex="-1"></a>    <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">50.00</span>                 <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="dv">49398</span>             <span class="dv">3</span>rd Qu.<span class="sc">:</span><span class="fl">2.000</span>  </span>
<span id="cb157-8"><a href="chapter-bayes.html#cb157-8" tabindex="-1"></a>    Max.   <span class="sc">:</span><span class="fl">66.00</span>                 Max.   <span class="sc">:</span><span class="dv">78399</span>             Max.   <span class="sc">:</span><span class="fl">3.000</span>  </span>
<span id="cb157-9"><a href="chapter-bayes.html#cb157-9" tabindex="-1"></a>           risk    </span>
<span id="cb157-10"><a href="chapter-bayes.html#cb157-10" tabindex="-1"></a>    good risk<span class="sc">:</span><span class="dv">123</span>  </span>
<span id="cb157-11"><a href="chapter-bayes.html#cb157-11" tabindex="-1"></a>    bad risk <span class="sc">:</span><span class="dv">123</span>  </span>
<span id="cb157-12"><a href="chapter-bayes.html#cb157-12" tabindex="-1"></a>                   </span>
<span id="cb157-13"><a href="chapter-bayes.html#cb157-13" tabindex="-1"></a>                   </span>
<span id="cb157-14"><a href="chapter-bayes.html#cb157-14" tabindex="-1"></a>                   </span>
<span id="cb157-15"><a href="chapter-bayes.html#cb157-15" tabindex="-1"></a>   </span></code></pre></div>
<p>This summary provides an overview of variable distributions and identifies any missing values or potential anomalies. Since the dataset appears clean and well-structured, we can proceed to data preparation before training the Naive Bayes classifier.</p>
</div>
<div id="data-preparation-for-modeling" class="section level3 unnumbered">
<h3>Data Preparation for Modeling<a class="anchor" aria-label="anchor" href="#data-preparation-for-modeling"><i class="fas fa-link"></i></a>
</h3>
<p>Before training the Naive Bayes classifier, we need to split the dataset into training and testing sets. This step ensures that we can evaluate how well the model generalizes to unseen data. We use an 80/20 split, allocating 80% of the data for training and 20% for testing. To maintain consistency with previous chapters, we apply the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function from the <strong>liver</strong> package:</p>
<div class="sourceCode" id="cb158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/partition.html">partition</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">risk</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">risk</span></span></code></pre></div>
<p>Setting <code>set.seed(5)</code> ensures reproducibility so that the same partitioning is achieved each time the code is run. The <code>train_set</code> will be used to train the Naive Bayes classifier, while the <code>test_set</code> will serve as unseen data to evaluate the model’s predictions. The <code>test_labels</code> vector contains the true class labels for the test set, which we will compare against the model’s predictions.</p>
<p>To verify that the training and test sets are representative of the original dataset, we compare the proportions of the <code>marital</code> variable across both sets. A chi-squared test is used to check whether the distribution of marital statuses (<code>single</code>, <code>married</code>, and <code>other</code>) is statistically similar between the training and test sets:</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="chapter-bayes.html#cb159-1" tabindex="-1"></a><span class="fu">chisq.test</span>(<span class="at">x =</span> <span class="fu">table</span>(train_set<span class="sc">$</span>marital), <span class="at">y =</span> <span class="fu">table</span>(test_set<span class="sc">$</span>marital))</span>
<span id="cb159-2"><a href="chapter-bayes.html#cb159-2" tabindex="-1"></a>   </span>
<span id="cb159-3"><a href="chapter-bayes.html#cb159-3" tabindex="-1"></a>    Pearson<span class="st">'s Chi-squared test</span></span>
<span id="cb159-4"><a href="chapter-bayes.html#cb159-4" tabindex="-1"></a><span class="st">   </span></span>
<span id="cb159-5"><a href="chapter-bayes.html#cb159-5" tabindex="-1"></a><span class="st">   data:  table(train_set$marital) and table(test_set$marital)</span></span>
<span id="cb159-6"><a href="chapter-bayes.html#cb159-6" tabindex="-1"></a><span class="st">   X-squared = 6, df = 4, p-value = 0.1991</span></span></code></pre></div>
<p>This statistical test evaluates whether the proportions of marital categories differ significantly between the training and test sets. The hypotheses for the test are:<br><span class="math display">\[
\begin{cases}
H_0:  \text{The proportions of marital categories are the same in both sets.}\\
H_a:  \text{At least one of the proportions is different.}
\end{cases}
\]</span>
Since the p-value exceeds <span class="math inline">\(\alpha = 0.05\)</span>, we fail to reject <span class="math inline">\(H_0\)</span>, meaning that the marital status distribution remains statistically similar between the training and test sets. This confirms that the partitioning process maintains the dataset’s characteristics, allowing for reliable model evaluation.</p>
<p>With a well-structured dataset and a validated partitioning process, we are now ready to train the Naive Bayes classifier and assess its predictive capabilities.</p>
</div>
<div id="applying-the-naive-bayes-classifier" class="section level3 unnumbered">
<h3>Applying the Naive Bayes Classifier<a class="anchor" aria-label="anchor" href="#applying-the-naive-bayes-classifier"><i class="fas fa-link"></i></a>
</h3>
<p>With the dataset partitioned and validated, we now proceed to train and evaluate the Naive Bayes classifier. The objective is to build a model using the training set and assess its ability to classify customers as <em>good risk</em> or <em>bad risk</em> in the test set.</p>
<p>Several R packages provide implementations of Naive Bayes, with two commonly used options being <a href="https://CRAN.R-project.org/package=naivebayes"><strong>naivebayes</strong></a> and <a href="https://CRAN.R-project.org/package=e1071"><strong>e1071</strong></a>. In this case study, we use the <strong>naivebayes</strong> package, which offers an efficient implementation of the classifier. The <code><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes()</a></code> function in this package supports various probability distributions depending on the nature of the features:</p>
<ul>
<li>
<em>Categorical distribution</em> for discrete variables such as <code>marital</code> and <code>mortgage</code>.<br>
</li>
<li>
<em>Bernoulli distribution</em> for binary features, a special case of the categorical distribution.<br>
</li>
<li>
<em>Poisson distribution</em> for count-based variables, such as the number of loans.<br>
</li>
<li>
<em>Gaussian distribution</em> for continuous features, such as <code>age</code> and <code>income</code>.<br>
</li>
<li>
<em>Non-parametric density estimation</em> for continuous features when no specific distribution is assumed.</li>
</ul>
<p>Unlike the k-NN algorithm in the previous chapter, which classifies new data without an explicit training phase, Naive Bayes follows a two-step process:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Training phase</strong> – The model learns probability distributions from the training data.<br>
</li>
<li>
<strong>Prediction phase</strong> – The trained model is used to classify new data points based on the learned probabilities.</li>
</ol>
<p>To train the model, we define a formula where <code>risk</code> is the target variable, and all other features serve as predictors:</p>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">risk</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">mortgage</span> <span class="op">+</span> <span class="va">nr.loans</span> <span class="op">+</span> <span class="va">marital</span></span></code></pre></div>
<p>We then apply the <code><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes()</a></code> function from the <strong>naivebayes</strong> package to train the classifier on the training dataset:</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="chapter-bayes.html#cb161-1" tabindex="-1"></a><span class="fu">library</span>(naivebayes)</span>
<span id="cb161-2"><a href="chapter-bayes.html#cb161-2" tabindex="-1"></a></span>
<span id="cb161-3"><a href="chapter-bayes.html#cb161-3" tabindex="-1"></a>naive_bayes <span class="ot">=</span> <span class="fu">naive_bayes</span>(formula, <span class="at">data =</span> train_set)</span>
<span id="cb161-4"><a href="chapter-bayes.html#cb161-4" tabindex="-1"></a></span>
<span id="cb161-5"><a href="chapter-bayes.html#cb161-5" tabindex="-1"></a>naive_bayes</span>
<span id="cb161-6"><a href="chapter-bayes.html#cb161-6" tabindex="-1"></a>   </span>
<span id="cb161-7"><a href="chapter-bayes.html#cb161-7" tabindex="-1"></a>   <span class="sc">==</span><span class="er">===============================</span> Naive Bayes <span class="sc">==</span><span class="er">================================</span></span>
<span id="cb161-8"><a href="chapter-bayes.html#cb161-8" tabindex="-1"></a>   </span>
<span id="cb161-9"><a href="chapter-bayes.html#cb161-9" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb161-10"><a href="chapter-bayes.html#cb161-10" tabindex="-1"></a>   <span class="fu">naive_bayes.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set)</span>
<span id="cb161-11"><a href="chapter-bayes.html#cb161-11" tabindex="-1"></a>   </span>
<span id="cb161-12"><a href="chapter-bayes.html#cb161-12" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-13"><a href="chapter-bayes.html#cb161-13" tabindex="-1"></a>    </span>
<span id="cb161-14"><a href="chapter-bayes.html#cb161-14" tabindex="-1"></a>   Laplace smoothing<span class="sc">:</span> <span class="dv">0</span></span>
<span id="cb161-15"><a href="chapter-bayes.html#cb161-15" tabindex="-1"></a>   </span>
<span id="cb161-16"><a href="chapter-bayes.html#cb161-16" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-17"><a href="chapter-bayes.html#cb161-17" tabindex="-1"></a>    </span>
<span id="cb161-18"><a href="chapter-bayes.html#cb161-18" tabindex="-1"></a>   A priori probabilities<span class="sc">:</span> </span>
<span id="cb161-19"><a href="chapter-bayes.html#cb161-19" tabindex="-1"></a>   </span>
<span id="cb161-20"><a href="chapter-bayes.html#cb161-20" tabindex="-1"></a>   good risk  bad risk </span>
<span id="cb161-21"><a href="chapter-bayes.html#cb161-21" tabindex="-1"></a>   <span class="fl">0.4923858</span> <span class="fl">0.5076142</span> </span>
<span id="cb161-22"><a href="chapter-bayes.html#cb161-22" tabindex="-1"></a>   </span>
<span id="cb161-23"><a href="chapter-bayes.html#cb161-23" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-24"><a href="chapter-bayes.html#cb161-24" tabindex="-1"></a>    </span>
<span id="cb161-25"><a href="chapter-bayes.html#cb161-25" tabindex="-1"></a>   Tables<span class="sc">:</span> </span>
<span id="cb161-26"><a href="chapter-bayes.html#cb161-26" tabindex="-1"></a>   </span>
<span id="cb161-27"><a href="chapter-bayes.html#cb161-27" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-28"><a href="chapter-bayes.html#cb161-28" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">age</span> (Gaussian) </span>
<span id="cb161-29"><a href="chapter-bayes.html#cb161-29" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-30"><a href="chapter-bayes.html#cb161-30" tabindex="-1"></a>         </span>
<span id="cb161-31"><a href="chapter-bayes.html#cb161-31" tabindex="-1"></a>   age    good risk  bad risk</span>
<span id="cb161-32"><a href="chapter-bayes.html#cb161-32" tabindex="-1"></a>     mean <span class="fl">46.453608</span> <span class="fl">35.470000</span></span>
<span id="cb161-33"><a href="chapter-bayes.html#cb161-33" tabindex="-1"></a>     sd    <span class="fl">8.563513</span>  <span class="fl">9.542520</span></span>
<span id="cb161-34"><a href="chapter-bayes.html#cb161-34" tabindex="-1"></a>   </span>
<span id="cb161-35"><a href="chapter-bayes.html#cb161-35" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-36"><a href="chapter-bayes.html#cb161-36" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">income</span> (Gaussian) </span>
<span id="cb161-37"><a href="chapter-bayes.html#cb161-37" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-38"><a href="chapter-bayes.html#cb161-38" tabindex="-1"></a>         </span>
<span id="cb161-39"><a href="chapter-bayes.html#cb161-39" tabindex="-1"></a>   income good risk  bad risk</span>
<span id="cb161-40"><a href="chapter-bayes.html#cb161-40" tabindex="-1"></a>     mean <span class="fl">48888.987</span> <span class="fl">27309.560</span></span>
<span id="cb161-41"><a href="chapter-bayes.html#cb161-41" tabindex="-1"></a>     sd    <span class="fl">9986.962</span>  <span class="fl">7534.639</span></span>
<span id="cb161-42"><a href="chapter-bayes.html#cb161-42" tabindex="-1"></a>   </span>
<span id="cb161-43"><a href="chapter-bayes.html#cb161-43" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-44"><a href="chapter-bayes.html#cb161-44" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">mortgage</span> (Bernoulli) </span>
<span id="cb161-45"><a href="chapter-bayes.html#cb161-45" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-46"><a href="chapter-bayes.html#cb161-46" tabindex="-1"></a>           </span>
<span id="cb161-47"><a href="chapter-bayes.html#cb161-47" tabindex="-1"></a>   mortgage good risk  bad risk</span>
<span id="cb161-48"><a href="chapter-bayes.html#cb161-48" tabindex="-1"></a>        yes <span class="fl">0.6804124</span> <span class="fl">0.7400000</span></span>
<span id="cb161-49"><a href="chapter-bayes.html#cb161-49" tabindex="-1"></a>        no  <span class="fl">0.3195876</span> <span class="fl">0.2600000</span></span>
<span id="cb161-50"><a href="chapter-bayes.html#cb161-50" tabindex="-1"></a>   </span>
<span id="cb161-51"><a href="chapter-bayes.html#cb161-51" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-52"><a href="chapter-bayes.html#cb161-52" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">nr.loans</span> (Gaussian) </span>
<span id="cb161-53"><a href="chapter-bayes.html#cb161-53" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-54"><a href="chapter-bayes.html#cb161-54" tabindex="-1"></a>           </span>
<span id="cb161-55"><a href="chapter-bayes.html#cb161-55" tabindex="-1"></a>   nr.loans good risk  bad risk</span>
<span id="cb161-56"><a href="chapter-bayes.html#cb161-56" tabindex="-1"></a>       mean <span class="fl">1.0309278</span> <span class="fl">1.6600000</span></span>
<span id="cb161-57"><a href="chapter-bayes.html#cb161-57" tabindex="-1"></a>       sd   <span class="fl">0.7282057</span> <span class="fl">0.7550503</span></span>
<span id="cb161-58"><a href="chapter-bayes.html#cb161-58" tabindex="-1"></a>   </span>
<span id="cb161-59"><a href="chapter-bayes.html#cb161-59" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-60"><a href="chapter-bayes.html#cb161-60" tabindex="-1"></a>   <span class="er">::</span> <span class="fu">marital</span> (Categorical) </span>
<span id="cb161-61"><a href="chapter-bayes.html#cb161-61" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span> </span>
<span id="cb161-62"><a href="chapter-bayes.html#cb161-62" tabindex="-1"></a>            </span>
<span id="cb161-63"><a href="chapter-bayes.html#cb161-63" tabindex="-1"></a>   marital    good risk   bad risk</span>
<span id="cb161-64"><a href="chapter-bayes.html#cb161-64" tabindex="-1"></a>     single  <span class="fl">0.38144330</span> <span class="fl">0.49000000</span></span>
<span id="cb161-65"><a href="chapter-bayes.html#cb161-65" tabindex="-1"></a>     married <span class="fl">0.52577320</span> <span class="fl">0.11000000</span></span>
<span id="cb161-66"><a href="chapter-bayes.html#cb161-66" tabindex="-1"></a>     other   <span class="fl">0.09278351</span> <span class="fl">0.40000000</span></span>
<span id="cb161-67"><a href="chapter-bayes.html#cb161-67" tabindex="-1"></a>   </span>
<span id="cb161-68"><a href="chapter-bayes.html#cb161-68" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span></span></code></pre></div>
<p>The <code><a href="https://majkamichal.github.io/naivebayes/reference/naive_bayes.html">naive_bayes()</a></code> function estimates the probability distributions for each feature, conditioned on the target class. Specifically:</p>
<ul>
<li>
<strong>Categorical features</strong> (e.g., <code>marital</code>, <code>mortgage</code>) – The function computes class-conditional probabilities.<br>
</li>
<li>
<strong>Continuous features</strong> (e.g., <code>age</code>, <code>income</code>, <code>nr.loans</code>) – The function assumes a Gaussian distribution and calculates the mean and standard deviation for each class.</li>
</ul>
<p>To inspect the model’s learned probability distributions, we summarize the trained model:</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="chapter-bayes.html#cb162-1" tabindex="-1"></a><span class="fu">summary</span>(naive_bayes)</span>
<span id="cb162-2"><a href="chapter-bayes.html#cb162-2" tabindex="-1"></a>   </span>
<span id="cb162-3"><a href="chapter-bayes.html#cb162-3" tabindex="-1"></a>   <span class="sc">==</span><span class="er">===============================</span> Naive Bayes <span class="sc">==</span><span class="er">================================</span> </span>
<span id="cb162-4"><a href="chapter-bayes.html#cb162-4" tabindex="-1"></a>    </span>
<span id="cb162-5"><a href="chapter-bayes.html#cb162-5" tabindex="-1"></a>   <span class="sc">-</span> Call<span class="sc">:</span> <span class="fu">naive_bayes.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set) </span>
<span id="cb162-6"><a href="chapter-bayes.html#cb162-6" tabindex="-1"></a>   <span class="sc">-</span> Laplace<span class="sc">:</span> <span class="dv">0</span> </span>
<span id="cb162-7"><a href="chapter-bayes.html#cb162-7" tabindex="-1"></a>   <span class="sc">-</span> Classes<span class="sc">:</span> <span class="dv">2</span> </span>
<span id="cb162-8"><a href="chapter-bayes.html#cb162-8" tabindex="-1"></a>   <span class="sc">-</span> Samples<span class="sc">:</span> <span class="dv">197</span> </span>
<span id="cb162-9"><a href="chapter-bayes.html#cb162-9" tabindex="-1"></a>   <span class="sc">-</span> Features<span class="sc">:</span> <span class="dv">5</span> </span>
<span id="cb162-10"><a href="chapter-bayes.html#cb162-10" tabindex="-1"></a>   <span class="sc">-</span> Conditional distributions<span class="sc">:</span> </span>
<span id="cb162-11"><a href="chapter-bayes.html#cb162-11" tabindex="-1"></a>       <span class="sc">-</span> Bernoulli<span class="sc">:</span> <span class="dv">1</span></span>
<span id="cb162-12"><a href="chapter-bayes.html#cb162-12" tabindex="-1"></a>       <span class="sc">-</span> Categorical<span class="sc">:</span> <span class="dv">1</span></span>
<span id="cb162-13"><a href="chapter-bayes.html#cb162-13" tabindex="-1"></a>       <span class="sc">-</span> Gaussian<span class="sc">:</span> <span class="dv">3</span></span>
<span id="cb162-14"><a href="chapter-bayes.html#cb162-14" tabindex="-1"></a>   <span class="sc">-</span> Prior probabilities<span class="sc">:</span> </span>
<span id="cb162-15"><a href="chapter-bayes.html#cb162-15" tabindex="-1"></a>       <span class="sc">-</span> good risk<span class="sc">:</span> <span class="fl">0.4924</span></span>
<span id="cb162-16"><a href="chapter-bayes.html#cb162-16" tabindex="-1"></a>       <span class="sc">-</span> bad risk<span class="sc">:</span> <span class="fl">0.5076</span></span>
<span id="cb162-17"><a href="chapter-bayes.html#cb162-17" tabindex="-1"></a>   </span>
<span id="cb162-18"><a href="chapter-bayes.html#cb162-18" tabindex="-1"></a>   <span class="sc">--------------------------------------------------------------------------------</span></span></code></pre></div>
<p>The summary output provides useful insights into how the classifier models each feature’s probability distribution. This forms the basis for making predictions on new data points, which we explore in the next section.</p>
</div>
<div id="prediction-and-model-evaluation" class="section level3 unnumbered">
<h3>Prediction and Model Evaluation<a class="anchor" aria-label="anchor" href="#prediction-and-model-evaluation"><i class="fas fa-link"></i></a>
</h3>
<p>After training the Naive Bayes classifier, we evaluate its performance by applying it to the test set, which contains customers unseen during training. The goal is to compare the predicted probabilities with the actual class labels stored in <code>test_labels</code>.</p>
<p>To obtain the predicted class probabilities, we use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function from the <strong>naivebayes</strong> package:</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prob_naive_bayes</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">naive_bayes</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span></code></pre></div>
<p>By specifying <code>type = "prob"</code>, the function returns posterior probabilities for each class instead of discrete predictions.</p>
<p>To inspect the model’s predictions, we display the first 10 probability estimates:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="chapter-bayes.html#cb164-1" tabindex="-1"></a><span class="co"># Display the first 10 predictions</span></span>
<span id="cb164-2"><a href="chapter-bayes.html#cb164-2" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">head</span>(prob_naive_bayes, <span class="at">n =</span> <span class="dv">10</span>), <span class="dv">3</span>)</span>
<span id="cb164-3"><a href="chapter-bayes.html#cb164-3" tabindex="-1"></a>         good risk bad risk</span>
<span id="cb164-4"><a href="chapter-bayes.html#cb164-4" tabindex="-1"></a>    [<span class="dv">1</span>,]     <span class="fl">0.001</span>    <span class="fl">0.999</span></span>
<span id="cb164-5"><a href="chapter-bayes.html#cb164-5" tabindex="-1"></a>    [<span class="dv">2</span>,]     <span class="fl">0.013</span>    <span class="fl">0.987</span></span>
<span id="cb164-6"><a href="chapter-bayes.html#cb164-6" tabindex="-1"></a>    [<span class="dv">3</span>,]     <span class="fl">0.000</span>    <span class="fl">1.000</span></span>
<span id="cb164-7"><a href="chapter-bayes.html#cb164-7" tabindex="-1"></a>    [<span class="dv">4</span>,]     <span class="fl">0.184</span>    <span class="fl">0.816</span></span>
<span id="cb164-8"><a href="chapter-bayes.html#cb164-8" tabindex="-1"></a>    [<span class="dv">5</span>,]     <span class="fl">0.614</span>    <span class="fl">0.386</span></span>
<span id="cb164-9"><a href="chapter-bayes.html#cb164-9" tabindex="-1"></a>    [<span class="dv">6</span>,]     <span class="fl">0.193</span>    <span class="fl">0.807</span></span>
<span id="cb164-10"><a href="chapter-bayes.html#cb164-10" tabindex="-1"></a>    [<span class="dv">7</span>,]     <span class="fl">0.002</span>    <span class="fl">0.998</span></span>
<span id="cb164-11"><a href="chapter-bayes.html#cb164-11" tabindex="-1"></a>    [<span class="dv">8</span>,]     <span class="fl">0.002</span>    <span class="fl">0.998</span></span>
<span id="cb164-12"><a href="chapter-bayes.html#cb164-12" tabindex="-1"></a>    [<span class="dv">9</span>,]     <span class="fl">0.378</span>    <span class="fl">0.622</span></span>
<span id="cb164-13"><a href="chapter-bayes.html#cb164-13" tabindex="-1"></a>   [<span class="dv">10</span>,]     <span class="fl">0.283</span>    <span class="fl">0.717</span></span></code></pre></div>
<p>The output contains two columns:</p>
<ul>
<li>The first column represents the probability that a customer is classified as “<code>good risk</code>.”<br>
</li>
<li>The second column represents the probability that a customer is classified as “<code>bad risk</code>.”</li>
</ul>
<p>For example, if the second row has a probability of 0.987 for “<code>bad risk</code>,” it indicates that the second customer in the test set is predicted to belong to the “<code>bad risk</code>” category with a probability of 0.987.</p>
<p>This probability-based output provides flexibility in decision-making. Instead of using a fixed threshold of 0.5, financial institutions can adjust the cutoff based on business objectives. For instance, if minimizing loan defaults is the priority, a more conservative threshold may be set. In the next section, we convert these probabilities into class predictions and evaluate the model using a confusion matrix and other performance metrics.</p>
<div id="confusion-matrix-1" class="section level4 unnumbered">
<h4>Confusion Matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix-1"><i class="fas fa-link"></i></a>
</h4>
<p>To assess the classification performance of our Naive Bayes model, we compute the confusion matrix using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> and <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.plot.html">conf.mat.plot()</a></code> functions from the <strong>liver</strong> package. The confusion matrix compares the predicted class probabilities with the actual class labels, allowing us to measure the model’s accuracy and analyze different types of errors.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="chapter-bayes.html#cb165-1" tabindex="-1"></a><span class="co"># Extract probability of "good risk"</span></span>
<span id="cb165-2"><a href="chapter-bayes.html#cb165-2" tabindex="-1"></a>prob_naive_bayes <span class="ot">=</span> prob_naive_bayes[, <span class="dv">1</span>] </span>
<span id="cb165-3"><a href="chapter-bayes.html#cb165-3" tabindex="-1"></a></span>
<span id="cb165-4"><a href="chapter-bayes.html#cb165-4" tabindex="-1"></a><span class="fu">conf.mat</span>(prob_naive_bayes, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"good risk"</span>)</span>
<span id="cb165-5"><a href="chapter-bayes.html#cb165-5" tabindex="-1"></a>              Actual</span>
<span id="cb165-6"><a href="chapter-bayes.html#cb165-6" tabindex="-1"></a>   Predict     good risk bad risk</span>
<span id="cb165-7"><a href="chapter-bayes.html#cb165-7" tabindex="-1"></a>     good risk        <span class="dv">24</span>        <span class="dv">3</span></span>
<span id="cb165-8"><a href="chapter-bayes.html#cb165-8" tabindex="-1"></a>     bad risk          <span class="dv">2</span>       <span class="dv">20</span></span>
<span id="cb165-9"><a href="chapter-bayes.html#cb165-9" tabindex="-1"></a></span>
<span id="cb165-10"><a href="chapter-bayes.html#cb165-10" tabindex="-1"></a><span class="fu">conf.mat.plot</span>(prob_naive_bayes, test_labels, <span class="at">cutoff =</span> <span class="fl">0.5</span>, <span class="at">reference =</span> <span class="st">"good risk"</span>)</span></code></pre></div>
<div class="inline-figure"><img src="bayes_files/figure-html/unnamed-chunk-16-1.png" width="65%" style="display: block; margin: auto;"></div>
<p>In this evaluation, we apply a <strong>classification threshold of 0.5</strong>, meaning that if a customer’s predicted probability of being a “<code>good risk</code>” is at least 50%, the model classifies them as “<code>good risk</code>”; otherwise, they are classified as “<code>bad risk</code>.” Additionally, we specify <strong>“<code>good risk</code>” as the reference class</strong>, meaning that performance metrics such as sensitivity and precision will be calculated with respect to this category.</p>
<p>The confusion matrix provides the following breakdown of model predictions:</p>
<ul>
<li>
<strong>True Positives (TP)</strong>: Customers correctly classified as “<code>good risk</code>.”<br>
</li>
<li>
<strong>True Negatives (TN)</strong>: Customers correctly classified as “<code>bad risk</code>.”<br>
</li>
<li>
<strong>False Positives (FP)</strong>: Customers incorrectly classified as “<code>good risk</code>” when they were actually “<code>bad risk</code>.”<br>
</li>
<li>
<strong>False Negatives (FN)</strong>: Customers incorrectly classified as “<code>bad risk</code>” when they were actually “<code>good risk</code>.”</li>
</ul>
<p>The values in the confusion matrix quantify the model’s classification accuracy and error rates at a cutoff of 0.5. Specifically, the model correctly predicts “24 + 20” cases and misclassifies “3 + 2” cases.</p>
<p>This matrix offers a structured way to assess classification performance, helping us understand how well the model differentiates between high- and low-risk customers. In the next section, we further analyze performance using additional evaluation metrics.</p>
</div>
<div id="roc-curve-and-auc-1" class="section level4 unnumbered">
<h4>ROC Curve and AUC<a class="anchor" aria-label="anchor" href="#roc-curve-and-auc-1"><i class="fas fa-link"></i></a>
</h4>
<p>To further evaluate the model, we compute the <em>Receiver Operating Characteristic (ROC) curve</em> and the <em>Area Under the Curve (AUC)</em> value. These metrics provide a comprehensive assessment of the model’s ability to distinguish between “<code>good risk</code>” and “<code>bad risk</code>” customers across different classification thresholds. The <strong>pROC</strong> package in R facilitates both calculations.</p>
<div class="sourceCode" id="cb166"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xrobin.github.io/pROC/">pROC</a></span><span class="op">)</span>          </span>
<span></span>
<span><span class="va">roc_naive_bayes</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">test_labels</span>, <span class="va">prob_naive_bayes</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc</a></span><span class="op">(</span><span class="va">roc_naive_bayes</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bayes_files/figure-html/unnamed-chunk-17-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The ROC curve plots the <strong>true positive rate (sensitivity)</strong> against the <strong>false positive rate (1 - specificity)</strong> at various threshold values. A curve that remains closer to the top-left corner indicates a well-performing model, while a curve near the diagonal suggests performance close to random guessing.</p>
<p>Next, we compute the <em>AUC</em> value, which summarizes the ROC curve into a single number:</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="chapter-bayes.html#cb167-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">auc</span>(roc_naive_bayes), <span class="dv">3</span>)</span>
<span id="cb167-2"><a href="chapter-bayes.html#cb167-2" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.957</span></span></code></pre></div>
<p>The AUC value, 0.957, represents the probability that a randomly selected “<code>good risk</code>” customer will receive a higher predicted probability than a randomly selected “<code>bad risk</code>” customer. An AUC closer to 1 indicates strong predictive performance, while an AUC of 0.5 suggests no better performance than random guessing.</p>
<p>By analyzing the ROC curve and AUC, financial institutions can adjust the decision threshold to align with business objectives. If minimizing false negatives (misclassifying high-risk customers as low-risk) is a priority, the threshold can be lowered to increase sensitivity. Conversely, if false positives (denying loans to eligible customers) are a concern, a higher threshold can be set to improve specificity.</p>
<p>Through this case study, we have demonstrated how Naive Bayes can be applied to financial risk assessment. By evaluating model performance using the confusion matrix, ROC curve, and AUC, we identified its strengths and limitations. This highlights the efficiency and interpretability of Naive Bayes, making it a valuable tool for probabilistic classification in financial decision-making.</p>
</div>
</div>
<div id="takeaways-from-the-case-study" class="section level3 unnumbered">
<h3>Takeaways from the Case Study<a class="anchor" aria-label="anchor" href="#takeaways-from-the-case-study"><i class="fas fa-link"></i></a>
</h3>
<p>This case study demonstrated how Naive Bayes can be applied to financial risk assessment by classifying customers as either <em>good risk</em> or <em>bad risk</em> based on demographic and financial attributes. Through key evaluation metrics such as the confusion matrix, ROC curve, and AUC, we analyzed the model’s predictive power and identified its strengths and limitations.</p>
<p>The results highlight the <strong>efficiency, simplicity, and interpretability</strong> of Naive Bayes, making it a valuable tool for probabilistic classification in financial decision-making. The model’s ability to provide probability estimates allows institutions to adjust decision thresholds based on business priorities—whether prioritizing sensitivity to minimize high-risk approvals or improving specificity to reduce false rejections.</p>
<p>While Naive Bayes performs well in this scenario, it relies on the assumption of feature independence, which may not always hold in real-world financial data. Future improvements could include using ensemble models or integrating additional financial indicators to refine predictions further.</p>
<p>By applying Naive Bayes to financial risk assessment, we demonstrated how probabilistic classification methods can support data-driven lending decisions, helping financial institutions manage risk effectively while optimizing credit policies.</p>
</div>
</div>
<div id="exercises-7" class="section level2" number="9.6">
<h2>
<span class="header-section-number">9.6</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-7"><i class="fas fa-link"></i></a>
</h2>
<div id="conceptual-questions-5" class="section level3 unnumbered">
<h3>Conceptual questions<a class="anchor" aria-label="anchor" href="#conceptual-questions-5"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>Why is Naive Bayes considered a probabilistic classification model?<br>
</li>
<li>What is the difference between prior probability, likelihood, and posterior probability in Bayes’ theorem?<br>
</li>
<li>What does it mean when we say Naive Bayes assumes feature independence?<br>
</li>
<li>In which situations does the feature independence assumption become problematic? Provide an example.<br>
</li>
<li>What are the key strengths of Naive Bayes? Why is it widely used in text classification and spam filtering?<br>
</li>
<li>What are the major limitations of Naive Bayes, and how do they impact its performance?<br>
</li>
<li>How does Laplace smoothing help in handling missing feature values in Naive Bayes?<br>
</li>
<li>When should you use multinomial Naive Bayes, Bernoulli Naive Bayes, or Gaussian Naive Bayes?<br>
</li>
<li>Compare the Naive Bayes classifier to k-Nearest Neighbors algorithm (Chapter <a href="chapter-knn.html#chapter-knn">7</a>). How do their assumptions and outputs differ?<br>
</li>
<li>How does the choice of probability threshold affect model predictions?<br>
</li>
<li>Why does Naive Bayes remain effective even when the independence assumption is violated?<br>
</li>
<li>What type of dataset characteristics make Naive Bayes perform poorly compared to other classifiers?<br>
</li>
<li>How does the Gaussian Naive Bayes classifier handle continuous data?<br>
</li>
<li>How can domain knowledge help improve Naive Bayes classification results?<br>
</li>
<li>How would Naive Bayes handle imbalanced datasets? What preprocessing techniques could help?<br>
</li>
<li>Explain how prior probabilities can be adjusted based on business objectives in a classification problem.</li>
</ol>
</div>
<div id="hands-on-implementation-with-the-churn-dataset" class="section level3 unnumbered">
<h3>Hands-on implementation with the churn dataset<a class="anchor" aria-label="anchor" href="#hands-on-implementation-with-the-churn-dataset"><i class="fas fa-link"></i></a>
</h3>
<p>For the following exercises, we will use the <em>churn</em> dataset from the <strong>liver</strong> package. This dataset contains information about customer subscriptions, and our goal is to predict whether a customer will churn (<code>churn = yes/no</code>) using the Naive Bayes classifier. In Section <a href="chapter-EDA.html#EDA-sec-churn">4.3</a>, we performed exploratory data analysis on this dataset to understand its structure and key features.</p>
<div id="data-preparation-2" class="section level4 unnumbered">
<h4>Data preparation<a class="anchor" aria-label="anchor" href="#data-preparation-2"><i class="fas fa-link"></i></a>
</h4>
<ol start="17" style="list-style-type: decimal">
<li>Load the <strong>liver</strong> package and the <em>churn</em> dataset:</li>
</ol>
<div class="sourceCode" id="cb168"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.uva.nl/profile/a.mohammadi">liver</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span></span></code></pre></div>
<ol start="18" style="list-style-type: decimal">
<li><p>Display the structure and summary statistics of the dataset to examine its variables and their distributions.</p></li>
<li><p>Split the dataset into an 80% training set and a 20% test set using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function from the <strong>liver</strong> package.</p></li>
<li><p>Verify that the partitioning maintains the distribution of the <code>churn</code> variable by comparing its proportions in the training and test sets.</p></li>
</ol>
</div>
</div>
<div id="training-and-evaluating-the-naive-bayes-classifier" class="section level3 unnumbered">
<h3>Training and evaluating the Naive Bayes classifier<a class="anchor" aria-label="anchor" href="#training-and-evaluating-the-naive-bayes-classifier"><i class="fas fa-link"></i></a>
</h3>
<ol start="21" style="list-style-type: decimal">
<li>Based on the exploratory data analysis in Section <a href="chapter-EDA.html#EDA-sec-churn">4.3</a>, select the following predictors for the Naive Bayes model: <code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>day.mins</code>, <code>eve.mins</code>, <code>night.mins</code>, and <code>customer.calls</code>. Define the model formula:</li>
</ol>
<div class="sourceCode" id="cb169"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">churn</span> <span class="op">~</span> <span class="va">account.length</span> <span class="op">+</span> <span class="va">voice.plan</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> </span>
<span>                 <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> </span>
<span>                 <span class="va">night.mins</span> <span class="op">+</span> <span class="va">customer.calls</span></span></code></pre></div>
<ol start="22" style="list-style-type: decimal">
<li><p>Train a Naive Bayes classifier on the training set using the <strong>naivebayes</strong> package.</p></li>
<li><p>Summarize the trained model. What insights can you gain from the estimated class-conditional probabilities?</p></li>
<li><p>Use the trained model to predict class probabilities for the test set using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function from the <strong>naivebayes</strong> package.</p></li>
<li><p>Extract and examine the first 10 probability predictions. Interpret what these values indicate about the likelihood of customer churn.</p></li>
<li><p>Compute the confusion matrix using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> function from the <strong>liver</strong> package with a classification threshold of 0.5.</p></li>
<li><p>Visualize the confusion matrix using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.plot.html">conf.mat.plot()</a></code> function from the <strong>liver</strong> package.</p></li>
<li><p>Compute key evaluation metrics, including accuracy, precision, recall, and F1-score, based on the confusion matrix.</p></li>
<li><p>Lower the classification threshold from 0.5 to 0.3 and recompute the confusion matrix. How does adjusting the threshold affect model performance?</p></li>
<li><p>Plot the ROC curve and compute the AUC value for the model. Interpret the results in terms of the model’s ability to distinguish between churn and non-churn customers.</p></li>
<li><p>Interpret the AUC value. What does it indicate about the model’s ability to distinguish between churn and non-churn customers?<br></p></li>
<li><p>Train a Naive Bayes model with Laplace smoothing (<code>laplace = 1</code>) and compare the results to the model without smoothing. How does smoothing affect predictions?</p></li>
<li><p>Compare the Naive Bayes classifier to the k-Nearest Neighbors algorithm (Chapter <a href="chapter-knn.html#chapter-knn">7</a>) trained on the same dataset. Evaluate their performance using accuracy, precision, recall, F1-score, and AUC. Which model performs better, and what factors might explain the differences in performance?</p></li>
<li><p>Experiment by removing one predictor variable at a time and retraining the model. How does this impact accuracy and other evaluation metrics?</p></li>
</ol>
</div>
<div id="real-world-application-and-critical-thinking" class="section level3 unnumbered">
<h3>Real-world application and critical thinking<a class="anchor" aria-label="anchor" href="#real-world-application-and-critical-thinking"><i class="fas fa-link"></i></a>
</h3>
<ol start="36" style="list-style-type: decimal">
<li><p>Suppose a telecommunications company wants to use this model to reduce customer churn. What business decisions could be made based on the model’s predictions?</p></li>
<li><p>If incorrectly predicting a false negative (missed churner) is more costly than a false positive, how should the decision threshold be adjusted?</p></li>
<li><p>A marketing team wants to offer promotional discounts to customers predicted to churn. How would you use this model to target the right customers?</p></li>
<li><p>Suppose the dataset included a new feature: customer satisfaction score (on a scale from 1 to 10). How could this feature improve the model?</p></li>
<li><p>What steps would you take if the model performed poorly on new customer data?</p></li>
<li><p>Explain why feature independence may or may not hold in this dataset. How could feature correlation impact the model’s reliability?</p></li>
<li><p>Would Naive Bayes be suitable for multi-class classification problems? If so, how would you extend this model to predict multiple churn reasons instead of just <code>yes/no</code>?</p></li>
<li><p>If given time-series data about customer interactions over months, would Naive Bayes still be appropriate? Why or why not?</p></li>
</ol>
</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></div>
<div class="next"><a href="chapter-regression.html"><span class="header-section-number">10</span> Regression Modeling: From Basics to Advanced Techniques</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li>
<a class="nav-link" href="#chapter-bayes"><span class="header-section-number">9</span> Naive Bayes Classifier</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#strengths-and-limitations">Strengths and Limitations</a></li>
<li><a class="nav-link" href="#what-this-chapter-covers">What This Chapter Covers</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bayes-theorem-and-probabilistic-foundations"><span class="header-section-number">9.1</span> Bayes’ Theorem and Probabilistic Foundations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-essence-of-bayes-theorem">The Essence of Bayes’ Theorem</a></li>
<li><a class="nav-link" href="#how-does-bayes-theorem-work">How Does Bayes’ Theorem Work?</a></li>
<li><a class="nav-link" href="#a-gateway-to-naive-bayes">A Gateway to Naive Bayes</a></li>
</ul>
</li>
<li><a class="nav-link" href="#why-is-it-called-naive"><span class="header-section-number">9.2</span> Why is it Called “Naive”?</a></li>
<li><a class="nav-link" href="#the-laplace-smoothing-technique"><span class="header-section-number">9.3</span> The Laplace Smoothing Technique</a></li>
<li><a class="nav-link" href="#types-of-naive-bayes-classifiers"><span class="header-section-number">9.4</span> Types of Naive Bayes Classifiers</a></li>
<li>
<a class="nav-link" href="#case-study-predicting-financial-risk-with-naive-bayes"><span class="header-section-number">9.5</span> Case Study: Predicting Financial Risk with Naive Bayes</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#problem-understanding-2">Problem Understanding</a></li>
<li><a class="nav-link" href="#data-understanding-1">Data Understanding</a></li>
<li><a class="nav-link" href="#data-preparation-for-modeling">Data Preparation for Modeling</a></li>
<li><a class="nav-link" href="#applying-the-naive-bayes-classifier">Applying the Naive Bayes Classifier</a></li>
<li><a class="nav-link" href="#prediction-and-model-evaluation">Prediction and Model Evaluation</a></li>
<li><a class="nav-link" href="#takeaways-from-the-case-study">Takeaways from the Case Study</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#exercises-7"><span class="header-section-number">9.6</span> Exercises</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#conceptual-questions-5">Conceptual questions</a></li>
<li><a class="nav-link" href="#hands-on-implementation-with-the-churn-dataset">Hands-on implementation with the churn dataset</a></li>
<li><a class="nav-link" href="#training-and-evaluating-the-naive-bayes-classifier">Training and evaluating the Naive Bayes classifier</a></li>
<li><a class="nav-link" href="#real-world-application-and-critical-thinking">Real-world application and critical thinking</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/RezaMoammadi/Book-Data-Science/blob/master/bayes.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/RezaMoammadi/Book-Data-Science/edit/master/bayes.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Uncovering Data Science with R</strong>" was written by Reza Mohammadi. It was last built on 2025-02-17.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
