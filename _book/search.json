[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Data science transforming way solve problems, make decisions, uncover insights data. Whether ’re beginner experienced professional, Uncovering Data Science R provides intuitive practical introduction exciting field—prior analytics programming experience required.book work progress, welcome feedback readers. comments, suggestions, corrections, please feel free contact us Contact Us.","code":""},{"path":"index.html","id":"why-this-book","chapter":"Preface","heading":"Why This Book?","text":"Data science rapidly evolving field leverages computational tools techniques transform raw data actionable insights. book, introduce fundamental skills needed work R, powerful freely available statistical programming language widely used data analysis, visualization, machine learning.Unlike many books data science, focus accessibility. aim provide intuitive practical introduction, making R data science concepts understandable little technical background. hands-approach ensures learn theoretical concepts also gain experience applying real-world datasets.Compared commercial software like SAS SPSS, R provides free, open-source, highly extensible platform statistical computing machine learning. rich ecosystem packages makes excellent alternative proprietary data mining tools.Inspired Free Open Source Software (FOSS) movement, content book open transparent, ensuring reproducibility. code, datasets, materials hosted CRAN accessible via liver package (https://CRAN.R-project.org/package=liver), allowing readers engage book interactively.","code":""},{"path":"index.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who Should Read This Book?","text":"book anyone interested learning data science, particularly new field. designed :Business professionals want leverage data decision-making,Students researchers looking apply data analysis work,Beginners prior programming experience,Anyone interested data science machine learning using R.","code":""},{"path":"index.html","id":"what-you-will-learn","chapter":"Preface","heading":"What You Will Learn","text":"primary goal book introduce data science concepts using R tool data analysis machine learning. R open-source language environment statistical computing graphics, offering vast collection packages data mining, visualization, modeling.hands-examples real-world datasets, learn:basics R set environment,core principles data science Data Science Methodology,clean, transform, explore data,fundamentals statistical analysis, machine learning, data visualization,build evaluate machine learning models, including classification, regression, clustering, neural networks,apply techniques real-world datasets.","code":""},{"path":"index.html","id":"the-data-science-process","chapter":"Preface","heading":"The Data Science Process","text":"Data science follows iterative structured methodology analyzing extracting insights data. book follows framework:Problem Understanding – Defining objective understanding data.Data Preparation – Preparing raw data analysis.Exploratory Data Analysis (EDA) – Identifying patterns relationships data.Preparing Data Modeling – Transforming data machine learning models.Modeling – Building predictive models using machine learning algorithms.Evaluation – Assessing model performance using various metrics.Deployment – Applying trained model real-world scenarios.end book, solid understanding phases able apply effectively.","code":""},{"path":"index.html","id":"how-this-book-is-structured","chapter":"Preface","heading":"How This Book Is Structured","text":"book structured hands-guide, designed take beginner practitioner R data science. chapters follow logical progression, starting foundational concepts gradually introducing advanced techniques.use real-world datasets (see Table 0.1) throughout book illustrate key concepts. datasets available liver package can accessed easily. brief overview book’s chapters:Chapter 1 – Introduction R, including installation basic operations.Chapter 2 – Introduction Data Science methodology.Chapter 3 – Data preparation techniques.Chapter 4 – Exploratory Data Analysis (EDA) using visualization summary statistics.Chapter 5 – Basics statistical analysis, including descriptive statistics hypothesis testing.Chapter 6 – Overview machine learning models.Chapter 7 – k-Nearest Neighbors (k-NN) algorithm.Chapter 8 – Model evaluation metrics techniques.Chapter 9 – Naïve Bayes classifier probabilistic modeling.Chapter 10 – Linear regression predictive modeling.Chapter 11 – Decision trees Random Forests.Chapter 12 – Neural networks deep learning basics.Chapter 13 – Clustering techniques, including k-means.end chapter, find practical exercises labs reinforce learning. exercises use real-world datasets provide step--step guidance ensure hands-experience.","code":""},{"path":"index.html","id":"how-to-use-this-book","chapter":"Preface","heading":"How to Use This Book","text":"book designed self-study classroom use. can read cover cover jump chapters interest . chapter builds previous ones, beginners encouraged follow sequence smooth learning experience.get book:Run code examples – code snippets designed executed interactively R.Complete exercises – Practical exercises reinforce key concepts improve problem-solving skills.Modify experiment – Try changing code explore different scenarios.Use reference – ’re familiar basics, use book guide working real-world data.book also used data science courses University Amsterdam. can serve textbook similar courses supplementary resource advanced analytics training.","code":""},{"path":"index.html","id":"datasets-used-in-this-book","chapter":"Preface","heading":"Datasets Used in This Book","text":"Table 0.1 lists datasets used book. real-world datasets used illustrate key concepts available liver package, can downloaded CRAN.\nTable 0.1: List datasets used case studies different chapters. Available R package liver.\n","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"prior programming experience required, basic understanding numbers logic helpful. run code book, need install R, RStudio, several R packages.","code":""},{"path":"chapter-into-R.html","id":"chapter-into-R","chapter":"1 The Basics for R","heading":"1 The Basics for R","text":"can analyze data, need way communicate computer. ’s programming languages like R Python come . Many data science teams use mix languages, R great starting point designed specifically data analysis statistical computing.","code":""},{"path":"chapter-into-R.html","id":"why-choose-r-for-data-science","chapter":"1 The Basics for R","heading":"Why Choose R for Data Science?","text":"R widely used statistics, data analysis, visualization due rich ecosystem libraries tools tailored data science. Unlike general-purpose programming languages, R built statistical analysis, allowing data scientists perform everything basic calculations advanced machine learning just lines code.Python another popular language data science, R particularly well-suited :\n- Statistical Computing – R built-statistical functions methods hypothesis testing, regression modeling, machine learning.\n- Data Visualization – Packages like ggplot2 provide powerful tools creating high-quality plots graphs minimal effort.\n- Reproducible Research – R Markdown Shiny make easy generate reports interactive dashboards directly R code.\n- Bioinformatics & Finance – Many researchers analysts fields use R due robust statistical libraries domain-specific packages.Beyond capabilities, R :Free & Open Source – Available everyone, vibrant community contributors.Cross-Platform – Runs Windows, macOS, Linux.Flexible & Powerful – Supports interactive data exploration, visualization, machine learning.R language, RStudio tool makes working R easier. RStudio integrated development environment (IDE) provides:console running R commands,script editor syntax highlighting auto-completion,Built-tools data visualization, debugging, package management.chapter, learn fundamental skills needed work R, installation running first commands. Let’s begin! 🚀","code":""},{"path":"chapter-into-R.html","id":"how-to-install-r","chapter":"1 The Basics for R","heading":"1.1 How to Install R","text":"get started R, first need install computer. Follow steps:Go CRAN website – Comprehensive R Archive Network.Select operating system – Click link Windows, macOS, Linux.Download install R – Follow -screen instructions complete installation.","code":""},{"path":"chapter-into-R.html","id":"keeping-r-up-to-date","chapter":"1 The Basics for R","heading":"Keeping R Up to Date","text":"R receives major update year, along 2-3 minor updates annually. updating R—especially major versions—requires reinstalling packages, staying date ensures :✅ Access latest features improvements,\n✅ Maintain compatibility new packages,\n✅ Benefit security patches performance enhancements.Keeping R updated might feel like hassle, postponing updates can make process cumbersome later. ’s best update regularly ensure smooth performance compatibility.","code":""},{"path":"chapter-into-R.html","id":"how-to-install-rstudio","chapter":"1 The Basics for R","heading":"1.2 How to Install RStudio","text":"RStudio open-source integrated development environment (IDE) makes working R easier, interactive, efficient. provides user-friendly interface, advanced script editor, various tools plotting, debugging, workspace management—significantly enhance R programming experience.","code":""},{"path":"chapter-into-R.html","id":"installing-rstudio","chapter":"1 The Basics for R","heading":"Installing RStudio","text":"Follow steps install RStudio:Go RStudio website.Download latest version RStudio Desktop (free, open-source edition).Run installer follow -screen instructions.Launch RStudio, ’re ready start coding R!RStudio updated several times year, notify new version available. Keeping RStudio date recommended take advantage new features performance improvements.","code":""},{"path":"chapter-into-R.html","id":"exploring-the-rstudio-interface","chapter":"1 The Basics for R","heading":"Exploring the RStudio Interface","text":"open RStudio, see window similar Figure 1.1.\nFigure 1.1: RStudio window first launch program.\nsee three panels, add fourth selecting File > New File > R Script. opens script editor can write save R code. ’s quick overview RStudio’s panels:Top-left: Script Editor – Write save R code.Bottom-left: Console – Run R commands see output.Top-right: Environment & History – View variables, datasets, past commands.Bottom-right: Plots, Help, & Files – Display graphs, access documentation, manage files.now, just know can type R code console press Enter run . progress book, ’ll become familiar RStudio’s features learn efficiently write, run, debug R code.","code":""},{"path":"chapter-into-R.html","id":"customizing-rstudio","chapter":"1 The Basics for R","heading":"Customizing RStudio","text":"RStudio highly customizable, allowing tailor workflow. adjust settings, go :Tools > Global Options – Access general settings.Appearance > Editor Theme – Change editor’s theme (e.g., “Tomorrow Night 80” dark mode).Font & Layout Settings – Modify font size, panel positions, interface options.\ncomfortable coding environment enhances productivity—feel free explore tweak settings suit preferences!","code":""},{"path":"chapter-into-R.html","id":"how-to-learn-r","chapter":"1 The Basics for R","heading":"1.3 How to Learn R","text":"Learning R exciting rewarding journey opens doors data science, statistics, machine learning. Fortunately, numerous resources—books, online courses, tutorials, forums—can help get started advance skills.","code":""},{"path":"chapter-into-R.html","id":"video-tutorials","chapter":"1 The Basics for R","heading":"1. Video Tutorials","text":"prefer learning watching, YouTube offers wealth R tutorials, ranging beginner advanced levels:R Programming – Covers R basics data science concepts.Data School – Focuses data analysis, machine learning, practical R applications.","code":""},{"path":"chapter-into-R.html","id":"books","chapter":"1 The Basics for R","heading":"2. Books","text":"Books great way build deep understanding R. top recommendations:Absolute Beginners: Hands-Programming R Garrett Grolemund1 – practical introduction new programming.Data Science R: R Data Science Hadley Wickham Garrett Grolemund2 – Covers data visualization, wrangling, modeling.Machine Learning: Machine Learning R Brett Lantz3 – comprehensive guide machine learning techniques using R.","code":""},{"path":"chapter-into-R.html","id":"online-courses","chapter":"1 The Basics for R","heading":"3. Online Courses","text":"prefer structured learning hands-exercises, online courses offer interactive experiences:DataCamp – Features beginner-friendly courses like Introduction R.Coursera – Offers courses R Programming Data Science Specialization.","code":""},{"path":"chapter-into-R.html","id":"r-communities-forums","chapter":"1 The Basics for R","heading":"4. R Communities & Forums","text":"Engaging online communities great way learn others, ask questions, get support:Stack Overflow – Find answers R-related coding questions.RStudio Community – Connect R users participate discussions.","code":""},{"path":"chapter-into-R.html","id":"practice-regularly","chapter":"1 The Basics for R","heading":"5. Practice Regularly","text":"best way learn R consistent practice. Start simple exercises, explore real-world datasets, experiment R code. combining structured learning hands-experience, ’ll quickly develop confidence proficiency R.🚀 Start today! Choose one resources begin R learning journey.","code":""},{"path":"chapter-into-R.html","id":"getting-help-and-learning-more","chapter":"1 The Basics for R","heading":"1.4 Getting Help and Learning More","text":"begin journey R, ’ll likely encounter challenges questions along way. Fortunately, many resources available help troubleshoot problems, deepen understanding, continue learning. Whether ’re stuck error message, exploring new function, looking best practices, combination built-documentation, online communities, external learning materials can guide .R comes extensive built-documentation provides details functions, packages, programming techniques. quickly look function, type ? followed function name R console. bring official documentation, including usage examples, argument details, additional references. can also use help() example() get context function works.Beyond R’s internal help system, R community invaluable resource. question, chances someone already asked (answered) . Platforms like Stack Overflow, RStudio Community, R-help mailing list contain thousands discussions common advanced topics R programming, data science, machine learning. Searching forums can often lead quick reliable solutions. don’t find existing answer, posting question clear explanation reproducible example increase chances getting helpful responses.simple Google search often fastest way troubleshoot issues. Searching error message function name usually direct blog posts, documentation, forum discussions relevant explanations. Additionally, AI tools like ChatGPT can assist R programming questions, debugging, conceptual explanations. AI-generated solutions aren’t always perfect, can provide useful insights, suggest alternative approaches, help clarify difficult concepts.Ultimately, best way master R hands-experience. Don’t afraid experiment—write code, test different functions, explore new datasets. Mistakes natural part learning, one helps reinforce understanding. practice, confident proficient ’ll become R. Keep coding, keep exploring, enjoy journey!","code":""},{"path":"chapter-into-R.html","id":"data-science-with-r","chapter":"1 The Basics for R","heading":"1.5 Data Science with R","text":"R provides strong foundation data science, real power comes extensive ecosystem packages—collections functions, datasets, documentation extend R’s capabilities. base version R includes many essential tools, come preloaded statistical machine learning algorithms may need. Instead, algorithms developed shared large community researchers practitioners free open-source R packages.package modular, reusable library enhances R’s functionality. Packages include well-documented functions, usage instructions, often sample datasets testing learning. book, frequently use liver package, developed specifically accompany book. contains datasets functions designed illustrate key data science concepts techniques. Additionally, machine learning algorithm covered book, introduce use appropriate R packages implement methods.interested exploring , Comprehensive R Archive Network (CRAN) hosts thousands packages statistical computing, data visualization, machine learning. full list available packages can browsed CRAN website, providing access tools tailored various domains data science beyond.","code":""},{"path":"chapter-into-R.html","id":"install-packages","chapter":"1 The Basics for R","heading":"1.6 How to Install R Packages","text":"two ways install R packages. first method RStudio’s graphical interface. Click “Tools” tab select “Install Packages…”. dialog box appears, enter name package(s) wish install “Packages” field click “Install” button. Make sure check “Install dependencies” option ensure necessary supporting packages installed well. See Figure 1.2 visual guide.\nFigure 1.2: visual guide installing R packages using ‘Tools’ tab RStudio.\nsecond method install packages directly using install.packages() function. example, install liver package, provides datasets functions used throughout book, enter following command R console:Press “Enter” execute command. R connect CRAN download package correct format operating system. encounter issues installation, ensure connected internet proxy firewall blocking access CRAN. first time install package, R may ask select CRAN mirror. Choose one geographically close faster downloads.install.packages() function also allows customization, installing package local file specific repository. learn , type following command R console:Packages need installed . installation, must loaded new R session using library() function. cover load packages next section.","code":"\ninstall.packages(\"liver\")\n?install.packages()"},{"path":"chapter-into-R.html","id":"how-to-load-r-packages","chapter":"1 The Basics for R","heading":"1.7 How to Load R Packages","text":"optimize memory usage, R automatically load installed packages. Instead, must explicitly load necessary packages new R session. ensures relevant functions datasets available, minimizing resource consumption.\nload package, use library() require() function. functions locate package system make functions, datasets, documentation accessible. example, load liver package, enter following command R console:Press Enter execute command. error message appears stating package found (e.g., \"package called 'liver'\"), indicates package installed. cases, refer previous section installing packages.Beyond liver, book utilizes several R packages, introduced progressively throughout chapters needed. However, R packages contain functions identical names. instance, liver* dplyr** packages include select() function. multiple packages loaded, R defaults using function recently loaded package.explicitly specify package function sourced , use :: operator. ensures clarity prevents conflicts. example, use select() function liver package, enter:approach particularly useful complex projects multiple packages required, preventing unintended overwrites functions name.","code":"\nlibrary(liver)\nliver::select()"},{"path":"chapter-into-R.html","id":"running-r-code","chapter":"1 The Basics for R","heading":"1.8 Running R Code","text":"R interactive language, allowing type commands directly console see results immediately. example, can perform basic arithmetic operations addition, subtraction, multiplication, division. add two numbers, type following R console:Press Enter execute command. R compute sum display result. can also store result variable later use:, <- assignment operator R, used assign values variables. users prefer = operator (result = 2 + 3), also works cases, <- remains recommended convention R programming.Variables R store values later use, allowing perform calculations efficiently. example, can multiply result 4:R retrieve stored value result compute multiplication.","code":"2 + 3\n   [1] 5\nresult <- 2 + 3result * 4\n   [1] 20"},{"path":"chapter-into-R.html","id":"using-comments-in-r","chapter":"1 The Basics for R","heading":"Using Comments in R","text":"Comments used explain code make easier understand. R, comment starts #, everything following line ignored interpreter.Comments affect execution code essential documentation, especially working complex projects collaborating others.","code":"\n# Store the sum of 2 and 3 in the variable `result`\nresult <- 2 + 3"},{"path":"chapter-into-R.html","id":"functions-in-r","chapter":"1 The Basics for R","heading":"1.8.1 Functions in R","text":"R provides rich set built-functions perform specific tasks. function takes input(s) (arguments), processes , returns output. example, c() function creates vectors:can apply functions vector. example, compute average numbers x, use mean() function:Functions R follow simple structure:functions require arguments, others optional. learn function, use ? followed function name:open R’s help documentation, providing details function’s purpose, usage, arguments, examples.Functions essential R programming, helping simplify complex operations making code reusable efficient. progress, also learn write functions automate tasks improve workflow.","code":"\nx <- c(1, 2, 3, 4, 5)  # Create a vectormean(x)  # Calculate the mean of x\n   [1] 3\nfunction_name(arguments)\n?mean  # or help(mean)"},{"path":"chapter-into-R.html","id":"how-to-import-data-into-r","chapter":"1 The Basics for R","heading":"1.9 How to Import Data into R","text":"performing analysis, first need load data R. R can read data multiple sources, including text files, Excel files, online datasets. Depending file format data source, can choose several methods importing data R.","code":""},{"path":"chapter-into-R.html","id":"using-rstudios-graphical-interface","chapter":"1 The Basics for R","heading":"Using RStudio’s Graphical Interface","text":"easiest way import data R RStudio’s graphical interface. Click Import Dataset button top-right panel RStudio (see Figure 1.3 visual guide). open dialog box can choose file type:\n- Text (base) – CSV tab-delimited files.\n- Excel – Microsoft Excel files.\n- formats available, depending installed packages.selecting file, RStudio display import settings window (see Figure 1.4). , can adjust column names, data types, options. first row contains column names, select Yes Heading option. Click Import, dataset appear RStudio’s Environment panel, ready analysis.\nFigure 1.3: visual guide loading dataset R using ‘Import Dataset’ tab RStudio.\n\nFigure 1.4: visual guide customizing import settings loading dataset R using ‘Import Dataset’ tab RStudio.\n","code":""},{"path":"chapter-into-R.html","id":"using-read.csv","chapter":"1 The Basics for R","heading":"Using read.csv()","text":"can also import data directly using read.csv() function, reads tabular data (CSV files) R data frame. data file stored locally, can load follows:Replace \"path///file.csv\" actual file path. file contain column names, use:","code":"\ndata <- read.csv(\"path/to/your/file.csv\")\ndata <- read.csv(\"path/to/your/file.csv\", header = FALSE)"},{"path":"chapter-into-R.html","id":"setting-the-working-directory","chapter":"1 The Basics for R","heading":"Setting the Working Directory","text":"default, R looks files current working directory. data located elsewhere, can specify full path read.csv() set working directory.check current working directory:set new working directory:Alternatively, RStudio, go Session > Set Working Directory > Choose Directory… select desired folder.","code":"\ngetwd()\nsetwd(\"~/Documents\")  # Adjust the path based on your system"},{"path":"chapter-into-R.html","id":"using-file.choose-with-read.csv","chapter":"1 The Basics for R","heading":"Using file.choose() with read.csv()","text":"interactively select file instead typing path manually, use file.choose():open file selection dialog, making convenient option working multiple datasets.","code":"\ndata <- read.csv(file.choose())"},{"path":"chapter-into-R.html","id":"loading-data-from-online-sources","chapter":"1 The Basics for R","heading":"Loading Data from Online Sources","text":"R also allows direct import datasets web sources. example, load publicly available COVID-19 dataset:approach useful accessing open datasets research institutions government agencies.","code":"\ncorona_data <- read.csv(\"https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\", na.strings = \"\", fileEncoding = \"UTF-8-BOM\")"},{"path":"chapter-into-R.html","id":"using-read_excel-for-excel-files","chapter":"1 The Basics for R","heading":"Using read_excel() for Excel Files","text":"import Excel files, use read_excel() function readxl package. First, install load package:, import Excel file:Unlike read.csv(), read_excel() supports multiple sheets within Excel file, can specified using sheet argument.\n### Loading Data R Packages {-}datasets available directly R packages require importing external file. example, liver package, developed book, contains multiple datasets. access churn dataset:Since many datasets used book included liver package (see Table 0.1), frequently use package examples demonstrations.section well-structured clearly explains fundamental data types R. concise informative, making accessible beginners maintaining professional tone suitable Springer publication. minor refinements improve clarity, consistency, readability.","code":"\ninstall.packages(\"readxl\")\n\nlibrary(readxl)\ndata <- read_excel(\"path/to/your/file.xlsx\")\nlibrary(liver)\ndata(churn)"},{"path":"chapter-into-R.html","id":"data-types-in-r","chapter":"1 The Basics for R","heading":"1.10 Data Types in R","text":"Data R can take various forms, correctly identifying types essential effective data manipulation, visualization, analysis. data type specific properties determine R processes , understanding helps avoid errors ensures accurate results.common data types R:Numeric: Represents real numbers, 3.14 -5.67. type used continuous numerical values, like heights, weights, temperatures.Integer: Represents whole numbers without decimals, 1, 42, -10. type useful count-based data, number customers items sold.Character: Represents text string data, \"Data Science\" \"R Programming\". type commonly used categorical labels, names, descriptive values.Logical: Represents Boolean values: TRUE FALSE. Logical data often used conditional statements filtering operations.Factor: Represents categorical data predefined levels. Factors commonly used storing variables \"Male\" \"Female\" dataset particularly useful statistical modeling.check data type variable, use class() function. example, determine type variable result, type:Press Enter, R display variable’s data type.Recognizing different data types essential choosing right analytical visualization techniques. explore later chapters (e.g., Chapters 4 5), numerical categorical variables require different approaches performing descriptive statistics, hypothesis testing, data visualization.","code":"class(result)\n   [1] \"numeric\""},{"path":"chapter-into-R.html","id":"data-structures-in-r","chapter":"1 The Basics for R","heading":"1.11 Data Structures in R","text":"Data structures fundamental working data R. define data stored manipulated, directly impacts efficiency accuracy data analysis. commonly used data structures R vectors, matrices, data frames, lists, illustrated Figure 1.4.\nFigure 1.5: visual guide different types data structures R.\n","code":""},{"path":"chapter-into-R.html","id":"vectors-in-r","chapter":"1 The Basics for R","heading":"Vectors in R","text":"vector simplest data structure R. one-dimensional array holds elements type (numeric, character, logical). Vectors building blocks data structures. can create vector using c() function:, x numeric vector containing five elements. .vector() function confirms x indeed vector, length(x) returns number elements vector.","code":"# Create a numeric vector\nx <- c(1, 2, 0, -3, 5)\n\n# Display the vector\nx\n   [1]  1  2  0 -3  5\n\n# Check if x is a vector\nis.vector(x)\n   [1] TRUE\n\n# Check the length of the vector\nlength(x)\n   [1] 5"},{"path":"chapter-into-R.html","id":"matrices-in-r","chapter":"1 The Basics for R","heading":"Matrices in R","text":"matrix two-dimensional array elements must type. Matrices useful mathematical operations structured numerical data. can create matrix using matrix() function:matrix m consists two rows three columns, filled row-wise. dim() function returns dimensions matrix. fill matrix column-wise, set byrow = FALSE.","code":"# Create a matrix with 2 rows and 3 columns\nm <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Display the matrix\nm\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n# Check if m is a matrix\nis.matrix(m)\n   [1] TRUE\n\n# Check the dimensions of the matrix\ndim(m)\n   [1] 2 3"},{"path":"chapter-into-R.html","id":"data-frames-in-r","chapter":"1 The Basics for R","heading":"Data Frames in R","text":"data frame two-dimensional table column can contain different data type (numeric, character, logical). makes data frames ideal storing tabular data, similar spreadsheets. can create data frame using data.frame() function:data frame students_df consists four columns: student_id, name, age, grade. class() function confirms object data frame, .data.frame() checks structure.inspect first rows data frame, use head() function. example, display first six rows churn dataset liver package:code loads liver package, retrieves churn dataset, provides overview structure. str() function particularly useful summarizing data frames, displays data types column values.","code":"# Create vectors for student data\nstudent_id <- c(101, 102, 103, 104)\nname       <- c(\"Emma\", \"Bob\", \"Alice\", \"Noah\")\nage        <- c(20, 21, 19, 22)\ngrade      <- c(\"A\", \"B\", \"A\", \"C\")\n\n# Create a data frame from the vectors\nstudents_df <- data.frame(student_id, name, age, grade)\n\n# Display the data frame\nstudents_df\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     A\n   4        104  Noah  22     Clibrary(liver)  # Load the liver package\ndata(churn)     # Load the churn dataset\n\n# Check the structure of the dataset\nstr(churn)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\n# Display the first six rows\nhead(churn)\n     state     area.code account.length voice.plan voice.messages intl.plan\n   1    KS area_code_415            128        yes             25        no\n   2    OH area_code_415            107        yes             26        no\n   3    NJ area_code_415            137         no              0        no\n   4    OH area_code_408             84         no              0       yes\n   5    OK area_code_415             75         no              0       yes\n   6    AL area_code_510            118         no              0       yes\n     intl.mins intl.calls intl.charge day.mins day.calls day.charge eve.mins\n   1      10.0          3        2.70    265.1       110      45.07    197.4\n   2      13.7          3        3.70    161.6       123      27.47    195.5\n   3      12.2          5        3.29    243.4       114      41.38    121.2\n   4       6.6          7        1.78    299.4        71      50.90     61.9\n   5      10.1          3        2.73    166.7       113      28.34    148.3\n   6       6.3          6        1.70    223.4        98      37.98    220.6\n     eve.calls eve.charge night.mins night.calls night.charge customer.calls churn\n   1        99      16.78      244.7          91        11.01              1    no\n   2       103      16.62      254.4         103        11.45              1    no\n   3       110      10.30      162.6         104         7.32              0    no\n   4        88       5.26      196.9          89         8.86              2    no\n   5       122      12.61      186.9         121         8.41              3    no\n   6       101      18.75      203.9         118         9.18              0    no"},{"path":"chapter-into-R.html","id":"lists-in-r","chapter":"1 The Basics for R","heading":"Lists in R","text":"list flexible data structure can contain elements different types, including vectors, matrices, data frames, even lists. Lists useful storing complex objects structured way. can create list using list() function:list my_list stores vector, matrix, data frame within single object. Lists allow efficient organization heterogeneous data. explore structure list, use str() function:Lists powerful tools R, especially handling nested hierarchical data. exploration, use ?list access documentation additional examples.","code":"# Create a list containing a vector, matrix, and data frame\nmy_list <- list(vector = x, matrix = m, data_frame = students_df)\n\n# Display the list\nmy_list\n   $vector\n   [1]  1  2  0 -3  5\n   \n   $matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n   \n   $data_frame\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     A\n   4        104  Noah  22     Cstr(my_list)\n   List of 3\n    $ vector    : num [1:5] 1 2 0 -3 5\n    $ matrix    : num [1:2, 1:3] 1 4 2 5 3 6\n    $ data_frame:'data.frame':  4 obs. of  4 variables:\n     ..$ student_id: num [1:4] 101 102 103 104\n     ..$ name      : chr [1:4] \"Emma\" \"Bob\" \"Alice\" \"Noah\"\n     ..$ age       : num [1:4] 20 21 19 22\n     ..$ grade     : chr [1:4] \"A\" \"B\" \"A\" \"C\""},{"path":"chapter-into-R.html","id":"accessing-records-or-variables-in-r","chapter":"1 The Basics for R","heading":"1.12 Accessing Records or Variables in R","text":"’ve imported data R, can easily access specific records variables using $ [] operators. tools essential extracting data data frames lists.$ operator allows extract specific column data frame specific element list. example, access name column students_df data frame, use:command retrieves displays name column students_df data frame.Similarly, can use $ operator access elements within list. example, access vector element my_list list:command retrieves displays vector element my_list list. $ operator straightforward powerful way access specific variables elements within data frames lists.Another method accessing specific records variables [] operator, allows subset data frames, matrices, lists based specific conditions. example, extract first three rows students_df data frame, can use:command display first three rows students_df data frame.can also use [] operator extract specific columns. instance, select name grade columns students_df data frame:command retrieves displays name grade columns students_df data frame.[] operator versatile, enabling subset data frames, matrices, lists precision. $ [] operators fundamental tools data manipulation R, allowing efficiently access manage data need.","code":"students_df$name\n   [1] \"Emma\"  \"Bob\"   \"Alice\" \"Noah\"my_list$vector\n   [1]  1  2  0 -3  5students_df[1:3, ]\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     Astudents_df[, c(\"name\", \"grade\")]\n      name grade\n   1  Emma     A\n   2   Bob     B\n   3 Alice     A\n   4  Noah     C"},{"path":"chapter-into-R.html","id":"visualizing-data-in-r","chapter":"1 The Basics for R","heading":"1.13 Visualizing Data in R","text":"Data visualization powerful tool exploring communicating insights data. plays crucial role exploratory data analysis (EDA), delve Chapter 4. saying goes, “picture worth thousand words,” data science, especially true. R provides broad array tools creating high-quality plots visualizations, allowing effectively present findings.R, two primary ways create plots: using base R graphics using ggplot2 package. Base R graphics offer simple direct way generate plots, ggplot2 provides greater flexibility customization. book primarily uses ggplot2, follows structured approach based grammar graphics, breaks plots three key components:Data: dataset visualized, data frame format using ggplot2.Aesthetics: visual properties data points, color, shape, size.Geometries: type plot created, scatter plots, bar plots, line plots.create plot using ggplot2, first install load package. Instructions installing packages provided Section 1.6. load ggplot2, use following command:Next, define data, aesthetics, geometries plot. example, create scatter plot miles per gallon (mpg) versus horsepower (hp) using built-mtcars dataset:code initializes plot ggplot() function, specifying dataset (mtcars). geom_point() function adds points plot, aes() function maps mpg x-axis hp y-axis.general template creating plots ggplot2 follows structure:Using template, variety visualizations can created.","code":"\nlibrary(ggplot2)\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp))ggplot(data = <DATA>) +\n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))"},{"path":"chapter-into-R.html","id":"geom-functions-in-ggplot2","chapter":"1 The Basics for R","heading":"Geom Functions in ggplot2","text":"Geom functions determine type plot created ggplot2. commonly used geom functions include:geom_point() scatter plotsgeom_bar() bar plotsgeom_line() line plotsgeom_boxplot() box plotsgeom_histogram() histogramsgeom_density() density plotsgeom_smooth() adding smoothed conditional means plotsFor example, create smoothed line plot mpg versus hp:Multiple geom functions can combined single plot. overlay scatter plot smoothed line:Alternatively, aes() function can placed inside ggplot() streamline code:Additional visualization examples can found Chapter 4. complete list geom functions, refer ggplot2 documentation.","code":"\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp))\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp)) + \n  geom_point(mapping = aes(x = mpg, y = hp))\nggplot(data = mtcars, mapping = aes(x = mpg, y = hp)) +\n  geom_smooth() + \n  geom_point()"},{"path":"chapter-into-R.html","id":"aesthetics-in-ggplot2","chapter":"1 The Basics for R","heading":"Aesthetics in ggplot2","text":"Aesthetics control visual properties data points, color, size, shape. properties specified within aes() function. example:, color = cyl maps color points number cylinders (cyl) mtcars dataset. ggplot2 automatically assigns unique color category adds corresponding legend.addition color, aesthetics size alpha (transparency) can used:Aesthetics can also set directly inside geom functions. example, make points blue triangles size 3:section introduced fundamentals data visualization R using ggplot2. next chapters explore visualization plays crucial role exploratory data analysis (Chapter 4) refine plots communication reporting. details visualization techniques, see ggplot2 documentation. interactive graphics, consider exploring plotly package Shiny web applications.","code":"\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, color = cyl))\n# Left plot: using the size aesthetic\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, size = cyl))\n\n# Right plot: using the alpha aesthetic\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, alpha = cyl))\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp), \n             color = \"blue\", size = 3, shape = 2)"},{"path":"chapter-into-R.html","id":"sec-formula-in-R","chapter":"1 The Basics for R","heading":"1.14 Formula in R","text":"Formulas R provide concise intuitive way specify relationships variables statistical modeling. widely used functions regression, classification, machine learning define response variable depends one predictors.R, formulas use tilde symbol ~ express relationships variables, response variable appears left-hand side predictor variables right-hand side. example, formula y ~ x specifies y modeled function x. multiple predictors, separated +.\ninstance, using diamonds dataset, formula:models price diamond based carat, cut, color.include variables dataset predictors, can use shorthand notation:approach particularly useful large datasets listing predictors manually impractical.formula R acts quoting operator, instructing R interpret variables symbolically rather evaluating immediately. variable left-hand side ~ dependent variable (response variable), variables right-hand side independent variables (predictor variables).Example 1.1  illustrate, suppose want predict price diamond using linear regression model. can pass formula lm() function:, formula price ~ carat + cut + color defines relationship, data argument specifies dataset use.defined, formulas can used various R functions statistical modeling machine learning. progress later chapters, encounter formulas functions regression, classification, (e.g., Chapters 7, 9, 10). Mastering formula syntax enable efficiently build, customize, interpret models throughout book.","code":"\nprice ~ carat + cut + color\nprice ~ .\nmodel <- lm(price ~ carat + cut + color, data = diamonds)"},{"path":"chapter-into-R.html","id":"reporting-with-r-markdown","chapter":"1 The Basics for R","heading":"1.15 Reporting with R Markdown","text":"Thus far, book covered interact R RStudio data analysis. section focuses equally important aspect: effectively communicating analytical findings. Data scientists must present results clearly teams, stakeholders, clients. Regardless depth analysis, impact limited communicated effectively. R Markdown facilitates process enabling seamless integration code, text, output dynamic, reproducible reports.R Markdown allows users write execute R code within document, producing reports, presentations, dashboards. Unlike traditional notebooks word processors, R Markdown ensures text, code, results remain synchronized data changes. book entirely written using R Markdown generated bookdown package, ensuring fully reproducible dynamic workflow.R Markdown documents can exported multiple formats, including HTML, PDF, Word, PowerPoint, making adaptable various audiences reporting needs. Furthermore, supports creation interactive documents using Shiny, allowing users build web applications facilitate exploratory data analysis.get started, following resources provide useful references:R Markdown Cheat Sheet: R Markdown Cheat Sheet offers concise reference creating documents, including syntax, formatting, output options. available RStudio Help > Cheatsheets > R Markdown Cheat Sheet.R Markdown Reference Guide: R Markdown Reference Guide provides detailed overview R Markdown’s features, including document structure customization.","code":""},{"path":"chapter-into-R.html","id":"r-markdown-basics","chapter":"1 The Basics for R","heading":"R Markdown Basics","text":"R Markdown follows literate programming approach, combining text executable code single document. Unlike word processors formatting visible writing, R Markdown requires compilation generate final report. approach ensures automation, plots figures generated dynamically inserted document. Since code embedded, analyses fully reproducible.create R Markdown document RStudio:\nFile > New File > R Markdown\ndialog box appear, allowing selection document type. standard report, choose “Document.” options include “Presentation” slides, “Shiny” interactive applications, “Template” predefined formats. selecting document type, enter title author name. output format can set HTML, PDF, Word; HTML often recommended debugging.R Markdown files use .Rmd extension, distinguishing .R script files. newly created file contains template can modified custom text, code, formatting.","code":""},{"path":"chapter-into-R.html","id":"the-header","chapter":"1 The Basics for R","heading":"The Header","text":"header defines metadata document’s title, author, date, output format. enclosed within three dashes (---).Title: document’s title.Author: name author.Date: date creation.Output format: format final document (html_document, pdf_document, word_document).Additional metadata can included customization, table contents options formatting preferences.","code":"---\ntitle: \"An Analysis of Customer Churn\"\nauthor: \"Reza Mohammadi\"\ndate: \"Aug 12, 2024\"\noutput: html_document\n---"},{"path":"chapter-into-R.html","id":"code-chunks-and-inline-code","chapter":"1 The Basics for R","heading":"Code Chunks and Inline Code","text":"R Markdown integrates R code within documents using code chunks, enclosed triple backticks (```{r}) followed code. example:compiled, R executes code displays output within document. Code chunks used analysis, visualizations, modeling. “Run” button RStudio allows individual execution chunks. See Figure 1.6 visual guide.\nFigure 1.6: Executing code chunk R Markdown using ‘Run’ button RStudio.\nCommon chunk options include:echo = FALSE: Displays output hides code.eval = FALSE: Shows code execute .message = FALSE: Suppresses messages.warning = FALSE: Suppresses warnings.error = FALSE: Hides error messages.include = FALSE: Omits code output.inline calculations, use backticks r keyword:renders dynamically :","code":"\n```r\n2 + 3\n   [1] 5\n```The factorial of 5 is 120.The factorial of 5 is 120."},{"path":"chapter-into-R.html","id":"styling-text","chapter":"1 The Basics for R","heading":"Styling Text","text":"R Markdown supports various text formatting options:Headings: Use # section titles.Bold: Enclose text double asterisks (**bold**).Italic: Use single asterisks (*italic*).Lists: Use * bullet points.Links: [R Markdown website](https://rmarkdown.rstudio.com)Images: ![Alt text](path//image.png)mathematical notation, use LaTeX-style equations:","code":"Inline: $y = \\beta_0 + \\beta_1 x$  \nBlock: $$ y = \\beta_0 + \\beta_1 x $$"},{"path":"chapter-into-R.html","id":"mastering-r-markdown","chapter":"1 The Basics for R","heading":"Mastering R Markdown","text":"learning:Books: R Markdown: Definitive Guide.Tutorials: R Markdown website.Courses: DataCamp R Markdown course.Forums: RStudio Community.leveraging R Markdown, data scientists can produce high-quality, reproducible reports enhance collaboration communication.","code":""},{"path":"chapter-into-R.html","id":"exercises","chapter":"1 The Basics for R","heading":"1.16 Exercises","text":"section provides hands-exercises reinforce understanding fundamental concepts covered chapter.","code":""},{"path":"chapter-into-R.html","id":"basic-exercises","chapter":"1 The Basics for R","heading":"Basic Exercises","text":"Install R RStudio computer.Use getwd() function check current working directory. , change new directory using setwd().Create numeric vector named numbers containing values 5, 10, 15, 20, 25. , calculate mean standard deviation vector.Create matrix 3 rows 4 columns, filled numbers 1 12.Create data frame containing following variables:student_id (integer)name (character)score (numeric)passed (logical, TRUE means student passed FALSE means failed)\nPrint first rows data frame using head().Install load liver ggplot2 packages R. encounter errors, check internet connection ensure CRAN accessible.Load churn dataset liver package display first rows using head() function.Report data types variables churn dataset using str() function.Report dimensions churn dataset using dim() function.Report summary statistics variables churn dataset using summary() function.Create scatter plot using ggplot2 visualizes relationship day.mins eve.mins churn dataset. Hint: See code Section 4.7.Create histogram day.calls variable churn dataset.Create boxplot day.mins variable churn dataset.Create boxplot day.mins variable churn dataset, grouped churn variable. Hint: See code Section 4.6.Use mean() function compute mean customer.calls variable churn dataset. , calculate mean customer.calls churner churn == yes.Create R Markdown document includes title, author, small analysis churn dataset. Generate HTML report.","code":""},{"path":"chapter-into-R.html","id":"more-challenges-exercise","chapter":"1 The Basics for R","heading":"More Challenges Exercise","text":"following R code generates simulated dataset 200 observations. use simulated dataset simple toy example Chapter 7 explain k-nearest neighbors algorithm works. simulated data patients three variables:Age: Age patients numeric variable range 15 75 years old.Ratio: Sodium/Potassium ratio patient’s blood numeric variable. ratio generated based Type variable.Type: factor three levels: \"\", \"B\", \"C\" representing type drug patient taking.Run code report summary statistics data.Visualize data using following ggplot2 code:Extend dataset drug_data adding new variable named Outcome, factor two levels (\"Good\" \"Bad\").Patients Type == \"\" higher probability \"Good\" outcomes.Patients Type == \"B\" Type == \"C\" lower probability \"Good\" outcomes.Use sample() appropriate probabilities generate Outcome variable.Create new scatter plot using ggplot2 visualizes relationship Age Ratio, colored Outcome variable.Create new variable Age_group drug_data dataset categorizes patients three groups:“Young” (\\(\\leq 30\\) years old)“Middle-aged” (31-50 years old)“Senior” (>50 years old).Calculate mean Ratio Age_group category drug_data dataset.Create bar chart using ggplot2 displays average Ratio Age_group.Modify drug_data dataset adding Risk_factor variable, calculated Ratio * Age / 10. Analyze Risk_factor differs Type.Create histogram Risk_factor variable, grouped Type.Generate boxplot visualize distribution Risk_factor across different Outcome categories.","code":"\n# Simulate data for kNN\nset.seed(10)\n\nn  = 200         # Number of patients\nn1 = 90          # Number of patients with drug A\nn2 = 60          # Number of patients with drug B \nn3 = n - n1 - n2 # Number of patients with drug C\n\n# Generate Age variable between 15 and 75\nAge = sample(x = 15:75, size = n, replace = TRUE)\n\n# Generate Drug Type variable with three levels\nType = sample(x = c(\"A\", \"B\", \"C\"), size = n, replace = TRUE, prob = c(n1, n2, n3))\n\n# Generate Sodium/Potassium Ratio based on Drug Type\nRatio = numeric(n)\n\nRatio[Type == \"A\"] = sample(x = 10:40, size = sum(Type == \"A\"), replace = TRUE)\nRatio[Type == \"B\"] = sample(x =  5:15, size = sum(Type == \"B\"), replace = TRUE)\nRatio[Type == \"C\"] = sample(x =  5:15, size = sum(Type == \"C\"), replace = TRUE)\n\n# Create a data frame with the generated variables\ndrug_data = data.frame(Age = Age, Ratio = Ratio, Type = Type)\nggplot(data = drug_data, aes(x = Age, y = Ratio)) +\n  geom_point(aes(color = Type, shape = Type)) + \n  labs(title = \"Age vs. Sodium/Potassium Ratio\", \n       x = \"Age\", y = \"Sodium/Potassium Ratio\")"},{"path":"chapter-intro-DS.html","id":"chapter-intro-DS","chapter":"2 Introduction to Data Science","heading":"2 Introduction to Data Science","text":"Data Science rapidly evolving field transforming industries leveraging computational, statistical, analytical techniques. 21st century, data become one valuable resources, often called “new oil” due potential drive innovation reshape future.Data science key unlocking potential. applying computational, statistical, analytical techniques, data scientists extract insights vast amounts data, enabling organizations make informed decisions, optimize processes, predict trends, develop intelligent systems. led groundbreaking advancements fields healthcare, finance, marketing, artificial intelligence (AI), beyond.Given rapid growth increasing demand, data science critical ever. chapter, ’ll explore fundamentals data science, discuss significance modern society, introduce Data Science Workflow—structured approach data scientists use transform raw data actionable insights.section well-structured provides clear introduction data science. effectively conveys interdisciplinary nature field highlights core components. However, areas clarity, consistency, flow can improved. suggestions:","code":""},{"path":"chapter-intro-DS.html","id":"what-is-data-science","chapter":"2 Introduction to Data Science","heading":"2.1 What is Data Science?","text":"Data science interdisciplinary field integrates computer science, statistics, domain expertise extract insights data. involves using analytical computational techniques process vast amounts raw data, transforming meaningful information supports decision-making strategic planning.\nFigure 2.1: Data science multidisciplinary field applies computational statistical methods extract insights data.\nAlthough term “data science” relatively new, foundations lie well-established disciplines statistics, data analysis, machine learning. exponential growth digital data, advancements computational power, increasing demand data-driven decision-making, data science emerged distinct essential field.core, data science concerned extracting knowledge data using combination statistical techniques, machine learning algorithms, domain-specific methodologies. helps organizations manage understand vast amounts information generated digital age.","code":""},{"path":"chapter-intro-DS.html","id":"key-components-of-data-science","chapter":"2 Introduction to Data Science","heading":"Key Components of Data Science","text":"field data science encompasses three main components:Data Engineering: foundation data science, responsible collecting, storing, structuring large datasets. includes development data pipelines infrastructure enable efficient analysis. crucial, data engineering beyond scope book.Data Analysis Statistics: application statistical methods explore analyze data. includes data visualization, hypothesis testing, predictive modeling. details topic covered Statistical Inference Hypothesis Testing Exploratory Data Analysis chapters.Machine Learning Artificial Intelligence: use algorithms identify patterns, make predictions, extract deeper insights. includes supervised unsupervised learning, deep learning, natural language processing. concepts discussed Modeling Process chapter.","code":""},{"path":"chapter-intro-DS.html","id":"why-data-science-matters","chapter":"2 Introduction to Data Science","heading":"2.2 Why Data Science Matters","text":"digital age, data become one valuable resources often referred “new oil” 21st century. comparison makes sense, world’s valuable companies today—including OpenAI, Google, Apple—driven artificial intelligence data science. Just wealthiest companies 20th century controlled oil energy, today’s leading enterprises leverage data key asset innovation competitive advantage.Across industries, data-driven decision-making become essential. Organizations generate vast amounts data every day, without right tools techniques, much data remain untapped. Data science helps organizations uncover patterns, detect trends, make informed decisions enhance efficiency, reduce costs, improve customer experiences.Data science plays crucial role wide range sectors, including:Finance: Financial institutions leverage data science risk assessment, fraud detection, algorithmic trading. Machine learning models identify anomalies transaction patterns, improving fraud detection regulatory compliance.Marketing: Businesses use data science analyze customer behavior, segment audiences, create targeted marketing campaigns. Platforms Facebook Google Ads leverage sophisticated algorithms match advertisements relevant audiences, improving engagement conversion rates.Retail E-commerce: Companies like Amazon Walmart use data science optimize inventory management, predict demand, personalize recommendations. analyzing purchase history browsing behavior, retailers can offer tailored promotions enhance customer satisfaction.Healthcare: Hospitals medical researchers use data science disease diagnosis, patient risk prediction, personalized treatment plans. analyzing large datasets medical records, institutions can identify high-risk patients take preventative measures improve health outcomes.example, Netflix applies data science analyze viewing patterns recommend personalized content users, supply chain optimization Amazon ensures faster deliveries leveraging predictive analytics.","code":""},{"path":"chapter-intro-DS.html","id":"the-data-science-workflow","chapter":"2 Introduction to Data Science","heading":"2.3 The Data Science Workflow","text":"data science workflow follows iterative cyclical approach, insights gained stage inform refine subsequent steps. Unlike strictly linear process, data science involves continuous refinement enhance accuracy efficiency. structured approach ensures data-driven projects conducted systematically, balancing exploratory analysis, model building, evaluation derive meaningful conclusions.data science workflow follows phased, adaptive approach within scientific framework, transforming raw data actionable knowledge. transformation often conceptualized using DIKW Pyramid (Data → Information → Knowledge → Wisdom), illustrated Figure 2.2.\nFigure 2.2: DIKW Pyramid illustrates transformation raw data actionable insights, progressing data information, knowledge, ultimately wisdom.\nspecifics may vary across projects, data science workflows follow common structure. book, adopt Data Science Workflow guiding framework structuring data science projects. workflow inspired Cross-Industry Standard Process Data Mining (CRISP-DM) model, widely recognized methodology data-driven projects. cyclic framework guides data scientists following key stages (see Figure 2.3):Problem Understanding – Defining business research question outlining objectives.Data Preparation – Collecting, cleaning, transforming, organizing data ensure suitable analysis. step includes handling missing values, addressing inconsistencies, detecting outliers, preparing features scaling, encoding, transformation.Exploratory Data Analysis (EDA) – Identifying patterns, distributions, relationships within data.Preparing Data Modeling – Engineering relevant features, normalizing data, selecting meaningful variables.Modeling – Applying machine learning statistical techniques develop predictive descriptive models.Evaluation – Assessing model performance using appropriate metrics validation techniques.Deployment – Integrating model production environment monitoring performance time.\nFigure 2.3: Data Science Workflow iterative framework structuring data science machine learning projects. Inspired CRISP-DM model, ensures systematic problem-solving continuous refinement.\ndata science inherently iterative, steps often revisited multiple times within single project. feedback loops stages allow continuous refinement—adjusting data preprocessing, modifying features, retraining models new insights emerge. following structured workflow, data scientists can ensure rigor, accuracy, efficiency transforming data valuable insights.","code":""},{"path":"chapter-intro-DS.html","id":"problem-understanding","chapter":"2 Introduction to Data Science","heading":"2.4 Problem Understanding","text":"first step data science project clearly define problem—whether business challenge research question. phase crucial data science just building models; solving real-world problems using data-driven approaches. well-defined problem ensures efforts aligned meaningful objectives, improving likelihood delivering actionable insights.stage, data scientists work closely stakeholders understand goals, clarify expectations, define success criteria. following questions help frame problem:research business question important?desired outcome impact?can data science techniques contribute addressing question?Focusing diving essential. Simon Sinek emphasizes TED talk “Great Leaders Inspire Action”, “People don’t buy ; buy .” concept applies data science well—understanding deeper motivation behind project provides clarity direction.example, data science team business analytics department may approached client wants predictive model lacks clarity specific problem trying solve. Without clear , becomes difficult develop solution delivers real value. Similarly, students working research projects often focus want build rather needed.Suppose company aims reduce customer churn. well-defined objective might develop predictive model identifies customers risk leaving targeted retention strategies can implemented. initial understanding helps frame problem guides selection relevant data, modeling techniques, evaluation metrics.Problem understanding analytical creative process. data science provides tools methodologies, defining right problem requires domain expertise critical thinking. following steps help ensure structured approach:Clearly articulate project objectives requirements terms overall goals business research entity.Break objectives outline specific expectations desired outcomes.Translate objectives data science problem can addressed using analytical techniques.Draft preliminary strategy achieve objectives, considering potential approaches methodologies.thoroughly defining problem, data scientists set stage effective workflow, ensuring subsequent analysis modeling efforts remain aligned meaningful outcomes.","code":""},{"path":"chapter-intro-DS.html","id":"data-preparation","chapter":"2 Introduction to Data Science","heading":"2.5 Data Preparation","text":"problem well-defined, next step data preparation, ensuring data accurate, complete, well-structured. Raw data often contains missing values, inconsistencies, outliers, making phase critical reliable analysis. Poorly prepared data can lead misleading insights, even sophisticated models.Data can originate various sources, including databases, spreadsheets, APIs, web scraping. may structured (e.g., numerical data databases) unstructured (e.g., text, images). Preprocessing essential analysis.Key steps data preparation include:Data Collection Integration: Merging data multiple sources ensuring consistency.Handling Missing Values: Removing, imputing, flagging incomplete data.Outlier Detection: Identifying managing extreme values using visualization.Resolving Inconsistencies: Standardizing formats, correcting errors, aligning categorical values.Feature Engineering: Transforming data encoding, scaling, normalization model compatibility.Data Summarization: Checking variable types, computing summary statistics, detecting duplicates.Though time-consuming, data preparation essential accurate modeling meaningful analysis. Chapter 3, explore techniques real-world examples.","code":""},{"path":"chapter-intro-DS.html","id":"exploratory-data-analysis-eda","chapter":"2 Introduction to Data Science","heading":"2.6 Exploratory Data Analysis (EDA)","text":"Exploratory Data Analysis (EDA) fundamental step data science workflow, providing initial understanding dataset formal modeling. primary objective EDA uncover patterns, relationships, anomalies data, helping data scientists refine hypotheses validate assumptions. systematically examining data, EDA ensures subsequent modeling process informed solid understanding dataset’s structure characteristics.Several key techniques commonly used EDA:Summary statistics – Measures mean, median, standard deviation, interquartile range provide insights distribution central tendencies numerical variables.Data visualization – Graphical techniques, including histograms, scatter plots, box plots, reveal data distributions, trends, potential outliers.Correlation analysis – Examining relationships numerical features using correlation coefficients helps identify dependencies may influence modeling decisions.EDA serves diagnostic exploratory functions. helps detect data quality issues, missing values inconsistencies, also guiding feature selection engineering. instance, strong correlation exists certain features target variable, features may prioritized modeling phase.thorough EDA process improves quality dataset also enhances interpretability reliability analytical results. Chapter 4, explore EDA techniques greater detail, applying real-world datasets illustrate practical applications.","code":""},{"path":"chapter-intro-DS.html","id":"preparing-data-for-modeling","chapter":"2 Introduction to Data Science","heading":"2.7 Preparing Data for Modeling","text":"insights EDA, next step prepare data modeling. stage involves feature engineering, feature selection, data splitting—crucial building effective models.Feature Engineering: Creating new features transforming existing ones enhance model performance. example, deriving new variables combining existing ones applying transformations can provide additional predictive power.Feature Selection: Identifying selecting relevant features improve model efficiency prevent overfitting. Removing irrelevant redundant features simplifies model enhances interpretability.Data Splitting: Dividing dataset training, validation, testing sets. training set used develop model, validation set helps fine-tune parameters, test set assesses final model performance.end stage, data structured well-prepared format, ensuring models can learn effectively. Chapter 6, explore techniques detail apply real-world datasets.","code":""},{"path":"chapter-intro-DS.html","id":"modeling","chapter":"2 Introduction to Data Science","heading":"2.8 Modeling","text":"Modeling stage data scientists apply machine learning statistical techniques prepared data create predictive descriptive model. goal build model effectively captures relationships within data generalizes well new, unseen data.modeling process typically involves:Choosing Model: Selecting appropriate model based problem type (e.g., regression, classification, clustering) characteristics dataset.Training Model: Fitting model training data learn patterns relationships.Tuning Hyperparameters: Adjusting model parameters optimize performance validation set.Common algorithms include linear regression (Chapter 10), decision trees (Chapter 11), Naïve Bayes classifier (Chapter 9), k-Nearest Neighbors (k-NN) algorithm (Chapter 7), neural networks (Chapter 12). method strengths limitations, selecting suitable model depends nature problem, data quality, computational constraints. Often, multiple models tested compared determine best-performing approach.","code":""},{"path":"chapter-intro-DS.html","id":"evaluation","chapter":"2 Introduction to Data Science","heading":"2.9 Evaluation","text":"model built, must rigorously evaluated ensure accuracy, generalizability, robustness deployment. evaluation process relies well-defined performance metrics, vary depending type problem. classification models, commonly used metrics include accuracy, precision, recall, F1-score, area receiver operating characteristic curve (ROC-AUC). regression tasks, measures mean squared error (MSE), mean absolute error (MAE), coefficient determination (\\(R^2\\)) assess model effectiveness.ensure model overfitting training data, cross-validation techniques, k-fold cross-validation, employed. methods provide reliable estimate model’s performance partitioning data multiple subsets training validation. Beyond numerical evaluation, error analysis plays crucial role diagnosing weaknesses, particularly confusion matrix interpretation classification problems residual analysis regression. careful examination errors often reveals underlying biases, data inconsistencies, model limitations require refinement.model fails meet expectations, adjustments may necessary, feature selection, hyperparameter tuning, exploring alternative modeling approaches. Chapter 8, explore techniques detail apply real-world datasets.","code":""},{"path":"chapter-intro-DS.html","id":"deployment","chapter":"2 Introduction to Data Science","heading":"2.10 Deployment","text":"model evaluated meets project goals, final step deployment, integrated production environment generate real-time insights predictions. phase crucial ensuring model contributes tangible value, whether supporting decision-making processes automating tasks within operational systems. Models can deployed various ways, embedding web applications, integrating enterprise software, automating processes large-scale data pipelines.Beyond initial integration, continuous monitoring essential track model’s performance detect potential issues. real-world data evolves, models may experience concept drift, predictive accuracy deteriorates due changes underlying patterns. mitigate , periodic model updates retraining necessary maintain reliability. Additionally, implementing robust logging performance tracking mechanisms helps ensure discrepancies predicted actual outcomes quickly identified addressed.Deployment one-time event ongoing process. Effective deployment strategies account scalability, interpretability, maintainability, allowing models remain useful dynamic environments. field data science advances, ability manage deployed models effectively continue critical factor transforming analytical insights real-world impact.","code":""},{"path":"chapter-intro-DS.html","id":"machine-learning","chapter":"2 Introduction to Data Science","heading":"2.11 Machine Learning","text":"Data science relies machine learning techniques extract insights data, make predictions, uncover patterns. methods enable data scientists move beyond descriptive analysis explore predictive prescriptive approaches, essential real-world applications. section, provide overview machine learning, including main types—supervised learning unsupervised learning—discuss machine learning differs statistical learning.Machine learning branch artificial intelligence focuses developing algorithms learn data make predictions. Rather explicitly programmed task, machine learning models identify patterns within data use make informed decisions. approach particularly useful complex problems rule-based programming impractical.instance, rather defining fixed set rules detect spam emails, machine learning model can trained labeled dataset emails classified “spam” “spam.” model learns distinguishing patterns can classify new emails high accuracy. ability generalize data makes machine learning invaluable fields finance, healthcare, marketing.","code":""},{"path":"chapter-intro-DS.html","id":"machine-learning-tasks-supervised-vs.-unsupervised-learning","chapter":"2 Introduction to Data Science","heading":"Machine Learning Tasks: Supervised vs. Unsupervised Learning","text":"Machine learning tasks can broadly categorized supervised learning unsupervised learning, differ terms models learn data objectives analysis.Supervised learning involves training model labeled dataset, data point associated known output. goal model learn relationship input features corresponding output, enabling make accurate predictions new data. Common supervised learning tasks include classification numeric prediction. classification, model assigns data points predefined categories, detecting whether email spam identifying whether patient particular disease. book covers classification techniques decision trees (Chapter 11), Naïve Bayes classifier (Chapter 9), k-Nearest Neighbors (k-NN) algorithm (Chapter 7). Numeric prediction, also known regression, focuses estimating continuous values, forecasting house prices based location size. detailed discussion regression techniques provided Chapter 10.Unsupervised learning, hand, applied datasets lack labeled outputs. objective uncover hidden patterns, relationships, structures within data. Clustering, common unsupervised learning technique, groups data points based similarity, segmenting customers according purchasing behavior. Another important unsupervised learning method pattern discovery, also known association rule learning, identifies relationships variables. technique widely used market basket analysis detect frequently co-purchased items. concepts explored detail Chapter 13.summary, supervised learning used labeled data available specific predictive outcome required, unsupervised learning beneficial exploratory data analysis, goal identify underlying structures unlabeled data. distinction two approaches fundamental selecting appropriate machine learning techniques given data science problem.","code":""},{"path":"chapter-intro-DS.html","id":"exercises-1","chapter":"2 Introduction to Data Science","heading":"2.12 Exercises","text":"following exercises help reinforce key concepts covered chapter. questions range fundamental definitions applied problem-solving related data science, data science workflow, machine learning.data-driven decision-making impact businesses? Give example real-world application.Data Science Workflow inspired CRISP-DM model. CRISP-DM stand , guide data-driven projects? key stages CRISP-DM model?Data Science Workflow CRISP-DM model standard processes data science projects. methodologies used industry?think can skip Problem Understanding phase directly jump Data Preparation data science project? Justify answer.Data Preparation considered one time-consuming steps data science project? common challenges faced phase?extent can Data Science projects automated without human intervention? risks limitations relying solely automated tools?following scenarios, identify appropriate stage data science workflow:\ncompany wants predict customer churn based historical data.\nresearcher exploring relationship air pollution respiratory diseases.\ne-commerce platform analyzing user behavior personalize product recommendations.\nhospital developing predictive model patient readmission rates.\ncompany wants predict customer churn based historical data.researcher exploring relationship air pollution respiratory diseases.e-commerce platform analyzing user behavior personalize product recommendations.hospital developing predictive model patient readmission rates.task, classify supervised unsupervised learning, explain reasoning, identify suitable machine learning algorithm applied.\nIdentifying fraudulent transactions credit card dataset.\nSegmenting customers based purchasing behavior.\nPredicting stock prices based historical data.\nGrouping news articles topics using natural language processing.\nIdentifying fraudulent transactions credit card dataset.Segmenting customers based purchasing behavior.Predicting stock prices based historical data.Grouping news articles topics using natural language processing.Define training dataset test dataset. important? improper splitting datasets affect model performance? Provide example real-world issue caused poor dataset partitioning.Many AI-driven systems criticized biased predictions, hiring algorithms favor certain demographics facial recognition models misidentify certain racial groups.\ncommon sources bias data science projects?\ncan data scientists ensure fairness mitigate biases models?\nGive example real-world case bias AI led negative consequences.\ncommon sources bias data science projects?can data scientists ensure fairness mitigate biases models?Give example real-world case bias AI led negative consequences.Accuracy common metric used evaluate models, always best indicator success. Consider binary classification problem 2% cases positive (e.g., detecting rare diseases fraud).\nmight accuracy misleading case?\nalternative evaluation metrics used?\ndecide whether model truly valuable decision-making?\nmight accuracy misleading case?alternative evaluation metrics used?decide whether model truly valuable decision-making?","code":""},{"path":"chapter-data-prep.html","id":"chapter-data-prep","chapter":"3 Data Preparation","heading":"3 Data Preparation","text":"Data preparation foundational step data science project, ensuring raw data transformed clean structured format suitable analysis. process often time-consuming yet crucial stage, quality data directly influences accuracy insights effectiveness predictive models.chapter explores key data preparation techniques, including handling missing values, detecting outliers, transforming data, feature engineering. end chapter, clear understanding preprocess raw data, enabling robust statistical modeling machine learning applications.illustrate concepts, use diamonds dataset ggplot2 package. dataset contains detailed attributes diamonds, carat, cut, color, clarity, price, making excellent case study data preprocessing. chapter, focus first two steps Data Science Workflow—data cleaning transformation—laying groundwork analysis subsequent chapters.","code":""},{"path":"chapter-data-prep.html","id":"problem-understanding","chapter":"3 Data Preparation","heading":"3.1 Problem Understanding","text":"preparing data analysis, essential define problem establish clear objectives. case, aim analyze diamonds dataset gain insights diamond pricing, critical factor industries jewelry retail, gemology, e-commerce. dataset includes attributes influence diamond value, allowing us explore key factors affecting pricing.","code":""},{"path":"chapter-data-prep.html","id":"objectives-and-key-questions","chapter":"3 Data Preparation","heading":"Objectives and Key Questions","text":"primary objectives diamonds dataset :Examine relationships diamond attributes (e.g., carat, cut, color, clarity) price.Identify patterns improve price estimation.Assess data quality, ensuring consistency detecting missing values outliers may affect analysis.achieve objectives, address key questions :attributes significant influence price?pricing trends based characteristics carat weight cut quality?inconsistencies, errors, missing values need corrected?","code":""},{"path":"chapter-data-prep.html","id":"framing-the-problem-as-a-data-science-task","chapter":"3 Data Preparation","heading":"Framing the Problem as a Data Science Task","text":"business perspective, understanding diamond pricing can provide valuable insights jewelers, e-commerce platforms, gemologists. data science perspective, problem can approached two ways:Predictive modeling: Developing model estimates diamond price based attributes.Exploratory data analysis (EDA): Identifying trends relationships without building predictive model.Clearly defining objectives ensures data preparation efforts align intended analytical approach, whether exploratory insights building robust predictive models generalize well unseen data. structured problem framing guide decisions data cleaning, transformation, feature engineering, ensuring analysis remains focused actionable.","code":""},{"path":"chapter-data-prep.html","id":"diamonds-dataset-overview","chapter":"3 Data Preparation","heading":"3.2 diamonds Dataset Overview","text":"diamonds dataset, included ggplot2 package, provides structured information various characteristics diamonds. row represents unique diamond, 54,940 entries total, contains 10 descriptive variables, including price, carat, cut, clarity, color. goal analysis gain deeper insights factors influence diamond pricing, understand distribution data across attributes, explore quantitative qualitative relationships variables.use diamonds dataset R, first ensure ggplot2 package installed. , install using:, load package dataset:inspect dataset structure, use:function reveals dataset 53940 observations 10 variables. summary key attributes:price: price US dollars ($326–$18,823).carat: weight diamond (0.2–5.01).cut: quality cut (Fair, Good, Good, Premium, Ideal).color: diamond color, D (best) J (worst).clarity: measurement clear diamond (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, (best)).x: length mm (0–10.74).y: width mm (0–58.9).z: depth mm (0–31.8).depth: total depth percentage = 2 * z / (x + y).table: width top diamond relative widest point.","code":"\ninstall.packages(\"ggplot2\") \nlibrary(ggplot2)  # Load ggplot2 package\ndata(diamonds)    # Load diamonds datasetstr(diamonds)   \n   tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n    $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n    $ cut    : Ord.factor w/ 5 levels \"Fair\"<\"Good\"<..: 5 4 2 4 2 3 3 3 1 3 ...\n    $ color  : Ord.factor w/ 7 levels \"D\"<\"E\"<\"F\"<\"G\"<..: 2 2 2 6 7 7 6 5 2 5 ...\n    $ clarity: Ord.factor w/ 8 levels \"I1\"<\"SI2\"<\"SI1\"<..: 2 3 5 4 2 6 7 3 4 5 ...\n    $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n    $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n    $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n    $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n    $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n    $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ..."},{"path":"chapter-data-prep.html","id":"types-of-features-in-the-diamonds-dataset","chapter":"3 Data Preparation","heading":"Types of Features in the diamonds Dataset","text":"Understanding types features dataset essential determining appropriate data preparation steps:Quantitative (Numerical) Variables: represented numbers can continuous discrete.\nContinuous Variables: variables can take value within range. dataset, carat, price, x, y, z, depth continuous.\nDiscrete Variables: variables take countable values, often integers. example, count customers number purchases discrete, though dataset doesn’t include variable.\nContinuous Variables: variables can take value within range. dataset, carat, price, x, y, z, depth continuous.Discrete Variables: variables take countable values, often integers. example, count customers number purchases discrete, though dataset doesn’t include variable.Categorical (Qualitative) Variables: describe data fits categories rather numerical value. divided three types:\nOrdinal Variables: Categorical variables meaningful order, intervals categories equal. instance, cut, color, clarity ordinal variables dataset. ordering levels variables (e.g., “Fair” “Ideal” cut) meaning.\nNominal Variables: Categorical variables without intrinsic ordering among categories. datasets, examples might include “gender” “product type,” diamonds dataset contain nominal variables.\nBinary Variables: Variables two levels, often coded 0 1. diamonds dataset doesn’t contain binary variables, example feature like “has_certificate” values “yes” “.”\nOrdinal Variables: Categorical variables meaningful order, intervals categories equal. instance, cut, color, clarity ordinal variables dataset. ordering levels variables (e.g., “Fair” “Ideal” cut) meaning.Nominal Variables: Categorical variables without intrinsic ordering among categories. datasets, examples might include “gender” “product type,” diamonds dataset contain nominal variables.Binary Variables: Variables two levels, often coded 0 1. diamonds dataset doesn’t contain binary variables, example feature like “has_certificate” values “yes” “.”Knowing type feature guides decisions data preparation. instance:\n- Numerical variables can normalized standardized using techniques like Min-Max Scaling Z-score Scaling.\n- Ordinal variables may encoded using ordinal encoding one-hot encoding, depending whether model recognize order.\n- Categorical variables without meaningful order typically one-hot encoded.understanding types variables diamonds dataset, can select appropriate transformations encoding methods prepare data effectively analysis modeling.","code":""},{"path":"chapter-data-prep.html","id":"key-considerations-for-data-preparation","chapter":"3 Data Preparation","heading":"Key Considerations for Data Preparation","text":"objectives mind, main priorities preparing dataset:Data Quality: Ensure data accurate, consistent, free major issues. involves checking missing values, outliers, inconsistencies bias analysis.Feature Engineering: Explore possibility creating new features improve predictive accuracy. instance, calculating volume (using product x, y, z dimensions) provide additional measure diamond’s size.Data Transformation: Ensure features appropriate formats. Categorical variables like cut color may need converted numeric codes dummy variables work machine learning algorithms effectively.","code":""},{"path":"chapter-data-prep.html","id":"Data-pre-outliers","chapter":"3 Data Preparation","heading":"3.3 Outliers","text":"Outliers data points significantly deviate general distribution dataset. can arise due measurement variability, data entry errors, genuinely unique observations. Identifying handling outliers crucial, can skew statistical analyses, affect model performance, lead misleading insights.Outliers play critical role multiple industries:Finance: Outliers transaction data can indicate fraud. Detecting unusually high spending patterns key fraud detection models.Healthcare: Medical records often contain anomalous lab results, may indicate rare diseases measurement errors.Manufacturing: Sensors factories may detect equipment failures unusual temperature spikes.many cases, outliers errors signals important events. Understanding role data analysis ensures don’t remove valuable insights unintentionally.","code":""},{"path":"chapter-data-prep.html","id":"identifying-outliers-using-visualization-techniques","chapter":"3 Data Preparation","heading":"Identifying Outliers Using Visualization Techniques","text":"","code":""},{"path":"chapter-data-prep.html","id":"boxplots-detecting-extreme-values","chapter":"3 Data Preparation","heading":"Boxplots: Detecting Extreme Values","text":"Boxplots visual tool detecting extreme values. boxplot y variable (diamond width) using ggplot() geom_boxplot() functions ggplot2 package:, boxplots highlight values beyond whiskers, may indicate potential outliers. Since diamonds width 0 mm, values like 32 mm 59 mm likely result data entry errors.","code":"\nggplot(data = diamonds) +\n    geom_boxplot(mapping = aes(y = y))"},{"path":"chapter-data-prep.html","id":"histograms-understanding-outlier-distribution","chapter":"3 Data Preparation","heading":"Histograms: Understanding Outlier Distribution","text":"Histograms provide another visual approach detecting outliers displaying frequency distribution values. histogram y variable using ggplot() geom_histogram() functions:enhance visibility, can zoom smaller frequencies using coord_cartesian() function ggplot2 package:useful visualization techniques include:Violin plots – Show outliers density distributions.Density plots – Provide smoother insights rare values multimodal distributions.","code":"\nggplot(data = diamonds) +\n    geom_histogram(aes(x = y), binwidth = 0.5, color = 'blue', fill = \"lightblue\")\nggplot(data = diamonds) +\n    geom_histogram(mapping = aes(x = y), binwidth = 0.5, color = 'blue', fill = \"lightblue\") + \n    coord_cartesian(ylim = c(0, 30))"},{"path":"chapter-data-prep.html","id":"handling-outliers-best-practices","chapter":"3 Data Preparation","heading":"Handling Outliers: Best Practices","text":"outliers identified, several strategies handling :Removing outliers: appropriate outlier clearly error (e.g., negative height, duplicate data entry).Transforming values: Techniques log transformation square root scaling can reduce influence extreme values preserving trends.Winsorization: Instead removing outliers, replace nearest percentile-based value (e.g., capping extreme values 95th percentile).Using robust statistical methods: algorithms, like median-based regression random forests, less sensitive outliers.Treating outliers separate category: fraud detection rare event prediction, outliers may contain valuable insights removed.Choosing right strategy depends context analysis potential impact outlier.","code":""},{"path":"chapter-data-prep.html","id":"expanded-code-example-handling-outliers-in-r","chapter":"3 Data Preparation","heading":"Expanded Code Example: Handling Outliers in R","text":"detecting outliers, can choose either replace NA values remove . , consider using mutate() function dplyr package. ’s example treating outliers missing values using mutate() ifelse():’s verify update:method ensures outliers distort dataset allowing imputation analysis.","code":"\ndiamonds_2 <- mutate(diamonds, y = ifelse(y == 0 | y > 30, NA, y))summary(diamonds_2$y)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     3.680   4.720   5.710   5.734   6.540  10.540       9"},{"path":"chapter-data-prep.html","id":"missing-values","chapter":"3 Data Preparation","heading":"3.4 Missing Values","text":"Missing values pose significant challenges data analysis, can lead biased results, reduce statistical power, impact performance machine learning models. handling missing data, typically consider two approaches:Imputation: Replacing missing values estimated values retain data integrity.Removal: Deleting records missing values, though may lead data loss potential bias.","code":""},{"path":"chapter-data-prep.html","id":"imputation-techniques","chapter":"3 Data Preparation","heading":"Imputation Techniques","text":"several strategies imputing missing values, different use cases:Mean, median, mode imputation: Replaces missing values mean, median, mode corresponding column.Random sampling: Fills missing values random observations drawn existing data distribution.Predictive imputation: Uses machine learning models regression k-nearest neighbors estimate missing values.Multiple imputation: Generates several possible values missing entries averages results reduce uncertainty.\n### Example: Random Sampling Imputation R {-}impute missing values y using random sampling, use impute() function Hmisc package:impute() function replaces missing values randomly sampled values existing distribution y, maintaining overall statistical properties dataset.","code":"\ndiamonds_2$y <- impute(diamonds_2$y, \"random\")"},{"path":"chapter-data-prep.html","id":"best-practices","chapter":"3 Data Preparation","heading":"Best Practices","text":"Use mean median imputation numerical variables missing values missing random (MAR).Use mode imputation categorical variables.Consider predictive models dataset large missing values completely random.Always assess proportion missing data—many values missing, removing variable may better approach imputation.","code":""},{"path":"chapter-data-prep.html","id":"feature-scaling","chapter":"3 Data Preparation","heading":"3.5 Feature Scaling","text":"Feature scaling, also known normalization standardization, crucial step data preprocessing. adjusts range distribution numerical features similar scale. Many machine learning algorithms, especially based distance metrics k-nearest neighbors, benefit significantly scaled input features, prevents variables larger ranges disproportionately influencing model’s outcome.instance, diamonds dataset, carat variable ranges 0.2 5, price ranges 326 18823. Without scaling, variables like price wider range can dominate model’s predictions, potentially leading suboptimal results. address , apply feature scaling techniques bring numeric variables onto comparable scale. section, explore two common scaling methods:Min-Max Scaling: Also known min-max normalization min-max transformation.Z-score Scaling: Also known standardization Z-score normalization.Feature scaling provides several benefits:Improved Model Performance: Ensures features contribute equally model, preventing features larger numerical ranges dominating learning algorithms.Better Model Convergence: Particularly useful gradient-based optimization methods logistic regression neural networks.Effective Distance-Based Learning: Algorithms k-means clustering support vector machines rely distance calculations, making feature scaling essential.Consistent Feature Interpretation: standardizing numerical values, models become easier compare interpret.However, feature scaling also drawbacks:Potential Loss Information: cases, scaling can obscure meaningful differences data points.Impact Outliers: Min-max scaling, particular, sensitive extreme values, can distort scaled representation.Additional Computation: Scaling adds preprocessing overhead, particularly working large datasets.Reduced Interpretability: original units measurement lost, making harder relate scaled values real-world meanings.Selecting right scaling method depends characteristics data requirements model. next sections, explore methods detail apply diamonds dataset.","code":""},{"path":"chapter-data-prep.html","id":"min-max-scaling","chapter":"3 Data Preparation","heading":"3.6 Min-Max Scaling","text":"Min-Max Scaling transforms values feature fixed range, typically \\([0, 1]\\). transformation ensures minimum value feature becomes 0 maximum value becomes 1. especially useful algorithms rely distance metrics, equalizes contributions features, making comparisons balanced.formula Min-Max Scaling :\\[\nx_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}},\n\\]\n\\(x\\) original feature value, \\(x_{\\text{min}}\\) \\(x_{\\text{max}}\\) minimum maximum values feature, \\(x_{\\text{scaled}}\\) scaled value, ranging 0 1.Min-Max Scaling particularly useful models require bounded input values, neural networks algorithms relying gradient-based optimization. However, method sensitive outliers, extreme values significantly affect scaled distribution.Example 3.1  demonstrate Min-Max Scaling, ’ll apply carat variable diamonds dataset, carat values range approximately 0.2 5. Using minmax() function liver package, can scale carat values fit within range [0, 1].first histogram (left) shows distribution carat without scaling, second histogram (right) shows Min-Max Scaling. scaling, carat values compressed range 0 1, allowing comparable features may different original scales. scaling method particularly beneficial distance-based algorithms, prevents features wider ranges undue influence.","code":"\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` without scaling\") + \n  xlab(\"Values for variable `carat`\")\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = minmax(carat)), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` with Min-Max Scaling\") + \n  xlab(\"Values for variable `carat`\")"},{"path":"chapter-data-prep.html","id":"z-score-scaling","chapter":"3 Data Preparation","heading":"3.7 Z-score Scaling","text":"Z-score Scaling, also known standardization, transforms feature values mean 0 standard deviation 1. method particularly useful algorithms assume normally distributed data, linear regression logistic regression, centers data around 0 normalizes spread values.formula Z-score Scaling :\\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)}\n\\]\\(x\\) original feature value, \\(\\text{mean}(x)\\) mean feature, \\(\\text{sd}(x)\\) standard deviation feature, \\(x_{\\text{scaled}}\\) standardized value, now mean 0 standard deviation 1.Z-score Scaling particularly beneficial models assume normality use gradient-based optimization, ensuring numerical features contribute equally. However, since relies mean standard deviation, sensitive outliers, can distort transformation.Example 3.2  Applying Z-score Scaling carat variable diamonds dataset, mean standard deviation carat approximately 0.8 0.47, respectively. use zscore() function liver package standardize values.first histogram (left) displays distribution carat without scaling, second histogram (right) shows distribution Z-score Scaling. transformation makes feature values comparable across different scales ensures feature contributes equally distance-based computations model training.Note: common misconception Z-score Scaling, data follows standard normal distribution. Z-score Scaling centers data mean 0 scales standard deviation 1, alter shape distribution. original distribution skewed, remain skewed scaling, seen histograms .choice Min-Max Scaling Z-score Scaling depends requirements model characteristics data. Min-Max Scaling preferable algorithms require fixed input range, Z-score Scaling better suited models assume normally distributed features. selecting appropriate scaling method, ensure balanced feature contributions improved model performance.","code":"\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` without scaling\") + \n  xlab(\"Values for variable `carat`\")\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = zscore(carat)), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` with Z-score Scaling\") + \n  xlab(\"Values for variable `carat`\")"},{"path":"chapter-data-prep.html","id":"how-to-reexpress-categorical-field-values","chapter":"3 Data Preparation","heading":"3.8 How to Reexpress Categorical Field Values","text":"data science, categorical features often need transformed numeric format can used machine learning models. Algorithms like decision trees, neural networks, linear regression require numeric inputs process data effectively. Converting categorical variables numerical representations ensures features contribute appropriately model, rather ignored treated incorrectly.process reexpressing categorical values crucial part data preparation, enables us leverage full range features dataset. section, explore several methods convert categorical fields numeric representations, focus techniques like one-hot encoding ordinal encoding. demonstrate techniques using diamonds dataset, includes several categorical features cut, color, clarity.","code":""},{"path":"chapter-data-prep.html","id":"why-reexpress-categorical-fields","chapter":"3 Data Preparation","heading":"3.8.1 Why Reexpress Categorical Fields?","text":"Categorical fields, also known nominal ordinal variables, often represent qualitative aspects data, product types, user locations, levels satisfaction. diamonds dataset, example:cut indicates quality diamond’s cut (e.g., “Fair,” “Good,” “Good,” “Premium,” “Ideal”).color represents diamond’s color grade (e.g., “D,” “E,” “F,” “D” colorless thus valuable).clarity describes diamond’s clarity, reflecting absence internal external flaws.fields essential understanding predicting diamond pricing, raw form text labels, suitable machine learning algorithms. Transforming numeric form allows us include valuable insights analysis.","code":""},{"path":"chapter-data-prep.html","id":"techniques-for-reexpressing-categorical-variables","chapter":"3 Data Preparation","heading":"3.8.2 Techniques for Reexpressing Categorical Variables","text":"several approaches converting categorical variables numeric representations. method choose depends type categorical variable nature data.","code":""},{"path":"chapter-data-prep.html","id":"ordinal-encoding","chapter":"3 Data Preparation","heading":"Ordinal Encoding","text":"Ordinal encoding suitable categorical variable meaningful order. example, cut feature diamonds dataset ordinal, natural hierarchy “Fair” “Ideal.” ordinal encoding, category assigned unique integer based rank level importance.example, might assign values follows:“Fair” → 1“Good” → 2“Good” → 3“Premium” → 4“Ideal” → 5This approach preserves order categories, can useful models interpret numeric values relative way, linear regression. However, important apply ordinal encoding order meaningful. non-ordinal variables, methods like one-hot encoding appropriate.","code":""},{"path":"chapter-data-prep.html","id":"one-hot-encoding","chapter":"3 Data Preparation","heading":"One-Hot Encoding","text":"One-hot encoding preferred technique nominal variables—categorical fields without intrinsic order. approach, unique category field transformed new binary (0 1) feature. method particularly useful variables like color clarity diamonds dataset, categories follow clear sequence.example, one-hot encode color feature, create set binary columns, one color grade:color_D: 1 diamond color “D,” 0 otherwise.color_E: 1 diamond color “E,” 0 otherwise.color_F: 1 diamond color “F,” 0 otherwise.One-hot encoding avoids introducing false ordinal relationships, ensuring model treats category independent entity. However, one downside can significantly increase dimensionality dataset categorical field many unique values.Note: Many machine learning libraries automatically drop one binary columns avoid multicollinearity (perfect correlation among features). instance, seven color categories, six binary columns created, missing category implied columns zero. approach, known dummy encoding, helps avoid redundancy keeps model simpler.","code":""},{"path":"chapter-data-prep.html","id":"frequency-encoding","chapter":"3 Data Preparation","heading":"Frequency Encoding","text":"Another useful approach, especially high-cardinality categorical variables (many unique values), frequency encoding. technique replaces category frequency dataset, allowing model capture information common category . Frequency encoding can particularly helpful fields like clarity want give model indication prevalent level .example:“VS2” appears 10,000 times dataset, encoded 10,000.“” appears 500 times, encoded 500.Frequency encoding less commonly used basic machine learning workflows can valuable dealing large datasets, one-hot encoding introduce many columns. However, cautious approach, may inadvertently add implicit weight common categories.","code":""},{"path":"chapter-data-prep.html","id":"choosing-the-right-encoding-technique","chapter":"3 Data Preparation","heading":"3.8.3 Choosing the Right Encoding Technique","text":"Selecting appropriate encoding technique depends nature categorical variable requirements analysis:Ordinal variables (like cut): Use ordinal encoding preserve natural order.Nominal variables unique values (like color clarity): Use one-hot encoding represent category binary column.High-cardinality categorical variables: Consider frequency encoding one-hot encoding introduce many features.Example 3.3  Applying techniques diamonds dataset:example:Ordinal Encoding: encoded cut variable based quality hierarchy.One-Hot Encoding: applied one-hot encoding color, creating binary columns color grade.encoding categorical fields way, transform dataset format compatible machine learning algorithms preserving essential information categorical feature.dataset now cleaned, scaled, encoded, ready move next stage data analysis. upcoming chapter, explore Exploratory Data Analysis (EDA), use visualizations summary statistics gain insights structure relationships within data. combining prepared data EDA techniques, can better understand features may hold predictive value model set stage successful machine learning outcomes.","code":"\n# Example: Ordinal encoding for `cut`\ndiamonds <- diamonds %>%\n  mutate(cut_encoded = as.integer(factor(cut, levels = c(\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"))))\n\n# Example: One-hot encoding for `color`\ndiamonds <- diamonds %>%\n  mutate(\n    color_D = ifelse(color == \"D\", 1, 0),\n    color_E = ifelse(color == \"E\", 1, 0),\n    color_F = ifelse(color == \"F\", 1, 0),\n    color_G = ifelse(color == \"G\", 1, 0),\n    color_H = ifelse(color == \"H\", 1, 0),\n    color_I = ifelse(color == \"I\", 1, 0),\n    color_J = ifelse(color == \"J\", 1, 0)\n  )"},{"path":"chapter-data-prep.html","id":"Data-pre-adult","chapter":"3 Data Preparation","heading":"3.9 Case Study: Who Can Earn More Than $50K Per Year?","text":"case study, explore Adult dataset, sourced US Census Bureau. dataset contains demographic information individuals, including age, education, occupation, income. dataset available liver package. details, refer documentation.goal study predict whether individual earns $50,000 per year based attributes. Section 11.5 Chapter 11, apply decision tree random forest algorithms build predictive model. applying techniques, need preprocess dataset handling missing values, encoding categorical variables, scaling numerical features. Let’s begin loading dataset examining structure.","code":""},{"path":"chapter-data-prep.html","id":"overview-of-the-dataset","chapter":"3 Data Preparation","heading":"Overview of the Dataset","text":"use Adult dataset, first ensure liver package installed. , install using:Next, load package dataset:inspect dataset structure, use:dataset contains 48598 records 15 variables. , 14 predictors, target variable, income, categorical variable two levels: <=50K >50K. features include numerical categorical variables:age: Age years (numerical).workclass: Employment type (categorical, 6 levels).demogweight: Census weighting factor (numerical).education: Highest level education (categorical, 16 levels).education.num: Number years education (numerical).marital.status: Marital status (categorical, 5 levels).occupation: Job category (categorical, 15 levels).relationship: Family relationship status (categorical, 6 levels).race: Racial background (categorical, 5 levels).gender: Gender identity (categorical, Male/Female).capital.gain: Capital gains (numerical).capital.loss: Capital losses (numerical).hours.per.week: Hours worked per week (numerical).native.country: Country origin (categorical, 42 levels).income: Target variable indicating annual income (<=50K >50K).clarity, categorize dataset’s variables:Nominal variables: workclass, marital.status, occupation, relationship, race, native.country, gender.Ordinal variable: education.Numerical variables: age, demogweight, education.num, capital.gain, capital.loss, hours.per.week.better understand dataset, generate summary statistics:summary provides insights distribution numerical variables, missing values, categorical variable levels, guiding us preparing data analysis.","code":"\ninstall.packages(\"liver\")\nlibrary(liver)  # Load liver package\ndata(adult)     # Load Adult datasetstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education.num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital.status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital.gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital.loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours.per.week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native.country: Factor w/ 42 levels \"?\",\"Cambodia\",..: 40 40 40 40 40 40 40 40 40 40 ...\n    $ income        : Factor w/ 2 levels \"<=50K\",\">50K\": 1 1 2 2 1 1 1 2 1 1 ...summary(adult)\n         age              workclass      demogweight             education    \n    Min.   :17.0   ?           : 2794   Min.   :  12285   HS-grad     :15750  \n    1st Qu.:28.0   Gov         : 6536   1st Qu.: 117550   Some-college:10860  \n    Median :37.0   Never-worked:   10   Median : 178215   Bachelors   : 7962  \n    Mean   :38.6   Private     :33780   Mean   : 189685   Masters     : 2627  \n    3rd Qu.:48.0   Self-emp    : 5457   3rd Qu.: 237713   Assoc-voc   : 2058  \n    Max.   :90.0   Without-pay :   21   Max.   :1490400   11th        : 1812  \n                                                          (Other)     : 7529  \n    education.num         marital.status            occupation   \n    Min.   : 1.00   Divorced     : 6613   Craft-repair   : 6096  \n    1st Qu.: 9.00   Married      :22847   Prof-specialty : 6071  \n    Median :10.00   Never-married:16096   Exec-managerial: 6019  \n    Mean   :10.06   Separated    : 1526   Adm-clerical   : 5603  \n    3rd Qu.:12.00   Widowed      : 1516   Sales          : 5470  \n    Max.   :16.00                         Other-service  : 4920  \n                                          (Other)        :14419  \n            relationship                   race          gender     \n    Husband       :19537   Amer-Indian-Eskimo:  470   Female:16156  \n    Not-in-family :12546   Asian-Pac-Islander: 1504   Male  :32442  \n    Other-relative: 1506   Black             : 4675                 \n    Own-child     : 7577   Other             :  403                 \n    Unmarried     : 5118   White             :41546                 \n    Wife          : 2314                                            \n                                                                    \n     capital.gain      capital.loss     hours.per.week        native.country \n    Min.   :    0.0   Min.   :   0.00   Min.   : 1.00   United-States:43613  \n    1st Qu.:    0.0   1st Qu.:   0.00   1st Qu.:40.00   Mexico       :  949  \n    Median :    0.0   Median :   0.00   Median :40.00   ?            :  847  \n    Mean   :  582.4   Mean   :  87.94   Mean   :40.37   Philippines  :  292  \n    3rd Qu.:    0.0   3rd Qu.:   0.00   3rd Qu.:45.00   Germany      :  206  \n    Max.   :41310.0   Max.   :4356.00   Max.   :99.00   Puerto-Rico  :  184  \n                                                        (Other)      : 2507  \n      income     \n    <=50K:37155  \n    >50K :11443  \n                 \n                 \n                 \n                 \n   "},{"path":"chapter-data-prep.html","id":"missing-values-1","chapter":"3 Data Preparation","heading":"3.9.1 Missing Values","text":"summary() function reveals variables workclass native.country contain missing values, represented \"?\" category. Specifically, 2794 records workclass 847 records native.country missing values. Since \"?\" used placeholder missing data, first convert entries NA:replacing \"?\" NA, remove unused factor levels clean dataset:visualize distribution missing values, use gg_miss_var() function naniar package:plot indicates workclass, occupation, native.country contain missing values. percentage missing values variables relatively low, workclass occupation less 0.06 percent missing data, native.country 0.02 percent.","code":"\nadult[adult == \"?\"] = NA\nadult = droplevels(adult)\nlibrary(naniar)  # Load package for visualizing missing values\n\ngg_miss_var(adult, show_pct = TRUE)"},{"path":"chapter-data-prep.html","id":"imputing-missing-values","chapter":"3 Data Preparation","heading":"Imputing Missing Values","text":"Instead removing records missing values, can lead information loss, apply random imputation, missing values filled randomly selected values existing distribution variable. maintains natural proportions category.use impute() function Hmisc package purpose:confirm missing values successfully imputed, generate another missing values plot:updated plot show missing values, indicating successful imputation.","code":"\nlibrary(Hmisc)  # Load package for imputation\n\n# Impute missing values using random sampling from existing categories\nadult$workclass      = impute(adult$workclass,      'random')\nadult$native.country = impute(adult$native.country, 'random')\nadult$occupation     = impute(adult$occupation,     'random')\ngg_miss_var(adult, show_pct = TRUE)"},{"path":"chapter-data-prep.html","id":"alternative-approaches","chapter":"3 Data Preparation","heading":"Alternative Approaches","text":"impute() function allows different statistical methods mean, median, mode imputation. default behavior median imputation. advanced techniques, aregImpute() function Hmisc package offers predictive imputation using additive regression, bootstrapping, predictive mean matching.Although removing records missing values using na.omit() option, generally discouraged unless missing values excessive biased way distort analysis.properly handling missing values, ensure data completeness maintain integrity dataset subsequent preprocessing steps, recoding categorical variables grouping country-level data broader regions.","code":""},{"path":"chapter-data-prep.html","id":"encoding-categorical-variables","chapter":"3 Data Preparation","heading":"3.9.2 Encoding Categorical Variables","text":"Categorical variables often contain large number unique values, making challenging use predictive models. Adult dataset, native.country workclass multiple categories, can introduce complexity redundancy. simplify variables, group similar categories together preserving interpretability.","code":""},{"path":"chapter-data-prep.html","id":"grouping-native.country-by-continent","chapter":"3 Data Preparation","heading":"Grouping native.country by Continent","text":"native.country variable contains 41 distinct countries. make manageable, categorize countries broader geographical regions:Europe: England, France, Germany, Greece, Netherlands, Hungary, Ireland, Italy, Poland, Portugal, Scotland, YugoslaviaAsia: China, Hong Kong, India, Iran, Cambodia, Japan, Laos, Philippines, Vietnam, Taiwan, ThailandNorth America: Canada, United States, Puerto RicoSouth America: Colombia, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Mexico, Nicaragua, Outlying US territories, Peru, Jamaica, Trinidad & TobagoOther: includes ambiguous “South” category, meaning unclear dataset documentation.\nuse fct_collapse() function forcats package reassign categories:confirm changes, display frequency distribution native.country:grouping original 42 countries 5 broader regions, simplify variable maintaining relevance analysis.","code":"\nlibrary(forcats)  # Load package for categorical variable transformation\n\n# To create a new factor variable with fewer levels for `native.country`\nEurope = c(\"England\", \"France\", \"Germany\", \"Greece\", \"Holand-Netherlands\", \"Hungary\", \"Ireland\", \"Italy\", \"Poland\", \"Portugal\", \"Scotland\", \"Yugoslavia\")\n\nAsia = c(\"China\", \"Hong\", \"India\", \"Iran\", \"Cambodia\", \"Japan\", \"Laos\", \"Philippines\", \"Vietnam\", \"Taiwan\", \"Thailand\")\n\nN.America = c(\"Canada\", \"United-States\", \"Puerto-Rico\")\n\nS.America = c(\"Columbia\", \"Cuba\", \"Dominican-Republic\", \"Ecuador\", \"El-Salvador\", \"Guatemala\", \"Haiti\", \"Honduras\", \"Mexico\", \"Nicaragua\", \"Outlying-US(Guam-USVI-etc)\", \"Peru\", \"Jamaica\", \"Trinadad&Tobago\")\n\n# Reclassify native.country into broader regions\nadult$native.country = fct_collapse(adult$native.country, \n                                    \"Europe\"    = Europe,\n                                    \"Asia\"      = Asia,\n                                    \"N.America\" = N.America,\n                                    \"S.America\" = S.America,\n                                    \"Other\"     = c(\"South\") )table(adult$native.country)\n   \n        Asia N.America S.America    Europe     Other \n         993     44747      1946       797       115"},{"path":"chapter-data-prep.html","id":"simplifying-workclass","chapter":"3 Data Preparation","heading":"Simplifying workclass","text":"workclass variable originally contains several employment categories. Since “Never-worked” “Without-pay” represent similar employment statuses, merge single category labeled “Unemployed”:verify updated categories, check frequency distribution:reducing number unique categories workclass native.country, improve model interpretability reduce risk overfitting applying machine learning algorithms.","code":"\nadult$workclass = fct_collapse(adult$workclass, \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))table(adult$workclass)\n   \n          Gov Unemployed    Private   Self-emp \n         6919         32      35851       5796"},{"path":"chapter-data-prep.html","id":"outliers","chapter":"3 Data Preparation","heading":"3.9.3 Outliers","text":"Detecting handling outliers essential step data preprocessing, extreme values can significantly impact statistical analysis model performance. , examine potential outliers capital.loss variable determine whether adjustments necessary.","code":""},{"path":"chapter-data-prep.html","id":"summary-statistics","chapter":"3 Data Preparation","heading":"Summary Statistics","text":"gain initial understanding capital.loss, compute summary statistics:summary output reveals following insights:minimum value 0, maximum 4356.median 0, significantly lower mean, indicating highly skewed distribution.75% observations capital loss 0, confirming strong right-skew.mean capital loss 87.94, influenced small number extreme values.","code":"summary(adult$capital.loss)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0.00    0.00    0.00   87.94    0.00 4356.00"},{"path":"chapter-data-prep.html","id":"visualizing-outliers","chapter":"3 Data Preparation","heading":"Visualizing Outliers","text":"investigate distribution capital.loss, use boxplot histogram:plots, observe:boxplot shows strong positive skew, many extreme values upper whisker.histogram indicates observations zero capital loss, cases around 2,000 4,000.Since large proportion observations report capital loss, examine nonzero cases.","code":"\nggplot(data = adult, aes(y = capital.loss)) +\n     geom_boxplot()\nggplot(data = adult, aes(x = capital.loss)) +\n     geom_histogram(bins = 30, color = \"blue\", fill = \"lightblue\")"},{"path":"chapter-data-prep.html","id":"zooming-into-the-nonzero-distribution","chapter":"3 Data Preparation","heading":"Zooming into the Nonzero Distribution","text":"better visualize spread nonzero values, focus observations capital.loss > 0:Key takeaways refined plots:majority nonzero values 500, small number extending beyond 4,000.distribution nonzero values approximately symmetric, suggesting extreme values, follow structured pattern rather random anomalies.","code":"\nggplot(data = adult, mapping = aes(x = capital.loss)) +\n    geom_histogram(bins = 30, color = \"blue\", fill = \"lightblue\") +\n    coord_cartesian(xlim = c(500, 4000), ylim = c(0, 1000))\nggplot(data = subset(adult, capital.loss > 0)) +\n     geom_boxplot(aes(y = capital.loss)) "},{"path":"chapter-data-prep.html","id":"handling-outliers","chapter":"3 Data Preparation","heading":"Handling Outliers","text":"Although capital.loss contains many high values, appear erroneous. Instead, reflect genuine cases within dataset. Since values provide meaningful information particular individuals, retain rather applying transformations removals.However, model performance significantly affected extreme values, might consider:Winsorization: Capping values reasonable percentile (e.g., 95th percentile).Log Transformation: Applying log transformation reduce skewness.Creating Binary Indicator: Introducing new variable indicating whether capital loss occurred (capital.loss > 0).Next, perform similar outlier analysis capital.gain variable. See exercises guided approach.","code":""},{"path":"chapter-data-prep.html","id":"exercises-2","chapter":"3 Data Preparation","heading":"3.10 Exercises","text":"section provides hands-exercises reinforce key concepts covered chapter. questions include theoretical, exploratory, practical challenges related data types, outliers, encoding techniques, feature engineering.","code":""},{"path":"chapter-data-prep.html","id":"understanding-data-types","chapter":"3 Data Preparation","heading":"Understanding Data Types","text":"difference continuous discrete numerical variables? Provide example real-world data.ordinal categorical variables differ nominal categorical variables? Give example .","code":""},{"path":"chapter-data-prep.html","id":"exploring-the-diamonds-dataset","chapter":"3 Data Preparation","heading":"Exploring the diamonds Dataset","text":"Report summary statistics diamonds dataset using summary() function. insights can derive output?diamonds dataset, variables nominal, ordinal, numerical? List accordingly.","code":""},{"path":"chapter-data-prep.html","id":"detecting-and-handling-outliers","chapter":"3 Data Preparation","heading":"Detecting and Handling Outliers","text":"Identify outliers variable x. exist, handle appropriately. Follow approach Section 3.3 y variable diamonds dataset.Repeat outlier detection process variable z. necessary, apply transformations filtering techniques.Check outliers depth variable. method use detect handle ?","code":""},{"path":"chapter-data-prep.html","id":"encoding-categorical-variables-1","chapter":"3 Data Preparation","heading":"Encoding Categorical Variables","text":"cut variable diamonds dataset ordinal. can encode properly using ordinal encoding?color variable diamonds dataset nominal. can encode using one-hot encoding?","code":""},{"path":"chapter-data-prep.html","id":"analyzing-the-adult-dataset","chapter":"3 Data Preparation","heading":"Analyzing the Adult Dataset","text":"Load Adult dataset liver package examine structure. Identify categorical variables classify nominal ordinal.Compute proportion individuals earn 50K (>50K). distribution tell income levels dataset?Adult dataset, generate summary statistics, boxplot, histogram variable capital.gain. observe?Based visualizations previous question, outliers capital.gain variable? , suggest strategy handle .","code":""},{"path":"chapter-data-prep.html","id":"feature-engineering-challenge","chapter":"3 Data Preparation","heading":"Feature Engineering Challenge","text":"Create new categorical variable Age_Group Adult dataset, grouping ages :Young (≤30 years old)Middle-aged (31-50 years old)Senior (>50 years old)\nUse cut() function implement transformation.Compute mean capital.gain Age_Group. insights gain income levels across different age groups?","code":""},{"path":"chapter-data-prep.html","id":"advanced-data-preparation-challenges","chapter":"3 Data Preparation","heading":"Advanced Data Preparation Challenges","text":"Adult dataset, education variable contains 16 distinct levels. Reduce categories broader groups “Diploma,” “High School Graduate,” “College,” “Postgraduate.” Implement transformation using fct_collapse() function.Adult dataset, education variable contains 16 distinct levels. Reduce categories broader groups “Diploma,” “High School Graduate,” “College,” “Postgraduate.” Implement transformation using fct_collapse() function.capital.gain capital.loss variables represent financial assets. Create new variable net.capital computes difference capital.gain capital.loss. Analyze distribution.capital.gain capital.loss variables represent financial assets. Create new variable net.capital computes difference capital.gain capital.loss. Analyze distribution.Perform Min-Max scaling numerical variables Adult dataset (age, capital.gain, capital.loss, hours.per.week). Use mutate() function apply transformation.Perform Min-Max scaling numerical variables Adult dataset (age, capital.gain, capital.loss, hours.per.week). Use mutate() function apply transformation.Perform Z-score normalization set numerical variables. Compare results Min-Max scaling. scenarios one approach preferable ?Perform Z-score normalization set numerical variables. Compare results Min-Max scaling. scenarios one approach preferable ?Construct logistic regression model predict whether individual earns 50K (>50K) based selected numerical features (age, education.num, hours.per.week). Preprocess data accordingly interpret coefficients model.Construct logistic regression model predict whether individual earns 50K (>50K) based selected numerical features (age, education.num, hours.per.week). Preprocess data accordingly interpret coefficients model.","code":""},{"path":"chapter-EDA.html","id":"chapter-EDA","chapter":"4 Exploratory Data Analysis","heading":"4 Exploratory Data Analysis","text":"Exploratory Data Analysis (EDA) process examining exploring data gain insights, identify patterns, understand relationships variables—applying formal hypotheses machine learning algorithms. critical step allows us “get know” data, using mix summary statistics, visualizations, preliminary analysis reveal structure potential insights. EDA foundational data science helps generate hypotheses informs decisions ’ll make later stages analysis.EDA rigid process strict rules; rather, flexible, iterative approach encourages curiosity open-ended exploration. stage, ’s important remain open investigating ideas arise. explorations may lead dead ends, others can uncover valuable insights guide rest project. time, become familiar data, ’ll naturally focus promising leads, help shape final analysis conclusions.EDA primarily exploration discovery—’s way find clues patterns data rather proving particular theory. Using tools like summary statistics, visualizations, basic correlations, can generate hypotheses start understand underlying structure data. However, insights just preliminary observations, formal conclusions; provide direction analysis, rigorous testing can confirm refine uncover EDA.interpreting patterns EDA, ’s essential balance statistical significance practical relevance. large datasets, even small correlations patterns may statistically significant, might meaningful implications problem hand. example, variable might show slight statistically significant association churn; however, association weak, may actionable real-world context. EDA encourages us consider perspectives: statistical tests can highlight interesting relationships, also need interpret findings way aligns practical goals domain knowledge.EDA also overlaps significantly data preparation cleaning. example, may notice missing values EDA decide address imputing removing data points. technically part data cleaning, ’s also aspect exploration, examine data make best decision handling issues. Identifying issues early allows us refine data set strong foundation analysis.EDA involves choosing appropriate tools techniques answer question pose data. choice visualization statistical summary depends type data specific aspect data ’re examining. example, histograms box plots helpful understanding distributions individual variables, scatter plots correlation matrices better exploring relationships pairs variables. following sections, ’ll see examples visualizations summaries help answer different types questions within EDA. ’ll also apply techniques churn dataset, help illustrate use EDA uncover patterns relevant customer retention.","code":""},{"path":"chapter-EDA.html","id":"key-areas-of-focus-in-eda","chapter":"4 Exploratory Data Analysis","heading":"4.1 Key Areas of Focus in EDA","text":"EDA, primary goals :Understand structure data: Determine data types, range, number observations, identify missing values anomalies.Analyze individual variable distributions: Explore variable understand distribution, central tendency, spread.Explore relationships variables: Identify correlations, dependencies, interactions may exist features.Identify patterns outliers: Spot unusual data points decide whether require special handling removal.objectives serve guide ensure gain comprehensive understanding dataset moving modeling.","code":""},{"path":"chapter-EDA.html","id":"types-of-eda-questions","chapter":"4 Exploratory Data Analysis","heading":"4.2 Types of EDA Questions","text":"performing EDA, ’s helpful approach data specific questions mind. questions typically fall two broad categories: univariate questions multivariate questions.Univariate Questions: questions focus understanding distribution individual variables. Examples include:distribution target variable?predictor variables like age, income, education distributed?missing values, distributed across variables?Answering univariate questions helps understand variable isolation, essential detecting skewness, outliers, data ranges. can use histograms, box plots, summary statistics explore characteristics.Multivariate Questions: questions examine relationships multiple variables. Examples include:relationship target variable predictors?certain predictors correlated , indicating potential multicollinearity?relationship missing values different variables?answer multivariate questions, use scatter plots, correlation matrices, pair plots visualize relationships. tools reveal interactions features can help detect patterns important building predictive models.questions guide exploratory process allow uncover insights inform later analysis. Keep mind EDA primarily exploration—finding clues rather proving theories.Note answer types questions can use different types visualizations statistical summaries. know visualization use? question hear lot students answer : depends type data question want answer. specific, following sections see examples visualizations statistical summaries can used answer univariate multi-variate questions. also see use practice churn dataset. end chapter guide line specificity answer question.","code":""},{"path":"chapter-EDA.html","id":"eda-as-data-storytelling","chapter":"4 Exploratory Data Analysis","heading":"4.3 EDA as Data Storytelling","text":"EDA technical exercise also form data storytelling. Data storytelling combines data, visuals, narrative communicate insights structured, compelling way. skill invaluable data science, enables present complex findings audiences accessible manner.Many scientific reports, journalistic pieces, public presentations rely data storytelling convey insights effectively. Figures like ones demonstrate data visualization can bring narrative life:Figure 4.1 illustrates global mean surface temperature changes Common Era, using visual cues communicate story temperature anomalies time. visualization, taken Raphael Neukom et al.4, shows data can transformed compelling visual narrative.\nFigure 4.1: Global mean surface temperature history Common Era. Temperature anomalies respect 1961–1990 CE. coloured lines represent 30-year low-pass-filtered ensemble medians individual reconstruction methods.\nFigure 4.2 shows animated scatter plot fertility rate versus life expectancy birth different world regions 1960 2015. Adapted Hans Rosling’s TED Talk “New insights poverty”, visualization effectively illustrates trends global health demography dynamic, multi-dimensional storytelling.\nFigure 4.2: Animated scatter plot fertility rate life expectancy birth different regions world 1960 2015.\nlearning interpret create visualizations, ’ll better equipped uncover key insights communicate findings persuasively.","code":""},{"path":"chapter-EDA.html","id":"eda-in-practice-working-with-the-churn-dataset","chapter":"4 Exploratory Data Analysis","heading":"4.4 EDA in Practice: Working with the Churn Dataset","text":"book, use churn dataset illustrate EDA process. churn dataset contains information customer behavior, including whether customer “churned” (.e., left service) various demographic behavioral attributes.EDA, aim :Uncover patterns related customer churn.Identify important predictors influence customer retention.Gain insights structure data guide us building predictive models.next sections, ’ll walk specific techniques exploring churn dataset, calculating summary statistics creating visualizations. following steps, ’ll able turn raw data actionable insights, building strong foundation analysis modeling. shortly frist second steps Data Science workflow.","code":""},{"path":"chapter-EDA.html","id":"problembusiness-understanding","chapter":"4 Exploratory Data Analysis","heading":"4.4.1 Problem/Business Understanding","text":"Companies interested know gonna get churned can proactively go customer provide better services turn customers’ decisions opposite direction. Companies interested know:losing ?causes reasons losing customers?stop leaving company?EDA help us answer questions provide insights company take actions.","code":""},{"path":"chapter-EDA.html","id":"data-understanding","chapter":"4 Exploratory Data Analysis","heading":"4.4.2 Data Understanding","text":"dataset comes IBM Sample Data Sets. data set contains 5000 rows (customers) 20 columns (features). “churn” column target indicates whether customer churned (left company) .\n20 variables :state: Categorical, 51 states District Columbia.area.code: Categorical.account.length: count, long account active.voice.plan: Categorical, yes , voice mail plan.voice.messages: Count, number voice mail messages.intl.plan: Categorical, yes , international plan.intl.mins: Continuous, minutes customer used service make international calls.intl.calls: Count, total number international calls.intl.charge: Continuous, total international charge.day.mins: Continuous, minutes customer used service day.day.calls: Count, total number calls day.day.charge: Continuous, total charge day.eve.mins: Continuous, minutes customer used service evening.eve.calls: Count, total number calls evening.eve.charge: Continuous, total charge evening.night.mins: Continuous, minutes customer used service night.night.calls: Count, total number calls night.night.charge: Continuous, total charge night.customer.calls: Count, number calls customer service.churn: Categorical, yes . Indicator whether customer left company (yes ).import dataset R follows:see overview dataset R using function str() follows:shows data data.frame object R 5000 observations 20 variables. last column (name churn) target variable indicates whether customers churned (left company) .using function summary() R, can see summary dataset followsIt shows summary 20 variables. dataset 19 predictors along target variable churn. type variables follows:dataset clean ready EDA. upcoming chapter, ’ll dive Exploratory Data Analysis (EDA), ’ll use visualizations summary statistics gain insights structure relationships within data. combining prepared data EDA techniques, can better understand features may hold predictive value model set stage successful machine learning outcomes.wonder can 51 states dataset just 3 area codes! still just getting know data set.","code":"\nlibrary(liver)\n\ndata(churn) # load the \"churn\" datasetstr(churn)   # Compactly display the structure of the data\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...summary(churn)\n        state              area.code    account.length  voice.plan\n    WV     : 158   area_code_408:1259   Min.   :  1.0   yes:1323  \n    MN     : 125   area_code_415:2495   1st Qu.: 73.0   no :3677  \n    AL     : 124   area_code_510:1246   Median :100.0             \n    ID     : 119                        Mean   :100.3             \n    VA     : 118                        3rd Qu.:127.0             \n    OH     : 116                        Max.   :243.0             \n    (Other):4240                                                  \n    voice.messages   intl.plan    intl.mins       intl.calls      intl.charge   \n    Min.   : 0.000   yes: 473   Min.   : 0.00   Min.   : 0.000   Min.   :0.000  \n    1st Qu.: 0.000   no :4527   1st Qu.: 8.50   1st Qu.: 3.000   1st Qu.:2.300  \n    Median : 0.000              Median :10.30   Median : 4.000   Median :2.780  \n    Mean   : 7.755              Mean   :10.26   Mean   : 4.435   Mean   :2.771  \n    3rd Qu.:17.000              3rd Qu.:12.00   3rd Qu.: 6.000   3rd Qu.:3.240  \n    Max.   :52.000              Max.   :20.00   Max.   :20.000   Max.   :5.400  \n                                                                                \n       day.mins       day.calls     day.charge       eve.mins       eve.calls    \n    Min.   :  0.0   Min.   :  0   Min.   : 0.00   Min.   :  0.0   Min.   :  0.0  \n    1st Qu.:143.7   1st Qu.: 87   1st Qu.:24.43   1st Qu.:166.4   1st Qu.: 87.0  \n    Median :180.1   Median :100   Median :30.62   Median :201.0   Median :100.0  \n    Mean   :180.3   Mean   :100   Mean   :30.65   Mean   :200.6   Mean   :100.2  \n    3rd Qu.:216.2   3rd Qu.:113   3rd Qu.:36.75   3rd Qu.:234.1   3rd Qu.:114.0  \n    Max.   :351.5   Max.   :165   Max.   :59.76   Max.   :363.7   Max.   :170.0  \n                                                                                 \n      eve.charge      night.mins     night.calls      night.charge   \n    Min.   : 0.00   Min.   :  0.0   Min.   :  0.00   Min.   : 0.000  \n    1st Qu.:14.14   1st Qu.:166.9   1st Qu.: 87.00   1st Qu.: 7.510  \n    Median :17.09   Median :200.4   Median :100.00   Median : 9.020  \n    Mean   :17.05   Mean   :200.4   Mean   : 99.92   Mean   : 9.018  \n    3rd Qu.:19.90   3rd Qu.:234.7   3rd Qu.:113.00   3rd Qu.:10.560  \n    Max.   :30.91   Max.   :395.0   Max.   :175.00   Max.   :17.770  \n                                                                     \n    customer.calls churn     \n    Min.   :0.00   yes: 707  \n    1st Qu.:1.00   no :4293  \n    Median :1.00             \n    Mean   :1.57             \n    3rd Qu.:2.00             \n    Max.   :9.00             \n   \n# skim(churn)"},{"path":"chapter-EDA.html","id":"chapter-EDA-categorical","chapter":"4 Exploratory Data Analysis","heading":"4.5 Investigating Categorical Variables","text":"Categorical variables represent discrete values, names, labels, categories. churn dataset, variables state, area.code, voice.plan, intl.plan categorical. explore variables, can use bar plots, pie charts, frequency tables visualize distributions. start target variable churn follows:bar plot presents distribution target variable churn, indicates whether customer churned (left company). plot reveals dataset imbalanced, customers staying (churn = \"\") leaving (churn = \"yes\"); presents proportion churner (churn = \"yes\") 1.4 percent proportion non-churner (churn = \"yes\") 8.6 percent. information crucial building predictive models, imbalance can affect model performance accuracy. Besides, one objective reduce proportion churners. need identify patterns data related customer churn, need investigate relationship target variable predictors.primary purpose exploratory data analysis (EDA) gain thorough understanding variables dataset examining distributions categorical variables, analyzing histograms numerical variables, exploring relationships variables. EDA focuses uncovering patterns understanding data structure, ultimate goal data mining project develop predictive model identifies customers likely churn, switch competitor. Today’s data analysis tools allow us familiarize dataset investigate potential associations churn, helping us explore data keeping project’s objective mind. begin examining categorical variables relationship churn, foundational analysis guide us building effective predictive model.begin analysis intl.plan (International Plan) variable, indicates whether customer subscribed international calling plan. binary variable, provides two categories—international plan without. explore relationship feature customer churn, can use bar plots visualize distribution churners non-churners based international plan selection:first plot (left) shows distribution churners non-churners across two categories international plan. plot allows us directly compare raw counts churners non-churners among customers without plan. second plot (right) displays proportions churners non-churners within category, y-axis scaled 0 1. proportion plot particularly useful comparing churn rates, normalizes differences group sizes. plots, observe customers international plan noticeably higher churn rate without , suggesting potential link international plan customer attrition.quantify relationship, can examine contingency table, provides detailed breakdown churners non-churners international plan status. Since intl.plan churn categorical variables, table helps us summarize compare distribution across categories:contingency table shows count churners non-churners customers without international plan. wish view counts proportions, can use prop.table() function, converts counts percentages provides clearer view relative churn rates within category.summary, exploratory analysis International Plan variable suggests two key takeaways:clear association international plan increased likelihood churn. may warrant investigation understand aspects international plan driving customers away.can expect international plan variable likely play important role predictive model churn, shows strong relationship target variable. Whatever data mining algorithm choose, ’s probable model include feature significant predictor customer churn.examining relationship international plan churn, ’ve taken important first step toward understanding dataset identifying potential predictors customer attrition. initial analysis lays groundwork building predictive models can help companies proactively address factors influencing churn ultimately improve customer retention.Next, ’ll continue exploration investigating voice.plan (Voice Mail Plan) variable relationship churn. Applying similar approach—visualizing summarizing data—provide additional insights customer behavior allow us refine understanding dataset. start, visualize distribution churners non-churners based Voice Mail Plan following bar plots:first plot (left) shows raw counts churners non-churners among customers without Voice Mail Plan, second plot (right) displays proportions. plots, observe customers without Voice Mail Plan appear likely churn opted feature.quantify observation, can create contingency table summarize counts churners non-churners Voice Mail Plan status:table shows number churners non-churners customers without Voice Mail Plan. example, reveals 102 customers Voice Mail Plan churn, 1221 . Similarly, can see counts customers without Voice Mail Plan. higher proportion churners among without Voice Mail Plan suggests feature may indeed relevant predictor customer attrition.summary, EDA Voice Mail Plan variable suggests two key takeaways:Enhancing Voice Mail Plan encouraging customers subscribe may help improve customer retention, plan seems associated lower churn rates.can expect Voice Mail Plan variable likely contribute meaningfully predictive model develop churn, though influence may strong International Plan.insights reinforce importance EDA uncovering patterns inform strategic decisions model-building efforts, helping companies address churn effectively.","code":"\nggplot(data = churn, aes(x = churn, label = scales::percent(prop.table(stat(count))))) +\n  geom_bar(fill = c(\"palevioletred1\", \"darkseagreen1\")) + \n  geom_text(stat = 'count', vjust = 0.2, size = 6)\nggplot(data = churn) + \n  geom_bar(aes(x = intl.plan, fill = churn)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = churn) + \n  geom_bar(aes(x = intl.plan, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) addmargins(table(churn$churn, churn$intl.plan, \n                 dnn = c(\"Churn\", \"International Plan\")))\n        International Plan\n   Churn  yes   no  Sum\n     yes  199  508  707\n     no   274 4019 4293\n     Sum  473 4527 5000\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) addmargins(table(churn$churn, churn$voice.plan, dnn = c(\"Churn\", \"Voice Mail Plan\")))\n        Voice Mail Plan\n   Churn  yes   no  Sum\n     yes  102  605  707\n     no  1221 3072 4293\n     Sum 1323 3677 5000"},{"path":"chapter-EDA.html","id":"EDA-sec-numeric","chapter":"4 Exploratory Data Analysis","heading":"4.6 Investigating Numerical Variables","text":"Next, turn exploration numeric predictive variables. Refer back summary statistics various predictors. start variable service.calls (Customer Service Calls), represents number calls made customer service. variable numerical discrete, making suitable histogram analysis. can use histograms visualize distribution customer service calls follows:histogram displays distribution customer service calls, x-axis representing number calls y-axis showing count customers. visualization allows us observe frequency different call counts. example, can see range calls 0 maximum 9 majority customers calls customer service numeric times distribution right-skewed, long tail right. skewness indicates customers made large number calls, potentially signaling dissatisfaction issues need addressed.initial histogram gives basic overview distribution customer service calls, investigate association churn, need incorporate target variable visualization. Overlaying histogram target variable provides clearer picture predictor relates churn. coloring histogram bars according churn status, can better discern patterns might suggest relationship customer service calls likelihood churn. ’s code create overlay histograms:first plot (left) shows distribution churners non-churners level customer service calls, second plot (right) presents values proportions within call count. left plot, suggestion churn prevalent among customers higher call counts, pattern isn’t immediately clear. normalized histogram right, however, makes relationship much apparent standardizing bar height 1, allowing us easily compare relative proportions churners within call level.normalized plot, can see striking pattern: customers made three fewer calls customer service notably lower churn rate made four calls. visualization indicates frequent customer service interactions strong indicator churn risk.analysis customer service calls variable suggests following actionable insights:Customer Retention Strategy: Monitor number customer service calls closely. third call, may beneficial offer specialized retention incentives, likelihood churn increases significantly fourth call.Predictive Modeling: expect number customer service calls important predictor model designed forecast churn, appears highly indicative customer dissatisfaction.Let’s now examine remaining numerical variables dataset, starting day.mins (Day Minutes). Since day.mins continuous variable, can visualize distribution using box plot density plot, providing different insights. , present two plots side side:box plot left allows us compare distribution day.mins churners non-churners, highlighting differences median, spread, potential outliers. density plot right provides smooth visualization distribution, making easier spot patterns day usage across groups.plots, observe high day-minute users likely churn. density plot, particular, shows noticeable peak among churners higher day.mins values, suggesting heavy daytime usage may linked customer dissatisfaction unmet needs.analysis suggests following actions:Monitor High Day-Minute Users: Track customers whose day-minute usage exceeds 200 minutes, high-usage customers appear greater risk churning.Investigate Reasons Churn Among Heavy Users: Explore heavy daytime users inclined leave. experiencing issues service quality, pricing, value?Include day.mins Churn Prediction Models: Given apparent predictive power, day.mins considered key feature churn prediction model, may help identify high-risk customers.summary, analyzing day.mins provided valuable insights customer behavior, highlighting opportunity improve customer retention focusing heavy daytime users. including variable predictive models, can enhance model’s accuracy take proactive steps reduce churn among high-usage customers.examine relationship eve.mins (Evening Minutes) variable churn, can use box plot density plot visualize distribution evening minutes churners non-churners:box plot (left) density plot (right) offer different perspectives distribution evening minutes across churn non-churn groups. box plot provides summary median, interquartile range, potential outliers, density plot shows overall shape distribution group.plots, observe slight tendency customers higher evening minutes greater likelihood churning. However, pattern subtle, graphical evidence alone strong enough make definitive conclusion relationship evening usage churn. situations like , visual trends ambiguous, ’s prudent withhold policy recommendations robust evidence statistical analyses predictive models.summary, may association evening usage churn, need additional evidence drawing conclusions. Proceeding caution essential data analysis, particularly visual trends weak uncertain. data-driven approach allow us validate findings making data-backed recommendations policy customer retention strategies.explore relationship night.mins (Night Minutes) churn, can visualize data using box plot density plot:box plot (left) density plot (right) suggest little discernible association churn night minutes. distributions night minutes churners non-churners quite similar, significant differences median, spread, overall shape. lack variation implies night.mins may strong predictor customer churn.fact, pattern minimal association churn appears hold several remaining numeric variables dataset. examined subset variables detail, may wish investigate others confirm whether exhibit meaningful relationship churn. Identifying variables little predictive value valuable part EDA process, allows us focus features likely enhance model’s accuracy.Ultimately, EDA serves guide prioritize features worth closer examination inclusion predictive modeling. features like night.mins, EDA indicates obvious link churn, might consider excluding final model keeping secondary variables.Note: absence obvious association predictor target variable EDA , , sufficient reason exclude predictor model. instance, although box plot density plot night.mins (Night Minutes) show clear visual relationship churn, necessarily mean night.mins lacks predictive value. Data mining models may still uncover useful patterns involving variable, particularly interacts features contributes complex, higher-dimensional associations. Therefore, unless compelling reason discard variable outright, generally best include initial stages modeling allow model determine predictive importance.example, night.mins display strong association churn EDA visuals, analysis using statistical tests may reveal otherwise. t-test, instance, show statistically significant difference mean number Night Minutes churners non-churners (see Chapter 3 details). finding suggest night.mins may indeed predictive power identifying churn, even though immediately apparent visualizations. Excluding variable based solely lack obvious association EDA stage lead less accurate model, subtle meaningful patterns might go unrecognized.confirm whether observed differences statistically significant, delve hypothesis testing statistical techniques next chapter. methods fall domain statistical inference model building, extending beyond scope exploratory data analysis. mention emphasize EDA one step modeling process; variables may reveal importance formal analysis. reason, ’s essential prematurely discard predictors simply relationship target variable isn’t immediately apparent EDA.","code":"\nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls), \n                 bins = 10, fill = \"skyblue\", color = \"black\")\nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls, fill = churn), position = \"stack\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n  \nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = day.mins), \n                 fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = day.mins, fill = churn), alpha = 0.3)\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = eve.mins), fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = eve.mins, fill = churn), alpha = 0.3)\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = night.mins), fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = night.mins, fill = churn), alpha = 0.3)"},{"path":"chapter-EDA.html","id":"EDA-sec-multivariate","chapter":"4 Exploratory Data Analysis","heading":"4.7 Envestigating Multivarate Relationships","text":"examine potential multivariate associations numeric variables churn using scatter plots. univariate exploration gives us insights individual variables, multivariate graphics can reveal interaction effects may apparent examining variables isolation.instance, consider scatter plot day.mins (Day Minutes) versus eve.mins (Evening Minutes), churn overlaid:Notice diagonal line partitioning plot. line, represented equation:\\[\n\\text{day.mins} = 400 - 0.6 \\times \\text{eve.mins}\n\\]separates upper-right section graph, observe higher concentration churners. region, encompassing customers high day minutes high evening minutes, appears high-churn zone compared rest data.investigate, isolate high-churn region visualize churn distribution within :high-churn subset, churn rate approximately 29%, significantly higher churn rate 86% observed across entire dataset—around five times overall churn rate. stark increase churn rate among customers high day evening usage suggests strong interaction effect wasn’t fully apparent univariate analyses.Interestingly, univariate exploration eve.mins alone clearly indicate high churn rate, multivariate analysis reveals evening minutes play role churn—particularly combined high day minutes. demonstrates value multivariate exploration uncovering hidden patterns interactions, providing us nuanced insights factors influencing customer churn.can investigate multivariate relationships examining scatter plot customer.calls versus day.mins, churn status overlaid:plot reveals interesting high-churn region upper left section, customers made numerous calls customer service relatively low day minutes. group customers, characterized frequent customer service interactions yet low usage, might represent dissatisfied segment prone leaving. insights difficult uncover univariate analysis alone, stem interaction two variables rather behavior one variable isolation.summary, previous univariate analysis showed customers make many calls customer service tend higher churn rate. However, scatter plot adds nuance finding. Among high-call customers, high day minutes seem somewhat “protected” churning—likely stay compared lower day minutes. words, upper right area plot, day minutes high customer calls also high, shows lower churn rate upper left, high customer calls coincide low day minutes.interaction suggests high usage (indicated high day minutes) may offset negative effect frequent customer service calls, perhaps reflecting engaged “sticky” customer segment. quantify observations scatter plot?next chapter, ’ll delve statistical techniques allow us rigorously test quantify multivariate relationships, determining whether significant predictive value churn model. help us transition exploratory insights actionable, data-driven decisions.","code":"\nggplot(data = churn) +\n    geom_point(aes(x = eve.mins, y = day.mins, color = churn), size = 0.7, alpha = 0.8) +\n    scale_color_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) +\n    geom_abline(intercept = 400, slope = -0.6, color = \"blue\", size = 1)\nsub_churn = subset(churn, (day.mins > 400 - 0.6 * eve.mins))\n\nggplot(data = sub_churn, aes(x = churn, label = scales::percent(prop.table(stat(count))))) +\n    geom_bar(fill = c(\"palevioletred1\", \"darkseagreen1\")) + \n    geom_text(stat = 'count', vjust = 0.2, size = 6)\nggplot(data = churn) +\n  geom_point(aes(x = day.mins, y = customer.calls, color = churn), alpha = 0.8) +\n  scale_color_manual(values = c(\"palevioletred1\", \"darkseagreen1\"))"},{"path":"chapter-EDA.html","id":"investigating-correlated-variables","chapter":"4 Exploratory Data Analysis","heading":"4.7.1 Investigating Correlated Variables","text":"data analysis, correlation measures relationship two variables—specifically, extent one variable changes relation another. Correlation helps us understand whether increases decreases one variable associated increases decreases another. three main types correlation outcomes: positive correlation (variables move direction), negative correlation (one variable increases decreases), correlation (clear relationship variables).example, ’ve ever wondered whether number hours study related exam scores, ’re thinking correlation. positive correlation mean hours studying associated higher scores, negative correlation imply studying associated lower scores (unusual, impossible). Correlation doesn’t imply causation, can reveal interesting associations worth exploring.strength direction linear relationship two variables \\(x\\) \\(y\\) can quantified using correlation coefficient \\(r\\). value ranges -1 1:\n- \\(r = 1\\) indicates perfect positive correlation, increases \\(x\\) always associated proportional increases \\(y\\).\n- \\(r = -1\\) indicates perfect negative correlation, increases \\(x\\) always associated proportional decreases \\(y\\).\n- \\(r = 0\\) suggests linear relationship \\(x\\) \\(y\\)., Figure 4.3 shows examples different correlation coefficients.\nFigure 4.3: Example scatterplots showing different correlation coefficients.\nlarge datasets—common data science—even small values \\(r\\) may statistically significant. datasets 1000 records, weak correlations (e.g., \\(-0.1 \\leq r \\leq 0.1\\)) can still hold statistical significance, though may always practical importance. evaluating correlations exploratory data analysis (EDA), ’s essential consider statistical practical implications.Using highly correlated predictors model can lead several issues:\n- Overemphasis Certain Data Patterns: Strongly correlated predictors can cause certain patterns data dominate model’s learning, potentially leading biased outcomes.\n- Instability Multicollinearity: Highly correlated predictors introduce redundancy, can destabilize models make interpretation difficult. problem, known multicollinearity, especially problematic linear models can result unreliable coefficient estimates.However, just two variables correlated doesn’t mean one automatically excluded model. Instead, EDA, ’s helpful adopt systematic approach managing correlated variables.handle correlated variables effectively EDA, consider following strategies:Identify Perfectly Correlated Variables: two variables perfect correlation (.e., \\(r = 1.0\\) \\(r = -1.0\\)), contain identical information. Including model redundant, retaining one sufficient.Identify Perfectly Correlated Variables: two variables perfect correlation (.e., \\(r = 1.0\\) \\(r = -1.0\\)), contain identical information. Including model redundant, retaining one sufficient.Group Highly Correlated Variables: Identify clusters variables strongly correlated . Later, modeling phase, can apply dimension reduction techniques, Principal Component Analysis (PCA), combine correlated variables smaller set uncorrelated components. approach reduces redundancy retaining essential information.Group Highly Correlated Variables: Identify clusters variables strongly correlated . Later, modeling phase, can apply dimension reduction techniques, Principal Component Analysis (PCA), combine correlated variables smaller set uncorrelated components. approach reduces redundancy retaining essential information.Consider Practical Relevance: Correlation alone dictate variable inclusion exclusion. example, two predictors highly correlated practical significance (e.g., age years experience), may choose retain evaluate impact modeling phase.Consider Practical Relevance: Correlation alone dictate variable inclusion exclusion. example, two predictors highly correlated practical significance (e.g., age years experience), may choose retain evaluate impact modeling phase.Note: strategy applies correlations among predictor variables, predictors target variable. Strong correlations predictor target variable often desirable, suggest predictor may informative modeling target.following steps, can reduce risk multicollinearity redundancy, helping ensure models reliable interpretable. approach allows EDA inform thoughtful, data-driven selection variables move modeling phase.Let’s apply process dataset. categories day, evening, night, international, three variables: minutes, calls, charge. According data description, charge variables likely calculated based minutes calls time period, naturally result high correlations . verify , ’ll compute correlation matrix variables visualize using correlation plot.correlation matrix reveals several key insights:\n- Perfect Correlations: observe perfect correlations (r = 1) day.mins day.charge, eve.mins eve.charge, night.mins night.charge, intl.mins intl.charge. indicates charge variable direct linear function corresponding minutes variable.\n- Correlation Minutes Calls: Interestingly, significant correlation minutes calls within time period (e.g., day.mins day.calls uncorrelated). one might expect customers making calls also spend minutes phone, data support assumption.Since day.charge, eve.charge, night.charge, intl.charge perfectly correlated respective minutes variables, remove one pair avoid redundancy. ’ll retain minutes variables eliminate charge variables, thus reducing number predictors 20 16. helps prevent issues multicollinearity reduces dimensionality data, can improve model efficiency.proceeded modeling phase without identifying addressing correlations, models produced unreliable results due multicollinearity. High multicollinearity can distort relationships variables model, making challenging determine true effect predictor. removing redundant variables, streamline model focus meaningful predictors, leading stable interpretable results.Important Note Correlation vs. Causation: Remember correlation imply causation. Just two variables correlated mean one causes . Causation implies changes one variable directly result changes , correlation simply indicates association. Always cautious interpreting correlations, especially considering causal inferences.summary, carefully investigating addressing correlated variables EDA, enhance reliability interpretability models. approach ensures modeling process data-driven thoughtful, setting solid foundation analyses insights follow.","code":"\nlibrary(ggcorrplot) # For correlation plots (ggcorrplot)\nvariable_list = c(\"intl.mins\",  \"intl.calls\",  \"intl.charge\", \n                  \"day.mins\",   \"day.calls\",   \"day.charge\",\n                  \"eve.mins\",   \"eve.calls\",   \"eve.charge\",\n                  \"night.mins\", \"night.calls\", \"night.charge\")\n\ncor_matrix = cor(churn[, variable_list])\n\nggcorrplot(cor_matrix, type = \"lower\", lab = TRUE, lab_size = 3)"},{"path":"chapter-EDA.html","id":"key-findings-and-insights","chapter":"4 Exploratory Data Analysis","heading":"4.8 Key Findings and Insights","text":"thorough exploratory data analysis (EDA) churn dataset, gained valuable insights factors may influence customer attrition. examining variable individually combination others, identified patterns relationships inform predictive modeling guide potential strategies reducing churn. key takeaways analysis:Redundant Variables:charge fields (day, evening, night, international) perfectly correlated respective minute fields, linear functions variables. avoid redundancy multicollinearity, exclude charge fields analysis, retaining minute variables.area code state fields appear add little value may redundant anomalous. Unless clarification purpose obtained, fields omitted model.Churn Insights:International Plan: Customers subscribed International Plan show significantly higher churn rate. suggests plan may meeting customer expectations, may attracting customer segment prone switching providers.Voice Mail Plan: contrast, customers Voice Mail Plan tend churn less frequently, indicating service may enhance customer loyalty.Customer Service Calls: high number customer service calls (four ) strongly associated higher churn. Customers contact customer service multiple times may experiencing unresolved issues dissatisfaction, ultimately leads leave.High Day Evening Minutes: Customers high day minutes high evening minutes churn elevated rate—six times overall churn rate. suggests heavy usage time periods may correlate dissatisfaction, possibly due cost quality issues.Combination Low Day Minutes High Customer Service Calls: Customers make frequent customer service calls relatively low day minutes also display higher churn rate. may indicate dissatisfaction among lower-usage customers still face issues requiring support.International Calls: Interestingly, customers fewer international calls tend churn frequently make international calls. suggest international usage may valuable service differentiator certain customer segments.Variables:remaining predictors, obvious association churn identified EDA. However, variables retained potential inputs downstream data mining models, may still contain useful predictive information combination variables within specific subsets data.exploratory analysis highlighted power EDA uncovering actionable insights applying advanced modeling techniques. Without sophisticated data mining algorithms, already identified several customer behaviors attributes correlate churn. findings provide solid foundation next steps analysis, involve applying predictive modeling techniques confirm quantify relationships observed.Additionally, insights EDA can translated actionable recommendations. instance:\n- company investigate International Plan understand correlates higher churn consider adjustments make appealing.\n- Specialized retention strategies deployed customers make frequent customer service calls, potentially offering tailored support resolve issues decide leave.\n- High-usage customers, particularly high day evening minutes, targeted incentives loyalty programs address dissatisfaction related cost service quality.identifying understanding patterns, company can take proactive steps reduce churn improve customer satisfaction, ultimately leading loyal customer base.","code":""},{"path":"chapter-statistics.html","id":"chapter-statistics","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5 Statistical Inference and Hypothesis Testing","text":"Statistical inference bridges gap observe sample want understand population. allows us assess whether patterns observed EDA reflect true relationships broader population—whether ’re just result chance. chapter, ’ll focus practical side statistical inference: ask right questions, apply appropriate techniques, use results guide meaningful decisions.goals statistical inference can summarized three fundamental tasks:Estimating unknown population characteristics, averages proportions.Quantifying Uncertainty measure confident can results.Testing Hypotheses evaluate whether observed patterns statistically meaningful simply due random variation.tasks lie heart data analysis, providing foundation robust conclusions data-driven decision-making. chapter, ’ll explore three pillars—estimation, uncertainty, hypothesis testing—using intuitive explanations practical examples.statistical inference isn’t just learning techniques—’s also critical thinking. end chapter, ’ll learn two essential skills:detect others misusing statistics, can identify misleading claims.avoid statistical missteps , , ’re feeling mischievous, “lie” statistics effectively.intrigued art spotting statistical trickery, consider reading Darrell Huff’s classic book, Lie Statistics. Although written humor journalistic insight, offers timeless lessons statistical skepticism—valuable skill age data overload.Let’s dive learn make inferences confidence, curiosity, just touch caution.","code":""},{"path":"chapter-statistics.html","id":"estimation-using-data-to-make-predictions","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.1 Estimation: Using Data to Make Predictions","text":"Estimation addresses question: can infer population based sample? example, churn dataset, may want estimate:average number customer service calls among churners.proportion customers subscribed International Plan.Estimation comes two main forms:Point Estimation: single best guess population parameter (e.g., sample mean proportion).Interval Estimation: range values (confidence interval) likely contain true population parameter.Let’s explore examples:Example 5.1  estimate proportion churners dataset, use sample proportion point estimate population proportion. ’s calculate R:proportion churners dataset 0.14. serves best single guess proportion churners population.Example 5.2  let’s estimate average number customer service calls customers churned. sample mean acts point estimate population mean:mean 4 calls, best single guess average number customer service calls among churners population.Key Insight: point estimates informative, don’t tell us precise reliable . , turn confidence intervals.","code":"\n# Calculate the proportion of churners\nprop.table(table(churn$churn))[\"yes\"]\n      yes \n   0.1414# Filter churners\nchurned_customers <- churn[churn$churn == \"yes\", ]\n\n# Calculate the mean\nmean_calls <- mean(churned_customers$customer.calls)\ncat(\"Point Estimate: Average Customer Service Calls for Churners:\", mean_calls)\n   Point Estimate: Average Customer Service Calls for Churners: 2.254597"},{"path":"chapter-statistics.html","id":"quantifying-uncertainty-confidence-intervals","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.2 Quantifying Uncertainty: Confidence Intervals","text":"Confidence intervals (CIs) provide way quantify uncertainty offering range plausible values population parameter. Instead saying, “average number customer service calls 4,” confidence interval might state, “95% confident true average 3.8 4.2.”confidence interval combines:point estimate (e.g., sample mean proportion).margin error, accounts variability uncertainty.general form confidence interval :\\[\n\\text{Point Estimate}  \\pm \\text{Margin Error}\n\\]example, confidence interval population mean calculated :\\[\n\\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\times \\left( \\frac{s}{\\sqrt{n}} \\right),\n\\]\nsample mean \\(\\bar{x}\\) point estimate quantity \\(z_{\\frac{\\alpha}{2}} \\times \\left( \\frac{s}{\\sqrt{n}} \\right)\\) margin error. z-score \\(z_{\\frac{\\alpha}{2}}\\) determined desired confidence level (e.g., 1.96 95% confidence), \\(s\\) sample standard deviation, \\(n\\) = sample size.visually represented 5.1, showing interval centered around point estimate width determined margin error.\nFigure 5.1: Confidence interval population mean. interval centered around point estimate, width determined margin error. confidence level specifies probability interval contains true population parameter.\nKey Factors Influence Confidence Intervals:Sample Size: Larger samples yield narrower intervals, increasing precision.Variability: Higher variability data results wider intervals.Confidence Level: Higher confidence levels (e.g., 99%) lead wider intervals lower levels (e.g., 90%).Example 5.3  Let’s calculate 95% confidence interval average number customer service calls among churners:confidence interval [ 2.12, 2.39 ], 95% confident true average lies within range.smaller sample sizes, use t-distribution instead normal distribution. t-distribution adjusts added uncertainty estimating population standard deviation. can calculate confidence intervals small samples R using t.test() function:approach automatically adjusts sample size underlying variability data, making robust alternative manual calculations.Interpretation: confidence intervals different groups (e.g., churners vs. non-churners) don’t overlap significantly, suggests meaningful differences behavior groups.summary, confidence intervals go beyond point estimates providing range plausible values population parameter, helping us account uncertainty predictions. Narrower intervals indicate greater precision, achieved larger sample sizes lower variability data. Confidence levels, 95%, quantify degree certainty interval contains true population parameter. smaller sample sizes, t-distribution offers reliable approach adjusting additional uncertainty inherent limited data. Confidence intervals, therefore, serve critical tool balancing precision uncertainty statistical inference.","code":"# Calculate mean and standard error\nmean_calls <- mean(churned_customers$customer.calls)\nse_calls <- sd(churned_customers$customer.calls) / sqrt(nrow(churned_customers))\n\n# Confidence Interval\nz_score <- 1.96  # For 95% confidence\nci_lower <- mean_calls - z_score * se_calls\nci_upper <- mean_calls + z_score * se_calls\n\ncat(\"95% Confidence Interval: [\", ci_lower, \",\", ci_upper, \"]\")\n   95% Confidence Interval: [ 2.120737 , 2.388457 ]t.test(churned_customers$customer.calls, conf.level = 0.95)$conf.int\n   [1] 2.120509 2.388685\n   attr(,\"conf.level\")\n   [1] 0.95"},{"path":"chapter-statistics.html","id":"hypothesis-testing","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3 Hypothesis Testing","text":"Hypothesis testing cornerstone inferential statistics, providing structured framework evaluating claims assumptions population parameters based sample data. allows us assess whether observed patterns data statistically significant merely result random chance. process lies heart data-driven decision-making, empowering us separate meaningful insights noise.core, hypothesis testing involves formulating two competing statements population parameter:Null Hypothesis (\\(H_0\\)): Represents default assumption status quo. example, might claim difference two groups, effect treatment, relationship variables.Alternative Hypothesis (\\(H_a\\)): Represents competing claim challenges null hypothesis. instance, might state difference, effect, relationship.goal hypothesis testing use evidence sample decide whether :Reject \\(H_0\\): Conclude evidence supports \\(H_a\\), null hypothesis unlikely true.Fail reject \\(H_0\\): Conclude insufficient evidence refute \\(H_0\\), though prove \\(H_0\\) true.make decisions, calculate measure evidence null hypothesis: p-value.p-value quantifies strength evidence \\(H_0\\). Specifically, represents probability observing sample data—something extreme—null hypothesis (\\(H_0\\)) true. Smaller p-values indicate stronger evidence \\(H_0\\), observed data highly unlikely assumption \\(H_0\\) true. interpret p-value follows:Small p-value (e.g., < 0.05): observed data unlikely \\(H_0\\). reject \\(H_0\\) conclude evidence support \\(H_a\\).Large p-value (e.g., > 0.05): observed data consistent \\(H_0\\). fail reject \\(H_0\\) conclude insufficient evidence support \\(H_a\\).p-value compared predefined threshold known significance level (\\(\\alpha\\)), commonly set 0.05 (5%). significance level represents maximum probability committing Type error—rejecting \\(H_0\\) actually true—willing tolerate. certain fields, medicine aerospace, cost Type error especially high, stricter thresholds (e.g., \\(\\alpha = 0.01\\)) may used minimize risk.leads us simple yet crucial takeaway, often referred key decision rule hypothesis testing. often tell students remember core message chapter:Reject \\(H_0\\) \\(p\\)-value < \\(\\alpha\\).Let’s see works practice:\\(p = 0.03\\) \\(\\alpha = 0.05\\): Reject \\(H_0\\) \\(p < \\alpha\\). evidence \\(H_0\\) strong enough conclude alternative hypothesis supported.\\(p = 0.12\\) \\(\\alpha = 0.05\\): Fail reject \\(H_0\\) \\(p > \\alpha\\). evidence insufficient refute \\(H_0\\), though prove \\(H_0\\) true.decision-making framework ensures hypothesis testing remains consistent, objective, aligned predefined level risk willing accept making errors.p-values useful tool, without limitations:p-value ≠ Importance: small p-value mean effect practically significant—indicates statistical significance. example, p-value 0.02 might suggest statistically detectable difference, effect size trivial, may justify action.Dependent Sample Size: Large samples can produce small p-values even negligible effects, small samples may fail detect meaningful differences.Binary Nature: dichotomous “reject/fail reject” decision oversimplifies data, often requires nuanced interpretation.Key Insight: p-values provide measure surprising sample data \\(H_0\\), must used alongside confidence intervals, effect sizes, domain knowledge robust conclusions.Hypothesis tests can take three forms depending research question nature alternative hypothesis (\\(H_a\\)):Left-Tailed Test: alternative hypothesis states parameter less null hypothesis value (\\(H_a: \\theta < \\theta_0\\)). type test focuses lower (left) tail distribution.\nExample: Testing whether average number customer service calls less 3.Left-Tailed Test: alternative hypothesis states parameter less null hypothesis value (\\(H_a: \\theta < \\theta_0\\)). type test focuses lower (left) tail distribution.\nExample: Testing whether average number customer service calls less 3.Right-Tailed Test: alternative hypothesis states parameter greater null hypothesis value (\\(H_a: \\theta > \\theta_0\\)). test focuses upper (right) tail distribution.\nExample: Testing whether churn rate greater 30%.Right-Tailed Test: alternative hypothesis states parameter greater null hypothesis value (\\(H_a: \\theta > \\theta_0\\)). test focuses upper (right) tail distribution.\nExample: Testing whether churn rate greater 30%.Two-Tailed Test: alternative hypothesis states parameter equal null hypothesis value (\\(H_a: \\theta \\neq \\theta_0\\)). test evaluates tails distribution determine whether parameter either significantly lower higher null value.\nExample: Testing whether mean monthly charges different $50.Two-Tailed Test: alternative hypothesis states parameter equal null hypothesis value (\\(H_a: \\theta \\neq \\theta_0\\)). test evaluates tails distribution determine whether parameter either significantly lower higher null value.\nExample: Testing whether mean monthly charges different $50.helpful analogy hypothesis testing criminal trial. null hypothesis (\\(H_0\\)) represents presumption innocence, alternative hypothesis (\\(H_a\\)) represents guilt. jury must weigh evidence decide whether reject \\(H_0\\) (declare guilt) fail reject \\(H_0\\) (declare innocence due insufficient evidence). Just jury can make errors, can hypothesis tests, possible outcomes summarized Table 5.1.Table 5.1:  Possible outcomes hypothesis testing two correct decisions two types errors.Type Error (\\(\\alpha\\)) occurs \\(H_0\\) rejected even though true, akin convicting innocent person. significance level (\\(\\alpha\\)), typically set 0.05, controls probability error. Conversely, Type II Error (\\(\\beta\\)) happens \\(H_0\\) rejected even though false, akin acquitting guilty person. likelihood Type II error depends factors sample size power test.chapter introduces seven widely used hypothesis tests (Table 5.2) applied across various data types scenarios. test paired practical examples demonstrate application interpretation. end section, tools confidently test hypotheses make informed decisions based statistical evidence.Table 5.2:  Seven commonly used hypothesis tests, null hypotheses (\\(H_0\\)), types variables apply .Let’s dive test practical example.","code":""},{"path":"chapter-statistics.html","id":"one-sample-t-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.1 One-sample T-test","text":"one-sample t-test evaluates whether mean (\\(\\mu\\)) numerical variable population equal specified value (\\(\\mu_0\\)). often used compare sample mean benchmark target. term “one-sample” refers fact comparing sample mean single specified value, “t-test” indicates test statistic follows t-distribution, used calculate \\(p\\)-value.null hypothesis (\\(H_0\\)) alternative hypothesis (\\(H_a\\)) formulated based research question, can take following forms:Two-Tailed Test:\n\\[\n\\bigg\\{\n\\begin{matrix}\n        H_0:  \\mu   =  \\mu_0 \\\\\n        H_a:  \\mu \\neq \\mu_0\n\\end{matrix}\n\\]Left-Tailed Test:\n\\[\n\\bigg\\{\n\\begin{matrix}\n        H_0:  \\mu \\geq \\mu_0 \\\\\n        H_a:  \\mu  <   \\mu_0\n\\end{matrix}\n\\]Right-Tailed Test:\n\\[\n\\bigg\\{\n\\begin{matrix}\n        H_0:  \\mu \\leq \\mu_0 \\\\\n        H_a:  \\mu >   \\mu_0\n\\end{matrix}\n\\]\\(p\\)-value represents probability observing sample mean (something extreme) assumption null hypothesis true. smaller \\(p\\)-value provides stronger evidence \\(H_0\\). \\(p\\)-value less significance level (\\(\\alpha = 0.05\\)), reject \\(H_0\\) conclude sample mean differs significantly specified value. Otherwise, fail reject \\(H_0\\).Example 5.4  Suppose company believes , average, customers make 2 service calls churning. want test whether true average number customer service calls among churners differs value.conduct test, set following hypotheses:Null Hypothesis (\\(H_0\\)): \\(H_0: \\mu = 2\\) (average number customer service calls 2.)Alternative Hypothesis (\\(H_a\\)): \\(H_a: \\mu \\neq 2\\) (average number customer service calls 2.)perform two-tailed one-sample t-test R using t.test() function. ’s implemented:output t-test includes p-value, test statistic, degrees freedom, confidence interval population mean. Let’s interpret results step step:p-value = 2^{-4} less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). indicate sufficient evidence, 5% significance level, conclude true average number customer service calls differs 2.Conversely, \\(p\\)-value greater 0.05, fail reject \\(H_0\\), concluding insufficient evidence support true mean different 2.example, p-value small enough reject \\(H_0\\), conclude:“sufficient evidence, 5% significance level, conclude population mean number customer service calls among churners different 2.”\nfail reject \\(H_0\\), conclusion phrased :“insufficient evidence, 5% significance level, conclude population mean number customer service calls among churners differs 2.”Additionally, test output provides following useful information:\n- 95% Confidence Interval = [2.12, 2.39]: interval represents range plausible values true population mean. value 2 lies outside interval, reinforces rejection \\(H_0\\).\n- Sample Mean = 2.25: point estimate population mean, calculated directly sample.side note, test statistic used compute p-value follows t-distribution \\(n - 1\\) degrees freedom (df). case, degrees freedom 706, depend sample size. test statistic 3.73, quantifies far sample mean deviates hypothesized mean (\\(2\\)) units standard error. larger absolute value test statistic indicates stronger evidence \\(H_0\\).summarize: one-sample t-test tells us whether reject \\(H_0\\), also provides additional insights confidence interval, sample mean, test statistic, giving comprehensive view data strength evidence.","code":"# Filter churned customers\nchurned_customers <- churn[churn$churn == \"yes\", ]\n\n# Perform One-sample T-test\nt_test <- t.test(churned_customers$customer.calls, mu = 2)\nt_test\n   \n    One Sample t-test\n   \n   data:  churned_customers$customer.calls\n   t = 3.7278, df = 706, p-value = 0.0002086\n   alternative hypothesis: true mean is not equal to 2\n   95 percent confidence interval:\n    2.120509 2.388685\n   sample estimates:\n   mean of x \n    2.254597"},{"path":"chapter-statistics.html","id":"hypothesis-testing-for-proportion","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.2 Hypothesis Testing for Proportion","text":"test proportion determines whether proportion (\\(\\pi\\)) category population matches hypothesized value (\\(\\pi_0\\)). especially useful binary categorical variables, observation falls one two categories (e.g., churned vs. churned). test allows us assess whether observed sample proportion deviates significantly specified benchmark, making practical tool business scientific contexts.example, company might want evaluate whether proportion churners population aligns expected value based historical data industry standards.Example 5.5  company estimates 15% customers churn. aim test whether actual proportion churners dataset differs estimate.Hypotheses:\n1. Null Hypothesis (\\(H_0\\)): \\(\\pi = 0.15\\) (population proportion churners 15%.)\n2. Alternative Hypothesis (\\(H_a\\)): \\(\\pi \\neq 0.15\\) (population proportion churners 15%.)can perform proportion test R using prop.test() function follows:Interpreting Output:P-value:\np-value = 0.0923 greater \\(\\alpha = 0.05\\), fail reject null hypothesis. means insufficient evidence conclude proportion churners population differs 15%. case, report:“statistically significant evidence suggest population proportion churners deviates 15%.”p-value smaller 0.05, reject null hypothesis conclude proportion churners significantly different 15%.Confidence Interval:\ntest output provides 95% confidence interval = [0.13, 0.15], represents plausible range true population proportion (\\(\\pi\\)). hypothesized value 0.15 lies within interval, supports failing reject \\(H_0\\). hand, 0.15 lies outside interval, strengthens case rejecting \\(H_0\\).Confidence Interval:\ntest output provides 95% confidence interval = [0.13, 0.15], represents plausible range true population proportion (\\(\\pi\\)). hypothesized value 0.15 lies within interval, supports failing reject \\(H_0\\). hand, 0.15 lies outside interval, strengthens case rejecting \\(H_0\\).Sample Proportion:\ntest also provides sample proportion = 0.14, point estimate population proportion (\\(\\pi\\)). value represents observed proportion churners dataset, calculated directly sample.Sample Proportion:\ntest also provides sample proportion = 0.14, point estimate population proportion (\\(\\pi\\)). value represents observed proportion churners dataset, calculated directly sample.summary, hypothesis test helps determine whether observed proportion churners aligns company’s estimate 15%. confidence interval sample proportion provide additional context, reinforcing conclusion drawn p-value. Furthermore, approach can extended one-tailed tests (e.g., testing whether churn rate higher lower 15%) used different confidence levels depending application.","code":"# Perform a proportion test\nprop_test <- prop.test(x = sum(churn$churn == \"yes\"), \n                       n = nrow(churn), \n                       p = 0.15)\nprop_test\n   \n    1-sample proportions test with continuity correction\n   \n   data:  sum(churn$churn == \"yes\") out of nrow(churn), null probability 0.15\n   X-squared = 2.8333, df = 1, p-value = 0.09233\n   alternative hypothesis: true p is not equal to 0.15\n   95 percent confidence interval:\n    0.1319201 0.1514362\n   sample estimates:\n        p \n   0.1414"},{"path":"chapter-statistics.html","id":"two-sample-t-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.3 Two-sample T-test","text":"two-sample t-test, also known Student’s t-test, statistical method used compare means numerical variable two independent groups. evaluates whether observed difference group means statistically significant simply due random variation. test named William Sealy Gosset, worked Guinness Brewery Dublin. Gosset published findings pseudonym “Student” employer wanted maintain secrecy innovative use statistics quality control. historical context highlights t-test’s original application solving practical, real-world problems, evaluating raw materials small samples.context churn dataset, can use two-sample t-test determine whether number international calls differs significantly customers churned . Understanding differences can help businesses identify potential predictors churn design effective interventions.conduct two-sample t-test, first establish hypotheses:Null Hypothesis (\\(H_0\\)): mean number international calls churners non-churners (\\(\\mu_1 = \\mu_2\\)).Alternative Hypothesis (\\(H_a\\)): mean number international calls differs churners non-churners (\\(\\mu_1 \\neq \\mu_2\\)).can also expressed mathematically :\n\\[\n\\bigg\\{\n\\begin{matrix}\n    H_0: \\mu_1 = \\mu_2   \\\\\n    H_a: \\mu_1 \\neq \\mu_2\n\\end{matrix}\n\\]begin, let’s visually explore relationship International Calls (intl.calls) churn status using boxplot:boxplot compares distribution international calls churners non-churners. medians spreads differ substantially, may indicate variable predictive importance. case, plot reveal strong visual evidence difference, formally assess , proceed two-sample t-test.perform test R, use t.test() function:function evaluates difference means two groups (churn = \"yes\" vs. churn = \"\") calculates test statistic corresponding p-value. Based output:p-value = 0.0014. Since value less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). indicates mean number international calls differs significantly churners non-churners.p-value = 0.0014. Since value less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). indicates mean number international calls differs significantly churners non-churners.test also provides 95% confidence interval = [-0.53, -0.13] difference means. Since interval include zero, reinforces conclusion difference statistically significant. confidence interval also quantifies magnitude difference two groups.test also provides 95% confidence interval = [-0.53, -0.13] difference means. Since interval include zero, reinforces conclusion difference statistically significant. confidence interval also quantifies magnitude difference two groups.Additionally, sample means two groups reported output:\nMean churners = 4.15\nMean non-churners = 4.48\nAdditionally, sample means two groups reported output:Mean churners = 4.15Mean non-churners = 4.48These sample means allow us directly compare average number international calls churners non-churners. example, churners made average 1.5 international calls non-churners made 2.3 calls, suggests churners tend make fewer international calls.two-sample t-test assumes two groups independent, numerical variable compared (e.g., intl.calls) follows approximately normal distribution within group. test robust minor deviations normality, assumptions always checked, especially small sample sizes.practical standpoint, result suggests number international calls significant predictor churn. Customers churn tend make fewer international calls average. insight can help businesses develop targeted strategies, offering discounts improved international calling plans customers show low usage. interventions potentially reduce churn rates addressing factors associated customer dissatisfaction.Finally, ’s worth noting example uses two-tailed test detect difference means (higher lower), one-tailed tests used research question specifies directional hypothesis. example, company specifically hypothesizes churners make fewer international calls non-churners, one-tailed test performed increase test’s sensitivity.summary, two-sample t-test powerful versatile tool comparing group means uncovering meaningful differences data. combining graphical exploration statistical testing, can make robust inferences translate actionable business insights.","code":"\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = intl.calls), fill = c(\"palevioletred1\", \"darkseagreen1\"))t_test_calls <- t.test(intl.calls ~ churn, data = churn)\nt_test_calls\n   \n    Welch Two Sample t-test\n   \n   data:  intl.calls by churn\n   t = -3.2138, df = 931.13, p-value = 0.001355\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    -0.5324872 -0.1287201\n   sample estimates:\n   mean in group yes  mean in group no \n            4.151344          4.481947"},{"path":"chapter-statistics.html","id":"two-sample-z-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.4 Two-Sample Z-test","text":"two-sample Z-test used compare proportions two groups determine whether observed difference proportions statistically significant. particularly useful binary categorical variables, goal evaluate whether proportions success (presence) one group differ another.context churn dataset, can apply two-sample Z-test investigate whether relationship target variable churn variable Voice Mail Plan (voice.plan). Specifically, aim test whether proportion churners Voice Mail Plan differs proportion non-churners plan.First, let’s visualize relationship Voice Mail Plan churn using bar plots:first bar plot shows raw counts churners non-churners across two categories Voice Mail Plan (Yes ), second plot provides proportions, making easier compare relative churn rates within category. visualizations suggest proportion churners might differ based whether Voice Mail Plan, formal hypothesis test needed confirm .test whether proportions significantly different, set following hypotheses:Null Hypothesis (\\(H_0\\)): \\(\\pi_1 = \\pi_2\\)\n(proportions customers Voice Mail Plan churners non-churners.)Null Hypothesis (\\(H_0\\)): \\(\\pi_1 = \\pi_2\\)\n(proportions customers Voice Mail Plan churners non-churners.)Alternative Hypothesis (\\(H_a\\)): \\(\\pi_1 \\neq \\pi_2\\)\n(proportions customers Voice Mail Plan differ churners non-churners.)Alternative Hypothesis (\\(H_a\\)): \\(\\pi_1 \\neq \\pi_2\\)\n(proportions customers Voice Mail Plan differ churners non-churners.)can also expressed mathematically :\n\\[\n\\bigg\\{\n\\begin{matrix}\n    H_0: \\pi_1 = \\pi_2   \\\\\n    H_a: \\pi_1 \\neq \\pi_2\n\\end{matrix}\n\\]perform Z-test R, first create contingency table summarize counts customers without Voice Mail Plan churner non-churner groups. can done using table() function:table displays count customers combination churn voice.plan. example, might show many churners non-churners subscribed Voice Mail Plan versus many .Next, apply prop.test() function conduct two-sample Z-test difference proportions:output test provides p-value, estimated proportions group, confidence interval difference proportions. Based result:p-value (0) less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). indicates difference proportions statistically significant, meaning proportion customers Voice Mail Plan differs churners non-churners.p-value (0) less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). indicates difference proportions statistically significant, meaning proportion customers Voice Mail Plan differs churners non-churners.test also provides 95% confidence interval = [-0.1702, -0.1101] difference proportions. interval contain zero, confirms proportions significantly different.test also provides 95% confidence interval = [-0.1702, -0.1101] difference proportions. interval contain zero, confirms proportions significantly different.Additionally, sample proportions churners (0.1443) non-churners (0.2844) reported output. represent observed proportions customers Voice Mail Plan group allow direct comparison.Since p-value less 0.05, reject \\(H_0\\) conclude sufficient evidence suggest proportion Voice Mail Plan members differs churners non-churners. result indicates variable Voice Mail Plan indeed useful predicting churn.business perspective, insight suggests customers without Voice Mail Plan may likely churn. Companies leverage information promoting Voice Mail Plans customers risk leaving investigating whether feature associated improved customer satisfaction retention.summary, two-sample Z-test provides formal method comparing proportions two groups. combining visual exploration hypothesis testing, can identify significant relationships use findings inform business strategies statistical modeling.","code":"\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) table_plan = table(churn$churn, churn$voice.plan, dnn = c(\"churn\", \"voice.plan\"))\ntable_plan\n        voice.plan\n   churn  yes   no\n     yes  102  605\n     no  1221 3072z_test = prop.test(table_plan)\nz_test\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  table_plan\n   X-squared = 60.552, df = 1, p-value = 7.165e-15\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.1701734 -0.1101165\n   sample estimates:\n      prop 1    prop 2 \n   0.1442716 0.2844165"},{"path":"chapter-statistics.html","id":"chi-square-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.5 Chi-square Test","text":"Chi-square test used evaluate whether association two categorical variables. assesses whether observed frequencies category differ significantly expected assumption independence. test particularly useful variables two categories, derives name Chi-square (\\(\\chi^2\\)) distribution based.illustrate, let’s analyze whether relationship variable marital target variable deposit bank dataset (available liver package). variable marital three categories: “divorced,” “married,” “single,” target variable deposit two categories: “yes” (customers purchased deposit) “” (customers ). goal determine whether marital status customers associated decision make deposit.start visualizing relationship marital deposit using bar plots:first bar plot shows raw counts deposits across marital categories, second plot shows proportions within marital group. visualizations suggest marital status might influence likelihood making deposit, formal hypothesis test needed confirm .create contingency table summarize counts observations across categories marital deposit:table provides observed frequencies deposits (“yes” “”) across marital categories. test whether proportions differ significantly, use Chi-square test following hypotheses:Null Hypothesis (\\(H_0\\)): proportions deposits across marital categories.\nMathematically:\\[\n\\pi_{divorced, \\ yes} = \\pi_{married, \\ yes} = \\pi_{single, \\ yes}\n\\]Alternative Hypothesis (\\(H_a\\)): least one proportions differs others.hypotheses can also expressed :\n\\[\n\\bigg\\{\n\\begin{matrix}\n    H_0: \\text{Deposit rates independent marital status.} \\\\\n    H_a: \\text{Deposit rates depend marital status.}\n\\end{matrix}\n\\]apply Chi-square test using chisq.test() function R:output provides Chi-square test statistic, degrees freedom, p-value. , p-value = 7.3735354^{-5} smaller significance level \\(\\alpha = 0.05\\). Therefore, reject null hypothesis (\\(H_0\\)) conclude statistically significant association marital status deposit behavior. words, proportion deposits differs across marital categories.Additionally, output includes expected frequencies null hypothesis, can compared observed frequencies assess differences lie. insights can guide analysis, investigating marital group contributes association.business perspective, result indicates marital status useful predictor whether customer purchase deposit. Marketing strategies can leverage information tailoring campaigns offers specific marital groups increase deposit adoption rates.summary, Chi-square test powerful tool assessing relationships categorical variables. combining visualizations, contingency tables, formal hypothesis testing, can draw meaningful conclusions associations data apply insights improve decision-making.","code":"\nggplot(data = bank) + \n    geom_bar(aes(x = marital, fill = deposit)) +\n    scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = bank) + \n    geom_bar(aes(x = marital, fill = deposit), position = \"fill\") +\n    scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) table_marital <- table(bank$deposit, bank$marital, dnn = c(\"deposit\", \"marital\"))\ntable_marital\n          marital\n   deposit divorced married single\n       no       451    2520   1029\n       yes       77     277    167chisq_test <- chisq.test(table_marital)\nchisq_test\n   \n    Pearson's Chi-squared test\n   \n   data:  table_marital\n   X-squared = 19.03, df = 2, p-value = 7.374e-05"},{"path":"chapter-statistics.html","id":"analysis-of-variance-anova-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.6 Analysis of Variance (ANOVA) Test","text":"Analysis Variance (ANOVA) test used compare means numerical variable across two groups. evaluates whether least one group mean differs significantly others. ANOVA especially useful analyzing relationship numerical variable categorical variable multiple levels, providing formal way determine categorical variable impacts numerical variable. test relies F-distribution assess whether observed differences means statistically significant.illustrate, let’s analyze relationship variable cut target variable price popular diamonds dataset (available ggplot2 package). variable cut five categories (“Fair,” “Good,” “Good,” “Premium,” “Ideal”), price numerical. objective test whether mean price diamonds differs across five cut categories.begin box plot visualize distribution diamond prices category cut:box plot displays spread median prices diamonds cut category. distributions appear distinct—example, noticeable differences medians ranges—suggests cut might influence price. However, visual inspection alone insufficient confirm whether differences statistically significant, proceed ANOVA test.formally test whether mean prices differ cut type, set following hypotheses:Null Hypothesis (\\(H_0\\)): group means equal.\nMathematically:\\[\n\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5\n\\]\n(average prices across cut types.)Null Hypothesis (\\(H_0\\)): group means equal.\nMathematically:\\[\n\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5\n\\]\n(average prices across cut types.)Alternative Hypothesis (\\(H_a\\)): least one group mean different.\n(average prices equal across cut categories.)Alternative Hypothesis (\\(H_a\\)): least one group mean different.\n(average prices equal across cut categories.)conduct ANOVA test R, use aov() function follows:output provides test statistic (F-value), degrees freedom, p-value. p-value smaller significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). instance, p-value = 8.4283073^{-150}, reject \\(H_0\\) conclude group means equal. indicates variable cut significant impact price diamonds.’s important note rejecting \\(H_0\\) ANOVA doesn’t identify specific groups differ. determine , can conduct post-hoc tests, Tukey’s Honestly Significant Difference (Tukey HSD) test, pinpoint pairs categories significant differences means. example, apply Tukey’s test identify cut categories (e.g., “Ideal” vs. “Good”) drive observed differences.summary, ANOVA test confirms whether categorical variable multiple levels influences numerical variable. case, relationship cut price suggests diamond cut type important predictor price, providing valuable insight quality impacts cost.","code":"\nggplot(data = diamonds) + \n  geom_boxplot(aes(x = cut, y = price, fill = cut)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\", \"skyblue1\", \"gold1\", \"lightcoral\"))# Perform ANOVA\nanova_test <- aov(price ~ cut, data = diamonds)\nsummary(anova_test)\n                  Df    Sum Sq   Mean Sq F value Pr(>F)    \n   cut             4 1.104e+10 2.760e+09   175.7 <2e-16 ***\n   Residuals   53935 8.474e+11 1.571e+07                   \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"chapter-statistics.html","id":"correlation-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.7 Correlation Test","text":"correlation test determines whether significant linear relationship two numerical variables testing null hypothesis population correlation coefficient (\\(\\rho\\)) equal zero. test evaluates direction strength relationship variables.diamonds dataset, let’s explore whether significant relationship variable carat (diamond weight) target variable price (diamond price). Visualizing relationship two variables often first step correlation analysis. scatter plot illustrating relationship:scatter plot shows clear upward trend, suggesting positive relationship carat price—weight diamond increases, price. formally test whether observed pattern statistically significant, establish following hypotheses:Null Hypothesis (\\(H_0\\)): \\(\\rho = 0\\) (linear correlation variables.)Alternative Hypothesis (\\(H_a\\)): \\(\\rho \\neq 0\\) (significant linear correlation variables.)hypotheses can expressed :\n\\[\n\\bigg\\{\n\\begin{matrix}\n    H_0: \\rho   =  0 \\\\\n    H_a: \\rho \\neq 0\n\\end{matrix}\n\\]perform correlation test R, use cor.test() function:output correlation test includes p-value, quantifies evidence null hypothesis. p-value = 0 smaller significance level (\\(\\alpha = 0.05\\)), reject \\(H_0\\). case, test result indicates strong evidence significant relationship carat price.test output also provides additional insights:Correlation Coefficient: correlation coefficient (\\(r = 0.92\\)) measures strength direction relationship. positive value near 1 indicates strong positive correlation.95% Confidence Interval: confidence interval [0.92, 0.92] provides plausible range true population correlation (\\(\\rho\\)). interval include 0, reinforces rejection \\(H_0\\) confirms presence significant correlation.correlation coefficient 0.92 indicates strong positive linear relationship carat price. Larger diamonds associated higher prices, aligns intuition business practices diamond industry. small p-value confirms relationship statistically significant, due random chance. Furthermore, confidence interval highlights precision estimate population correlation, offering range plausible values \\(\\rho\\).combining visualization, hypothesis testing, confidence intervals, gain comprehensive understanding relationship carat price, can inform analysis predictive modeling.","code":"\nggplot(data = diamonds) +\n    geom_point(aes(x = carat, y = price), colour = \"blue\") +\n    labs(x = \"Carat\", y = \"Price\") cor_test <- cor.test(diamonds$carat, diamonds$price)\ncor_test\n   \n    Pearson's product-moment correlation\n   \n   data:  diamonds$carat and diamonds$price\n   t = 551.41, df = 53938, p-value < 2.2e-16\n   alternative hypothesis: true correlation is not equal to 0\n   95 percent confidence interval:\n    0.9203098 0.9228530\n   sample estimates:\n         cor \n   0.9215913"},{"path":"chapter-statistics.html","id":"wrapping-up","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.4 Wrapping Up","text":"chapter, laid foundation statistical inference, starting estimation, explored point estimates confidence intervals provide valuable insights population parameters accounting uncertainty. turned hypothesis testing, learning formulate null alternative hypotheses, calculate test statistics, interpret p-values make data-driven decisions. practical examples, applied variety techniques, t-test comparing means, tests evaluating proportions, ANOVA assessing group differences, Chi-square test correlation analysis uncovering relationships variables. Together, tools form robust framework answering research questions drawing meaningful conclusions data.Statistical inference hypothesis testing lie core data analysis, offering structured approach distinguish meaningful patterns random noise. methods indispensable tasks like testing effectiveness marketing strategy, evaluating performance product, predicting customer behavior. continue apply techniques, remember reliability results depends checking assumptions, interpreting findings within broader context data, incorporating domain expertise add depth conclusions.next chapter, ’ll build concepts prepare machine learning learning partition datasets effectively. ensure partitions valid reliable, rely hypothesis testing. connecting statistical inference model-building, ’ll see techniques chapter form foundation creating validating predictive models.","code":""},{"path":"chapter-modeling.html","id":"chapter-modeling","chapter":"6 Preparing Data for Modeling","heading":"6 Preparing Data for Modeling","text":"advance Data Science Process, illustrated Chapter 2, Figure 2.3, ’ve completed foundational steps pave way effective modeling:Problem Understanding: Chapter 2.4 highlighted importance clearly defining problem aligning objectives data-driven strategies.Data Preparation: Chapter 3, addressed challenges missing values, outliers, data transformation ensure dataset clean ready analysis.Exploratory Data Analysis: Chapter 4 guided us visualizing summarizing data uncover patterns generate meaningful insights.Statistical Inference: Chapter 5, explored hypothesis testing feature selection, tools ’ll leverage chapter validate data partitioning.diving building machine learning models, ’s essential establish robust foundation ’ll refer Setup Phase. phase includes three critical tasks must completed modeling begins:Partitioning Data: Dividing dataset training testing subsets create clear separation model building evaluation.Validating Partition: Ensuring partition appropriate representative, allowing reliable insights emerge training testing process.Balancing Training Dataset: Addressing imbalances training data (e.g., class imbalances categorical targets) ensure fair accurate model training.tasks often overlooked play crucial role ensuring modeling process rigorous effective.introducing topic, students often ask, “necessary partition data?” “need follow specific steps?”. insightful questions—ones ’ll address depth throughout chapter. However, diving details partitioning validating datasets, ’s worth stepping back examine Data Science Process aligns diverges Statistical Inference. understanding similarities differences, can better appreciate importance steps bridge principles traditional statistics practical demands modern machine learning.","code":""},{"path":"chapter-modeling.html","id":"statistical-inference-in-the-context-of-data-science","chapter":"6 Preparing Data for Modeling","heading":"6.1 Statistical Inference in the Context of Data Science","text":"Chapter 5, explored statistical inference helps us make conclusions populations based sample data. foundation remains valuable data science process, goals applications differ preparing data modeling.Statistical inference data science diverge two key ways applied modeling tasks:Significance Practicality: data science, datasets often contain thousands even millions observations, nearly difference relationship becomes statistically significant. However, statistical significance necessarily equate practical significance. example, machine learning model might identify tiny effect size (e.g., minor improvement predictions) statistically significant meaningful impact decision-making. Thus, modeling, focus shifts practical relevance predictive power mere statistical significance.Significance Practicality: data science, datasets often contain thousands even millions observations, nearly difference relationship becomes statistically significant. However, statistical significance necessarily equate practical significance. example, machine learning model might identify tiny effect size (e.g., minor improvement predictions) statistically significant meaningful impact decision-making. Thus, modeling, focus shifts practical relevance predictive power mere statistical significance.Exploration vs. Hypothesis Testing: Traditional statistical inference begins specific hypothesis mind, testing whether new treatment improves outcomes compared control. contrast, data science often takes exploratory approach, using data uncover patterns, relationships, actionable insights without predefined hypothesis. instance, preparing data modeling, might investigate features predictive target variable assess relationships variables refine dataset.Exploration vs. Hypothesis Testing: Traditional statistical inference begins specific hypothesis mind, testing whether new treatment improves outcomes compared control. contrast, data science often takes exploratory approach, using data uncover patterns, relationships, actionable insights without predefined hypothesis. instance, preparing data modeling, might investigate features predictive target variable assess relationships variables refine dataset.said, statistical inference still plays critical role data science process, particularly validating key steps data preparation. example:Partition Validation: splitting data training testing sets, statistical tests can confirm whether two subsets representative original dataset.Feature Selection: Hypothesis tests can help identify features strong relationships target variable, aiding selecting predictors modeling.understanding differences leveraging statistical inference strategically, can ensure data preparation process supports building robust, reliable, interpretable models. proceed chapter, ’ll see inference data science methods work together create datasets meaningful ready modeling.","code":""},{"path":"chapter-modeling.html","id":"why-is-it-necessary-to-partition-the-data","chapter":"6 Preparing Data for Modeling","heading":"6.2 Why Is It Necessary to Partition the Data?","text":"Partitioning dataset critical step preparing data modeling. common question students ask learning topic , “need partition data?”. answer lies principle generalization—ability model perform well unseen data. Without proper partitioning, risk building models excel training data fail make accurate predictions real-world scenarios. Partitioning ensures model’s performance evaluated data hasn’t seen training, providing unbiased measure ability generalize effectively.goal partitioning divide data two distinct subsets: training set, used build model, testing set, used evaluate performance. separation simulates real-world conditions, model must make predictions new, unseen data. Partitioning helps us detect address common modeling pitfalls like overfitting underfitting. trade-offs illustrated Figure 6.1, highlights balance model complexity performance training testing datasets.\nFigure 6.1: trade-model complexity accuracy training test sets. highlights optimal model complexity (sweet spot), test set accuracy reaches highest value unseen data.\nOverfitting occurs model learns training data well, including noise random fluctuations, instead capturing general patterns. Overfitted models achieve high accuracy training set fail miserably unseen data. example, churn prediction model might memorize customer IDs irrelevant details instead identifying meaningful behavioral trends. model struggle predict churn new customers little practical value.Underfitting, hand, arises model simplistic capture underlying patterns data. might happen model lacks complexity preprocessing removes much useful information. Underfitted models perform poorly training testing sets, failing capture signal within data. instance, churn model predicts overall churn rate customers, without considering individual characteristics, lack predictive power.Partitioning addresses issues enabling us evaluate model’s performance unseen data (testing set). comparing accuracy training testing sets, can identify whether model overfitting (high training accuracy low testing accuracy) underfitting (low accuracy datasets). evaluation helps us iteratively refine model strike right balance complexity generalization.Partitioning also protects data leakage, critical issue information testing set inadvertently influences training process. Data leakage inflates performance metrics, leading false sense confidence model’s abilities. strictly separating testing set training process, can obtain realistic reliable assessment model’s generalization performance.Beyond simple train-test splits, cross-validation powerful technique improves robustness partitioning. cross-validation, dataset divided multiple subsets, “folds.” model trained subset data tested different fold, process repeated across folds. results averaged provide reliable estimate model performance. Cross-validation particularly useful working smaller datasets tuning hyperparameters, minimizes bias variance introduced single train-test split.Partitioning data isn’t just procedural step—’s cornerstone building models perform well real-world applications. systematically addressing overfitting, underfitting, data leakage, incorporating techniques like cross-validation, ensure model well-suited training data also capable making accurate predictions unseen data.summarize, general strategy supervised machine learning models consists three key steps, illustrated Figure 6.2:Partitioning dataset training testing sets, followed validating partition.Building machine learning models training data.Evaluating performance models testing data identify effective approach.\nFigure 6.2: general predictive machine learning process building evaluating models. 80-20 split ratio example may vary based dataset task.\nfollowing process, create models reliable capable generalizing unseen data. chapter, focus crucial first step: partitioning data effectively, validating partition, preparing balanced training dataset. steps form foundation robust model building evaluation, paving way impactful, data-driven decision-making.","code":""},{"path":"chapter-modeling.html","id":"sec-partitioning","chapter":"6 Preparing Data for Modeling","heading":"6.3 Partitioning the Data","text":"Partitioning data crucial step preparing machine learning. common method train-test split, also known holdout method, dataset divided two subsets: training set testing set (Figure 6.2). training set used build model, testing set reserved evaluating performance. split ratio typically 70-30, 80-20, 90-10, depending dataset size specific modeling task.process, training set includes records complete information, including target variable. testing set, however, target variable temporarily hidden simulate unseen data. Machine learning models trained exclusively training set, learning patterns trends. models applied test set predict hidden target values. Finally, predictions compared actual (restored) target values test set evaluate model’s performance. approach ensures model evaluated unseen data, providing unbiased measure generalization ability. Cross-validation safeguards overfitting minimizing chance random variations present sets.example, consider churn dataset, goal predict customer churn based various features, ’ll explore Chapter 7. case, train-test split divides data two subsets:\n- training set, includes customer features known churn status.\n- testing set, includes customer features omits churn status (temporarily treated unknown).R, liver package offers partition() function creating train-test split. ’s churn dataset can split training testing sets:example:set.seed(43): Sets seed ensure reproducibility, split remains consistent across runs.partition(): Splits dataset 80% training data (train_set) 20% testing data (test_set).actual_test: Stores true target values testing set later evaluation.Reproducibility essential data science. setting seed, ensure random split can recreated exactly, allowing others replicate results grade assignments reliably. integer can used seed value—doesn’t need match one used . Setting seed best practice, particularly sharing code collaborating others, eliminates inconsistencies ensures precise reproducibility.applying train-test split, establish reliable framework evaluating model performance ensure robustness results. next steps, foundation allow us validate partitions, balance training datasets, build effective machine learning models.","code":"\nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test  = test_set$deposit"},{"path":"chapter-modeling.html","id":"sec-validate-partition","chapter":"6 Preparing Data for Modeling","heading":"6.4 Validating the Partition","text":"success entire modeling process depends quality data partition. Validating partition ensures training testing sets representative original dataset, enabling model learn diverse examples generalize effectively unseen data. Without validation, modeling process risks bias—either model fails generalize training set isn’t representative, testing set doesn’t provide accurate evaluation real-world performance.Validation involves comparing training testing sets confirm distributions statistically similar, particularly key variables. Since datasets often include many variables, step typically focuses small set randomly selected features features particular importance, target variable. choice statistical test depends type variable compared, shown Table 6.1.Table 6.1:  Suggested hypothesis tests validating partitions, based type target variable.Validating partition procedural step—safeguard biased modeling. training testing sets differ significantly, model’s performance compromised. example, training set isn’t representative original dataset, model may fail generalize. Conversely, testing set isn’t representative, evaluation results may overly optimistic. Ensuring split reflects original dataset’s characteristics allows fair reliable model evaluation.Example 6.1  Let’s consider churn dataset introduced previous section. target variable, churn (whether customer churned ), binary. According Table 6.1, appropriate statistical test validate partition variable Two-Sample Z-Test, compares proportion churned customers training testing sets. ’s can implemented R:example:\\(x_1\\) \\(x_2\\) represent number churned customers training testing sets, respectively.\\(n_1\\) \\(n_2\\) represent total number observations training testing sets, respectively.prop.test() function compares proportions churned customers two subsets.hypotheses test :\\[\n\\bigg\\{\n\\begin{matrix}\nH_0:  \\pi_{\\text{churn, train}} = \\pi_{\\text{churn, test}} \\quad \\text{(Proportions equal)} \\\\\nH_a:  \\pi_{\\text{churn, train}} \\neq \\pi_{\\text{churn, test}} \\quad \\text{(Proportions equal)}\n\\end{matrix}\n\\]test result provides p-value = 0.69. Since p-value greater significance level (\\(\\alpha = 0.05\\)), fail reject null hypothesis (\\(H_0\\)). indicates statistically significant difference proportions churned customers training testing sets. failing reject \\(H_0\\), confirm partition valid respect target variable churn. proportions churned customers consistent across subsets, ensuring model trained tested representative data.Beyond validating target variable, can extend process key features dataset. example, might compare numerical features like customer.calls day.mins using two-sample t-test validate categorical features two categories using Chi-square test. broader validation ensures partition representative across relevant variables.Partition Invalid? example, significant differences found training testing sets—several corrective actions can taken. revisit partitioning process changing random seed, adjusting split ratio, using stratified sampling ensure key features proportionally represented subsets. Alternatively, techniques like k-fold cross-validation can provide robust approach using observations training testing across multiple iterations.Validating partition critical step data preparation process. ensures modeling process fair, reliable, capable producing generalizable results. addressing potential discrepancies early, set stage robust machine learning models perform effectively real-world, unseen data.","code":"x1 <- sum(train_set$churn == \"yes\")\nx2 <- sum(test_set$churn == \"yes\")\n\nn1 <- nrow(train_set)\nn2 <- nrow(test_set)\n\ntest_churn <- prop.test(x = c(x1, x2), n = c(n1, n2))\ntest_churn\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.1566, df = 1, p-value = 0.6923\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.0190317  0.0300317\n   sample estimates:\n   prop 1 prop 2 \n   0.1425 0.1370"},{"path":"chapter-modeling.html","id":"balancing-the-training-dataset","chapter":"6 Preparing Data for Modeling","heading":"6.5 Balancing the Training Dataset","text":"real-world classification problems, one class target variable often significantly underrepresented compared (s). imbalance can lead biased models perform well majority class poorly minority class. example, fraud detection dataset, fraudulent transactions may account tiny fraction data, legitimate transactions dominate. Similarly, churn prediction dataset, majority customers might churn, small percentage . left unaddressed, imbalance can result models fail accurately predict minority class, despite appearing perform well overall.Imbalanced datasets problematic machine learning algorithms optimize overall accuracy, can favor majority class. model trained imbalanced churn dataset, example, might predict “churn” almost customers, resulting high accuracy completely missing minority class customers likely churn. failure can significant implications, particularly cases minority class—fraud cases, churners, patients rare disease—critical importance.address issue, balancing training dataset ensures classes adequately represented model training. exposing model sufficient examples minority class, balancing helps model learn patterns trends classes, improving ability generalize make accurate predictions minority class. Common techniques balancing include:Oversampling: Increasing number minority class examples duplicating existing observations generating synthetic samples (e.g., using SMOTE).Undersampling: Reducing number majority class examples randomly removing observations.Hybrid Methods: Combining oversampling undersampling achieve balanced dataset.Class Weights: Modifying algorithm penalize misclassifications minority class heavily training.choice technique depends factors dataset size, degree imbalance, specific machine learning algorithm used. Let’s demonstrate balancing training dataset example using churn dataset.First, examine distribution target variable, churn, training dataset determine whether balancing necessary. can done R follows:Suppose output reveals proportion customers churn (churn = \"yes\") 0.14 proportion non-churners (churn = \"\") 0.86. significant imbalance suggests balancing may beneficial, particularly predicting churners priority.balance training dataset, use ROSE package R oversample minority class (churn = \"yes\") constitutes 30% training dataset. ’s can implemented:example, ovun.sample() function used oversample minority class, achieving desired class distribution. parameter p = 0.3 specifies churners comprise 30% balanced training dataset. Notably, formula notation churn ~ . indicates balancing performed based target variable churn (refer Section 1.14 details formula notation). oversampling, new class distribution examined ensure dataset reflects specified proportions.Balancing ensures model exposed sufficient number examples churners non-churners training. instance, decision tree trained balanced dataset give equal importance features predictive churn, rather dominated majority class.’s important note balancing applied training dataset, test dataset. test dataset remain representative original data distribution provide unbiased evaluation model’s generalization performance. Balancing test set can introduce bias lead overly optimistic performance metrics, reflect real-world conditions.Moreover, balancing must performed partitioning dataset training testing sets. Balancing partitioning can lead data leakage, information test set influences training process. compromises integrity modeling process inflates performance metrics, creating false sense confidence model’s ability generalize.said, balancing always necessary. Many modern machine learning algorithms, random forests gradient boosting machines, robust class imbalances incorporate techniques like class weighting handle minority classes effectively. Additionally, evaluation metrics precision, recall, F1-score, AUC-ROC designed account class imbalance, providing fairer assessment model performance even dataset remains imbalanced.summary, balancing training dataset can address class imbalance, especially minority class critical analysis. However, always required used judiciously. balancing deemed necessary, must performed training dataset partitioning maintain validity reliability modeling process. carefully balancing training data, ensure models better equipped handle minority class, resulting fairer effective predictions.","code":"# Check the class distribution\ntable(train_set$churn)\n   \n    yes   no \n    570 3430\nprop.table(table(train_set$churn))\n   \n      yes     no \n   0.1425 0.8575# Load the ROSE package\nlibrary(ROSE)\n\n# Oversample the training set to balance the classes with 30% churners\nbalanced_train_set <- ovun.sample(churn ~ ., data = train_set, method = \"over\", \n                                  p = 0.3)$data\n\n# Check the new class distribution\ntable(balanced_train_set$churn)\n   \n     no  yes \n   3430 1444\nprop.table(table(balanced_train_set$churn))\n   \n          no       yes \n   0.7037341 0.2962659"},{"path":"chapter-knn.html","id":"chapter-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7 Classification using k-Nearest Neighbors","text":"k-Nearest Neighbors (kNN) algorithm simple yet effective machine learning technique, widely used solving classification problems. intuitive approach ease implementation make go-choice beginners reliable tool experienced practitioners. chapter, delve details kNN algorithm, demonstrate implementation R, discuss practical applications. focus kNN, ’s essential revisit fundamental concept classification, one cornerstone tasks machine learning.","code":""},{"path":"chapter-knn.html","id":"classification","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.1 Classification","text":"ever wondered email app effortlessly filters spam, streaming service seems know exactly want watch next, banks detect fraudulent credit card transactions real-time? seemingly magical predictions made possible classification, fundamental task machine learning.core, classification involves assigning label category observation based features. example, given customer data, classification can predict whether likely churn stay loyal. Unlike regression, predicts continuous numerical values (e.g., house prices), classification deals discrete outcomes. target variable, often called class label, can either :Binary: Two possible categories (e.g., spam vs. spam).Multi-class: two categories (e.g., car, bicycle, pedestrian image recognition).diagnosing diseases identifying fraudulent activities, classification versatile tool used across countless domains solve practical problems.","code":""},{"path":"chapter-knn.html","id":"where-is-classification-used","chapter":"7 Classification using k-Nearest Neighbors","heading":"Where Is Classification Used?","text":"Classification algorithms power many everyday applications cutting-edge technologies. examples:\n- Email filtering: Sorting spam non-spam messages.\n- Fraud detection: Identifying suspicious credit card transactions.\n- Customer retention: Predicting whether customer churn.\n- Medical diagnosis: Diagnosing diseases based patient records.\n- Object recognition: Detecting pedestrians vehicles self-driving cars.\n- Recommendation systems: Suggesting movies, songs, products based user preferences.Every time interact technology “predicts” something , chances , classification model working behind scenes.","code":""},{"path":"chapter-knn.html","id":"how-does-classification-work","chapter":"7 Classification using k-Nearest Neighbors","heading":"How Does Classification Work?","text":"Classification involves two critical phases:Training Phase: algorithm learns patterns labeled dataset, contains predictor variables (features) target class labels. instance, fraud detection system, algorithm might learn transactions involving unusually high amounts originating foreign locations likely fraudulent.Prediction Phase: model trained, applies learned patterns classify new, unseen data. example, given new transaction, model predicts whether fraudulent legitimate.good classification model just memorize training data—generalizes well, meaning performs accurately new, unseen data. example, model trained historical medical data able correctly diagnose new patient never seen .","code":""},{"path":"chapter-knn.html","id":"which-classification-algorithm-should-you-use","chapter":"7 Classification using k-Nearest Neighbors","heading":"Which Classification Algorithm Should You Use?","text":"Different classification algorithms designed different kinds problems datasets. commonly used algorithms include:\n- k-Nearest Neighbors (kNN): simple, distance-based algorithm (covered chapter).\n- Logistic Regression: Popular binary classification tasks, predicting customer churn.\n- Decision Trees Random Forests: Versatile, interpretable methods complex problems.\n- Naive Bayes: Particularly useful text classification, like spam filtering.\n- Neural Networks: Effective handling high-dimensional complex data, images natural language.choice algorithm depends factors like dataset size, feature relationships, desired trade-interpretability performance. example, ’re working small dataset need easy--interpret solution, kNN Decision Trees might ideal. hand, ’re analyzing high-dimensional data like images, Neural Networks suitable.see classification action, imagine bank dataset goal predict whether customer make deposit (deposit = yes) (deposit = ). features might include customer details like age, education, job, marital status. training classification model data, bank can identify target potential customers likely invest, improving marketing strategy.","code":""},{"path":"chapter-knn.html","id":"why-is-classification-important","chapter":"7 Classification using k-Nearest Neighbors","heading":"Why Is Classification Important?","text":"Classification forms backbone countless machine learning applications drive smarter decisions actionable insights industries like finance, healthcare, retail, technology. Understanding works critical step mastering machine learning applying solve real-world problems.rest chapter, ’ll explore k-Nearest Neighbors (kNN) algorithm, straightforward yet powerful method classification. simplicity intuitive nature make excellent choice beginners foundational building block advanced algorithms. Let’s dive !","code":""},{"path":"chapter-knn.html","id":"how-k-nearest-neighbors-works","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.2 How k-Nearest Neighbors Works","text":"ever tried make decision asking trusted friends advice? k-Nearest Neighbors (kNN) algorithm works similar way—“asks” nearest data points neighborhood determine category new observation. simple yet powerful idea makes kNN one intuitive methods machine learning.Unlike many algorithms require complex training phase, kNN lazy learning instance-based method. doesn’t build explicit model training; instead, stores entire training dataset makes predictions --fly finding nearest neighbors given observation. parameter \\(k\\) determines many neighbors consider, majority class among neighbors becomes prediction.","code":""},{"path":"chapter-knn.html","id":"how-does-knn-classify-a-new-observation","chapter":"7 Classification using k-Nearest Neighbors","heading":"How Does kNN Classify a New Observation?","text":"new observation needs classified, kNN calculates distance every data point training set using specified distance metric, Euclidean distance. algorithm identifies \\(k\\)-nearest neighbors predicts class based majority vote among neighbors.better understand works, let’s look Figure 7.1, illustrates simple example two classes: Class (red circles) Class B (blue squares).new data point, represented dark star, needs classified. figure demonstrates predictions two different values \\(k\\):\\(k = 3\\): algorithm looks 3 closest neighbors dark star—two blue squares one red circle. Since majority neighbors belong Class B (blue squares), new point classified Class B.\\(k = 6\\): algorithm now considers larger neighborhood 6 neighbors. case, four red circles two blue squares nearest neighbors. majority vote shifting Class (red circles), new point classified Class .\nFigure 7.1: two-dimensional toy dataset two classes (Class Class B) new data point (dark star), illustrating k-Nearest Neighbors algorithm k = 3 k = 6.\nKey Takeaway Figure:Increasing \\(k\\) smooths predictions incorporating neighbors decision-making process. However, may lead less sensitivity local patterns.example, \\(k = 6\\), larger neighborhood includes red circles, shifting majority class Class . demonstrates majority voting larger neighborhoods can significantly affect outcome.","code":""},{"path":"chapter-knn.html","id":"strengths-and-limitations-of-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"Strengths and Limitations of kNN","text":"simplicity kNN makes excellent starting point understanding classification. relying distance metrics majority voting, avoids complexity training explicit models. However, simplicity comes trade-offs:Strengths:\nEasy understand implement.\nEffective small datasets clear patterns.\nEasy understand implement.Effective small datasets clear patterns.Limitations:\nSensitive irrelevant noisy features, distance calculations may become less meaningful.\nComputationally expensive large datasets, since algorithm must compute distances training points prediction.\nRequires careful choice \\(k\\) balance sensitivity local patterns robustness noise.\nSensitive irrelevant noisy features, distance calculations may become less meaningful.Computationally expensive large datasets, since algorithm must compute distances training points prediction.Requires careful choice \\(k\\) balance sensitivity local patterns robustness noise.","code":""},{"path":"chapter-knn.html","id":"a-practical-example-of-knn-in-action","chapter":"7 Classification using k-Nearest Neighbors","heading":"A Practical Example of kNN in Action","text":"illustrate kNN, consider toy simulated example real-world scenario involving drug prescription classification. dataset 200 patients includes age, sodium--potassium (Na/K) ratio, drug type prescribed. Figure 7.2 shows scatter plot data, drug types represented :Red circles Drug ,Green triangles Drug B, andBlue squares Drug C.\nFigure 7.2: Scatter plot Age vs. Sodium/Potassium Ratio 200 patients, drug type indicated color shape.\nSuppose now three new patients whose drug classifications unknown. details follows:Patient 1: 40 years old Na/K ratio 30.5,Patient 2: 28 years old Na/K ratio 9.6, andPatient 3: 61 years old Na/K ratio 10.5.patients represented orange circles Figure 7.3. Using kNN, classify drug type patient.\nFigure 7.3: Scatter plot Age vs. Sodium/Potassium Ratio 200 patients, drug type indicated color shape. three new patients represented large orange circles.\nPatient 1, located deep within cluster red-circle points (Drug ), classification straightforward: Drug . nearest neighbors belong Drug , making easy decision.Patient 2, situation nuanced.\\(k = 1\\): nearest neighbor blue square, classification Drug C.\\(k = 2\\): tie Drug B Drug C, leaving clear majority.\\(k = 3\\): Two three nearest neighbors blue squares, resulting majority vote Drug C.Patient 3, scenario becomes even ambiguous:\\(k = 1\\): closest neighbor blue square, classification Drug C.\\(k = 2 3\\): neighbors belong multiple classes, resulting ties uncertainty.\nFigure 7.4: Zoom-plots three new patients nearest neighbors. left plot Patient 1, middle plot Patient 2, right plot Patient 3.\nexamples illustrate several key considerations kNN:value \\(k\\) determines sensitive algorithm local patterns noise.Distance metrics, Euclidean distance, affect neighbors selected.Proper feature scaling essential ensure variables contribute fairly distance calculation.classify new observation, kNN relies measuring similarity data points. brings us question: define calculate similarity?","code":""},{"path":"chapter-knn.html","id":"distance-metrics","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.3 Distance Metrics","text":"k-Nearest Neighbors (kNN) algorithm, classification new data point determined identifying similar records training dataset. define measure similarity? similarity might seem intuitive, applying machine learning requires precise distance metrics. metrics quantify “closeness” “distance” two data points multidimensional space, directly influencing neighbors selected classification.Imagine ’re shopping online looking recommendations. ’re 50-year-old married female—’s “similar” : 40-year-old single female 30-year-old married male? answer depends measure distance person. kNN, distance calculated based features age marital status. smaller distance, “similar” two individuals , influence determining recommendation classification.widely used distance metric kNN Euclidean distance, measures straight-line distance two points. Think “--crow-flies” distance, similar shortest path two locations map. metric intuitive aligns often perceive distance real world.mathematical terms, Euclidean distance two points, \\(x\\) \\(y\\), \\(n\\)-dimensional space given :\\[\n\\text{dist}(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\ldots + (x_n - y_n)^2}\n\\]:\\(x = (x_1, x_2, \\ldots, x_n)\\) \\(y = (y_1, y_2, \\ldots, y_n)\\) represent feature vectors two points.differences corresponding features (\\(x_i - y_i\\)) squared, summed, square-rooted calculate distance.","code":""},{"path":"chapter-knn.html","id":"example-calculating-euclidean-distance","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.3.1 Example: Calculating Euclidean Distance","text":"Let’s calculate Euclidean distance two patients based age sodium/potassium (Na/K) ratio:Patient 1: \\(x = (40, 30.5)\\)Patient 2: \\(y = (28, 9.6)\\)Using formula:\\[\n\\text{dist}(x, y) = \\sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \\sqrt{(12)^2 + (20.9)^2} = 24.11\n\\]result quantifies dissimilarity two patients. kNN, distance help determine similar Patient 1 Patient 2 whether Patient 1 classified drug class Patient 2.","code":""},{"path":"chapter-knn.html","id":"a-note-on-choosing-distance-metrics","chapter":"7 Classification using k-Nearest Neighbors","heading":"A Note on Choosing Distance Metrics","text":"many distance metrics, Manhattan Distance, Hamming Distance, Cosine Similarity, default, Euclidean distance commonly used kNN. works well many scenarios, particularly features continuous properly scaled. Choosing right distance measure somewhat beyond scope book, general purposes, Euclidean distance reliable choice. dataset unique characteristics categorical features, might need explore alternative metrics; details, refer dist() function R.","code":""},{"path":"chapter-knn.html","id":"how-to-choose-an-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.4 How to Choose an Optimal \\(k\\)","text":"many opinions seek making important decision? might lead biased perspective, many might dilute relevance advice. Similarly, k-Nearest Neighbors (kNN) algorithm, choice \\(k\\)—number neighbors considered classification—directly impacts model’s performance. find right \\(k\\)?universally “correct” value \\(k\\). optimal choice depends specific dataset problem hand, requiring careful consideration trade-offs involved.","code":""},{"path":"chapter-knn.html","id":"balancing-overfitting-and-underfitting","chapter":"7 Classification using k-Nearest Neighbors","heading":"Balancing Overfitting and Underfitting","text":"\\(k\\) set small value, \\(k = 1\\), algorithm becomes highly sensitive outliers training data. new observation classified solely based single closest neighbor. can lead overfitting, model memorizes training data struggles generalize unseen data. example, small cluster mislabeled data points disproportionately influence predictions, reducing model’s reliability.Conversely, \\(k\\) increases, algorithm incorporates neighbors classification decision. Larger \\(k\\) values smooth decision boundary, making model less sensitive noise outliers. However, \\(k\\) becomes large, model may -simplify, averaging meaningful patterns data. instance, \\(k\\) comparable size training set, majority class dominate predictions, leading underfitting.Finding right \\(k\\) involves striking balance extremes. Smaller \\(k\\) values capture local patterns effectively, larger \\(k\\) values provide robustness expense detail.","code":""},{"path":"chapter-knn.html","id":"choosing-k-through-validation","chapter":"7 Classification using k-Nearest Neighbors","heading":"Choosing \\(k\\) Through Validation","text":"practice, selecting \\(k\\) iterative process. common approach evaluate algorithm’s performance multiple \\(k\\) values using validation set cross-validation. Performance metrics like accuracy, precision, recall, F1-score guide selection \\(k\\) works best dataset.illustrate, let’s use churn dataset evaluate accuracy kNN algorithm across \\(k\\) values ranging 1 30. Figure 7.5 shows accuracy fluctuates \\(k\\) increases. plot generated using kNN.plot() function liver package R.\nFigure 7.5: Accuracy k-Nearest Neighbors algorithm different values k range 1 30.\nplot, observe accuracy kNN algorithm fluctuates \\(k\\) increases. example, highest accuracy achieved \\(k = 5\\). value, kNN algorithm balances sensitivity local patterns robustness noise, delivering accuracy 0.932 error rate 0.068.Choosing optimal \\(k\\) much art science. ’s universal rule selecting \\(k\\), experimentation validation key. Start range plausible \\(k\\) values, test model’s performance, select one provides best results based chosen metric.Keep mind optimal \\(k\\) may vary across datasets, ’s essential repeat process whenever applying kNN new problem. carefully tuning \\(k\\), can ensure kNN model accurate generalizable, striking perfect balance overfitting underfitting.","code":"   Setting levels: reference = \"yes\", case = \"no\"\n   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-knn.html","id":"preparing-data-for-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5 Preparing Data for kNN","text":"effectiveness k-Nearest Neighbors (kNN) algorithm relies heavily dataset prepared. Since kNN uses distance metrics evaluate similarity data points, proper preprocessing crucial ensure accurate meaningful results. Two essential steps process feature scaling one-hot encoding, enable algorithm handle numerical categorical features effectively.","code":""},{"path":"chapter-knn.html","id":"feature-scaling-1","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.1 Feature Scaling","text":"datasets, numerical features often vastly different ranges. instance, age may range 20 70, income range 20,000 150,000. Without proper scaling, features larger ranges (like income) dominate distance calculations, leading biased predictions. address , numerical features must transformed comparable scales.widely used scaling method min-max scaling, transforms feature specified range, typically [0, 1], using formula:\\[\nx_{\\text{scaled}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\], \\(x\\) represents original feature value, \\(\\min(x)\\) \\(\\max(x)\\) feature’s minimum maximum values. ensures features contribute equally distance metric. Another commonly used method z-score standardization, scales features mean 0 standard deviation 1:\\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)}\n\\]method particularly useful features follow different distributions varying units. methods prevent single feature dominating distance calculations, ensuring fair treatment numerical variables.Important: Scaling must always performed partitioning dataset training test sets. Scaling parameters (e.g., minimum, maximum, mean, standard deviation) must calculated using training set applied consistently training test sets. ensures test data remains independent, avoiding information leakage bias results.","code":""},{"path":"chapter-knn.html","id":"scaling-training-and-test-data-the-same-way","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.2 Scaling Training and Test Data the Same Way","text":"illustrate importance consistent scaling, consider patient drug classification problem, involves two features: age sodium/potassium (Na/K) ratio. Figure 7.3 shows dataset 200 patients training set, three additional patients test set. Using minmax() function liver package, demonstrate correct incorrect ways scale data:difference illustrated Figure ??. middle panel shows results proper scaling, test set scaled using parameters derived training set. ensures consistency distance calculations across datasets. contrast, right panel shows improper scaling, test set scaled independently. leads distorted relationships training test data, can cause unreliable predictions.Key Insight: Proper scaling ensures distance metrics remain valid, improper scaling creates inconsistencies undermine kNN algorithm’s performance. Always derive scaling parameters training set apply consistently test set.","code":"\n# A proper way to scale the data\ntrain_scaled = minmax(train_data, col = c(\"Age\", \"Ratio\"))\n\ntest_scaled = minmax(test_data, col = c(\"Age\", \"Ratio\"), \n                     min = c(min(train_data$Age), min(train_data$Ratio)), \n                     max = c(max(train_data$Age), max(train_data$Ratio)))\n\n# An incorrect way to scale the data\ntrain_scaled_wrongly = minmax(train_data, col = c(\"Age\", \"Ratio\"))\ntest_scaled_wrongly  = minmax(test_data , col = c(\"Age\", \"Ratio\"))"},{"path":"chapter-knn.html","id":"one-hot-encoding-1","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.3 One-Hot Encoding","text":"Categorical features, marital status subscription type, directly used distance calculations distance metrics like Euclidean distance work numerical data. overcome , use one-hot encoding, converts categorical variables binary (dummy) variables. example, categorical variable voice.plan, levels yes , can encoded :\\[\n\\text{voice.plan-yes} =\n\\bigg\\{\n\\begin{matrix}\n1 \\quad \\text{voice plan = yes}  \\\\\n0 \\quad \\text{voice plan = }\n\\end{matrix}\n\\]Similarly, variable like marital status three levels (single, married, divorced) can encoded two binary features:\\[\n\\text{marital-single} =\n\\bigg\\{\n\\begin{matrix}\n1 \\quad \\text{marital status = single}  \\\\\n0 \\quad \\text{otherwise}\n\\end{matrix}\n\\]\\[\n\\text{marital-married} =\n\\bigg\\{\n\\begin{matrix}\n1 \\quad \\text{marital status = married}  \\\\\n0 \\quad \\text{otherwise}\n\\end{matrix}\n\\]absence marital_single marital_married implies third category (divorced). approach ensures categorical variable fully represented, maintaining scale numerical features. categorical variable \\(k\\) levels, \\(k-1\\) binary features created avoid redundancy.liver package R provides one.hot() function perform one-hot encoding automatically. identifies categorical variables encodes binary columns, leaving numerical features unchanged. example, applying one-hot encoding marital variable bank dataset adds binary columns encoded categories:Note: One-hot encoding unnecessary ordinal features, categories natural order (e.g., low, medium, high). Ordinal variables instead assigned numerical values preserve order (e.g., low = 1, medium = 2, high = 3), enabling kNN algorithm treat numerical features.","code":"# To perform one-hot encoding on the \"marital\" variable\nbank_encoded <- one.hot(bank, cols = c(\"marital\"), dropCols = FALSE)\n\nstr(bank_encoded)\n   'data.frame':    4521 obs. of  20 variables:\n    $ age             : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job             : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital         : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ marital_divorced: int  0 0 0 0 0 0 0 0 0 0 ...\n    $ marital_married : int  1 1 0 1 1 0 1 1 1 1 ...\n    $ marital_single  : int  0 0 1 0 0 1 0 0 0 0 ...\n    $ education       : Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance         : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing         : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan            : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact         : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day             : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month           : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration        : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign        : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays           : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous        : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome        : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-knn.html","id":"sec-kNN-churn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6 Applying kNN Algorithm in Practice","text":"Applying kNN algorithm involves several key steps, preparing data training model, making predictions, evaluating performance. section, demonstrate entire workflow using churn dataset liver package R. target variable, churn, indicates whether customer churned (yes) (), predictors include customer characteristics like account length, international plan status, call details. strcure dataset:shows data data.frame object R 5000 observations 19 features along target binary variable (last column) name churn indicates whether customers churned (left company) . goal build kNN model accurately predicts customer churn based features.Chapter 4, explored churn dataset identified key features influence customer churn. Based results use following features build kNN model:account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, customer.calls.Let’s start preparing data kNN algorithm performing feature scaling one-hot encoding. proceed selecting optimal \\(k\\), training kNN model, evaluating performance.","code":"str(churn)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-knn.html","id":"step-1-preparing-the-data","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.1 Step 1: Preparing the Data","text":"first step applying kNN partition data training test sets, followed preprocessing tasks like feature scaling one-hot encoding. Since dataset already clean contains missing values, can proceed directly steps.split dataset 80% training set 20% test set using partition() function liver package:partition() function ensures randomized split, preserving overall distribution target variable training test sets. Note proceeding, validate partitions. skip step Section 6.4.","code":"\nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test  = test_set$churn"},{"path":"chapter-knn.html","id":"one-hot-encoding-2","chapter":"7 Classification using k-Nearest Neighbors","heading":"One-Hot Encoding","text":"Categorical variables, voice.plan intl.plan, converted binary (dummy) variables using one.hot() function. ensures kNN algorithm can handle categorical data effectively:instance, voice.plan variable transformed voice.plan_yes voice.plan_no. However, since presence one category implies absence , retain one dummy variable (e.g., voice.plan_yes) simplicity.","code":"categorical_vars = c(\"voice.plan\", \"intl.plan\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_vars)\ntest_onehot  = one.hot(test_set,  cols = categorical_vars)\n\nstr(test_onehot)\n   'data.frame':    1000 obs. of  22 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 2 50 14 46 10 4 25 15 11 32 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 3 2 1 3 2 2 2 2 2 1 ...\n    $ account.length: int  118 141 85 76 147 130 20 142 72 149 ...\n    $ voice.plan_yes: int  0 1 1 1 0 0 0 0 1 0 ...\n    $ voice.plan_no : int  1 0 0 0 1 1 1 1 0 1 ...\n    $ voice.messages: int  0 37 27 33 0 0 0 0 37 0 ...\n    $ intl.plan_yes : int  1 1 0 0 0 0 0 0 0 0 ...\n    $ intl.plan_no  : int  0 0 1 1 1 1 1 1 1 1 ...\n    $ intl.mins     : num  6.3 11.2 13.8 10 10.6 9.5 6.3 14.2 14.7 11.1 ...\n    $ intl.calls    : int  6 5 4 5 4 19 6 6 6 9 ...\n    $ intl.charge   : num  1.7 3.02 3.73 2.7 2.86 2.57 1.7 3.83 3.97 3 ...\n    $ day.mins      : num  223 259 196 190 155 ...\n    $ day.calls     : int  98 84 139 66 117 112 109 95 80 94 ...\n    $ day.charge    : num  38 44 33.4 32.2 26.4 ...\n    $ eve.mins      : num  221 222 281 213 240 ...\n    $ eve.calls     : int  101 111 90 65 93 99 84 63 102 92 ...\n    $ eve.charge    : num  18.8 18.9 23.9 18.1 20.4 ...\n    $ night.mins    : num  203.9 326.4 89.3 165.7 208.8 ...\n    $ night.calls   : int  118 97 75 108 133 78 102 148 71 108 ...\n    $ night.charge  : num  9.18 14.69 4.02 7.46 9.4 ...\n    $ customer.calls: int  0 0 1 1 0 0 0 2 3 1 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-knn.html","id":"feature-scaling-2","chapter":"7 Classification using k-Nearest Neighbors","heading":"Feature Scaling","text":"kNN relies distance metrics, sensitive scale features. ensure fair contributions features, scale numerical variables using min-max scaling. minmax() function liver package applied training test sets, using scaling parameters derived training set:minmax() function scales features range [0, 1]. deriving scaling parameters (minimum maximum) training set, ensure consistency avoid data leakage.","code":"\nnumeric_vars = c(\"account.length\", \"voice.messages\", \"intl.mins\", \"intl.calls\", \n                 \"day.mins\", \"day.calls\", \"eve.mins\", \"eve.calls\", \n                 \"night.mins\", \"night.calls\", \"customer.calls\")\n\nmin_train = sapply(train_set[, numeric_vars], min)\nmax_train = sapply(train_set[, numeric_vars], max)\n\ntrain_scaled = minmax(train_onehot, col = numeric_vars, min = min_train, max = max_train)\ntest_scaled  = minmax(test_onehot,  col = numeric_vars, min = min_train, max = max_train)"},{"path":"chapter-knn.html","id":"step-2-choosing-an-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.2 Step 2: Choosing an Optimal \\(k\\)","text":"choice \\(k\\), number neighbors, significantly affects performance kNN algorithm. identify optimal \\(k\\), evaluate model’s accuracy different values \\(k\\) using kNN.plot() function:kNN.plot() function generates plot accuracy versus \\(k\\) values, allowing us visually identify optimal \\(k\\). case, highest accuracy achieved \\(k = 5\\), striking balance sensitivity local patterns (small \\(k\\)) robustness noise (large \\(k\\)).","code":"formula = churn ~ account.length + voice.plan_yes + voice.messages + \n                  intl.plan_yes + intl.mins + intl.calls + \n                  day.mins + day.calls + eve.mins + eve.calls + \n                  night.mins + night.calls + customer.calls\n\nkNN.plot(formula = formula, train = train_scaled, test = test_scaled, \n         k.max = 30, set.seed = 43)\n   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-knn.html","id":"step-3-training-the-model-and-making-predictions","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.3 Step 3: Training the Model and Making Predictions","text":"Using optimal \\(k = 5\\), train kNN model make predictions test set kNN() function:kNN() function computes distances test point training points, identifies 5 nearest neighbors, assigns majority class among neighbors predicted class test point.","code":"\nkNN_predict = kNN(formula = formula, train = train_scaled, test = test_scaled, k = 5)"},{"path":"chapter-knn.html","id":"step-4-evaluating-the-model","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.4 Step 4: Evaluating the Model","text":"Model evaluation essential assess well kNN algorithm performs unseen data. , display confusion matrix test set predictions using conf.mat() function:confusion matrix summarizes number correct incorrect predictions. case, model achieves “54 + 856” correct predictions “7 + 83” incorrect predictions. provides insights model’s performance highlights areas improvement.","code":"conf.mat(kNN_predict, actual_test)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-knn.html","id":"final-remarks","chapter":"7 Classification using k-Nearest Neighbors","heading":"Final Remarks","text":"step--step implementation kNN algorithm, demonstrated importance data preprocessing, parameter tuning, proper evaluation. kNN simple intuitive, effectiveness relies heavily steps. evaluation metrics performance analysis, explore topics next chapter (Chapter 8).","code":""},{"path":"chapter-knn.html","id":"summary","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.7 Summary","text":"chapter, explored k-Nearest Neighbors (kNN) algorithm, simple yet effective method solving classification problems. began revisiting concept classification real-world applications, highlighting difference binary multi-class problems. delved mechanics kNN, emphasizing reliance distance metrics identify similar data points. Critical preprocessing steps, feature scaling one-hot encoding, discussed ensure accurate meaningful distance calculations. also covered select optimal \\(k\\) value demonstrated implementation kNN using liver package R churn dataset. practical examples, highlighted importance proper data preparation parameter tuning reliable effective classification performance.simplicity interpretability kNN make excellent starting point understanding classification exploring dataset structure. However, algorithm notable limitations, including sensitivity noise, computational inefficiency large datasets, requirement proper scaling feature selection. challenges make kNN less practical large-scale applications, remains valuable tool small medium-sized datasets serves benchmark evaluating advanced algorithms.kNN easy understand implement, prediction speed scalability constraints often make unsuitable modern, large-scale datasets. Nonetheless, helpful baseline method stepping stone sophisticated techniques. upcoming chapters, explore advanced classification algorithms, Decision Trees, Random Forests, Logistic Regression, address limitations kNN provide enhanced performance scalability wide range applications.","code":""},{"path":"chapter-knn.html","id":"exercises-3","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.8 Exercises","text":"…","code":""},{"path":"chapter-evaluation.html","id":"chapter-evaluation","chapter":"8 Model Evaluation","heading":"8 Model Evaluation","text":"progress Data Science Process, introduced Chapter 2 illustrated Figure 2.3, ’ve already completed first five phases Data Science Workflow:Problem Understanding: Defining problem aim solve.Data Preparation: Cleaning, transforming, organizing data analysis.Exploratory Data Analysis (EDA): Gaining insights uncovering patterns data.Preparing Data Model: Setting data modeling scaling, encoding, partitioning.Modeling: Applying algorithms make predictions extract insights—kNN classification method explored previous chapter.Now, arrive Model Evaluation phase, pivotal step Data Science Process. phase answers critical question: well model perform?","code":""},{"path":"chapter-evaluation.html","id":"why-is-model-evaluation-important","chapter":"8 Model Evaluation","heading":"Why Is Model Evaluation Important?","text":"Building model just beginning. true test model lies ability generalize new, unseen data. Without proper evaluation, model may appear successful development fail real-world applications.Consider example:\n’ve built model detect fraudulent credit card transactions, achieves 95% accuracy. Impressive, right? 1% transactions actually fraudulent, model might simply classify every transaction legitimate, ignoring fraud cases. highlights crucial point: accuracy alone can misleading, especially imbalanced datasets.Model evaluation goes beyond simplistic metrics like accuracy. provides nuanced understanding model’s:Strengths: model well (e.g., detecting true positives).Weaknesses: falls short (e.g., missing fraud cases generating false alarms).Trade-offs: balance competing priorities, sensitivity vs. specificity precision vs. recall.short, model evaluation ensures model aligns real-world goals problem. helps answer questions :well model handle imbalanced datasets?good identifying true positives (e.g., detecting cancer)?minimize false positives (e.g., incorrectly flagging legitimate emails spam)?George Box famously said, “models wrong, useful.” model always simplification reality. capture every nuance complexity, properly evaluated, can provide actionable insights guide decisions effectively. Evaluation metrics help us judge whether model “useful enough” meet needs problem ’re solving.chapter, ’ll explore evaluate classification models, starting binary classification, target variable two categories (e.g., spam vs. spam). ’ll discuss metrics multi-class classification, two categories (e.g., types vehicles: car, truck, bike). Finally, ’ll touch evaluation metrics regression models, target variable continuous (e.g., predicting house prices).goal build strong foundation model evaluation, helping confidently assess model performance make data-driven decisions. Let’s begin cornerstone classification evaluation: Confusion Matrix.","code":""},{"path":"chapter-evaluation.html","id":"confusion-matrix","chapter":"8 Model Evaluation","heading":"8.1 Confusion Matrix","text":"confusion matrix cornerstone evaluating classification models. provides detailed snapshot well model’s predictions align actual outcomes categorizing predictions four distinct groups. binary classification problems, confusion matrix typically organized shown Table 8.1.classification tasks, often focus model’s ability distinguish one class interest (positive class) another (negative class). instance, fraud detection scenario, fraudulent transactions might positive class, legitimate ones negative class.Table 8.1:  Confusion matrix summarizing correct incorrect predictions binary classification problems. positive class refers class interest, negative class represents category.Let’s break terms:True Positives (TP): Cases model correctly predicts positive class (e.g., fraud detected fraud).False Positives (FP): Cases model incorrectly predicts positive class (e.g., legitimate transactions flagged fraud).True Negatives (TN): Cases model correctly predicts negative class (e.g., legitimate transactions classified legitimate).False Negatives (FN): Cases model fails predict positive class (e.g., fraud classified legitimate).structure feels familiar, ’s mirrors concept type type II errors introduced Chapter 5 hypothesis testing. diagonal elements confusion matrix (TP TN) represent correct predictions, -diagonal elements (FP FN) capture incorrect ones.","code":""},{"path":"chapter-evaluation.html","id":"calculating-key-metrics","chapter":"8 Model Evaluation","heading":"Calculating Key Metrics","text":"Using counts confusion matrix, can calculate basic performance metrics model, accuracy (also know success rate) error rate:\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}\n\\]\\[\n\\text{Error Rate} = 1 - \\text{Accuracy} = \\frac{\\text{FP} + \\text{FN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}\n\\]Accuracy proportion correct predictions (TP TN) among predictions made model. gives general sense well model performs. conversely, Error Rate proportion incorrect predictions (FP FN) among predictions. accuracy gives overall sense model performance, differentiate types errors. example, imbalanced datasets one class dominates, accuracy may appear high even model performs poorly detecting minority class. need nuanced metrics, sensitivity, specificity, precision, recall, ’ll explore later sections.Example 8.1  Let’s revisit k-Nearest Neighbors (kNN) model built Chapter 7 classify churn dataset. Using confusion matrix, can evaluate well model performs test data.’s apply kNN model generate confusion matrix predictions:details kNN model built, refer Section 7.6.Now, ’ll generate confusion matrix predictions using conf.mat() function liver package:confusion matrix summarizes model’s performance. example:True Positives (TP): 54 cases churn correctly predicted.True Negatives (TN): 856 cases non-churn correctly predicted.False Positives (FP): 83 cases model falsely predicted churn.False Negatives (FN): 7 cases churn missed.can also visualize confusion matrix using conf.mat.plot() function liver package:Using confusion matrix, can calculate following metrics kNN model:\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total Predictions}} = \\frac{54 + 856}{1000} = 0.91\n\\]\\[\n\\text{Error Rate} = \\frac{\\text{FP} + \\text{FN}}{\\text{Total Predictions}} = \\frac{83 + 7}{1000} = 0.09\n\\]values give us sense overall performance model. However, accuracy error rate don’t provide insights specific errors, well model detects true positives avoids false positives. insights, need explore additional metrics like sensitivity, specificity, precision, recall, ’ll cover next.","code":"\n# Load the churn dataset\ndata(churn)\n\n# Partition the data into training and testing sets\nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\nactual_test = test_set$churn\n\n# Build and predict using the kNN model\nformula = churn ~ account.length + voice.plan + voice.messages + \n                  intl.plan + intl.mins + intl.calls + \n                  day.mins + day.calls + eve.mins + eve.calls + \n                  night.mins + night.calls + customer.calls\n\nkNN_predict = kNN(formula = formula, train = train_set, \n                  test = test_set, k = 5, scaler = \"minmax\")conf.mat(kNN_predict, actual_test)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856   Setting levels: reference = \"yes\", case = \"no\"conf.mat.plot(kNN_predict, actual_test)\n   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-evaluation.html","id":"sensitivity-and-specificity","chapter":"8 Model Evaluation","heading":"8.2 Sensitivity and Specificity","text":"classification, ’s important evaluate just many predictions correct overall, well model identifies specific classes. Sensitivity Specificity two complementary metrics focus model’s ability distinguish positive negative classes.","code":""},{"path":"chapter-evaluation.html","id":"sensitivity","chapter":"8 Model Evaluation","heading":"Sensitivity","text":"Sensitivity (also called Recall fields, like information retrieval) measures model’s ability correctly identify positive cases. answers question:“actual positives, many model correctly predict?”Mathematically, sensitivity defined :\\[\n\\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n\\]Let’s compute sensitivity k-Nearest Neighbors (kNN) model built Chapter 7, predicted whether customers churned (churn = yes). Sensitivity case reflects percentage churners correctly identified model. Using confusion matrix Example 8.1:\\[\n\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{54}{54 + 7} = 0.885\n\\]means model correctly identified 88.5% actual churners.perfect model achieve sensitivity 1.0 (100%), meaning correctly identifies positive cases. However, ’s important note even naïve model classifies customers churners also achieve 100% sensitivity. illustrates sensitivity alone isn’t enough evaluate model’s performance—must paired metrics capture full picture.","code":""},{"path":"chapter-evaluation.html","id":"specificity","chapter":"8 Model Evaluation","heading":"Specificity","text":"sensitivity focuses positive class, Specificity measures model’s ability correctly identify negative cases. answers question:“actual negatives, many model correctly predict?”Specificity particularly important situations avoiding false positives critical. example, spam detection, incorrectly marking legitimate email spam (false positive) can severe consequences missing spam messages. Mathematically, specificity defined :\\[\n\\text{Specificity} = \\frac{\\text{True Negatives (TN)}}{\\text{True Negatives (TN)} + \\text{False Positives (FP)}}\n\\]Using kNN model confusion matrix Example 8.1, let’s calculate specificity identifying non-churners (churn = ):\\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{856}{856 + 83} = 0.912\n\\]means model correctly classified 91.2% actual non-churners leaving company.good classification model ideally achieve high sensitivity high specificity, relative importance metrics depends problem domain. example, medical diagnostics, sensitivity often prioritized ensure disease cases missed, credit scoring, specificity might take precedence avoid mistakenly classifying reliable customers risks. kNN model Example 8.1, sensitivity 0.885 specificity 0.912. trade-may acceptable instance, identifying churners (sensitivity) might critical avoiding false positives (specificity). next section, ’ll explore metrics like precision recall, refine model evaluation.","code":""},{"path":"chapter-evaluation.html","id":"precision-recall-and-f1-score","chapter":"8 Model Evaluation","heading":"8.3 Precision, Recall, and F1-Score","text":"addition sensitivity specificity, Precision, Recall, F1-Score offer deeper insights classification model’s performance. metrics particularly valuable scenarios imbalanced datasets, simple accuracy can misleading.Precision (also called positive predictive value) measures many model’s predicted positives actually positive. answers question: “model predicts positive, often correct?” formula :\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\nPrecision especially important applications false positives costly. example, fraud detection, flagging legitimate transactions fraudulent can lead customer dissatisfaction unnecessary investigations.Recall (equivalent sensitivity) measures model’s ability identify positive cases. answers question: “actual positives, many model correctly predict?” formula :\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\nrecall often used interchangeably sensitivity medical diagnostics, commonly referred recall areas like information retrieval, spam detection, text classification. Recall particularly useful cases missing positive cases (false negatives) serious consequences, failing diagnose disease missing spam emails.inherent trade-precision recall: increasing one often decreases . example, model high recall might correctly flag fraudulent transactions also mislabel many legitimate transactions fraud (low precision). Conversely, model high precision might flag transactions fraud (mostly correct), miss many actual fraud cases (low recall).balance trade-, F1-Score combines precision recall single metric. harmonic mean precision recall, emphasizing balance:\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n   = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}\n\\]\nF1-Score particularly useful cases imbalanced datasets, one class dominates . Unlike accuracy, considers false positives false negatives, providing balanced evaluation model’s predictive performance.Let’s calculate precision, recall, F1-Score k-Nearest Neighbors (kNN) model Example 8.1, predicts customer churn (churn = yes). First, precision quantifies often model’s predicted churners actual churners:\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{54}{54 + 83} = 0.394\n\\]\nmeans model predicts churn, correct 39.4% time.Next, recall measures many actual churners correctly identified model:\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{54}{54 + 7} = 0.885\n\\]\nshows model successfully identifies 88.5% actual churners.Finally, F1-Score provides single measure balances precision recall:\n\\[\nF1 = \\frac{2 \\cdot 54}{2 \\cdot 54 + 83 + 7} = 0.545\n\\]\nF1-Score indicates well model balances precision recall, offering comprehensive evaluation ability correctly identify churners minimizing false predictions. balance makes F1-Score especially useful comparing multiple models, provides single number summarizing performance.F1-Score valuable metric, assumes precision recall equally important, may always align priorities particular problem. instance, medical diagnostics, recall (ensuring cases missed) might critical precision, whereas spam filtering, precision (avoiding false positives) might take precedence. , F1-Score used alongside metrics fully understand model’s strengths weaknesses. comprehensive evaluation, now turn metrics assess performance across thresholds, described coming sections.","code":""},{"path":"chapter-evaluation.html","id":"taking-uncertainty-into-account","chapter":"8 Model Evaluation","heading":"8.4 Taking Uncertainty into Account","text":"evaluating classification model, confusion matrix derived metrics like precision, recall, F1-score provide valuable insights performance. However, metrics based discrete predictions, model already classified observations either positive negative. , lose critical layer information: uncertainty confidence behind prediction. classification models, including k-Nearest Neighbors (kNN), can provide probabilities class instead binary predictions. probabilities quantify confident model predictions, offering powerful tool fine-tuning behavior better align requirements task.Making prediction can viewed thresholding model’s probability output. default, threshold 0.5 used: probability belonging positive class 50% greater, model predicts positive class. Otherwise, predicts negative class. default threshold works many cases, universal. Adjusting threshold can significantly impact model’s performance, allowing better align business goals domain-specific needs. example, applications, false negatives may far costly false positives—vice versa. experimenting different thresholds, can explore trade-offs sensitivity, specificity, precision, recall optimize model’s performance.Example 8.2  Let’s revisit k-Nearest Neighbors (kNN) model Example 8.1 predict customer churn (churn = yes). time, instead making discrete predictions, ’ll obtain probabilities positive class setting type parameter \"prob\" kNN() function:output lists first 10 probabilities class: first column corresponds positive class (churn = yes), second column corresponds negative class (churn = ). example, first row, probability 0.4 indicates model 40% confident customer churn, probability 0.6 suggests 60% confidence customer churn. modifying threshold classification, can adjust model determines whether prediction positive negative.Now, let’s calculate confusion matrix model two different thresholds: 0.5 (default) 0.7:threshold 0.5, model classifies customer churner probability churn least 50%. threshold, confusion matrix match one Example 8.1, use default cutoff. raising threshold 0.7, model requires least 70% confidence classify customer churner. change shifts balance true positives, true negatives, false positives, false negatives. example:Lowering threshold increases sensitivity, allowing model catch true positives potentially leading false positives.Raising threshold increases specificity, reducing false positives potentially missing true positives.Adjusting threshold especially important costs false positives false negatives differ. instance, spam detection, false positives (marking legitimate emails spam) can frustrate users, raising threshold prioritize specificity may preferable. Conversely, fraud detection, missing fraudulent transaction (false negatives) may far costly, lowering threshold prioritize sensitivity make sense.Fine-tuning threshold allows us align model’s behavior business objectives application-specific goals. Suppose require sensitivity 90%. iteratively adjusting threshold recalculating sensitivity, can identify value meets target. process known defining operating point model.However, adjusting threshold always involves trade-offs. threshold maximizes sensitivity may lower precision, false positives classified positive. Similarly, threshold maximizes specificity may reduce recall, true positives misclassified negative. example, setting threshold 0.9 may result extremely high specificity cost missing true positives.Ultimately, choice threshold depends context problem specific priorities application. Whether focus minimizing errors, achieving regulatory compliance, balancing precision recall, threshold tailored meet objectives. experimenting different thresholds, can optimize model’s performance best suit needs task hand. next section, ’ll explore tools like Receiver Operating Characteristic (ROC) curve Area Curve (AUC), provide systematic way evaluate model performance across range thresholds.","code":"kNN_prob = kNN(formula = formula, train = train_set, \n               test = test_set, k = 5, scaler = \"minmax\",\n               type = \"prob\")\nkNN_prob[1:10, ]\n      yes  no\n   6  0.4 0.6\n   10 0.2 0.8\n   17 0.0 1.0\n   19 0.0 1.0\n   21 0.0 1.0\n   23 0.2 0.8\n   29 0.0 1.0\n   31 0.0 1.0\n   36 0.0 1.0\n   40 0.0 1.0conf.mat(kNN_prob[, 1], actual_test, cutoff = 0.5)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856\nconf.mat(kNN_prob[, 1], actual_test, cutoff = 0.7)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  22   1\n       no  115 862"},{"path":"chapter-evaluation.html","id":"roc-curve-and-auc","chapter":"8 Model Evaluation","heading":"8.5 ROC Curve and AUC","text":"Manually experimenting thresholds insightful often impractical. Additionally, metrics like sensitivity, specificity, precision, recall attempt summarize model’s performance, provide snapshots specific thresholds. need way evaluate model performs across range thresholds, offering broader view behavior. Models often vary achieve accuracy; two models similar overall accuracy may excel entirely different aspects prediction. example, one model might identify positives misclassify many negatives, another might opposite. systematically evaluate model’s performance across thresholds, use Receiver Operating Characteristic (ROC) curve associated metric, Area Curve (AUC). tools provide visual quantitative way assess model’s ability distinguish positive negative classes.ROC curve graphical representation trade-sensitivity (true positive rate) specificity (true negative rate) across different thresholds. plots True Positive Rate (Sensitivity) False Positive Rate (1 - Specificity). concept originated World War II measure radar receiver performance, distinguishing true signals false alarms. modern machine learning, ’s invaluable tool evaluating classifier effectiveness.characteristics ROC curve illustrated Figure 8.1. vertical axis, True Positive Rate (Sensitivity) plotted, False Positive Rate (1 - Specificity) plotted horizontal axis. Several key scenarios highlighted figure:\n- Optimal Performance (Green Curve): model near-perfect performance passes top-left corner, achieving high sensitivity high specificity.\n- Good Performance (Blue Curve): model decent perfect performance curve remains closer top-left corner diagonal line.\n- Random Classifier (Diagonal Line): diagonal line (gray dashed) represents model predictive value, classifying purely random. classifier close line offers little utility.\nFigure 8.1: ROC curve illustrates trade-sensitivity specificity different thresholds. diagonal line represents classifier predictive value (gray dashed line), curves represent varying levels performance: green optimal blue good.\npoint ROC curve corresponds specific threshold. thresholds vary, True Positive Rate (Sensitivity) False Positive Rate (1 - Specificity) change, tracing curve. closer curve top-left corner, better model’s performance distinguishing classes.construct ROC curve, classifier’s predictions sorted estimated probabilities positive class. Starting origin, prediction’s impact sensitivity specificity plotted. Correct predictions (true positives) result vertical movements, incorrect predictions (false positives) lead horizontal shifts.Let’s apply concept k-Nearest Neighbors (kNN) model Example 8.2, obtained probabilities positive class (churn = yes). ’ll use probabilities generate ROC curve model. pROC package R simplifies process. Ensure package installed using install.packages(\"pROC\") proceeding.create ROC curve, two inputs needed: estimated probabilities positive class actual class labels. Using roc() function pROC package, can create ROC curve object follows:can visualize ROC curve using ggroc() function ggplot2 package plot() function basic display. ’s ROC curve kNN model:\nFigure 8.2: ROC curve KNN k = 5, based churn data.\nROC curve visually demonstrates model’s performance across different thresholds. curve closer top-left corner indicates better performance, achieves high sensitivity specificity. diagonal line represents random classifier, providing baseline comparison. case, kNN model’s ROC curve much closer top-left corner, suggesting strong performance distinguishing churners non-churners.Another critical metric derived ROC curve Area Curve (AUC). AUC quantifies overall performance model, summarizing ROC curve single number. AUC value represents probability randomly chosen positive instance higher predicted score randomly chosen negative instance.\nFigure 8.3: AUC summarizes ROC curve single number, representing model’s ability rank positive cases higher negative ones. AUC = 1: Perfect model. AUC = 0.5: better random guessing.\n’s AUC values interpreted:AUC = 1: Perfect classifier.AUC = 0.5: better random guessing.kNN model, can calculate AUC follows:AUC value model 0.849, indicating model ranks positive cases higher negative ones probability 0.849.ROC curve AUC offer comprehensive systematic way evaluate classification models, enabling comparisons models helping identify optimal threshold specific tasks. tools particularly valuable working imbalanced datasets, account trade-offs sensitivity specificity across thresholds. combining insights metrics like precision, recall, F1-score, can develop deeper understanding model performance select best approach problem hand.","code":"\nlibrary(pROC)\n\nroc_knn <- roc(response = actual_test, predictor = kNN_prob[, 1])\nggroc(roc_knn, colour = \"blue\") +\n    ggtitle(\"ROC curve for KNN with k = 5, based on churn data\")auc(roc_knn)\n   Area under the curve: 0.8494"},{"path":"chapter-evaluation.html","id":"metrics-for-multi-class-classification","chapter":"8 Model Evaluation","heading":"8.6 Metrics for Multi-Class Classification","text":"far, ’ve focused binary classification, target variable two categories. However, many real-world problems involve multi-class classification, target variable can belong three categories. Examples include classifying species ecological studies identifying different types vehicles. Evaluating models requires extending metrics handle multiple categories effectively.confusion matrix multi-class classification expands include classes, row representing actual class column representing predicted class. Correct predictions appear diagonal, -diagonal elements indicate misclassifications. structure highlights classes model struggles distinguish.Metrics like Accuracy, Precision, Recall, F1-Score can adapted multi-class problems. class, model evaluated class “positive” class others “negative.” Precision, Recall, F1-Score calculated per class. summarize overall performance, compute averages :Macro-Average: Treats classes equally taking unweighted mean metrics across classes.Micro-Average: Aggregates predictions across classes, giving weight larger classes.Weighted-Average: Weights class’s metric frequency dataset.metrics ensure balanced evaluation, especially dealing imbalanced datasets, classes may significantly fewer samples others.metrics like ROC curve AUC designed binary classification, can adapted multi-class problems using techniques like one-vs-(evaluating class others). However, applications, metrics like macro-averaged F1-Score provide clear practical summary multi-class model’s performance.using metrics, can evaluate well model performs across categories, identify weaknesses specific classes, ensure model aligns requirements task.","code":""},{"path":"chapter-evaluation.html","id":"evaluation-metrics-for-continuous-targets","chapter":"8 Model Evaluation","heading":"8.7 Evaluation Metrics for Continuous Targets","text":"’ve focused evaluating classification models far, many real-world problems involve predicting continuous target variables, house prices, stock market trends, weather forecasts. problems require regression models, assessed using metrics designed continuous data.One common evaluation metrics regression models Mean Squared Error (MSE):\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n, \\(y_i\\) represents actual value, \\(\\hat{y}_i\\) predicted value, \\(n\\) number observations. MSE measures average squared difference predicted actual values, penalizing larger errors heavily. Lower MSE values indicate better model performance, zero representing perfect fit.Although widely used, MSE limitations, particularly sensitivity outliers. robust evaluation, can use Mean Absolute Error (MAE), calculates average absolute difference predicted actual values:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^{n} |y_i - \\hat{y}_i|\n\\]\nUnlike MSE, MAE treats errors equally, making less sensitive extreme values interpretable certain contexts. ’s particularly useful target variable skewed distribution outliers present.Another widely used metric regression models \\(R^2\\) score, coefficient determination. \\(R^2\\) score measures proportion variance target variable model explains. ranges 0 1, higher values indicate better fit. \\(R^2\\) 1 implies model perfectly predicts target variable, value 0 suggests model provides better predictions mean.metrics provide starting point evaluating regression models, choice metric depends specific problem goals. instance, prioritizing interpretability, MAE might meaningful, whereas MSE useful larger errors must penalized. explore metrics greater depth Chapter 10, dive regression modeling techniques.","code":""},{"path":"chapter-evaluation.html","id":"summary-1","chapter":"8 Model Evaluation","heading":"8.8 Summary","text":"chapter, explored critical step Model Evaluation, determines well model performs whether meets requirements problem hand. Starting foundational concepts, examined metrics evaluating classification models, including binary, multi-class, regression models.","code":""},{"path":"chapter-evaluation.html","id":"key-takeaways","chapter":"8 Model Evaluation","heading":"Key Takeaways","text":"Binary Classification Metrics:\nbegan understanding confusion matrix, categorizes predictions true positives, true negatives, false positives, false negatives. , derived metrics like accuracy, sensitivity (recall), specificity, precision, F1-Score evaluate trade-offs different types errors.Binary Classification Metrics:\nbegan understanding confusion matrix, categorizes predictions true positives, true negatives, false positives, false negatives. , derived metrics like accuracy, sensitivity (recall), specificity, precision, F1-Score evaluate trade-offs different types errors.Threshold Tuning:\nRecognizing impact probability thresholds model predictions, discussed adjusting thresholds can help align model specific goals, maximizing sensitivity critical applications prioritizing specificity avoid false positives.Threshold Tuning:\nRecognizing impact probability thresholds model predictions, discussed adjusting thresholds can help align model specific goals, maximizing sensitivity critical applications prioritizing specificity avoid false positives.ROC Curve AUC:\nevaluate model performance across possible thresholds, introduced Receiver Operating Characteristic (ROC) curve Area Curve (AUC). tools provide systematic visual way assess model’s ability distinguish classes, particularly useful comparing multiple models.ROC Curve AUC:\nevaluate model performance across possible thresholds, introduced Receiver Operating Characteristic (ROC) curve Area Curve (AUC). tools provide systematic visual way assess model’s ability distinguish classes, particularly useful comparing multiple models.Multi-Class Classification:\nmulti-class problems, extended metrics like precision, recall, F1-Score calculating per-class metrics aggregating using methods macro-average, micro-average, weighted-average. ensures balanced evaluation, even imbalanced datasets.Multi-Class Classification:\nmulti-class problems, extended metrics like precision, recall, F1-Score calculating per-class metrics aggregating using methods macro-average, micro-average, weighted-average. ensures balanced evaluation, even imbalanced datasets.Regression Metrics:\nFinally, covered metrics evaluating regression models, Mean Squared Error (MSE), Mean Absolute Error (MAE), \\(R^2\\) score, measure accuracy predictions continuous target variables. metrics offer flexibility depending whether minimizing large errors achieving interpretability important.Regression Metrics:\nFinally, covered metrics evaluating regression models, Mean Squared Error (MSE), Mean Absolute Error (MAE), \\(R^2\\) score, measure accuracy predictions continuous target variables. metrics offer flexibility depending whether minimizing large errors achieving interpretability important.","code":""},{"path":"chapter-evaluation.html","id":"closing-thoughts","chapter":"8 Model Evaluation","heading":"Closing Thoughts","text":"chapter emphasized single metric can capture full picture model’s performance. Instead, evaluation consider specific goals constraints problem, whether minimizing errors, handling imbalanced data, aligning business objectives. Proper evaluation ensures model accurate also actionable reliable real-world applications.mastering evaluation techniques, now equipped critically assess model performance, optimize thresholds, select right model task hand. following chapters, build foundation explore advanced modeling techniques evaluation greater detail.","code":""},{"path":"chapter-bayes.html","id":"chapter-bayes","chapter":"9 Naive Bayes Classifier","heading":"9 Naive Bayes Classifier","text":"Naive Bayes Classifier one simplest yet surprisingly powerful algorithms machine learning. family probabilistic classifiers based Bayes’ Theorem, key assumption—often referred “naive”—features conditionally independent given target class. Despite oversimplified assumption, Naive Bayes often delivers strong performance practice, especially domains like text classification, spam detection, sentiment analysis, medical diagnosis.Naive Bayes celebrated speed, scalability, interpretability. efficient training prediction phases, making suitable large-scale datasets high-dimensional feature spaces. example, text classification tasks, thousands features (e.g., words tokens) may involved, Naive Bayes can classify data points milliseconds. simplicity ease implementation make foundational tool beginners go-algorithm many real-world tasks.roots algorithm lie Bayes’ Theorem 5, principle introduced 18th-century mathematician Thomas Bayes. theorem provides mathematical framework updating probability hypothesis new evidence observed. core, Bayes’ Theorem refines understanding event combining prior knowledge (known prior distribution) new information observed data (resulting posterior distribution). ideas form foundation Bayesian methods, wide-ranging applications machine learning, statistics, beyond.","code":""},{"path":"chapter-bayes.html","id":"strengths-and-limitations","chapter":"9 Naive Bayes Classifier","heading":"Strengths and Limitations","text":"strength Naive Bayes lies simplicity computational efficiency. particularly effective :High-dimensional datasets (e.g., text data thousands features).Tasks requiring quick predictions, real-time spam detection.Problems feature independence approximately true independence assumption major limitation.However, limitations also important acknowledge:independence assumption often hold real-world data, especially features highly correlated.Naive Bayes may struggle scenarios continuous data unless Gaussian distributions assumed.tends underperform complex datasets compared sophisticated algorithms like random forests gradient boosting.Despite limitations, Naive Bayes remains reliable, interpretable, robust algorithm. often first choice quick prototyping serves benchmark advanced models.","code":""},{"path":"chapter-bayes.html","id":"what-will-this-chapter-cover","chapter":"9 Naive Bayes Classifier","heading":"What Will This Chapter Cover?","text":"chapter, :Explore mathematical foundations Naive Bayes, including Bayes’ Theorem application classification.Examine Naive Bayes works, step--step explanations examples.Discuss different variants algorithm, including Gaussian Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes, use cases.Highlight strengths, limitations, practical applications.Provide implementation guide R, using real-world datasets (risk dataset liver package) demonstrate effectiveness.end chapter, solid understanding Naive Bayes Classifier, theoretical underpinnings, practical applications, enabling confidently apply real-world problems.","code":""},{"path":"chapter-bayes.html","id":"bayes-theorem-and-probabilistic-foundations","chapter":"9 Naive Bayes Classifier","heading":"9.1 Bayes’ Theorem and Probabilistic Foundations","text":"explain uncertainty predict outcomes using single, elegant equation? presented Equation (9.1), Bayes’ Theorem cornerstone probabilistic reasoning, offering mathematical framework updating beliefs light new evidence. author “Everything Predictable: Bayesian Statistics Explain World” argues Bayesian statistics help predict future also explain fabric rational decision-making. heart powerful framework lies work Thomas Bayes, 18th-century Presbyterian minister self-taught mathematician, whose contributions provided systematic way refine probabilities new information becomes available.","code":""},{"path":"chapter-bayes.html","id":"the-essence-of-bayes-theorem","chapter":"9 Naive Bayes Classifier","heading":"The Essence of Bayes’ Theorem","text":"Bayes’ Theorem formula calculating probability event (\\(\\)) based prior knowledge new evidence (\\(B\\)). answers question: Given already know, belief hypothesis change observe new data?Mathematically, expressed :\\[\\begin{equation}\nP(|B) = P() \\cdot \\frac{P(B|)}{P(B)}\n\\tag{9.1}\n\\end{equation}\\]:\\(P(|B)\\): posterior probability—probability event \\(\\) (hypothesis) given event \\(B\\) (evidence) occurred.\\(P()\\): prior probability—belief \\(\\) observing \\(B\\).\\(P(B|)\\): likelihood—probability observing \\(B\\) assuming \\(\\) true.\\(P(B)\\): evidence—total probability observing \\(B\\).Bayes’ Theorem elegantly combines prior knowledge new evidence refine understanding uncertainty. foundational principle probabilistic learning, quantifying data adjust expectations.see Bayes’ Theorem action, consider practical example risk dataset liver package. , calculate probability customer good risk profile (\\(\\)) given mortgage (\\(B\\)).Example 9.1  Suppose tasked estimating probability customer good risk mortgage. risk dataset contains relevant information:Adding margins contingency table clarity:Now, define events:\\(\\): Customer “good risk”.\\(B\\): Customer mortgage (mortgage = yes).prior probability customer good risk :\\[\nP() = \\frac{\\text{Total Good Risk Cases}}{\\text{Total Cases}} = \\frac{123}{246} = 0.5\n\\]Using Bayes’ Theorem, probability good risk given customer mortgage :\\[\\begin{equation}\n\\label{eq1}\n\\begin{split}\nP(\\text{Good Risk} | \\text{Mortgage = Yes}) & = \\frac{P(\\text{Good Risk} \\cap \\text{Mortgage = Yes})}{P(\\text{Mortgage = Yes})} \\\\\n& = \\frac{\\text{Good Risk Mortgage Cases}}{\\text{Total Mortgage Cases}} \\\\\n& = \\frac{81}{175} \\\\\n& = 0.463\n\\end{split}\n\\end{equation}\\]demonstrates customers mortgages lower probability classified good risk compared overall population.","code":"data(risk)\n\nxtabs(~ risk + mortgage, data = risk)\n              mortgage\n   risk        yes no\n     good risk  81 42\n     bad risk   94 29addmargins(xtabs(~ risk + mortgage, data = risk))\n              mortgage\n   risk        yes  no Sum\n     good risk  81  42 123\n     bad risk   94  29 123\n     Sum       175  71 246"},{"path":"chapter-bayes.html","id":"how-does-bayes-theorem-work","chapter":"9 Naive Bayes Classifier","heading":"How Does Bayes’ Theorem Work?","text":"Bayes’ Theorem leverages conditional probability describe likelihood event changes based specific conditions. example:\n- medical diagnostics, estimates probability disease (\\(\\)) given positive test result (\\(B\\)), accounting test’s reliability disease prevalence.\n- spam detection, computes probability email spam (\\(\\)) based occurrence certain keywords (\\(B\\)).Probability theory provides rigorous mathematical structure reasoning uncertainty, Bayes’ Theorem transforms framework learning data making rational decisions.","code":""},{"path":"chapter-bayes.html","id":"a-gateway-to-naive-bayes","chapter":"9 Naive Bayes Classifier","heading":"A Gateway to Naive Bayes","text":"Naive Bayes Classifier builds directly Bayes’ Theorem. assuming features conditionally independent given target class, simplifies computation probabilities large, high-dimensional datasets. assumption often violated practice, frequently works well enough yield highly effective results, especially applications like text classification spam filtering.proceed, ’ll see Bayes’ Theorem forms foundation Naive Bayes algorithm, enabling handle complex datasets efficiently maintaining simplicity interpretability.","code":""},{"path":"chapter-bayes.html","id":"why-is-it-called-naive","chapter":"9 Naive Bayes Classifier","heading":"9.2 Why is it Called “Naive”?","text":"“naive” Naive Bayes reflects algorithm’s simplifying assumption features conditionally independent , given target class. reality, features often correlated (e.g., income age), assumption dramatically simplifies computations, making algorithm efficient scalable.illustrate, consider risk dataset liver package:can see dataset includes features age, income, marital status, mortgage, number loans. Naive Bayes assumes features independent conditioned target class (risk), can either good risk bad risk. Let’s express mathematically. target variable \\(Y\\) represents risk, possible values \\(y_1 = \\text{good risk}\\) \\(y_2 = \\text{bad risk}\\), predictors \\(X_1, X_2, \\dots, X_5\\). Using Bayes’ Theorem (Equation (9.1)), probability \\(Y = y_1\\) given features :\\[\nP(Y = y_1 | X_1 \\cap \\dots \\cap X_5) = \\frac{P(Y = y_1) \\cdot P(X_1 \\cap \\dots \\cap X_5 | Y = y_1)}{P(X_1 \\cap \\dots \\cap X_5)}\n\\]However, calculating \\(P(X_1 \\cap X_2 \\cap \\dots \\cap X_5 | Y = y_1)\\) computationally challenging, especially number predictors grows. example, datasets hundreds thousands features (common domains like text classification) require enormous amounts memory store probabilities possible combinations features.naive assumption conditional independence simplifies treating feature independent others, given target class. allows joint probability term \\(P(X_1 \\cap \\dots \\cap X_5 | Y = y_1)\\) expressed product individual probabilities:\\[\nP(X_1 \\cap \\dots \\cap X_5 | Y = y_1) = P(X_1 | Y = y_1) \\cdot \\dots \\cdot P(X_5 | Y = y_1)\n\\]transformation eliminates need compute complex joint probabilities allows algorithm operate efficiently, even high-dimensional datasets. Instead handling exponential number combinations, Naive Bayes calculates conditional probabilities feature independently, given class.practice, independence assumption rarely true—features often exhibit degree correlation. However, Naive Bayes frequently performs surprisingly well despite limitation. excels domains like text classification, independence assumption approximately holds, slight violations assumption significantly affect predictive accuracy. example, spam detection systems sentiment analysis often rely Naive Bayes due simplicity, speed, effectiveness.combining strengths ability handle high-dimensional data, Naive Bayes strikes balance computational efficiency predictive power, making foundational algorithm machine learning.","code":"str(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr.loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-bayes.html","id":"the-laplace-smoothing-technique","chapter":"9 Naive Bayes Classifier","heading":"9.3 The Laplace Smoothing Technique","text":"One primary challenges Naive Bayes algorithm vulnerability zero probabilities. issue arises feature category present test data missing training data. happens, algorithm assigns probability zero unseen category, since Naive Bayes multiplies probabilities prediction, even single zero probability results overall prediction probability zero affected class. effectively eliminates class possible prediction can significantly degrade classifier’s performance.address issue, Laplace Smoothing (also known add-one smoothing) employed. Named French mathematician Pierre-Simon Laplace, technique ensures every class-feature combination non-zero probability, even missing training data. Laplace smoothing works adding small constant (commonly \\(k = 1\\)) count frequency table, ensuring category left zero probability.illustrate necessity Laplace smoothing, consider marital variable risk dataset. Suppose category married entirely absent class bad risk training data due imbalance sampling limitations. Let’s visualize situation:case, probability \\(P(\\text{bad risk} | \\text{married})\\) zero. creates significant problem: Naive Bayes classifier completely ignore instance marital = married predicting bad risk class. However, intuitively, even examples absent training data, probability still small non-zero value reflect possibility combination occur test data.Laplace smoothing resolves modifying calculation. adds small constant \\(k\\) (usually \\(k = 1\\)) count frequency table. smoothed probability calculated :\\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married}) + k}{\\text{count}(\\text{bad risk}) + k \\cdot \\text{total unique categories } \\text{marital}}\n\\]adjustment ensures :Counts adjusted: category receives additional \\(k\\) count numerator.Denominator expanded: total count increased \\(k \\times \\text{number categories}\\), ensuring probability distribution remains valid.guarantees every feature-class combination small, non-zero probability, thus preventing zero probabilities dominating predictions.R, naivebayes package provides laplace argument apply Laplace smoothing. default, laplace = 0, meaning smoothing applied. apply smoothing, simply set laplace = 1. instance:ensures class-feature combination zero probability, improving robustness Naive Bayes classifier, especially dealing small imbalanced training datasets. \\(k = 1\\) commonly used, value \\(k\\) can adjusted based specific domain knowledge requirements. However, practice, setting \\(k = 1\\) suffices use cases.Laplace smoothing simple yet effective technique highlights minor adjustments can address critical limitations machine learning algorithms. ensuring probabilities remain non-zero, enhances reliability robustness Naive Bayes real-world scenarios.","code":"            risk\n   marital   good risk bad risk\n     single         21       11\n     married        51        0\n     other           8       10\nlibrary(naivebayes)\n\n# Fit Naive Bayes with Laplace smoothing\nmodel <- naive_bayes(risk ~ age + income + marital + mortgage + nr.loans, \n                     data = risk, \n                     laplace = 1)"},{"path":"chapter-bayes.html","id":"types-of-naive-bayes-classifiers","chapter":"9 Naive Bayes Classifier","heading":"9.4 Types of Naive Bayes Classifiers","text":"Naive Bayes flexible algorithm variants tailored different types data problem domains. choice Naive Bayes classifier depends nature features assumptions underlying distribution. three common types :Multinomial Naive Bayes: Best suited categorical discrete count features, word frequencies text data. example, marital variable risk dataset categorical, making good fit variant.Multinomial Naive Bayes: Best suited categorical discrete count features, word frequencies text data. example, marital variable risk dataset categorical, making good fit variant.Bernoulli Naive Bayes: Designed binary features (e.g., 0s 1s). variant ideal datasets yes/presence/absence features. instance, mortgage variable risk dataset, two categories (yes ), fits variant.Bernoulli Naive Bayes: Designed binary features (e.g., 0s 1s). variant ideal datasets yes/presence/absence features. instance, mortgage variable risk dataset, two categories (yes ), fits variant.Gaussian Naive Bayes: Used continuous features assumed follow normal (Gaussian) distribution. example, age income variables risk dataset continuous thus suitable variant.Gaussian Naive Bayes: Used continuous features assumed follow normal (Gaussian) distribution. example, age income variables risk dataset continuous thus suitable variant.variant optimized different data types, making essential choose one aligns dataset’s characteristics. understanding distinctions, can select appropriate Naive Bayes classifier achieve optimal performance. following sections, delve variant, exploring unique characteristics use cases.","code":""},{"path":"chapter-bayes.html","id":"case-study-predicting-risk-profiles","chapter":"9 Naive Bayes Classifier","heading":"9.5 Case Study: Predicting Risk Profiles","text":"case study, apply Naive Bayes classifier predict financial risk using real-world risk dataset liver package R. goal classify customers either “good risk” “bad risk” based several predictors.","code":""},{"path":"chapter-bayes.html","id":"overview-of-the-dataset-1","chapter":"9 Naive Bayes Classifier","heading":"Overview of the Dataset","text":"start loading dataset examining structure:risk dataset data.frame 6 variables 246 observations. contains 5 predictors target variable risk, binary factor two levels: “good risk” “bad risk.” predictors dataset :age: Age years.marital: Marital status (levels: “single,” “married,” “”).income: Yearly income.mortgage: Whether customer mortgage (levels: “yes,” “”).nr_loans: Number loans customer .risk: target variable (levels: “good risk,” “bad risk”).additional details dataset, refer documentation .","code":"data(risk)\n\nstr(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr.loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-bayes.html","id":"data-preparation-1","chapter":"9 Naive Bayes Classifier","heading":"Data Preparation","text":"evaluate model’s performance, partition dataset training testing sets, using 80/20 split. ensures classifier trained one subset tested unseen data:set.seed() function ensures reproducibility data split.validate partitioning, test whether proportions marital variable similar training testing sets. use chi-squared test compare proportions across three categories marital:hypotheses test :\\[\n\\begin{aligned}\nH_0 &: \\text{proportions `marital` categories training test sets.} \\\\\nH_a &: \\text{least one category different proportion.}\n\\end{aligned}\n\\]Since p-value exceeds \\(\\alpha = 0.05\\), fail reject \\(H_0\\). indicates proportions marital categories statistically similar training test sets, confirming partitioning valid.","code":"\nset.seed(5)\n\ndata_sets = partition(data = risk, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test = test_set$riskchisq.test(x = table(train_set$marital), y = table(test_set$marital))\n   \n    Pearson's Chi-squared test\n   \n   data:  table(train_set$marital) and table(test_set$marital)\n   X-squared = 6, df = 4, p-value = 0.1991"},{"path":"chapter-bayes.html","id":"applying-the-naive-bayes-classifier","chapter":"9 Naive Bayes Classifier","heading":"Applying the Naive Bayes Classifier","text":"specify model formula, risk target variable predictors features:Using naivebayes package, train Naive Bayes classifier training data:naive_bayes() function computes conditional probabilities feature given target class. categorical predictors (e.g., marital, mortgage), calculates class-conditional probabilities. continuous predictors (e.g., age, income, nr.loans), assumes Gaussian distribution calculates mean standard deviation class.summary output provides following details:Categorical predictors (e.g., marital, mortgage): Class-conditional probabilities.Continuous predictors (e.g., age, income, nr.loans): Means standard deviations Gaussian distributions.information forms basis making predictions trained model.","code":"\nformula = risk ~ age + income + mortgage + nr.loans + maritallibrary(naivebayes)\n\nnaive_bayes = naive_bayes(formula, data = train_set)\n\nnaive_bayes\n   \n   ================================= Naive Bayes ==================================\n   \n   Call:\n   naive_bayes.formula(formula = formula, data = train_set)\n   \n   -------------------------------------------------------------------------------- \n    \n   Laplace smoothing: 0\n   \n   -------------------------------------------------------------------------------- \n    \n   A priori probabilities: \n   \n   good risk  bad risk \n   0.4923858 0.5076142 \n   \n   -------------------------------------------------------------------------------- \n    \n   Tables: \n   \n   -------------------------------------------------------------------------------- \n   :: age (Gaussian) \n   -------------------------------------------------------------------------------- \n         \n   age    good risk  bad risk\n     mean 46.453608 35.470000\n     sd    8.563513  9.542520\n   \n   -------------------------------------------------------------------------------- \n   :: income (Gaussian) \n   -------------------------------------------------------------------------------- \n         \n   income good risk  bad risk\n     mean 48888.987 27309.560\n     sd    9986.962  7534.639\n   \n   -------------------------------------------------------------------------------- \n   :: mortgage (Bernoulli) \n   -------------------------------------------------------------------------------- \n           \n   mortgage good risk  bad risk\n        yes 0.6804124 0.7400000\n        no  0.3195876 0.2600000\n   \n   -------------------------------------------------------------------------------- \n   :: nr.loans (Gaussian) \n   -------------------------------------------------------------------------------- \n           \n   nr.loans good risk  bad risk\n       mean 1.0309278 1.6600000\n       sd   0.7282057 0.7550503\n   \n   -------------------------------------------------------------------------------- \n   :: marital (Categorical) \n   -------------------------------------------------------------------------------- \n            \n   marital    good risk   bad risk\n     single  0.38144330 0.49000000\n     married 0.52577320 0.11000000\n     other   0.09278351 0.40000000\n   \n   --------------------------------------------------------------------------------summary(naive_bayes)\n   \n   ================================= Naive Bayes ================================== \n    \n   - Call: naive_bayes.formula(formula = formula, data = train_set) \n   - Laplace: 0 \n   - Classes: 2 \n   - Samples: 197 \n   - Features: 5 \n   - Conditional distributions: \n       - Bernoulli: 1\n       - Categorical: 1\n       - Gaussian: 3\n   - Prior probabilities: \n       - good risk: 0.4924\n       - bad risk: 0.5076\n   \n   --------------------------------------------------------------------------------"},{"path":"chapter-bayes.html","id":"prediction-and-model-evaluation","chapter":"9 Naive Bayes Classifier","heading":"Prediction and Model Evaluation","text":"now use trained model predict posterior probabilities test set:output contains predicted probabilities class:first column shows probability “good risk.”second column shows probability “bad risk.”example, first row probability 0.995 “bad risk,” indicates high likelihood first customer test set belongs “bad risk” class.","code":"prob_naive_bayes = predict(naive_bayes, test_set, type = \"prob\")\n\nround(head(prob_naive_bayes, n = 10), 3)\n         good risk bad risk\n    [1,]     0.001    0.999\n    [2,]     0.013    0.987\n    [3,]     0.000    1.000\n    [4,]     0.184    0.816\n    [5,]     0.614    0.386\n    [6,]     0.193    0.807\n    [7,]     0.002    0.998\n    [8,]     0.002    0.998\n    [9,]     0.378    0.622\n   [10,]     0.283    0.717"},{"path":"chapter-bayes.html","id":"confusion-matrix-1","chapter":"9 Naive Bayes Classifier","heading":"Confusion Matrix","text":"evaluate classification performance, compute confusion matrix using cutoff 0.5:confusion matrix summarizes model’s predictions:True Positives (TP): Correctly predicted “good risk.”True Negatives (TN): Correctly predicted “bad risk.”False Positives (FP): Predicted “good risk” “bad risk.”False Negatives (FN): Predicted “bad risk” “good risk.”values confusion matrix reveal model’s accuracy types errors cutoff 0.5. example, classifier makes “24 + 20” correct predictions “3 + 2” incorrect predictions.","code":"prob_naive_bayes = prob_naive_bayes[, 1] # Probability of \"good risk\"\n\nconf.mat(prob_naive_bayes, actual_test, cutoff = 0.5, \n         reference = \"good risk\")\n              Actual\n   Predict     good risk bad risk\n     good risk        24        3\n     bad risk          2       20\n\nconf.mat.plot(prob_naive_bayes, actual_test, cutoff = 0.5, \n              reference = \"good risk\")"},{"path":"chapter-bayes.html","id":"roc-curve-and-auc-1","chapter":"9 Naive Bayes Classifier","heading":"ROC Curve and AUC","text":"evaluate model, compute ROC curve AUC value:ROC curve illustrates trade-sensitivity specificity different thresholds. closer curve top-left corner, better model’s performance.Finally, calculate AUC value:AUC value, 0.957, quantifies model’s ability rank positive instances higher negative ones. value closer 1 indicates excellent performance, value 0.5 represents random guessing.applying Naive Bayes classifier risk dataset, demonstrated ability efficiently classify customers “good risk” “bad risk.” metrics confusion matrix, ROC curve, AUC, evaluated model’s predictive power identified strengths limitations. process highlights simplicity interpretability Naive Bayes, making practical choice many real-world classification problems.","code":"\nroc_naive_bayes = roc(actual_test, prob_naive_bayes)\n\nggroc(roc_naive_bayes)round(auc(roc_naive_bayes), 3)\n   [1] 0.957"},{"path":"chapter-bayes.html","id":"exercises-4","chapter":"9 Naive Bayes Classifier","heading":"9.6 Exercises","text":"..","code":""},{"path":"chapter-regression.html","id":"chapter-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10 Regression Modeling: From Basics to Advanced Techniques","text":"Regression modelling cornerstone statistical analysis centuries, evolving one powerful versatile tools data science. origins trace back Isaac Newton’s work 1700s, term “regression” later introduced Francis Galton 19th century describe biological phenomena. Early pioneers like Legendre Gauss laid mathematical groundwork development least squares method, today, thanks advancements computing programming languages like R, regression analysis accessible, scalable, integral solving real-world problems.Charles Wheelan eloquently puts book “Naked Statistics”:Regression modelling hydrogen bomb statistics arsenal.makes regression powerful ability quantify relationships variables, uncover patterns, make predictions. Whether ’re estimating impact advertising spend sales, forecasting housing prices, identifying risk factors disease, regression modelling provides foundation evidence-based decision-making.chapter, explore essentials regression, simplest form—simple linear regression—advanced techniques like generalized linear models (GLMs) non-linear regression. end, understand mathematical principles behind regression models also gain practical experience applying R analyze interpret real-world data.","code":""},{"path":"chapter-regression.html","id":"sec-simple-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.1 Simple Linear Regression","text":"explore regression methods estimation prediction, use marketing dataset liver package. dataset provides straightforward practical example real-world scenario, company seeks optimize advertising spending maximize revenue. small size clean structure make ideal learning tool understanding regression concepts. dataset contains information advertising campaigns, including spending, clicks, impressions, transactions, daily revenue. target variable, revenue, represents daily revenue generated, seven variables serve predictors.marketing dataset, formatted data.frame, consists 40 observations (rows) 8 variables (columns):spend: Daily spending pay-per-click (PPC) advertising.clicks: Number clicks ads.impressions: Number ad impressions per day.display: Whether display campaign running (yes ).transactions: Number transactions per day.click.rate: Click-rate (CTR).conversion.rate: Conversion rate.revenue: Daily revenue (target variable).Let’s load examine structure dataset:dataset includes 8 variables 40 observations, 7 predictors one numerical-continuous target variable (revenue). clean dataset serves perfect starting point regression analysis.understand relationships variables, use pairs.panels() function psych package create visualization:plot includes: Bivariate scatter plots (bottom-left) showing relationships pairs variables; Histograms (diagonal) showing distribution variable; Correlation coefficients (top-right) quantifying strength linear relationships. example, variables spend revenue exhibit strong positive linear relationship, correlation coefficient 0.79. indicates higher spending generally associated higher revenue, supporting hypothesis linear relationship variables.","code":"data(marketing, package = \"liver\")\n\nstr(marketing)\n   'data.frame':    40 obs. of  8 variables:\n    $ spend          : num  22.6 37.3 55.6 45.4 50.2 ...\n    $ clicks         : int  165 228 291 247 290 172 68 112 306 300 ...\n    $ impressions    : int  8672 11875 14631 11709 14768 8698 2924 5919 14789 14818 ...\n    $ display        : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ transactions   : int  2 2 3 2 3 2 1 1 3 3 ...\n    $ click.rate     : num  1.9 1.92 1.99 2.11 1.96 1.98 2.33 1.89 2.07 2.02 ...\n    $ conversion.rate: num  1.21 0.88 1.03 0.81 1.03 1.16 1.47 0.89 0.98 1 ...\n    $ revenue        : num  58.9 44.9 141.6 209.8 197.7 ...\npairs.panels(marketing)"},{"path":"chapter-regression.html","id":"fitting-a-simple-linear-regression-model","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Fitting a Simple Linear Regression Model","text":"begin analysis, focus simple linear regression model examines relationship single predictor (spend) target variable (revenue). provides foundational understanding regression expanding complex models involving multiple predictors. First, let’s visualize relationship scatter plot overlay regression line:\nFigure 10.1: Scatter plot daily revenue (€) versus daily spend (€) 40 observations, fitted least-squares regression line (blue) showing linear relationship.\nFigure 10.1 displays scatter plot spend versus revenue marketing dataset, fitted least-squares regression line.regression equation :\\[\n\\hat{y} = b_0 + b_1x\n\\]\n:\\(b_0\\): Intercept y-axis (estimated revenue spending zero).\\(b_1\\): Slope line (change revenue one-unit increase spending).\\(\\hat{y}\\): Predicted value dependent variable (revenue) given independent variable (spend).\\(x\\): Independent variable (spend).","code":""},{"path":"chapter-regression.html","id":"estimating-the-model-in-r","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Estimating the Model in R","text":"use lm() function estimate regression coefficients:regression results summarized using summary() function:output includes estimated coefficients, standard errors, t-statistics, p-values, goodness--fit metrics. estimated regression equation :\\[\n\\text{revenue} = 15.71 + 5.25 \\cdot \\text{spend}\n\\]means :intercept (\\(b_0\\)) 15.71, representing estimated daily revenue money spent PPC advertising.slope (\\(b_1\\)) 5.25, meaning every additional €1 spent, daily revenue increases approximately 5.25.","code":"\nsimple_reg = lm(revenue ~ spend, data = marketing)summary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09"},{"path":"chapter-regression.html","id":"interpreting-the-regression-line","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Interpreting the Regression Line","text":"regression line serves linear approximation relationship spending revenue. can use make predictions. instance, suppose want predict revenue day €25 spent advertising. Using regression equation:\\[\n\\hat{y} = 15.71 + 5.25 \\cdot 25 = 147\n\\]Thus, estimated daily revenue €147. Suppose marketing team wants plan budget upcoming campaign. spend €25 PPC advertising, model predicts estimated revenue €147. insight can help guide spending decisions maximize returns.","code":""},{"path":"chapter-regression.html","id":"residuals-and-model-fit","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Residuals and Model Fit","text":"Residuals (\\(y - \\hat{y}\\)) represent vertical distances observed data points regression line. example, one day dataset spend €25 actual revenue 185.36. prediction error (residual) :\\[\n\\text{Residual} = y - \\hat{y} = 185.36 - 147 = 38.36\n\\]Residuals help us identify potential patterns deviations linear model may capture, ensuring model’s assumptions hold. example, residuals exhibit systematic patterns, curvature, may indicate relationship variables truly linear, signaling need model adjustments.least-squares method minimizes sum squared residuals (SSE):\\[\\begin{equation}\n\\text{SSE} = \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2,\n\\tag{10.1}\n\\end{equation}\\]\\(y_i\\) observed value, \\(\\hat{y}_i\\) predicted value, \\(n\\) number observations. Minimizing SSE ensures regression line best linear fit data, providing accurate predictions. approach remains commonly used linear regression due simplicity efficiency.","code":""},{"path":"chapter-regression.html","id":"key-insights","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Key Insights","text":"important takeaways simple linear regression analysis:Intercept (\\(b_0\\)): intercept represents estimated revenue spending zero. dataset, \\(b_0 = 15.71\\), makes sense days spending.Slope (\\(b_1\\)): slope indicates every €1 increase spending, revenue expected increase approximately 5.25.Prediction Accuracy: regression line provides excellent linear approximation, individual residuals reveal prediction errors specific data points.summary, simple linear regression provides effective way model interpret relationship two variables. analyzing marketing dataset, demonstrated estimate, interpret, apply regression model make predictions. next sections expand concepts exploring techniques assess model quality extend regression multiple predictors.foundational understanding simple linear regression, now ready explore techniques assess quality regression models extend methods multiple predictors next sections.","code":""},{"path":"chapter-regression.html","id":"hypothesis-testing-in-simple-linear-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.2 Hypothesis Testing in Simple Linear Regression","text":"regression analysis, use estimated slope \\(b_1\\) sample regression equation draw inferences unknown slope \\(\\beta_1\\) population regression equation. population regression equation provides linear approximation relationship predictor (\\(x\\)) response (\\(y\\)) entire population, just sample data. expressed :\\[\ny = \\beta_0 + \\beta_1x + \\epsilon\n\\]equation:\\(\\beta_0\\): population intercept, representing expected value \\(y\\) \\(x = 0\\).\\(\\beta_1\\): population slope, representing change \\(y\\) one-unit increase \\(x\\).\\(\\epsilon\\): random error term captures variability \\(y\\) explained linear relationship.goal hypothesis testing regression determine whether \\(\\beta_1\\), slope population regression line, significantly different zero. \\(\\beta_1 = 0\\), regression equation simplifies :\\[\ny = \\beta_0 + \\epsilon\n\\]indicates linear relationship predictor \\(x\\) response \\(y\\). Conversely, \\(\\beta_1 \\neq 0\\), linear relationship exists \\(x\\) \\(y\\). formally test , conduct following hypothesis test:\\[\n\\bigg\\{\n\\begin{matrix}\n  H_0: \\beta_1 =  0 \\quad \\text{(linear relationship \\(x\\) \\(y\\))} \\qquad  \\\\\n  H_a: \\beta_1 \\neq 0 \\quad \\text{(linear relationship exists \\(x\\) \\(y\\))}\n\\end{matrix}\n\\]simple linear regression, summary output model provides necessary information test hypotheses. Specifically, slope regression line estimated sample data \\(b_1\\), along following key components:Standard error: Measures variability slope estimate \\(b_1\\).t-statistic: Quantifies many standard errors \\(b_1\\) away 0.p-value: Indicates probability observing t-statistic extreme one calculated, assuming \\(H_0\\) (.e., \\(\\beta_1 = 0\\)) true.Let’s revisit simple linear regression results marketing dataset, modeled revenue (daily revenue) function spend (daily advertising spend). estimated slope \\(b_1\\) spend 5.25, following corresponding hypothesis test results:output:t-statistic slope 7.93.p-value 1.4150362^{-9} (close zero).p-value, 1.4150362^{-9}, extremely small (near zero). represents probability observing t-statistic extreme one calculated, assuming relationship spend revenue (\\(\\beta_1 = 0\\)).Since p-value much smaller commonly used significance level (\\(\\alpha = 0.05\\)), reject null hypothesis \\(H_0\\). leads conclusion slope \\(\\beta_1\\) significantly different zero, providing strong evidence linear relationship spend revenue.hypothesis test confirms predictor spend statistically significant impact response revenue. Specifically:slope estimate \\(b_1 = 5.25\\) suggests additional €1 spent advertising, daily revenue increases approximately 5.25 units.result validates use spend predictor revenue, providing quantitative measure relationship.Hypothesis testing simple linear regression provides formal way evaluate significance relationship predictor response variable. statistically significant slope (\\(\\beta_1\\)) indicates changes predictor \\(x\\) associated changes response \\(y\\).next sections, explore additional techniques evaluate regression model quality extend concepts multiple predictors, enabling comprehensive analyses better predictions.","code":"summary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09"},{"path":"chapter-regression.html","id":"measuring-the-quality-of-a-regression-model","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.3 Measuring the Quality of a Regression Model","text":"can evaluate effectiveness regression model? reject null hypothesis (\\(H_0: \\beta_1 = 0\\)) hypothesis test, model provides evidence linear relationship predictor response variable, making ineffective. However, establish \\(\\beta_1 \\neq 0\\), rely additional metrics assess quality regression model. Two key statistics purpose Residual Standard Error (SSE) \\(R^2\\) (R-squared) statistic.Residual Standard Error (RSE) indicates measure size “typical” prediction error. defined :\\[\nRSE = \\sqrt{\\frac{1}{n-p-1} SSE}\n\\]\nSSE sum squared errors defined Equation (10.1), \\(n\\) number observations, \\(p\\) number predictors model. RSE provides estimate typical prediction error, smaller values indicating better fit.smaller SSE indicates regression model accurately captures variability response variable, producing better predictions. example, simple linear regression model marketing dataset, SSE 3.3447117^{5}. value represents total squared error predicting revenue spend. closer SSE zero, smaller typical prediction error, indicates higher-quality model.\\(R^2\\) (R-squared) statistic evaluates well regression model explains variability response variable. defined :\\[\nR^2 = 1 - \\frac{SSE}{SST}\n\\]\\(SST\\) total variability response variable (\\(y\\)) fitting model, \\(SSE\\) variability remains unexplained fitting model. \\(R^2\\) represents proportion variability \\(y\\) accounted linear relationship \\(x\\). instance, \\(R^2 = 0.80\\), means 80% variability \\(y\\) explained model, remaining 20% due factors captured model.simple linear regression model marketing dataset, \\(R^2 = 62\\)% (0.62 decimal). means 62% variability daily revenue explained linear relationship daily spend. higher \\(R^2\\) value suggests better fit, values close 100% indicating excellent fit. However, \\(R^2\\) alone assess whether model generalize well new data, important combine diagnostics validation techniques.also important understand relationship \\(R^2\\) correlation coefficient (\\(r\\)). simple linear regression, square correlation coefficient (\\(r^2\\)) equal \\(R^2\\). example, correlation coefficient spend revenue marketing dataset 0.79, squaring value gives 0.62—value equals \\(R^2\\). highlights \\(R^2\\) reflects strength linear relationship predictor response variables.might also wonder difference Multiple R-squared Adjusted R-squared. \\(R^2\\) measures proportion variance \\(y\\) explained model, Adjusted \\(R^2\\) accounts number predictors model. adjustment prevents \\(R^2\\) artificially inflating irrelevant predictors added model. Adjusted \\(R^2\\) calculated :\\[\n\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\cdot \\frac{n-1}{n-p-1}\n\\]\\(n\\) number observations, \\(p\\) number predictors. Adjusted \\(R^2\\) penalizes models unnecessary predictors, providing reliable measure model performance.simple linear regression (one predictor), \\(R^2\\) Adjusted \\(R^2\\) one predictor. However, multiple linear regression, Adjusted \\(R^2\\) typically lower better indicator well model generalizes new data. particularly helpful comparing models different numbers predictors, adjusts model complexity.conclusion, Residual Standard Error (SSE), \\(R^2\\), Adjusted \\(R^2\\) essential metrics evaluating quality regression model. SSE provides measure model’s prediction accuracy, \\(R^2\\) Adjusted \\(R^2\\) assess well model explains variability response variable. metrics help guide model interpretation selection, especially extending complex models.","code":""},{"path":"chapter-regression.html","id":"sec-multiple-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.4 Multiple Linear Regression","text":"many real-world scenarios, datasets often include numerous variables, many may linear relationship target (response) variable. Multiple regression modeling provides method analyze quantify relationships incorporating multiple predictors single model. , multiple regression offers greater precision estimation prediction compared simple regression, much like regression improves upon univariate estimates.illustrate multiple regression using marketing dataset, add predictor display simple regression model (previously used spend predictor) evaluate whether improves model’s quality. general equation multiple regression model :\\[\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\\(p\\) number predictors, \\(\\beta_0\\) intercept, \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) coefficients (slopes) representing relationship predictor response variable.case, equation two predictors (spend display) becomes:\\[\n\\hat{y} = b_0 + b_1 \\cdot \\text{spend} + b_2 \\cdot \\text{display}\n\\]R, fit model using lm() function:estimated regression equation output :\\[\n\\text{revenue} = -41.44 + 5.36 \\cdot \\text{spend} + 104.29 \\cdot \\text{display}\n\\]model:intercept (\\(b_0\\)) -41.44, representing estimated revenue predictors (spend display) zero.coefficient spend (\\(b_1\\)) 5.36, indicating every additional €1 spent, revenue increases approximately 5.36 euros, holding display constant.coefficient display (\\(b_2\\)) 104.29, meaning running display campaign (display = 1) increases revenue approximately 104.29 euros, holding spend constant.illustrate, consider day company spends €25 advertising runs display campaign (display = 1). Using regression equation, predicted revenue :\\[\n\\hat{y} = -41.44 + 5.36 \\cdot 25 + 104.29 \\cdot 1 = 196.74\n\\]Thus, predicted revenue day approximately €196.74.prediction error (residual) specific day calculated difference actual revenue \\(y\\) predicted revenue \\(\\hat{y}\\):\\[\n\\text{Residual} = y - \\hat{y} = 185.36 - 196.74 = -11.49\n\\]Interestingly, prediction error smaller one produced simpler regression model, used spend predictor. simple regression model, residual 38.26. improvement prediction accuracy due inclusion display additional predictor, provides information variability revenue.Adding predictor display reduces prediction error also improves overall quality regression model. example:Residual Standard Error (RSE): RSE measures typical size prediction errors. simple regression model, RSE approximately 93.82. multiple regression model, RSE decreased approximately 78.14, indicating smaller, accurate prediction errors.Residual Standard Error (RSE): RSE measures typical size prediction errors. simple regression model, RSE approximately 93.82. multiple regression model, RSE decreased approximately 78.14, indicating smaller, accurate prediction errors.\\(R^2\\) (R-squared): \\(R^2\\) statistic measures proportion variability response variable (revenue) explained predictors. simple regression model, \\(R^2 = 62\\%\\). adding display, \\(R^2 = 75\\%\\). indicates larger proportion variability revenue now explained model.\\(R^2\\) (R-squared): \\(R^2\\) statistic measures proportion variability response variable (revenue) explained predictors. simple regression model, \\(R^2 = 62\\%\\). adding display, \\(R^2 = 75\\%\\). indicates larger proportion variability revenue now explained model.Adjusted \\(R^2\\): Unlike \\(R^2\\), Adjusted \\(R^2\\) accounts number predictors, making reliable metric comparing models different numbers predictors. Adjusted \\(R^2\\) increased 61% simple regression model 73% multiple regression model. confirms additional predictor contributes meaningfully model.Adjusted \\(R^2\\): Unlike \\(R^2\\), Adjusted \\(R^2\\) accounts number predictors, making reliable metric comparing models different numbers predictors. Adjusted \\(R^2\\) increased 61% simple regression model 73% multiple regression model. confirms additional predictor contributes meaningfully model.summary, multiple regression model demonstrates clear improvements simple regression model. including display additional predictor, achieve:Better Fit: model explains larger proportion variability revenue, reflected higher \\(R^2\\) Adjusted \\(R^2\\).Reduced Prediction Error: smaller RSE indicates model provides accurate predictions.Enhanced Interpretability: coefficients allow us quantify effect running display campaign increasing spending revenue.Multiple regression modeling offers robust framework analyzing relationships response variable multiple predictors. added flexibility makes indispensable tool understanding complex datasets improving predictive performance. next sections, explore evaluate model assumptions, perform diagnostics, refine regression models ensure validity reliability.","code":"multiple_reg = lm(revenue ~ spend + display, data = marketing)\n\nsummary(multiple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend + display, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -189.420  -45.527    5.566   54.943  154.340 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) -41.4377    32.2789  -1.284 0.207214    \n   spend         5.3556     0.5523   9.698 1.05e-11 ***\n   display     104.2878    24.7353   4.216 0.000154 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 78.14 on 37 degrees of freedom\n   Multiple R-squared:  0.7455, Adjusted R-squared:  0.7317 \n   F-statistic: 54.19 on 2 and 37 DF,  p-value: 1.012e-11"},{"path":"chapter-regression.html","id":"generalized-linear-models-glms","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5 Generalized Linear Models (GLMs)","text":"linear regression powerful tool modeling continuous outcomes, limited dealing non-continuous response variables, binary count data. Generalized Linear Models (GLMs) extend linear regression framework introducing link function variance function, enabling model wide range response variable distributions. makes GLMs versatile widely applicable data science, can handle binary, count, non-continuous outcomes.section, focus two widely used GLMs:Logistic regression, used binary classification tasks.Poisson regression, used modeling count data.","code":""},{"path":"chapter-regression.html","id":"logistic-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.1 Logistic Regression","text":"Logistic regression GLM designed model binary outcomes, response variable takes two values, 0/1 yes/. Instead predicting response variable directly, logistic regression estimates probability outcome one class versus . ensure predicted probabilities lie 0 1, model uses logit function, defined :\\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\], \\(p\\) represents probability outcome 1, logit function transforms linear combination predictors probability.","code":""},{"path":"chapter-regression.html","id":"logistic-regression-in-r","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.2 Logistic Regression in R","text":"illustrate logistic regression, consider task want predict whether customer churn (leave service) based set predictors. churn dataset, use example, contains several variables related customer behavior. However, based prior knowledge, select following predictors model:account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, customer.calls.use glm() function fit logistic regression model R, specifying response variable churn (binary variable) predictors specified ::churn binary response variable.predictors specified formula.family = binomial indicates logistic regression model.view model’s summary interpret results, use summary() function:summary output provides estimated coefficients, standard errors, z-statistics, p-values predictor. Predictors high p-values (typically > 0.05) statistically significant reconsidered removal. instance, account.length high p-value, exclude model re-run regression. iterative process ensures meaningful predictors included final model.","code":"\ndata(churn)\n\nformula = churn ~ account.length + voice.messages + day.mins + eve.mins + \n                  night.mins + intl.mins + customer.calls + intl.plan + voice.plan\n\nlogreg_1 = glm(formula, data = churn, family = binomial)summary(logreg_1)\n   \n   Call:\n   glm(formula = formula, family = binomial, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n   (Intercept)     8.8917584  0.6582188  13.509  < 2e-16 ***\n   account.length -0.0013811  0.0011453  -1.206   0.2279    \n   voice.messages -0.0355317  0.0150397  -2.363   0.0182 *  \n   day.mins       -0.0136547  0.0009103 -15.000  < 2e-16 ***\n   eve.mins       -0.0071210  0.0009419  -7.561 4.02e-14 ***\n   night.mins     -0.0040518  0.0009048  -4.478 7.53e-06 ***\n   intl.mins      -0.0882514  0.0170578  -5.174 2.30e-07 ***\n   customer.calls -0.5183958  0.0328652 -15.773  < 2e-16 ***\n   intl.planno     2.0958198  0.1214476  17.257  < 2e-16 ***\n   voice.planno   -2.1637477  0.4836735  -4.474 7.69e-06 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for binomial family taken to be 1)\n   \n       Null deviance: 4075.0  on 4999  degrees of freedom\n   Residual deviance: 3174.3  on 4990  degrees of freedom\n   AIC: 3194.3\n   \n   Number of Fisher Scoring iterations: 6"},{"path":"chapter-regression.html","id":"poisson-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.3 Poisson Regression","text":"Poisson regression GLM used modeling count data, response variable represents number occurrences event within fixed interval. Examples include number customer service calls, website visits, product purchases. Poisson regression model assumes :response variable follows Poisson distribution.mean distribution equals variance.log mean (\\(\\ln(\\lambda)\\)) can expressed linear combination predictors.Poisson regression model expressed :\\[\n\\ln(\\lambda) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\], \\(\\lambda\\) expected count (mean) response variable, predictors (\\(x_1, x_2, \\dots, x_p\\)) influence log \\(\\lambda\\).","code":""},{"path":"chapter-regression.html","id":"poisson-regression-in-r","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.4 Poisson Regression in R","text":"demonstrate Poisson regression, consider task want predict number customer service calls (customer.calls) based following predictors churn dataset:churn, account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins.Since customer.calls integer-valued variable, Poisson regression appropriate linear regression task. fit Poisson regression model using glm() function::\n- customer.calls response variable.\n- predictors specified formula.\n- family = poisson indicates Poisson regression model.evaluate model, view summary regression output:summary output provides estimated coefficients, standard errors, z-statistics, p-values predictor. Predictors high p-values (typically > 0.05) statistically significant reconsidered. instance, predictors voice.messages, night.mins, voice.plan high p-values, can excluded model.example, coefficient significant predictor (e.g., intl.plan) can interpreted expected percentage change mean customer.calls one-unit change predictor, holding variables constant.Generalized Linear Models (GLMs) provide flexible framework modeling response variables non-normal distributions. Logistic regression Poisson regression two common GLMs allow us model binary count data, respectively. models extend applicability regression techniques wide range practical problems data science, customer churn prediction event counts. iteratively refining model excluding non-significant predictors, ensure final model interpretable predictive.next sections, explore techniques validating improving regression models ensure robustness practical utility.","code":"\nformula = customer.calls ~ churn + voice.messages + day.mins + eve.mins + \n                           night.mins + intl.mins + intl.plan + voice.plan\n\nreg_pois = glm(formula, data = churn, family = poisson)summary(reg_pois)\n   \n   Call:\n   glm(formula = formula, family = poisson, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n   (Intercept)     0.9957186  0.1323004   7.526 5.22e-14 ***\n   churnno        -0.5160641  0.0304013 -16.975  < 2e-16 ***\n   voice.messages  0.0034062  0.0028294   1.204 0.228646    \n   day.mins       -0.0006875  0.0002078  -3.309 0.000938 ***\n   eve.mins       -0.0005649  0.0002237  -2.525 0.011554 *  \n   night.mins     -0.0003602  0.0002245  -1.604 0.108704    \n   intl.mins      -0.0075034  0.0040886  -1.835 0.066475 .  \n   intl.planno     0.2085330  0.0407760   5.114 3.15e-07 ***\n   voice.planno    0.0735515  0.0878175   0.838 0.402284    \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for poisson family taken to be 1)\n   \n       Null deviance: 5991.1  on 4999  degrees of freedom\n   Residual deviance: 5719.5  on 4991  degrees of freedom\n   AIC: 15592\n   \n   Number of Fisher Scoring iterations: 5"},{"path":"chapter-regression.html","id":"sec-stepwise-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.6 Model Selection Using Stepwise Regression","text":"Selecting appropriate predictors crucial step building regression model accurate interpretable. process, known model specification, ensures model captures underlying relationships data without overfitting including irrelevant predictors. Proper model specification enhances model’s ability make reliable predictions provides meaningful insights decision-making.many real-world applications, particularly business data science, datasets often contain dozens even hundreds potential predictors. Managing complexity requires systematic methods identify relevant predictors. One method stepwise regression, popular algorithm iterative model selection. Stepwise regression begins evaluating predictors one time, adding improve model removing contribute meaningfully. iterative process helps handle issues multicollinearity ensures helpful predictors remain final model. Additionally, stepwise regression computationally efficient small medium-sized datasets, making practical choice many scenarios.assess model quality selection process, use metrics like Akaike Information Criterion (AIC). AIC provides balance model complexity goodness fit, lower AIC value indicating better model. AIC defined :\\[\nAIC = 2p + n \\log\\left(\\frac{SSE}{n}\\right)\n\\]\\(p\\) number estimated predictors model, \\(n\\) number observations, \\(SSE\\) sum squared errors, quantifies unexplained variability response variable. Using AIC ensures select model explains data well penalizing unnecessary complexity. Unlike \\(R^2\\), always increases predictors added, AIC introduces penalty model complexity, favoring simpler models generalize better new data. makes AIC robust criterion selecting parsimonious interpretable models.demonstrate process model specification, apply stepwise regression marketing dataset, contains seven predictors. goal identify best regression model predicting revenue (daily revenue) based predictors.Example 10.1  start building linear regression model includes available predictors. allows us assess initial model use results baseline stepwise regression process. formula revenue ~ . lm() function specifies predictors dataset included initial model:output model reveals several predictors high p-values (much greater significance level \\(\\alpha = 0.05\\)), suggesting variables may significantly contribute model. High multicollinearity among predictors also affecting model’s performance.Next, apply stepwise regression using step() function. setting direction argument \"\", algorithm iteratively adds removes predictors, evaluating contribution model step:first iteration, stepwise algorithm evaluates contribution predictor removes variable spend, highest p-value initial model. Subsequent iterations continue process, adding removing variables based impact model performance. stepwise process concludes changes improve model.view final selected model, use summary() function:stepwise regression process selects simpler model two predictors: clicks display. final estimated regression equation :\\[\n\\text{revenue} = -33.63 + 0.9 \\cdot \\text{clicks} + 95.51 \\cdot \\text{display}\n\\]model demonstrates better fit compared initial full model. Specifically:Residual Standard Error (RSE) decreased approximately 93.82 72.29, indicating typical prediction error now smaller.R-squared (\\(R^2\\)) value increased 62% 77%, meaning greater proportion variability revenue explained final model.stepwise regression approach identifies relevant predictors (clicks display) discarding others significantly improve model performance. ensures final model parsimonious interpretable.example, stepwise regression helped us build efficient regression model marketing dataset iteratively adding removing predictors. using AIC selection criterion, identified model best trade-complexity goodness fit.stepwise regression popular intuitive method model selection, important recognize limitations. First, algorithm evaluates predictors sequentially rather considering possible combinations, can sometimes result suboptimal models. Second, stepwise regression prone overfitting, particularly working small datasets containing many predictors. Overfitting occurs model captures noise rather meaningful patterns, reducing generalizability new data. Finally, multicollinearity can distort p-values used stepwise regression, leading misleading results. reasons, alternative methods like LASSO (Least Absolute Shrinkage Selection Operator) Ridge Regression often preferred complex scenarios, particularly high-dimensional datasets. methods beyond scope book, exploration, refer Introduction Statistical Learning Applications R.conclusion, model specification critical step regression analysis. selecting right predictors using systematic techniques like stepwise regression, can build models accurate interpretable. stepwise regression without limitations, remains practical widely used method selecting predictors regression models, particularly datasets manageable number variables. process improves model’s predictive performance maintaining simplicity interpretability, making valuable tool data-driven decision-making.","code":"ml_all = lm(revenue ~ ., data = marketing)\n\nsummary(ml_all)\n   \n   Call:\n   lm(formula = revenue ~ ., data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -138.00  -59.12   15.16   54.58  106.99 \n   \n   Coefficients:\n                     Estimate Std. Error t value Pr(>|t|)\n   (Intercept)     -25.260020 246.988978  -0.102    0.919\n   spend            -0.025807   2.605645  -0.010    0.992\n   clicks            1.211912   1.630953   0.743    0.463\n   impressions      -0.005308   0.021588  -0.246    0.807\n   display          79.835729 117.558849   0.679    0.502\n   transactions     -7.012069  66.383251  -0.106    0.917\n   click.rate      -10.951493 106.833894  -0.103    0.919\n   conversion.rate  19.926588 135.746632   0.147    0.884\n   \n   Residual standard error: 77.61 on 32 degrees of freedom\n   Multiple R-squared:  0.7829, Adjusted R-squared:  0.7354 \n   F-statistic: 16.48 on 7 and 32 DF,  p-value: 5.498e-09ml_stepwise = step(ml_all, direction = \"both\")\n   Start:  AIC=355.21\n   revenue ~ spend + clicks + impressions + display + transactions + \n       click.rate + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - spend            1       0.6 192760 353.21\n   - click.rate       1      63.3 192822 353.23\n   - transactions     1      67.2 192826 353.23\n   - conversion.rate  1     129.8 192889 353.24\n   - impressions      1     364.2 193123 353.29\n   - display          1    2778.1 195537 353.79\n   - clicks           1    3326.0 196085 353.90\n   <none>                         192759 355.21\n   \n   Step:  AIC=353.21\n   revenue ~ clicks + impressions + display + transactions + click.rate + \n       conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - click.rate       1      67.9 192828 351.23\n   - transactions     1      75.1 192835 351.23\n   - conversion.rate  1     151.5 192911 351.24\n   - impressions      1     380.8 193141 351.29\n   - display          1    2787.2 195547 351.79\n   - clicks           1    3325.6 196085 351.90\n   <none>                         192760 353.21\n   + spend            1       0.6 192759 355.21\n   \n   Step:  AIC=351.23\n   revenue ~ clicks + impressions + display + transactions + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - transactions     1      47.4 192875 349.24\n   - conversion.rate  1     129.0 192957 349.25\n   - impressions      1     312.9 193141 349.29\n   - clicks           1    3425.7 196253 349.93\n   - display          1    3747.1 196575 350.00\n   <none>                         192828 351.23\n   + click.rate       1      67.9 192760 353.21\n   + spend            1       5.2 192822 353.23\n   \n   Step:  AIC=349.24\n   revenue ~ clicks + impressions + display + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - conversion.rate  1      89.6 192965 347.26\n   - impressions      1     480.9 193356 347.34\n   - display          1    5437.2 198312 348.35\n   <none>                         192875 349.24\n   + transactions     1      47.4 192828 351.23\n   + click.rate       1      40.2 192835 351.23\n   + spend            1      13.6 192861 351.23\n   - clicks           1   30863.2 223738 353.17\n   \n   Step:  AIC=347.26\n   revenue ~ clicks + impressions + display\n   \n                     Df Sum of Sq    RSS    AIC\n   - impressions      1       399 193364 345.34\n   <none>                         192965 347.26\n   - display          1     14392 207357 348.13\n   + conversion.rate  1        90 192875 349.24\n   + click.rate       1        52 192913 349.24\n   + spend            1        33 192932 349.25\n   + transactions     1         8 192957 349.25\n   - clicks           1     35038 228002 351.93\n   \n   Step:  AIC=345.34\n   revenue ~ clicks + display\n   \n                     Df Sum of Sq    RSS    AIC\n   <none>                         193364 345.34\n   + impressions      1       399 192965 347.26\n   + transactions     1       215 193149 347.29\n   + conversion.rate  1         8 193356 347.34\n   + click.rate       1         6 193358 347.34\n   + spend            1         2 193362 347.34\n   - display          1     91225 284589 358.80\n   - clicks           1    606800 800164 400.15summary(ml_stepwise)\n   \n   Call:\n   lm(formula = revenue ~ clicks + display, data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -141.89  -55.92   16.44   52.70  115.46 \n   \n   Coefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) -33.63248   28.68893  -1.172 0.248564    \n   clicks        0.89517    0.08308  10.775 5.76e-13 ***\n   display      95.51462   22.86126   4.178 0.000172 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 72.29 on 37 degrees of freedom\n   Multiple R-squared:  0.7822, Adjusted R-squared:  0.7704 \n   F-statistic: 66.44 on 2 and 37 DF,  p-value: 5.682e-13"},{"path":"chapter-regression.html","id":"extending-linear-models-to-capture-non-linear-relationships","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.7 Extending Linear Models to Capture Non-Linear Relationships","text":"Thus far, focused linear regression models, simple, interpretable, easy implement. models work well relationships predictors response variables approximately linear, predictive power limited cases relationships inherently non-linear. scenarios, relying linearity assumption can lead suboptimal results.earlier sections, explored techniques like stepwise regression (Section 10.6) improve linear models reducing complexity addressing multicollinearity. However, techniques operate within constraints linear framework. capture non-linear patterns retaining interpretability, turn polynomial regression, simple yet powerful extension linear regression incorporates non-linear terms.","code":""},{"path":"chapter-regression.html","id":"the-need-for-non-linear-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"The Need for Non-Linear Regression","text":"Linear regression assumes straight-line relationship predictors response variable. However, many real-world datasets, relationships complex. example, consider scatter plot Figure 10.2, depicts relationship unit.price (house price per unit area) house.age (age house) house dataset. orange line represents fit simple linear regression model. Clearly, data exhibits curved, non-linear pattern, linear fit fails capture.address limitation, can enhance model incorporating non-linear terms. instance, data suggests quadratic relationship, can model :\\[\nunit.price = b_0 + b_1 \\cdot house.age + b_2 \\cdot house.age^2\n\\]equation predicts unit.price using house.age square (house.age^2). model accommodates non-linear relationships, remains linear model coefficients (\\(b_0, b_1, b_2\\)) estimated using linear least squares. blue curve Figure 10.2 illustrates quadratic fit, captures data’s pattern far better linear fit.\nFigure 10.2: Scatter plot house price ($) versus house age (years) house dataset, fitted simple linear regression line orange quadratic regression curve blue.\n","code":""},{"path":"chapter-regression.html","id":"polynomial-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.7.1 Polynomial Regression","text":"Polynomial regression extends linear model adding higher-order terms predictors, squared (\\(x^2\\)), cubic (\\(x^3\\)), higher-degree terms. example, cubic regression model takes form:\\[\n\\hat{y} = b_0 + b_1 \\cdot x + b_2 \\cdot x^2 + b_3 \\cdot x^3\n\\]allows polynomial regression capture increasingly complex patterns. Importantly, although model non-linear terms predictors, remains linear coefficients (\\(b_0, b_1, b_2, b_3\\)), making compatible standard least squares estimation. However, care must taken using high-degree polynomials (\\(d > 3\\)) can become overly flexible, fitting noise data rather meaningful patterns (overfitting), especially near boundaries predictor range.","code":""},{"path":"chapter-regression.html","id":"example-polynomial-regression-on-the-house-dataset","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.7.2 Example: Polynomial Regression on the House Dataset","text":"illustrate polynomial regression, use house dataset, contains information house prices features house age, distance MRT stations, number convenience stores. objective model unit.price (house price per unit area) function house.age compare performance simple linear regression polynomial regression.dataset consists six features 414 observations. target variable unit.price, predictors include:house.age: Age house (years).distance..MRT: Distance nearest MRT station.stores.number: Number convenience stores.latitude: Latitude.longitude: Longitude.unit.price: House price per unit area (target variable).First, examine structure dataset:dataset includes 414 observations 6 variables, 5 predictors one numerical target variable (unit.price).","code":"data(house)\n\nstr(house)\n   'data.frame':    414 obs. of  6 variables:\n    $ house.age      : num  32 19.5 13.3 13.3 5 7.1 34.5 20.3 31.7 17.9 ...\n    $ distance.to.MRT: num  84.9 306.6 562 562 390.6 ...\n    $ stores.number  : int  10 9 5 5 5 3 7 6 1 3 ...\n    $ latitude       : num  25 25 25 25 25 ...\n    $ longitude      : num  122 122 122 122 122 ...\n    $ unit.price     : num  37.9 42.2 47.3 54.8 43.1 32.1 40.3 46.7 18.8 22.1 ..."},{"path":"chapter-regression.html","id":"fitting-simple-linear-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Fitting Simple Linear Regression","text":"begin fitting simple linear regression model predict unit.price using house.age:R-squared (\\(R^2\\)) value model 0.04, indicating 4.43% variability house prices explained house.age. low \\(R^2\\) reflects poor fit data.","code":"simple_reg_house = lm(unit.price ~ house.age, data = house)\n\nsummary(simple_reg_house)\n   \n   Call:\n   lm(formula = unit.price ~ house.age, data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -31.113 -10.738   1.626   8.199  77.781 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) 42.43470    1.21098  35.042  < 2e-16 ***\n   house.age   -0.25149    0.05752  -4.372 1.56e-05 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 13.32 on 412 degrees of freedom\n   Multiple R-squared:  0.04434,    Adjusted R-squared:  0.04202 \n   F-statistic: 19.11 on 1 and 412 DF,  p-value: 1.56e-05"},{"path":"chapter-regression.html","id":"fitting-polynomial-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Fitting Polynomial Regression","text":"Next, fit quadratic polynomial regression model better capture non-linear relationship:\\[\nunit.price = b_0 + b_1 \\cdot house.age + b_2 \\cdot house.age^2\n\\]implement model R:quadratic regression model achieves significantly higher R-squared (\\(R^2\\)) value 0.2, compared simple linear model. indicates quadratic model explains variability data. Additionally, Residual Standard Error (RSE) lower, reflecting smaller prediction errors.example, polynomial regression outperforms simple linear regression, evidenced higher \\(R^2\\) lower RSE. incorporating non-linear terms, polynomial regression provides greater flexibility modeling complex relationships. However, care must taken avoid overfitting adding higher-degree terms.Polynomial regression offers straightforward way extend linear models non-linear relationships. advanced techniques, splines generalized additive models, refer Chapter 7 Introduction Statistical Learning Applications R.","code":"reg_nonlinear_house = lm(unit.price ~ poly(house.age, 2), data = house)\n\nsummary(reg_nonlinear_house)\n   \n   Call:\n   lm(formula = unit.price ~ poly(house.age, 2), data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -26.542  -9.085  -0.445   8.260  79.961 \n   \n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           37.980      0.599  63.406  < 2e-16 ***\n   poly(house.age, 2)1  -58.225     12.188  -4.777 2.48e-06 ***\n   poly(house.age, 2)2  109.635     12.188   8.995  < 2e-16 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 12.19 on 411 degrees of freedom\n   Multiple R-squared:  0.2015, Adjusted R-squared:  0.1977 \n   F-statistic: 51.87 on 2 and 411 DF,  p-value: < 2.2e-16"},{"path":"chapter-regression.html","id":"diagnosing-and-validating-regression-models","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.8 Diagnosing and Validating Regression Models","text":"deploying regression model, essential validate assumptions. Ignoring assumptions akin building house unstable foundation: predictions poorly validated model can lead erroneous, overoptimistic, misleading results costly consequences. Model diagnostics ensure model robust, reliable, appropriate making predictions.Linear regression models built following key assumptions:Independence (Random Sampling): observations independent , meaning response one observation depend response another.Linearity: relationship predictor(s) response variable linear. scatter plot predictor(s) response can help verify assumption.Normality: residuals (errors) model follow normal distribution. can assessed visually using Q-Q plot.Constant Variance (Homoscedasticity): residuals constant variance every level predictor(s). residuals vs. fitted values plot used check assumption.Violations assumptions may compromise validity model, leading inaccurate estimates, unreliable predictions, invalid statistical inferences.","code":""},{"path":"chapter-regression.html","id":"example-diagnosing-the-regression-model-for-the-marketing-dataset","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.8.1 Example: Diagnosing the Regression Model for the Marketing Dataset","text":"demonstrate model diagnostics, evaluate assumptions multiple regression model created Example 10.1 using marketing dataset. fitted model predicts daily revenue (revenue) based clicks display.generate diagnostic plots model follows:\nFigure 10.3: Diagnostic plots assessing regression model assumptions.\ndiagnostic plots help us assess four key assumptions:Normality Residuals: Normal Q-Q plot (upper-right) shows closely residuals align theoretical normal distribution. points fall along straight line, residuals approximately normally distributed. example, majority points lie close line, indicating normality assumption satisfied.Normality Residuals: Normal Q-Q plot (upper-right) shows closely residuals align theoretical normal distribution. points fall along straight line, residuals approximately normally distributed. example, majority points lie close line, indicating normality assumption satisfied.Linearity Constant Variance (Homoscedasticity): Residuals vs. Fitted plot (upper-left) helps check linearity homoscedasticity:\npoints form random scatter discernible pattern, linearity assumption holds.\nvertical spread residuals roughly uniform across fitted values, constant variance assumption satisfied.\ncase, residuals appear randomly scattered without systematic curvature changes variance, supporting validity assumptions.\nLinearity Constant Variance (Homoscedasticity): Residuals vs. Fitted plot (upper-left) helps check linearity homoscedasticity:points form random scatter discernible pattern, linearity assumption holds.vertical spread residuals roughly uniform across fitted values, constant variance assumption satisfied.case, residuals appear randomly scattered without systematic curvature changes variance, supporting validity assumptions.Independence: independence assumption pertains dataset rather diagnostic plots. marketing dataset, revenue one day unlikely depend revenue another day, making independence assumption reasonable example.Independence: independence assumption pertains dataset rather diagnostic plots. marketing dataset, revenue one day unlikely depend revenue another day, making independence assumption reasonable example.Based diagnostic plots characteristics dataset:Normality Assumption: residuals approximately normally distributed, evidenced Normal Q-Q plot.Linearity Constant Variance: residuals vs. fitted values plot shows visible patterns heteroscedasticity, validating assumptions.Independence: nature data (daily revenue) suggests observations independent one another.validating assumptions, confirm regression model marketing dataset appropriate inference prediction. Failing check assumptions lead unreliable results, emphasizing importance diagnostics regression modeling.Model diagnostics validation fundamental ensuring integrity regression model. models violate one assumptions, alternative approaches robust regression, non-linear regression, transformations variables may considered. Additionally, validation techniques cross-validation --sample testing can used evaluate model’s performance generalizability unseen data.adhering diagnostic best practices, ensure models deploy built solid statistical foundations, capable providing accurate trustworthy predictions.","code":"\nml_stepwise = lm(revenue ~ clicks + display, data = marketing)\n\nplot(ml_stepwise)  "},{"path":"chapter-regression.html","id":"exercises-5","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.9 Exercises","text":"Hands-exercises section reinforce concepts.","code":""},{"path":"chapter-tree.html","id":"chapter-tree","chapter":"11 Decision Trees and Random Forests","heading":"11 Decision Trees and Random Forests","text":"part Data Science workflow, already explored several powerful algorithms classification regression. include k-Nearest Neighbors Naive Bayes classification, well linear regression models continuous outcomes classification tasks. chapter, introduce two additional widely-used techniques: Decision Trees Random Forests. algorithms highly versatile, capable tackling classification regression problems, built around idea making series hierarchical decisions classify data predict outcomes.core, Decision Trees employ simple decision-making rules split dataset smaller, homogeneous subsets, resulting tree-like structure decisions. step--step partitioning allows trees capture complex relationships data remaining interpretable intuitive. Random Forests, extension Decision Trees, enhance predictive power combining outputs multiple trees. ensemble learning approach improves accuracy, reduces overfitting, provides robust solution compared single tree.illustrate concept, consider simple Decision Tree shown Figure 11.1, predicts whether customer’s credit risk classified “good” “bad” based features age income. example uses risk dataset introduced Chapter 9. node tree represents question, “yearly income lower 36K? (income < 36e+3)” “age >= 29?”, terminal nodes, also known leaves, represent final predictions (e.g., “Good risk” “Bad risk”). Along predictions, tree also provides uncertainty values. process begins root node, data iteratively split branches based feature values reaches terminal nodes.\nFigure 11.1: Decision tree predicting credit risk based age income.\nshown Figure 11.1, decision trees provide visually intuitive interpretable structure require advanced statistical knowledge. makes especially valuable business contexts, simplicity, interpretability, actionable insights critical. Whether ’s customer segmentation, risk assessment, process optimization, decision trees serve user-friendly tool deriving insights informing decisions.end chapter, gain:deep understanding mechanics behind Decision Trees Random Forests,Practical knowledge building, evaluating, tuning Decision Trees using algorithms like CART C5.0, andThe ability leverage Random Forests solving real-world problems, identifying risky loans, detecting fraudulent transactions, classifying images.start exploring Decision Trees constructed, learning key concepts algorithms. , delve Random Forests, powerful ensemble learning method builds upon strengths Decision Trees deliver state---art performance across wide range applications.","code":""},{"path":"chapter-tree.html","id":"how-decision-trees-work","chapter":"11 Decision Trees and Random Forests","heading":"11.1 How Decision Trees Work","text":"Decision Trees classify data predict outcomes recursively dividing dataset smaller subsets based feature values. divide conquer approach aims maximize homogeneity subsets step, creating groups similar possible. split, algorithm identifies feature threshold best separate data, using criteria Gini Index, Entropy, Variance Reduction, depending whether task classification regression. process continues one following conditions met:tree reaches predefined maximum depth,observations subset belong class (classification) share value (regression), orFurther splits fail improve model’s performance.iterative process creates binary tree, node represents decision test (e.g., “\\(x_1 < 10\\)?”), leaf represents final prediction (e.g., “Class ” “Class B”). Decision Trees highly interpretable, structure visually represents decision-making process, making particularly useful understanding predictions made.better understand Decision Trees work, consider toy dataset two features (\\(x_1\\) \\(x_2\\)) two classes (Class Class B), shown Figure 11.2. dataset contains 50 data points, goal classify points respective classes using Decision Tree.\nFigure 11.2: two-dimensional toy dataset (50 observations) two classes (Class Class B), used illustrate build Decision Trees.\nprocess starts identifying feature threshold best separate two classes. algorithm evaluates possible thresholds feature selects split maximizes homogeneity resulting subsets. dataset, best split based feature \\(x_1\\), decision boundary \\(x_1 = 10\\).shown Figure 11.3, split divides dataset two regions:Left region: Data points \\(x_1 < 10\\), 80% Class 20% Class B.Right region: Data points \\(x_1 \\geq 10\\), 28% Class 72% Class B.test \\(x_1 < 10\\) forms root node tree, two regions correspond left right branches tree.\nFigure 11.3: Left: Decision boundary tree depth 1. Right: corresponding Decision Tree.\nAlthough first split significantly separates two classes, regions still contain points classes. improve classification accuracy, algorithm recursively evaluates additional splits within region.Figure 11.4, second split made using feature \\(x_2\\). left region (\\(x_1 < 10\\)), optimal threshold \\(x_2 = 6\\), right region (\\(x_1 \\geq 10\\)), threshold \\(x_2 = 8\\). additional splits refine boundaries feature space, creating smaller homogeneous regions.\nFigure 11.4: Left: Decision boundary tree depth 2. Right: corresponding Decision Tree.\nsplitting process continues tree meets stopping condition, reaching maximum depth producing pure leaf nodes (regions containing data points one class). Figure 11.5 shows final tree, grown depth 5. decision boundaries left illustrate feature space partitioned regions, associated specific class.\nFigure 11.5: Left: Decision boundary tree depth 5. Right: corresponding Decision Tree.\ndepth, tree created highly specific decision boundaries closely match training data. However, specificity often leads overfitting, model captures noise outliers data instead general patterns. Overfitted trees may perform poorly unseen data.make predictions Decision Tree, algorithm evaluates test conditions node follows corresponding branch reaches leaf. prediction depends type task:classification, predicted class majority class points leaf.regression, predicted value mean target variable points leaf.example, Figure 11.4, new data point \\(x_1 = 8\\) \\(x_2 = 4\\) traverse left region (\\(x_1 < 10\\)), bottom-left region (\\(x_2 < 6\\)), ultimately landing leaf labeled Class 80% confidence 20% error rate.Decision Trees can also handle regression tasks following splitting process minimizing variance target variable instead maximizing class homogeneity. regression, prediction new data point average target value training points corresponding leaf. allows Decision Trees model non-linear relationships effectively.Controlling complexity Decision Tree crucial prevent overfitting. Fully growing tree leaves pure often results highly complex model perfectly fits training data performs poorly unseen data. can observed Figure 11.5, decision boundaries overfit training set, capturing outliers noise.address , two strategies commonly used:Pre-pruning: Stop tree-building process early based criteria limiting maximum depth, number leaf nodes, minimum number points required split node.Post-pruning: Build full tree simplify removing collapsing nodes provide little additional value.effectiveness strategies depends dataset application. choice split criterion—Gini Index, Entropy, Variance Reduction—also plays crucial role determining tree’s performance. criteria foundational two widely used Decision Tree algorithms, CART C5.0, explored following sections.","code":""},{"path":"chapter-tree.html","id":"classification-and-regression-trees-cart","chapter":"11 Decision Trees and Random Forests","heading":"11.2 Classification and Regression Trees (CART)","text":"Classification Regression Trees (CART) algorithm, introduced Breiman et al. 1984,6 one widely used methods constructing decision trees. CART generates binary trees, meaning decision node splits data exactly two branches. recursively partitions training dataset subsets records share similar values target variable. partitioning guided splitting criterion designed minimize impurity resulting subsets. classification tasks, CART employs measures Gini Index Entropy evaluate splits, regression tasks, minimizes Variance target variable.example, Gini Index commonly used measure impurity classification tasks. Gini Index node calculated :\\[\nGini = 1 - \\sum_{=1}^k p_i^2\n\\]\\(p_i\\) represents proportion samples node belong class \\(\\), \\(k\\) total number classes. node considered “pure” data points belong single class, resulting Gini Index 0. tree construction, CART selects feature threshold result largest reduction impurity, splitting data create two homogeneous child nodes.recursive nature CART can result highly detailed trees perfectly fit training data. ensures lowest possible error rate training set, can lead overfitting, tree becomes overly complex fails generalize unseen data. mitigate , CART employs technique called pruning simplify tree.Pruning involves cutting back branches tree contribute meaningfully predictive accuracy validation set. achieved finding adjusted error rate penalizes overly complex trees many leaf nodes. goal pruning strike balance accuracy simplicity, enhancing tree’s ability generalize new data. pruning process described detail Breiman et al..7Despite simplicity, CART powerful algorithm widely adopted practice. key strengths include:Interpretability: tree structure intuitive easy visualize, making CART models highly explainable.Versatility: CART can handle classification regression tasks effectively.Ability handle mixed data types: CART works seamlessly datasets containing numerical categorical variables.However, CART also limitations. algorithm tends produce deep trees may overfit training data, especially dataset small noisy. Additionally, CART’s reliance greedy splitting can result suboptimal splits, evaluates one split time rather considering possible combinations.address shortcomings, advanced algorithms developed, C5.0, incorporates improvements splitting pruning techniques, Random Forests, combine multiple decision trees create robust models. approaches build foundations CART, improving performance reducing susceptibility overfitting. explore methods subsequent sections.","code":""},{"path":"chapter-tree.html","id":"the-c5.0-algorithm-for-building-decision-trees","chapter":"11 Decision Trees and Random Forests","heading":"11.3 The C5.0 Algorithm for Building Decision Trees","text":"C5.0 algorithm one well-known widely used decision tree implementations. Developed J. Ross Quinlan, C5.0 advanced iteration earlier algorithms, C4.5 ID3 (Iterative Dichotomiser 3). Building upon strengths predecessors, C5.0 introduces several improvements efficiency, flexibility, accuracy, making popular choice academic commercial applications. Quinlan offers commercial version C5.0 (available RuleQuest), single-threaded implementation made publicly available incorporated open-source tools R.C5.0 differs decision tree algorithms, CART, several key ways. Unlike CART, produces binary trees, C5.0 allows flexible tree structures non-binary splits. categorical attributes, C5.0 can create separate branches unique value attribute, can lead highly “bushy” trees attribute many categories. Another major distinction lies way node homogeneity measured. CART uses metrics like Gini Index Variance Reduction, C5.0 employs Entropy Information Gain, concepts rooted information theory, evaluate optimal splits.Entropy measures level disorder randomness dataset. High entropy indicates dataset high diversity (e.g., mix classes), whereas low entropy signifies greater homogeneity (e.g., samples belong class). goal C5.0 algorithm identify splits reduce entropy, creating purer subsets data step tree-building process. Formally, entropy variable \\(x\\) \\(k\\) classes defined :\\[\nEntropy(x) = - \\sum_{=1}^k p_i \\log_2(p_i)\n\\], \\(p_i\\) proportion samples belonging class \\(\\). example, dataset contains even split two classes, entropy maximum. Conversely, samples belong single class, entropy zero. concept extends naturally calculation Information Gain, quantifies reduction entropy achieved splitting data particular feature. Given candidate split \\(S\\) divides dataset \\(T\\) subsets \\(T_1, T_2, \\dots, T_c\\), entropy split calculated weighted sum entropies subsets:\\[\nH_S(T) = \\sum_{=1}^c \\frac{|T_i|}{|T|} \\cdot Entropy(T_i)\n\\]Information Gain split \\(S\\) :\\[\ngain(S) = H(T) - H_S(T)\n\\]\\(H(T)\\) represents entropy dataset split. decision node, C5.0 algorithm evaluates possible splits selects one maximizes information gain. process ensures splits lead progressively purer subsets, improving accuracy model.illustrate C5.0 algorithm, consider application risk dataset, predicts credit risk (“good” “bad”) based features like age income. Figure 11.6 shows resulting decision tree, created using C5.0 function C50 package R. node tree represents decision based feature value, branches lead subsets data become progressively homogeneous.\nFigure 11.6: C5.0 Decision Tree predicting credit risk based age income.\ntree Figure 11.6 demonstrates algorithm uses entropy information gain construct splits best separate classes. Unlike strictly binary splits produced CART, C5.0 allows multi-way splits working categorical attributes, can create trees variable shapes. flexibility often leads compact trees easier interpret, especially datasets categorical variables.C5.0 offers several advantages decision tree algorithms. computationally efficient, making suitable large datasets, flexibility handling non-binary splits allows nuanced tree structures. Additionally, C5.0 incorporates advanced features feature weighting, allows algorithm prioritize relevant features tree-building process. can improve model performance focusing important predictors.Despite strengths, C5.0 algorithm without limitations. Trees generated C5.0 can become overly complex “bushy,” particularly working categorical attributes many unique values. complexity can make trees harder interpret may lead overfitting. address issues, pruning techniques can applied simplify tree improve generalizability. Additionally, computational cost evaluating multiple splits categorical features may increase large datasets high cardinality, although mitigated C5.0’s overall efficiency.summary, C5.0 algorithm powerful versatile tool building decision trees. leveraging concepts like entropy information gain, constructs models accurate interpretable. shares many similarities CART, ability handle multi-way splits use information theory make distinct valuable alternative. C5.0 algorithm widely used fields finance, healthcare, marketing, decision tree models provide actionable insights transparent decision-making processes. next section, explore Random Forests, ensemble learning technique builds upon decision trees enhance accuracy robustness.","code":""},{"path":"chapter-tree.html","id":"random-forests-an-ensemble-approach","chapter":"11 Decision Trees and Random Forests","heading":"11.4 Random Forests: An Ensemble Approach","text":"Decision Trees powerful intuitive, prone overfitting, particularly grown full depth. Random Forests8 address limitation adopting ensemble approach combines predictions multiple Decision Trees produce robust accurate model. Instead relying single tree, Random Forests aggregate predictions many trees, reducing overfitting enhancing performance complex datasets.Random Forest algorithm introduces two key elements randomness improve model diversity:Bootstrap Aggregation (Bagging): tree trained random subset training data, created sampling replacement. means observations appear multiple times tree’s training data, others may excluded. diversity ensures tree learns slightly different patterns.Random Feature Selection: split, algorithm considers random subset features instead evaluating features. decorrelates trees, tree forced rely different combinations features make decisions.forest built, predictions trees aggregated produce final output:classification, final prediction determined majority voting, tree votes class, common class selected.regression, final output average predictions trees.strength Random Forests lies principle “wisdom crowd.” Individually, tree weak learner, trained limited subset data features. However, combined, collective predictions form strong learner. leveraging diversity individual trees, Random Forests reduce likelihood errors made single tree dominate overall model.Additionally, randomness introduced feature selection ensures single feature dominates model, making Random Forests particularly effective datasets correlated redundant features. feature-level decorrelation enhances ensemble’s ability generalize unseen data.Random Forests several notable advantages:Reduced Overfitting: averaging predictions multiple trees, Random Forests smooth noise variance present individual trees, leading better generalization.High Accuracy: Random Forests perform well classification regression tasks, particularly datasets non-linear relationships high-dimensional feature spaces.Feature Importance: algorithm provides feature importance scores, enabling us identify influential predictors. especially useful feature selection gaining insights underlying data.Robustness: Random Forests resilient noise outliers, ensemble effect reduces impact anomalies final prediction.Flexibility: Random Forests can handle numerical categorical data adapt well diverse types problems.Despite strengths, Random Forests limitations:Computational Complexity: Training hundreds thousands trees can computationally intensive, especially large datasets. However, can mitigated parallel processing, tree built independently.Reduced Interpretability: individual Decision Trees highly interpretable, ensemble nature Random Forests makes difficult understand collective decision-making process model.Bias-Variance Tradeoff: Although Random Forests reduce variance bagging, may sometimes smooth complex relationships data single, well-tuned Decision Tree capture.Random Forests strike balance accuracy robustness, addressing many weaknesses individual Decision Trees retaining strengths. well-suited classification regression tasks particularly effective scenarios noisy high-dimensional data. Moreover, ability compute feature importance scores provides valuable insights drivers model’s predictions, making predictive tool also exploratory one.Random Forests become one widely used machine learning algorithms due versatility, reliability, strong performance across variety applications. next section, apply Random Forests, along Decision Trees, adult dataset explore question: can earn $50K per year? case study provide practical demonstration models work can evaluated compared real-world scenario.","code":""},{"path":"chapter-tree.html","id":"tree-case-study","chapter":"11 Decision Trees and Random Forests","heading":"11.5 Case Study: Who Can Earn More Than $50K Per Year?","text":"demonstrate practical application Decision Trees Random Forests, use adult dataset, provides demographic income information individuals. dataset, sourced US Census Bureau, widely used predict whether individual earns $50,000 per year based features education, hours worked per week, marital status, . objective binary classification problem categorize individuals one two income groups: <=50K >50K, features serving predictors target variable income.","code":""},{"path":"chapter-tree.html","id":"overview-of-the-dataset-2","chapter":"11 Decision Trees and Random Forests","heading":"Overview of the Dataset","text":"begin loading dataset examining structure:dataset contains 48598 records 15 variables. , 14 predictors, target variable, income, binary categorical variable two levels: <=50K >50K. features include numerical categorical variables:age: Age years (numerical).workclass: Type employment (categorical, 6 levels).demogweight: Demographic weight (categorical).education: Highest education level (categorical, 16 levels).education.num: Years education (numerical).marital.status: Marital status (categorical, 5 levels).occupation: Type occupation (categorical, 15 levels).relationship: Type relationship (categorical, 6 levels).race: Race (categorical, 5 levels).gender: Gender (categorical, Male/Female).capital.gain: Capital gains (numerical).capital.loss: Capital losses (numerical).hours.per.week: Weekly hours worked (numerical).native.country: Country origin (categorical, 42 levels).income: Target variable, representing annual income (<=50K >50K).additional details dataset, visit documentation.","code":"data(adult)\n\nstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education.num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital.status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital.gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital.loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours.per.week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native.country: Factor w/ 42 levels \"?\",\"Cambodia\",..: 40 40 40 40 40 40 40 40 40 40 ...\n    $ income        : Factor w/ 2 levels \"<=50K\",\">50K\": 1 1 2 2 1 1 1 2 1 1 ..."},{"path":"chapter-tree.html","id":"data-cleaning-and-preparation","chapter":"11 Decision Trees and Random Forests","heading":"Data Cleaning and Preparation","text":"dataset includes missing values represented character \"?\". simplicity, rely prior data cleaning steps (see Chapter 3) handle issues. steps include recoding categorical variables, grouping country-level data broader regions, imputing missing values, demonstrated :partition cleaned dataset training (80%) testing (20%) subsets ensure models evaluated unseen data:use set.seed() ensures reproducibility.","code":"\nset.seed(6)\n\ndata_sets = partition(data = adult, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test = test_set$income"},{"path":"chapter-tree.html","id":"decision-tree-with-cart","chapter":"11 Decision Trees and Random Forests","heading":"Decision Tree with CART","text":"predict whether individual’s income exceeds $50K, fit Decision Tree using CART algorithm. following predictors used:age, education.num, capital.gain, capital.loss, hours.per.week, marital.status, workclass, race, gender.tree built using rpart() function rpart package:resulting tree visualized using rpart.plot() function:tree identifies marital.status important predictor, followed capital.gain, education.num, capital.loss. tree contains 6 decision nodes 7 leaves, providing interpretable insights predictors.","code":"formula = income ~ age + education.num + capital.gain + capital.loss + \n                   hours.per.week + marital.status + workclass + race + gender\n\ntree_cart = rpart(formula = formula, data = train_set, method = \"class\")\n\nprint(tree_cart)\n   n= 38878 \n   \n   node), split, n, loss, yval, (yprob)\n         * denotes terminal node\n   \n    1) root 38878 9217 <=50K (0.76292505 0.23707495)  \n      2) marital.status=Divorced,Never-married,Separated,Widowed 20580 1282 <=50K (0.93770651 0.06229349)  \n        4) capital.gain< 7055.5 20261  978 <=50K (0.95172992 0.04827008) *\n        5) capital.gain>=7055.5 319   15 >50K (0.04702194 0.95297806) *\n      3) marital.status=Married 18298 7935 <=50K (0.56634605 0.43365395)  \n        6) education.num< 12.5 12944 4163 <=50K (0.67838381 0.32161619)  \n         12) capital.gain< 5095.5 12350 3582 <=50K (0.70995951 0.29004049)  \n           24) education.num< 8.5 2159  231 <=50K (0.89300602 0.10699398) *\n           25) education.num>=8.5 10191 3351 <=50K (0.67118045 0.32881955)  \n             50) capital.loss< 1846 9813 3059 <=50K (0.68827066 0.31172934) *\n             51) capital.loss>=1846 378   86 >50K (0.22751323 0.77248677) *\n         13) capital.gain>=5095.5 594   13 >50K (0.02188552 0.97811448) *\n        7) education.num>=12.5 5354 1582 >50K (0.29548001 0.70451999) *\nrpart.plot(tree_cart, type = 4, extra = 104)"},{"path":"chapter-tree.html","id":"decision-tree-with-c5.0","chapter":"11 Decision Trees and Random Forests","heading":"Decision Tree with C5.0","text":"next use C5.0 algorithm build Decision Tree, starting predictors. tree constructed using C5.0() function C50 package:output provides summary tree. full tree visualization omitted , highlights importance marital.status root node, consistent CART results.","code":"tree_C50 = C5.0(formula, data = train_set) \n\nprint(tree_C50)\n   \n   Call:\n   C5.0.formula(formula = formula, data = train_set)\n   \n   Classification Tree\n   Number of samples: 38878 \n   Number of predictors: 9 \n   \n   Tree size: 93 \n   \n   Non-standard options: attempt to group attributes"},{"path":"chapter-tree.html","id":"random-forest","chapter":"11 Decision Trees and Random Forests","heading":"Random Forest","text":"Random Forest algorithm used build ensemble Decision Trees, aggregating predictions. Using predictors, construct Random Forest model 100 trees using randomForest() function:can visualize variable importance error rate Random Forest model:","code":"\nrandom_forest = randomForest(formula = formula, data = train_set, ntree = 100)\nvarImpPlot(random_forest)\n\nplot(random_forest)"},{"path":"chapter-tree.html","id":"model-evaluation","chapter":"11 Decision Trees and Random Forests","heading":"Model Evaluation","text":"evaluate model performance, calculate confusion matrix, ROC curve, AUC three models (CART, C5.0, Random Forest):","code":""},{"path":"chapter-tree.html","id":"cart","chapter":"11 Decision Trees and Random Forests","heading":"CART:","text":"","code":"predict_cart = predict(tree_cart, test_set, type = \"class\")\n\nconf.mat(predict_cart, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\n          Actual\n   Predict <=50K >50K\n     <=50K  7061 1035\n     >50K    433 1191\nconf.mat.plot(predict_cart, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\""},{"path":"chapter-tree.html","id":"c5.0","chapter":"11 Decision Trees and Random Forests","heading":"C5.0:","text":"","code":"predict_C50 = predict(tree_C50, test_set, type = \"class\")\n\nconf.mat(predict_C50, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\n          Actual\n   Predict <=50K >50K\n     <=50K  7089  887\n     >50K    405 1339\nconf.mat.plot(predict_C50, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\""},{"path":"chapter-tree.html","id":"random-forest-1","chapter":"11 Decision Trees and Random Forests","heading":"Random Forest:","text":"Finally, ROC curves AUC models compared:black curve represents CART, red curve represents C5.0, green curve represents Random Forest. Based AUC values, C5.0 performs slightly better, three models show comparable accuracy, making reliable classification task.","code":"predict_random_forest = predict(random_forest, test_set)\n\nconf.mat(predict_random_forest, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\n          Actual\n   Predict <=50K >50K\n     <=50K  7069  913\n     >50K    425 1313\nconf.mat.plot(predict_random_forest, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\nprob_cart = predict(tree_cart, test_set, type = \"prob\")[, 1]\nprob_C50 = predict(tree_C50, test_set, type = \"prob\")[, 1]\nprob_random_forest = predict(random_forest, test_set, type = \"prob\")[, 1]\n\nroc_cart = roc(actual_test, prob_cart)\nroc_C50 = roc(actual_test, prob_C50)\nroc_random_forest = roc(actual_test, prob_random_forest)\n\nggroc(list(roc_cart, roc_C50, roc_random_forest), size = 0.8) + \n    theme_minimal() + ggtitle(\"ROC Curves with AUC for Three Models\") +\n  scale_color_manual(values = 1:3, \n    labels = c(paste(\"CART; AUC=\", round(auc(roc_cart), 3)), \n                paste(\"C5.0; AUC=\", round(auc(roc_C50), 3)), \n                paste(\"Random Forest; AUC=\", round(auc(roc_random_forest), 3)))) +\n  theme(legend.title = element_blank()) +\n  theme(legend.position = c(.7, .3), text = element_text(size = 17))"},{"path":"chapter-tree.html","id":"exercises-6","chapter":"11 Decision Trees and Random Forests","heading":"11.6 Exercises","text":"…","code":""},{"path":"chapter-nn.html","id":"chapter-nn","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12 Neural Networks: The Building Blocks of Artificial Intelligence","text":"centuries, humans dreamed creating machines capable mimicking human intelligence. Philosophers, scientists, storytellers long grappled possibilities consequences creations, weaving myths, fiction, philosophical discourse. roots fascination can traced far back ancient Greece, inventors like Daedalus Hero Alexandria said constructed mechanical devices write, generate sounds, even play music. Today, 21st century, age-old dreams longer confined realm imagination. become reality form Artificial Intelligence (AI), transformative force now deeply integrated daily lives. ChatGPT generative AI (GenAI) self-driving cars digital assistants like Siri Alexa, AI revolutionized way work, interact, make decisions. unprecedented progress driven advancements computational power, availability vast datasets, breakthroughs algorithm design.heart many cutting-edge AI systems lies class algorithms known neural networks. past decade, neural networks undergone dramatic resurgence umbrella deep learning, ushering revolutionary advancements across diverse fields like computer vision, natural language processing, generative modeling. deep learning now represents forefront machine learning, foundation rests simpler neural network architectures. chapter, focus feed-forward neural networks, also known multilayer perceptrons (MLPs). foundational models form building blocks sophisticated deep learning systems.Neural networks computational models inspired human brain. Just brain composed billions interconnected neurons work together enable complex tasks like reasoning, learning, perception, artificial neural networks replicate structure using layers interconnected nodes (artificial neurons). architecture allows neural networks process learn data identifying patterns, making uniquely suited solving problems involve complex, high-dimensional, unstructured data—images, text, sound. Unlike traditional machine learning models like Decision Trees k-Nearest Neighbors, neural networks excel ability automatically discover features representations within data, often surpassing human-engineered solutions.","code":""},{"path":"chapter-nn.html","id":"why-neural-networks-are-powerful","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Why Neural Networks Are Powerful","text":"Neural networks particularly adept solving complex nonlinear problems, making indispensable tasks involve large, diverse, intricate datasets. unique design capabilities provide several key advantages:Pattern Recognition Complex Data: Neural networks shine comes detecting patterns unstructured data, recognizing objects images, understanding spoken language, generating coherent text. tasks traditional algorithms struggle.Robustness Noise: Thanks dense networks neurons adaptable weights, neural networks can identify meaningful patterns even noisy incomplete datasets, effectively filtering irrelevant erroneous data.Scalability: Neural networks can handle vast amounts data adapt increasing complexity adding layers nodes, enabling model highly nonlinear relationships solve challenging problems.Despite strengths, neural networks without challenges. Unlike interpretable models decision trees, neural networks often referred “black boxes” due distributed opaque decision-making processes. can difficult understand neural network makes specific prediction, reasoning embedded across countless weights activations. Additionally, training neural networks can computationally intensive, often requiring specialized hardware like GPUs TPUs handle enormous volume calculations efficiently.power neural networks lies biological inspiration. Just interconnected neurons brain collaborate perform complex tasks, artificial neurons network combine outputs solve problems simpler algorithms . ability emulate nonlinear, adaptive learning brain positioned neural networks forefront academic research industry applications.","code":""},{"path":"chapter-nn.html","id":"whats-ahead","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"What’s Ahead","text":"chapter, explore key concepts behind neural networks examine transformative applications following topics:Biological Inspiration: Understanding structure function human brain inspired artificial neural networks.Core Algorithmic Principles: Exploring foundational mechanics neural networks, including layers, nodes, weights.Activation Functions: Unpacking importance introducing non-linearity enable neural networks model complex patterns.Training Neural Networks: Learning neural networks adjust parameters iterative optimization minimize errors.Case Study: Applying neural networks solve real-world problem—predicting whether customer subscribe term deposit using bank marketing dataset.Neural networks represent paradigm shift modern computing, enabling machines tackle problems considered insurmountable. powering recommendation systems driving autonomous vehicles, models transforming industries shaping future AI. chapter, uncover fundamentals neural networks, demonstrating extraordinary capabilities laying groundwork understanding operate. Let’s begin exploring inspiration behind neural networks connection biology human brain.","code":""},{"path":"chapter-nn.html","id":"neural-networks-inspired-by-biological-neurons","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.1 Neural Networks: Inspired by Biological Neurons","text":"foundation neural networks deeply rooted structure function biological neurons, form basis learning decision-making animal brains. individual neurons relatively simple structure, true power lies dense intricate connectivity. networks interconnected neurons enable brain perform highly complex tasks, pattern recognition, classification, reasoning, decision-making. example, human brain contains approximately \\(10^{11}\\) neurons, neuron forming connections average 10,000 others. creates astonishing \\(10^{15}\\) synaptic connections—vast, dynamic network capable extraordinary learning adaptation.Artificial Neural Networks (ANNs) computational abstractions biological system. far simpler biological counterparts, ANNs replicate fundamental principle learning interconnected units. leveraging dense networks artificial neurons, ANNs can model nonlinear dynamic processes, enabling tackle complex problems domains image recognition, speech processing, decision-making. particularly adept uncovering patterns relationships data, even cases traditional algorithms struggle.shown Figure 12.1, biological neuron designed process transmit information. Dendrites act input channels, collecting signals neurons. signals processed integrated cell body, decision made: combined input surpasses certain threshold, neuron “fires” sends output signal axon connected neurons. nonlinear behavior—firing certain input threshold exceeded—plays critical role brain’s ability process information efficiently.Similarly, artificial neuron (illustrated Figure 12.2) emulates process using mathematical model. receives inputs (\\(x_i\\)) either artificial neurons directly dataset. inputs combined using weighted summation (\\(\\sum w_i x_i\\)), weights (\\(w_i\\)) represent strength input’s influence. combined signal passed activation function (\\(f(.)\\)) introduce non-linearity, determining final output (\\(\\hat{y}\\)). output either passed downstream artificial neurons used final result model. activation function crucial, enables neural networks learn model complex, nonlinear relationships data.\nFigure 12.1: Visualization biological neuron, processes input signals dendrites sends outputs axon.\n\nFigure 12.2: Illustration artificial neuron, designed emulate structure function biological neuron simplified way.\nOne key advantages artificial neural networks robustness. Unlike traditional algorithms, neural networks can handle noisy incomplete data effectively. network’s many interconnected neurons weighted connections allow adapt “learn around” noise, focusing underlying patterns. However, flexibility comes cost. Neural networks often require large amounts data computational power train effectively, decision-making process less interpretable traditional models like decision trees.following sections, delve deeper mechanics neural networks, starting core structure algorithms enable learn data.","code":""},{"path":"chapter-nn.html","id":"how-neural-networks-work","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.2 How Neural Networks Work","text":"Neural networks can understood extension linear models incorporate multiple layers processing produce predictions decisions. core, build upon fundamental concepts linear regression. Recall linear regression model makes predictions using following equation:\\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_p x_p\n\\]\n\\(p\\) represents number predictors, \\(b_0\\) intercept, \\(b_1\\) \\(b_p\\) learned coefficients. setup, \\(\\hat{y}\\) weighted sum input features (\\(x_1\\) \\(x_p\\)), weights (\\(b_1\\) \\(b_p\\)) determine relative influence feature prediction. simple linear relationship can visualized shown Figure 12.3, input features prediction represented nodes, coefficients visualized connecting weights.\nFigure 12.3: graphical representation regression model: input features predictions shown nodes, coefficients represented connections nodes.\nFigure 12.3, nodes left represent input features, lines connecting represent coefficients (\\(w_i\\)), single node right represents output (\\(\\hat{y}\\)), weighted sum inputs. neural network generalizes idea introducing additional layers nodes input output, allowing model capture complex, nonlinear patterns data. structure illustrated Figure 12.4.structure neural network includes following key components:Input Layer: input layer entry point data. node layer corresponds input feature (e.g., age, income, image pixels).Hidden Layers: intermediate layers process data extract patterns. hidden layer contains multiple nodes (artificial neurons), node connected every node preceding succeeding layers. nodes hidden layers perform mathematical transformations data, enabling network learn complex relationships.Output Layer: final layer produces model’s prediction. example, classification task, output might predicted probability specific class, regression tasks, might continuous numerical value.\nFigure 12.4: Visualization multilayer neural network model two hidden layers.\nFigure 12.4, input layer passes features network, hidden layer transforms information passes next layer. output layer aggregates information generate final prediction. Every connection network associated weight (\\(w_i\\)), determines strength relationship two nodes. weights adjusted training optimize model’s accuracy.behavior artificial neuron can mathematically expressed :\\[\n\\hat{y} = f\\left( \\sum_{=1}^{p} w_i x_i + b \\right)\n\\]:\n- \\(x_i\\) represents input features,\n- \\(w_i\\) represents corresponding weights,\n- \\(b\\) bias term helps shift activation threshold,\n- \\(\\sum\\) represents summation weighted inputs,\n- \\(f(.)\\) activation function, \n- \\(\\hat{y}\\) output neuron.activation function plays vital role introducing non-linearity model. Without , neural network simply linear model, regardless complexity number layers. applying non-linear transformation combined input signals, activation functions enable neural networks approximate highly complex patterns data.","code":""},{"path":"chapter-nn.html","id":"key-characteristics-of-neural-networks","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Key Characteristics of Neural Networks","text":"Despite diversity neural network architectures, neural networks share three key characteristics define functionality (see Figure 12.2):Activation Functions:\nactivation function transforms neuron’s net input output signal passed next layer. Activation functions introduce non-linearity, critical modeling complex relationships data. Examples include sigmoid function, ReLU (Rectified Linear Unit), hyperbolic tangent (tanh).Activation Functions:\nactivation function transforms neuron’s net input output signal passed next layer. Activation functions introduce non-linearity, critical modeling complex relationships data. Examples include sigmoid function, ReLU (Rectified Linear Unit), hyperbolic tangent (tanh).Network Architecture:\narchitecture defines overall structure neural network, including number layers, number nodes layer, way nodes connected. instance, deep neural network many hidden layers, enabling learn hierarchical abstract representations data.Network Architecture:\narchitecture defines overall structure neural network, including number layers, number nodes layer, way nodes connected. instance, deep neural network many hidden layers, enabling learn hierarchical abstract representations data.Training Algorithm:\nTraining neural network involves adjusting weights (\\(w_i\\)) biases (\\(b\\)) model minimize error predicted actual outputs. achieved iterative optimization algorithms gradient descent, uses gradient loss function update weights.Training Algorithm:\nTraining neural network involves adjusting weights (\\(w_i\\)) biases (\\(b\\)) model minimize error predicted actual outputs. achieved iterative optimization algorithms gradient descent, uses gradient loss function update weights.following sections, ’ll explore components greater detail, starting activation functions role enabling neural networks learn complex, non-linear patterns.","code":""},{"path":"chapter-nn.html","id":"activation-functions","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.3 Activation Functions","text":"activation function critical component neural network, defining artificial neuron processes incoming signals passes information network. Much like biological counterpart, artificial neuron aggregates input signals, applies transformation, determines output signal send forward. biological neurons, transformation akin summing input signals dendrites deciding whether neuron “fires” based whether cumulative signal exceeds certain threshold.artificial neurons, process implemented mathematically. activation function determines whether, extent, neuron “activates” response inputs. Early models neural networks often used threshold activation function, mirrors biological concept. threshold function activates input signal surpasses certain threshold value. Mathematically, defined :\\[\nf(x) =\n\\begin{cases}\n1 & \\text{} x \\geq 0 \\\\\n0 & \\text{} x < 0\n\\end{cases}\n\\]Figure 12.5 visualizes threshold activation function. , neuron outputs value 1 input least zero, 0 otherwise. Due step-like shape, sometimes referred unit step function.\nFigure 12.5: Visualization threshold activation function (unit step).\nbiologically intuitive, threshold activation function rarely used modern neural networks handle nuanced relationships input output. rigid, providing binary outputs (0 1), differentiable, prevents use optimization algorithms like gradient descent.","code":""},{"path":"chapter-nn.html","id":"the-sigmoid-activation-function","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"The Sigmoid Activation Function","text":"widely used alternative sigmoid activation function, also known logistic sigmoid. sigmoid function provides smoother, non-binary output maps input value 0 1. defined mathematically :\\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\], \\(e\\) base natural logarithm (approximately 2.72). sigmoid function “S-shaped” curve, shown Figure 12.6, makes particularly useful modeling probabilities continuous values. Unlike threshold function, sigmoid differentiable, makes compatible modern training algorithms.\nFigure 12.6: Visualization sigmoid activation function.\nsigmoid function effective producing smooth outputs certain limitations. example, suffers vanishing gradient problem, large small input values cause gradient approach zero, slowing learning process.","code":""},{"path":"chapter-nn.html","id":"other-common-activation-functions","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Other Common Activation Functions","text":"addition sigmoid function, several activation functions commonly used, depending specific needs task. Figure 12.7 provides overview three widely used activation functions:Hyperbolic Tangent (tanh): Similar sigmoid function output range \\((-1, 1)\\). symmetry around zero often leads faster learning practice.Gaussian: Produces bell-shaped curve centered zero. less common useful specific applications like radial basis function networks.Linear Activation: Outputs input , often used final layer regression tasks.\nFigure 12.7: Comparison common activation functions: Sigmoid, tanh, Gaussian.\n","code":""},{"path":"chapter-nn.html","id":"choosing-the-right-activation-function","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Choosing the Right Activation Function","text":"choice activation function significantly impacts performance behavior neural network:Sigmoid commonly used output layer binary classification problems, output represents probability.Tanh often used hidden layers zero-centered, leading faster convergence optimization.ReLU (Rectified Linear Unit): discussed , ReLU one popular activation functions modern neural networks. outputs \\(f(x) = \\max(0, x)\\), helps address vanishing gradient problem accelerates training.Gaussian linear functions specialized chosen specific tasks, regression radial basis function networks.Activation functions like sigmoid tanh compress input values narrow output range (e.g., 0 1 sigmoid, \\(-1\\) \\(1\\) tanh). compression leads saturation problem, input values far zero produce near-constant outputs. instance:\n- Sigmoid outputs close 0 inputs \\(-5\\) close 1 inputs \\(+5\\).\n- results gradients near zero, slowing learning (vanishing gradient problem).One solution issue preprocess data normalizing standardizing input features fall within small range centered around zero. ensures inputs sensitive (non-saturated) range activation function, leading faster convergence better learning.","code":""},{"path":"chapter-nn.html","id":"network-architecture","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.4 Network Architecture","text":"capacity neural network learn make predictions deeply tied architecture, topology. refers arrangement neurons connections , define data flows network. neural networks can take countless forms, architecture primarily characterized three elements:number layers network,number neurons (nodes) layer, andThe connections neurons across layers.architecture neural network determines ability model complexity. Larger networks layers neurons can capture intricate patterns decision boundaries. However, power network just matter size also components organized interconnected.understand network architecture, let us consider simple example illustrated Figure 12.3. basic network consists :Input nodes, receive raw feature values dataset. input node corresponds one feature passes value network.Output nodes, provide network’s final prediction (denoted \\(p\\)).single-layer network, input nodes connected directly output node set weights (\\(w_1, w_2, \\dots, w_p\\)), representing influence input feature prediction. simple architecture works well basic classification regression tasks struggles complex patterns.handle sophisticated tasks, can add hidden layers, shown Figure 12.4. intermediate layers introduce additional processing steps, enabling network model nonlinear relationships discover complex patterns data.multilayer network typically consists three types layers:input layer, raw features enter network,One hidden layers, extract refine patterns, andThe output layer, combines processed information generate network’s final prediction.fully connected network, every neuron one layer connected every neuron next layer, connection assigned weight. weights determine much influence one neuron another, adjusted training optimize network’s performance.addition hidden layers allows network process input data hierarchically. Early layers may learn basic features, edges image simple word patterns text, deeper layers capture abstract representations, shapes semantic meaning. network contains multiple hidden layers, referred deep neural network (DNN). practice training networks known deep learning, enabled breakthroughs fields computer vision, speech recognition, natural language processing.number input output nodes network determined problem:\n- input nodes match number features dataset. example, dataset 20 features 20 input nodes.\n- output nodes depend task. regression, typically one output node predicted value. classification, number output nodes corresponds number classes.number hidden nodes layer predefined must decided user. larger number hidden nodes increases network’s capacity learn complex patterns, also increases risk overfitting—situation model performs well training data poorly unseen data. Overly large networks can also computationally expensive slow train.complex network may appear powerful, crucial strike balance complexity simplicity. often guided principle Occam’s Razor, suggests simplest model adequately explains data usually best choice. optimal network architecture often requires trial error, combined domain knowledge validation techniques evaluate performance unseen data.summary, architecture neural network defines capacity solve problems. simple single-layer network sophisticated deep neural networks, architectures offer flexibility model wide variety tasks, ranging basic regression highly complex problems like image recognition text generation. move forward, explore architectures trained optimize performance learn data.","code":""},{"path":"chapter-nn.html","id":"how-neural-networks-learn","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.5 How Neural Networks Learn","text":"neural network begins untrained structure—blank slate. Similar newborn child learning experience, neural network must trained data adjust internal connections. connections, represented weights, strengthened weakened network processes data, enabling learn patterns relationships time. Just baby’s brain develops interacting environment, neural network refines iteratively improving predictions based data encounters.Training neural network involves computationally intensive process adjusting weights connect neurons. neural networks studied since mid-20th century, remained impractical real-world applications 1980s, major breakthrough—backpropagation algorithm—made feasible train multilayer networks efficiently. Backpropagation, technique “back-propagating” errors network, revolutionized neural networks enabling learn complex tasks. Despite computationally expensive relative simpler algorithms, backpropagation become cornerstone modern neural network training, powering applications computer vision natural language processing.core, backpropagation works iteratively refining network’s weights process cycles two phases: forward phase backward phase. cycle, referred epoch, begins random initialization weights, network starts prior knowledge. successive epochs, network learns adjusting weights minimize prediction errors.forward phase, input data passed network, layer layer, starting input layer propagating hidden layers reaching output layer. neuron processes input applying weights, summing weighted inputs, transforming result using activation function. output layer produces network’s prediction, compared actual target value training data. comparison generates error signal—measure far network’s prediction .backward phase, error signal propagated backward network update weights. goal adjust weights way network produces predictions closer true target values subsequent forward passes. achieved using technique called gradient descent, determines optimal direction magnitude weight changes minimize error. Gradient descent relies derivative activation function compute gradient error respect weight. gradient indicates steeply error changes small adjustment weight, providing “slope” guide weight updates. process akin finding fastest route downhill mountainous terrain: always stepping direction steepest descent, network gradually approaches point minimum error.size weight adjustment controlled parameter called learning rate. high learning rate allows network make large, rapid updates weights, potentially speeding training risking overshooting optimal solution. Conversely, low learning rate ensures precise updates may result slower convergence. Striking right balance learning rate crucial effective training.successfully apply gradient descent backpropagation, network’s activation functions must differentiable. requirement smooth, non-linear activation functions sigmoid, hyperbolic tangent, ReLU (Rectified Linear Unit) widely used. differentiable nature enables network compute gradients efficiently, allowing algorithm make meaningful updates weights.repeated cycles forward backward propagation, network refines weights, reducing overall error improving ability generalize unseen data. training process may sound complex, modern machine learning libraries like TensorFlow PyTorch automate implementation backpropagation, gradient descent, weight updates. tools simplify process, enabling practitioners focus designing network architecture preparing data.development backpropagation algorithm marked turning point neural networks, enabling tackle real-world problems impressive accuracy. Although training remains computationally demanding, advancements hardware—GPUs TPUs—made practical train even large, complex networks. progress driven breakthroughs fields speech recognition, image classification, predictive modeling.Now foundation neural networks trained, explore application real-world scenarios, demonstrating can extract meaningful patterns data make predictions.","code":""},{"path":"chapter-nn.html","id":"case-study-bank-marketing","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.6 Case Study: Bank Marketing","text":"bank marketing dataset contains information customers contacted Portuguese banking institution subscribe term deposit. primary objective predict whether customer subscribe term deposit based available features. analysis helps identify profiles likely subscribers also provides insights improving future marketing campaigns.","code":""},{"path":"chapter-nn.html","id":"business-context","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Business Context","text":"Banks often rely two approaches promote products:Mass Campaigns: Targeting general public indiscriminately, typically resulting low response rates (e.g., <1%).Directed Marketing: Focusing specific, likely customers, improves effectiveness raises concerns privacy intrusion.case, goal refine effectiveness directed marketing campaigns analyzing patterns past campaign data. identifying customers likely subscribe, bank can reduce costs, minimize intrusive communications, maintain success rates.term deposit savings product fixed interest rate specified period. Customers benefit better interest rates compared regular savings accounts, banks use term deposits increase financial assets. term deposits can found .","code":""},{"path":"chapter-nn.html","id":"overview-of-the-dataset-3","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Overview of the Dataset","text":"dataset, sourced repository, includes information direct phone-based marketing campaigns. customers contacted multiple times campaign. goal classify whether customer subscribed term deposit (deposit = \"yes\" \"\").load inspect dataset:dataset contains 4521 observations 17 variables. includes 16 predictors one target variable (deposit). summary key variables:Demographic Features:\n- age: Age customer (numeric).\n- job: Type job (e.g., “admin.”, “blue-collar”, “management”).\n- marital: Marital status (e.g., “married”, “single”).\n- education: Level education (e.g., “secondary”, “tertiary”).\n- default: Whether customer credit default (binary: “yes”, “”).\n- balance: Average yearly balance euros (numeric).Loan Information:\n- housing: Whether customer housing loan (binary).\n- loan: Whether customer personal loan (binary).Campaign Details:\n- contact: Type communication (e.g., “telephone”, “cellular”).\n- duration: Last contact duration seconds (numeric).\n- campaign: Number contacts performed campaign (numeric).\n- pdays: Days since customer last contacted (numeric).\n- previous: Number contacts current campaign (numeric).\n- poutcome: Outcome previous campaign (e.g., “success”, “failure”).Target Variable:\n- deposit: Indicates whether customer subscribed term deposit (binary: “yes”, “”).","code":"data(bank)   # Load the bank marketing dataset \n\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-nn.html","id":"data-cleaning-and-preparation-1","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Data Cleaning and Preparation","text":"prepare dataset Neural Network algorithm, variables must scaled range 0 1. apply min-max normalization using minmax() function liver package:applying min-max normalization, can compare distribution age example:Next, partition dataset training (80%) test (20%) subsets using partition() function:validate split, compare proportion deposit = \"yes\" training test sets using two-sample Z-test:test confirms proportions subsets statistically similar (p-value > 0.05), validating split.’s improved version subsection:","code":"bank_mm = minmax(bank, col = \"all\")\nstr(bank_mm)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : num  0.162 0.206 0.235 0.162 0.588 ...\n    $ job      : num  0.9091 0.6364 0.3636 0.3636 0.0909 ...\n    $ marital  : num  0.5 0.5 1 0.5 0.5 1 0.5 0.5 0.5 0.5 ...\n    $ education: num  0 0.333 0.667 0.667 0.333 ...\n    $ default  : num  0 0 0 0 0 0 0 0 0 0 ...\n    $ balance  : num  0.0685 0.1088 0.0626 0.0643 0.0445 ...\n    $ housing  : num  0 1 1 1 1 0 1 1 1 1 ...\n    $ loan     : num  0 1 0 1 0 0 0 0 0 1 ...\n    $ contact  : num  0 0 0 1 1 0 0 0 1 0 ...\n    $ day      : num  0.6 0.3333 0.5 0.0667 0.1333 ...\n    $ month    : num  0.909 0.727 0 0.545 0.727 ...\n    $ duration : num  0.0248 0.0715 0.0599 0.0645 0.0735 ...\n    $ campaign : num  0 0 0 0.0612 0 ...\n    $ pdays    : num  0 0.39 0.38 0 0 ...\n    $ previous : num  0 0.16 0.04 0 0 0.12 0.08 0 0 0.08 ...\n    $ poutcome : num  1 0 0 1 1 ...\n    $ deposit  : num  0 0 0 0 0 0 0 0 0 0 ...\nggplot(data = bank) +\n  geom_histogram(mapping = aes(x = age), colour = \"darkblue\", fill = \"lightblue\") +\n  ggtitle(\"Variable 'age' before normalization\")\n\nggplot(data = bank_mm) +\n  geom_histogram(mapping = aes(x = age), colour = \"darkblue\", fill = \"lightblue\") +\n  ggtitle(\"Variable 'age' after normalization\")\nset.seed(500)\n\ndata_sets = partition(data = bank_mm, ratio = c(0.8, 0.2))\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\nactual_test = test_set$depositx1 = sum(train_set$deposit == 1)\nx2 = sum(test_set$deposit == 1)\n\nn1 = nrow(train_set)\nn2 = nrow(test_set)\n\nprop.test(x = c(x1, x2), n = c(n1, n2))\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.0014152, df = 1, p-value = 0.97\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.02516048  0.02288448\n   sample estimates:\n      prop 1    prop 2 \n   0.1150124 0.1161504"},{"path":"chapter-nn.html","id":"applying-the-neural-network-algorithm","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Applying the Neural Network Algorithm","text":"objective classify customers either likely (deposit = \"yes\") unlikely (deposit = \"\") subscribe term deposit, based following predictors:age, default, balance, housing, loan, duration, campaign, pdays, previous.implement neural network, use neuralnet package R. package offers straightforward flexible way build neural networks provides functionality visualizing network topology. neuralnet great learning tool, also powerful enough practical applications.haven’t already installed neuralnet package, can typing:installed, load session:Next, apply neuralnet() function training dataset build model:’s argument function call :formula: Specifies target variable (deposit) predictors.data: Indicates dataset used training (train_set).hidden: Defines number hidden layers nodes (1 hidden layer 1 node case).err.fct: Sets error function minimize training; use “sse” (Sum Squared Errors).linear.output: Ensures nonlinear activation function output layer, appropriate classification tasks.training, visualize network examine topology:visualization shows network consists :\n- 9 input nodes, corresponding 9 predictors,\n- 1 hidden layer containing single node, \n- 1 output node representing classification result (yes ).training process converged 2578 steps, final error rate 142.97. Upon analyzing weights network, found duration predictor greatest influence model’s output, making significant factor determining whether customer likely subscribe term deposit.straightforward setup demonstrates neural networks process input data multiple layers extract meaningful patterns make predictions. next section, evaluate model’s performance interpret results.","code":"\ninstall.packages(\"neuralnet\")\nlibrary(neuralnet)\nformula = deposit ~ age + default + balance + housing + loan + duration + \n                    campaign + pdays + previous\n\nneuralnet_bank = neuralnet(\n  formula = formula,\n  data = train_set,\n  hidden = 1,                # Single hidden layer with 1 node\n  err.fct = \"sse\",           # Error function: Sum of Squared Errors\n  linear.output = FALSE      # Nonlinear activation function\n)\nplot(neuralnet_bank, rep = \"best\")"},{"path":"chapter-nn.html","id":"prediction-and-model-evaluation-1","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Prediction and Model Evaluation","text":"use trained Neural Network predict outcomes test dataset:evaluate predictions using confusion matrix cutoff value 0.5:confusion matrix reveals number correct incorrect predictions (e.g., true positives, false positives). Finally, assess model’s performance plotting ROC curve calculating AUC:ROC curve illustrates model’s ability distinguish two classes. high AUC score indicates strong predictive performance.","code":"prob_nn = predict(neuralnet_bank, test_set)\nhead(prob_nn)\n            [,1]\n   12 0.01460510\n   16 0.02158387\n   17 0.11371069\n   19 0.02391761\n   32 0.03149115\n   38 0.01552204conf.mat(prob_nn, actual_test, cutoff = 0.5)\n   Setting levels: reference = \"0\", case = \"1\"\n          Actual\n   Predict   0   1\n         0  16  22\n         1 783  83\nroc_nn = roc(actual_test, prob_nn)\n\nggroc(roc_nn, size = 0.8) + \n  theme_minimal() + \n  ggtitle(\"ROC for Neural Network Algorithm\") +\n  theme(legend.title = element_blank(), legend.position = c(.7, .3), text = element_text(size = 17))"},{"path":"chapter-nn.html","id":"exercises-7","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.7 Exercises","text":"…","code":""},{"path":"chapter-cluster.html","id":"chapter-cluster","chapter":"13 Clustering","heading":"13 Clustering","text":"Every day, interact systems organize vast amounts data without explicit instructions. Netflix recommend movies tailored taste? Amazon categorize millions products? real-world examples clustering, machine learning technique groups similar items based shared characteristics—without predefined labels.many real-world scenarios, deal large datasets structure unknown. Unlike classification, assigns predefined labels data points (e.g., distinguishing spam non-spam emails), clustering exploratory—helps uncover hidden patterns, making powerful tool knowledge discovery. identifying meaningful groups, clustering allows us make sense complex data extract valuable insights.Clustering widely used across multiple domains, including:Customer segmentation – Identifying distinct customer groups personalize marketing campaigns.Market research – Understanding consumer behavior enhance product recommendations.Fraud detection – Detecting suspicious financial transactions may indicate fraudulent activity.Document organization – Automatically grouping large collections text meaningful categories.Bioinformatics – Clustering genes similar expression patterns uncover biological insights.chapter provides comprehensive introduction clustering, covering:fundamental principles clustering differs classification.mechanics clustering algorithms define similarity.K-means clustering, one widely used clustering techniques.practical case study: segmenting cereal brands based nutritional content.end chapter, understand clustering works, apply , implement real-world scenarios. Let’s dive !\n## Cluster Analysis? {#cluster-}Clustering unsupervised machine learning technique groups data points clusters based similarity. Unlike supervised learning, models learn labeled examples, clustering exploratory—uncovers hidden structures data without predefined labels. goal form groups data points within cluster highly similar, different clusters distinct.computer determine data points belong together? Clustering relies similarity measures quantify close distant two points . One commonly used approaches distance metrics, Euclidean distance, defined :\\[\n\\text{dist}(x, y) = \\sqrt{ \\sum_{=1}^n (x_i - y_i)^2}\n\\]\\(x = (x_1, x_2, \\ldots, x_n)\\) \\(y = (y_1, y_2, \\ldots, y_n)\\) represent two data points \\(n\\) features. closer two points, similar .However, Euclidean distance always appropriate. categorical variables, alternative strategies one-hot encoding transform categories numerical values, enabling distance-based clustering. Additionally, features often require scaling (e.g., min-max normalization) ensure single variable dominates clustering process.Clustering often compared classification, serve different purposes. Classification assigns predefined labels new data points based past examples, whereas clustering discovers groupings raw data. Classification typically used prediction, clustering primarily exploration pattern discovery. clustering generates labels rather predicting existing ones, sometimes referred unsupervised classification. cluster assignments can used inputs analysis, refining predictions neural network decision tree model.clustering algorithms aim achieve high intra-cluster similarity (data points within cluster close together) low inter-cluster similarity (clusters well separated). concept visually illustrated Figure 13.1, effective clusters minimize internal variation maximizing separation groups.\nFigure 13.1: Clustering algorithms aim minimize intra-cluster variation maximizing inter-cluster separation.\nBeyond role data exploration, clustering widely used preprocessing step machine learning. Given massive scale modern datasets, clustering helps reduce complexity identifying smaller number representative groups, leading several benefits:Reduced computation time downstream models.Improved interpretability summarizing large datasets.Enhanced predictive performance structuring inputs supervised learning.following sections, explore K-means clustering, one widely used clustering algorithms. also discuss methods selecting optimal number clusters apply clustering real-world dataset.","code":""},{"path":"chapter-cluster.html","id":"kmeans","chapter":"13 Clustering","heading":"13.1 K-means Clustering","text":"K-means clustering one simplest widely used clustering algorithms. aims partition dataset \\(k\\) clusters iteratively refining cluster centers, ensuring data points within cluster similar possible. algorithm operates iterative process assigning points clusters updating cluster centers based assignments. process stops assignments stabilize, meaning data points switch clusters.K-means algorithm requires user specify number clusters, \\(k\\), advance. follows steps:Initialize: Randomly select \\(k\\) data points initial cluster centers.Assignment: Assign data point nearest cluster center. creates \\(k\\) groups.Update: Compute centroid (mean) cluster move cluster centers new locations.Repeat: Iterate steps 2 3 convergence—cluster assignments longer change.Although K-means simple efficient, limitations. final clusters depend heavily initial choice cluster centers, meaning different runs algorithm may produce different results. Additionally, K-means sensitive outliers assumes clusters spherical similar size, may always case real-world data.illustrate K-means works, consider dataset 50 records two features, \\(x_1\\) \\(x_2\\), shown Figure 13.2. goal partition data three clusters.\nFigure 13.2: simple dataset 50 records two features, ready clustering.\nfirst step randomly select three initial cluster centers (red stars), shown left panel Figure 13.3. data point assigned nearest cluster, forming three groups labeled blue (Cluster ), green (Cluster B), orange (Cluster C). right panel Figure 13.3 displays initial assignments. dashed lines represent Voronoi diagram, divides space regions associated cluster center.\nFigure 13.3: Initial random cluster centers (left) first cluster assignments (right).\nSince K-means sensitive initialization, poor placement initial cluster centers can lead suboptimal clustering. mitigate issue, K-means++9 introduced 2007. method strategically selects initial centers improve convergence reduce randomness.initial cluster assignments made, K-means enters update phase. first step recompute centroid cluster, mean position points assigned cluster. cluster centers moved new centroid locations, shown left panel Figure 13.4. right panel illustrates Voronoi boundaries shift, causing data points reassigned different cluster.\nFigure 13.4: Updated cluster centers (left) new assignments centroid adjustment (right).\nprocess—reassigning points updating centroids—continues iteratively. another update, points switch clusters , leading refined Voronoi partition, shown Figure 13.5.\nFigure 13.5: Updated cluster centers assignments another iteration.\nalgorithm continues iterating cluster assignments stabilize—points switch clusters, shown Figure 13.6. point, algorithm converges, final clusters established.\nFigure 13.6: Final cluster assignments K-means convergence.\nclustering complete, results can presented two ways:Cluster Assignments: data point labeled belonging Cluster , B, C.Centroid Coordinates: final positions cluster centers can reported.final cluster centroids act representative points, summarizing dataset enabling analysis. K-means clustering widely used applications customer segmentation, image compression, document clustering. next section, explore methods selecting optimal number clusters ensure meaningful partitions real-world datasets.","code":""},{"path":"chapter-cluster.html","id":"kmeans-choose","chapter":"13 Clustering","heading":"13.2 Choosing the Number of Clusters","text":"One key challenges K-means clustering selecting appropriate number clusters, \\(k\\). choice \\(k\\) significantly impacts results—clusters may fail capture meaningful structures, many clusters risk overfitting creating overly fragmented groups. Unlike supervised learning, evaluation metrics like accuracy guide model selection, clustering absolute ground truth, making selection \\(k\\) subjective.cases, domain knowledge can provide useful guidance. example, clustering movies, reasonable starting point might number well-known genres. business setting, marketing teams may set \\(k = 3\\) plan design three distinct advertising campaigns. Similarly, seating arrangements conference might determine number groups based available tables. However, clear intuition exists, data-driven methods needed determine optimal \\(k\\).One widely used technique choosing \\(k\\) elbow method, evaluates within-cluster variation changes number clusters increases. clusters added, clusters become homogeneous (internal similarity increases), overall heterogeneity (difference clusters) decreases. However, improvement follows diminishing returns pattern. idea find point adding another cluster longer significantly reduces within-cluster variance.critical point, known elbow point, represents natural number clusters. concept illustrated Figure 13.7, curve shows total within-cluster sum squares (WCSS) function \\(k\\). “elbow” curve—rate improvement slows—strong candidate \\(k\\).\nFigure 13.7: elbow method helps determine optimal number clusters K-means clustering.\nelbow method provides useful heuristic, limitations. datasets, curve may exhibit clear elbow, making choice \\(k\\) ambiguous. Additionally, evaluating many different values \\(k\\) can computationally expensive, especially large datasets.techniques can supplement refine selection \\(k\\):Silhouette Score: Measures well point fits within assigned cluster compared others. higher silhouette score suggests well-defined clustering structure.Gap Statistic: Compares clustering result reference distribution assess whether structure significant.Cross-validation clustering tasks: applications clustering feeds downstream task (e.g., classification), impact different \\(k\\) values can evaluated context.Ultimately, choice \\(k\\) driven data characteristics practical considerations. Clustering often used exploratory analysis, meaning useful \\(k\\) necessarily mathematically “optimal” one rather one yields meaningful, interpretable insights.Observing cluster characteristics evolve \\(k\\) varies can informative. groups may remain stable across different \\(k\\) values, indicating strong natural boundaries, others may appear disappear, suggesting fluid structures data.Rather aiming perfect cluster count, often sufficient find reasonable interpretable clustering solution. next section, apply clustering real-world dataset, demonstrating practical knowledge can guide choice \\(k\\) actionable insights.Now explored K-means clustering methods selecting optimal number clusters, apply concepts real-world dataset.","code":""},{"path":"chapter-cluster.html","id":"kmeans-cereal","chapter":"13 Clustering","heading":"13.3 Case Study: Clustering Cereal Data","text":"case study, apply K-means clustering cereal dataset liver package. dataset contains nutritional information 77 cereal brands, including calories, protein, fat, sodium, fiber, sugar content. Understanding nutritional profiles valuable marketing strategies, consumer targeting, product positioning. goal segment cereals distinct groups based nutritional similarities.","code":""},{"path":"chapter-cluster.html","id":"dataset-overview","chapter":"13 Clustering","heading":"13.3.1 Dataset Overview","text":"cereal dataset includes 77 observations 16 variables, covering various nutritional attributes. can accessed liver package, shown :can examine structure using:dataset contains following variables:name: Name cereal (categorical).manuf: Manufacturer cereal (categorical).type: Cereal type (hot cold, categorical).calories: Calories per serving (numerical).protein: Grams protein per serving (numerical).fat: Grams fat per serving (numerical).sodium: Milligrams sodium per serving (numerical).fiber: Grams dietary fiber per serving (numerical).carbo: Grams carbohydrates per serving (numerical).sugars: Grams sugar per serving (numerical).potass: Milligrams potassium per serving (numerical).vitamins: Percentage FDA-recommended vitamins (categorical: 0, 25, 100).shelf: Display shelf position (categorical: 1, 2, 3).weight: Weight one serving ounces (numerical).cups: Number cups per serving (numerical).rating: Cereal rating score (numerical).","code":"\nlibrary(liver)  # Load the liver package\n\ndata(cereal)    # Load the cereal datasetstr(cereal)\n   'data.frame':    77 obs. of  16 variables:\n    $ name    : Factor w/ 77 levels \"100% Bran\",\"100% Natural Bran\",..: 1 2 3 4 5 6 7 8 9 10 ...\n    $ manuf   : Factor w/ 7 levels \"A\",\"G\",\"K\",\"N\",..: 4 6 3 3 7 2 3 2 7 5 ...\n    $ type    : Factor w/ 2 levels \"cold\",\"hot\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ calories: int  70 120 70 50 110 110 110 130 90 90 ...\n    $ protein : int  4 3 4 4 2 2 2 3 2 3 ...\n    $ fat     : int  1 5 1 0 2 2 0 2 1 0 ...\n    $ sodium  : int  130 15 260 140 200 180 125 210 200 210 ...\n    $ fiber   : num  10 2 9 14 1 1.5 1 2 4 5 ...\n    $ carbo   : num  5 8 7 8 14 10.5 11 18 15 13 ...\n    $ sugars  : int  6 8 5 0 8 10 14 8 6 5 ...\n    $ potass  : int  280 135 320 330 -1 70 30 100 125 190 ...\n    $ vitamins: int  25 0 25 25 25 25 25 25 25 25 ...\n    $ shelf   : int  3 3 3 3 3 1 2 3 1 3 ...\n    $ weight  : num  1 1 1 1 1 1 1 1.33 1 1 ...\n    $ cups    : num  0.33 1 0.33 0.5 0.75 0.75 1 0.75 0.67 0.67 ...\n    $ rating  : num  68.4 34 59.4 93.7 34.4 ..."},{"path":"chapter-cluster.html","id":"data-preprocessing","chapter":"13 Clustering","heading":"13.3.2 Data Preprocessing","text":"applying K-means clustering, need clean preprocess data. start summarizing dataset:Upon inspection, notice unusual values variables sugars, carbo, potass, entries set -1. Since negative values invalid nutritional attributes, replace NA:Next, handle missing values using K-nearest neighbors (KNN) imputation knnImputation() function DMwR2 package:clustering, exclude categorical identifier variables (name, manuf, rating), retaining numerical features:Since dataset includes features different scales, apply min-max scaling using minmax() function liver package ensure variables contribute equally clustering process:visualize effect normalization, plot sodium distribution scaling:scaling, values fall within 0–1 range, making distance-based clustering reliable.","code":"summary(cereal)\n                           name    manuf    type       calories    \n    100% Bran                : 1   A: 1   cold:74   Min.   : 50.0  \n    100% Natural Bran        : 1   G:22   hot : 3   1st Qu.:100.0  \n    All-Bran                 : 1   K:23             Median :110.0  \n    All-Bran with Extra Fiber: 1   N: 6             Mean   :106.9  \n    Almond Delight           : 1   P: 9             3rd Qu.:110.0  \n    Apple Cinnamon Cheerios  : 1   Q: 8             Max.   :160.0  \n    (Other)                  :71   R: 8                            \n       protein           fat            sodium          fiber       \n    Min.   :1.000   Min.   :0.000   Min.   :  0.0   Min.   : 0.000  \n    1st Qu.:2.000   1st Qu.:0.000   1st Qu.:130.0   1st Qu.: 1.000  \n    Median :3.000   Median :1.000   Median :180.0   Median : 2.000  \n    Mean   :2.545   Mean   :1.013   Mean   :159.7   Mean   : 2.152  \n    3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:210.0   3rd Qu.: 3.000  \n    Max.   :6.000   Max.   :5.000   Max.   :320.0   Max.   :14.000  \n                                                                    \n        carbo          sugars           potass          vitamins     \n    Min.   :-1.0   Min.   :-1.000   Min.   : -1.00   Min.   :  0.00  \n    1st Qu.:12.0   1st Qu.: 3.000   1st Qu.: 40.00   1st Qu.: 25.00  \n    Median :14.0   Median : 7.000   Median : 90.00   Median : 25.00  \n    Mean   :14.6   Mean   : 6.922   Mean   : 96.08   Mean   : 28.25  \n    3rd Qu.:17.0   3rd Qu.:11.000   3rd Qu.:120.00   3rd Qu.: 25.00  \n    Max.   :23.0   Max.   :15.000   Max.   :330.00   Max.   :100.00  \n                                                                     \n        shelf           weight          cups           rating     \n    Min.   :1.000   Min.   :0.50   Min.   :0.250   Min.   :18.04  \n    1st Qu.:1.000   1st Qu.:1.00   1st Qu.:0.670   1st Qu.:33.17  \n    Median :2.000   Median :1.00   Median :0.750   Median :40.40  \n    Mean   :2.208   Mean   :1.03   Mean   :0.821   Mean   :42.67  \n    3rd Qu.:3.000   3rd Qu.:1.00   3rd Qu.:1.000   3rd Qu.:50.83  \n    Max.   :3.000   Max.   :1.50   Max.   :1.500   Max.   :93.70  \n   cereal[cereal == -1] <- NA\nfind.na(cereal)  # Check missing values\n        row col\n   [1,]  58   9\n   [2,]  58  10\n   [3,]   5  11\n   [4,]  21  11library(DMwR2)\ncereal <- knnImputation(cereal, k = 3, scale = TRUE)\nfind.na(cereal)  # Verify missing values are filled\n   [1] \" No missing values (NA) in the dataset.\"\nselected_variables <- colnames(cereal)[-c(1, 2, 16)]\ncereal_subset <- cereal[, selected_variables]cereal_mm <- minmax(cereal_subset, col = \"all\")\nstr(cereal_mm)  # Check the transformed dataset\n   'data.frame':    77 obs. of  13 variables:\n    $ type    : num  0 0 0 0 0 0 0 0 0 0 ...\n    $ calories: num  0.182 0.636 0.182 0 0.545 ...\n    $ protein : num  0.6 0.4 0.6 0.6 0.2 0.2 0.2 0.4 0.2 0.4 ...\n    $ fat     : num  0.2 1 0.2 0 0.4 0.4 0 0.4 0.2 0 ...\n    $ sodium  : num  0.4062 0.0469 0.8125 0.4375 0.625 ...\n    $ fiber   : num  0.7143 0.1429 0.6429 1 0.0714 ...\n    $ carbo   : num  0 0.167 0.111 0.167 0.5 ...\n    $ sugars  : num  0.4 0.533 0.333 0 0.533 ...\n    $ potass  : num  0.841 0.381 0.968 1 0.122 ...\n    $ vitamins: num  0.25 0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ...\n    $ shelf   : num  1 1 1 1 1 0 0.5 1 0 1 ...\n    $ weight  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.83 0.5 0.5 ...\n    $ cups    : num  0.064 0.6 0.064 0.2 0.4 0.4 0.6 0.4 0.336 0.336 ...\nggplot(data = cereal) +\n    geom_histogram(aes(x = sodium), color = \"blue\", fill = \"lightblue\") +\n    theme_minimal() + ggtitle(\"Before min-max normalization\")\n\nggplot(data = cereal_mm) +\n    geom_histogram(aes(x = sodium), color = \"blue\", fill = \"lightblue\") + \n    theme_minimal() + ggtitle(\"After min-max normalization\")"},{"path":"chapter-cluster.html","id":"applying-k-means-clustering","chapter":"13 Clustering","heading":"13.3.3 Applying K-means Clustering","text":"","code":""},{"path":"chapter-cluster.html","id":"choosing-the-optimal-number-of-clusters","chapter":"13 Clustering","heading":"Choosing the Optimal Number of Clusters","text":"clustering, need determine optimal number clusters. use elbow method, plots within-cluster sum squares (WCSS) different values \\(k\\). elbow point—improvement WCSS slows—suggests ideal \\(k\\):plot, observe \\(k = 4\\) clusters reasonable choice, adding clusters beyond point yields diminishing improvements WCSS.","code":"\nlibrary(factoextra)\n\nfviz_nbclust(cereal_mm, kmeans, method = \"wss\", k.max = 15) + \n  geom_vline(xintercept = 4, linetype = 2, color = \"gray\")"},{"path":"chapter-cluster.html","id":"performing-k-means-clustering","chapter":"13 Clustering","heading":"Performing K-means Clustering","text":"now apply K-means algorithm \\(k = 4\\) clusters:check cluster sizes:","code":"\nset.seed(3)  # Ensure reproducibility\ncereal_kmeans <- kmeans(cereal_mm, centers = 4)cereal_kmeans$size\n   [1] 36 10 13 18"},{"path":"chapter-cluster.html","id":"visualizing-the-clusters","chapter":"13 Clustering","heading":"Visualizing the Clusters","text":"better understand clustering results, visualize clusters using fviz_cluster() function factoextra package:scatter plot displays four clusters, point representing cereal brand. Different colors indicate distinct clusters, ellipses represent spread cluster based standard deviation.","code":"\nfviz_cluster(cereal_kmeans, cereal_mm, geom = \"point\", ellipse.type = \"norm\", palette = \"custom_palette\")"},{"path":"chapter-cluster.html","id":"interpreting-the-results","chapter":"13 Clustering","heading":"Interpreting the Results","text":"clusters reveal natural groupings among cereals based nutritional content. example:\n- clusters may contain low-sugar, high-fiber cereals, appealing health-conscious consumers.\n- Others may group high-calorie, high-sugar cereals, often marketed children.\n- Another group may include balanced cereals, offering mix moderate calories nutrients.examine cereals belong specific cluster (e.g., Cluster 1), can use:command lists names cereals assigned Cluster 1, helping us interpret characteristics group.case study demonstrated K-means clustering can segment cereals meaningful groups based nutritional content. data preprocessing, feature scaling, cluster visualization, successfully grouped cereals similar characteristics. clustering techniques widely applicable marketing, consumer analytics, product positioning, providing actionable insights businesses researchers alike.chapter, explored fundamentals clustering, mechanics K-means algorithm, methods choosing optimal number clusters. applied concepts real-world dataset, demonstrating K-means can extract meaningful insights. Clustering remains powerful tool across various domains, marketing bioinformatics, making essential technique modern data science toolkit.","code":"\ncereal$name[cereal_kmeans$cluster == 1]"},{"path":"chapter-cluster.html","id":"exercises-8","chapter":"13 Clustering","heading":"13.4 Exercises","text":"…","code":""}]
