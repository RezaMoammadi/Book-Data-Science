[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Data science transforming way solve problems, make decisions, uncover insights data. Whether ’re beginner experienced professional, Uncovering Data Science R provides intuitive practical introduction exciting field—prior analytics programming experience required.book work progress, welcome feedback readers. comments, suggestions, corrections, please feel free contact us Contact Us.","code":""},{"path":"index.html","id":"why-this-book","chapter":"Preface","heading":"Why This Book?","text":"Data science rapidly evolving field leverages computational tools techniques transform raw data actionable insights. book, introduce fundamental skills needed work R, powerful freely available statistical programming language widely used data analysis, visualization, machine learning.Unlike many books data science, focus accessibility. aim provide intuitive practical introduction, making R data science concepts understandable little technical background. hands-approach ensures learn theoretical concepts also gain experience applying real-world datasets.Compared commercial software like SAS SPSS, R provides free, open-source, highly extensible platform statistical computing machine learning. rich ecosystem packages makes excellent alternative proprietary data mining tools.Inspired Free Open Source Software (FOSS) movement, content book open transparent, ensuring reproducibility. code, datasets, materials hosted CRAN accessible via liver package (https://CRAN.R-project.org/package=liver), allowing readers engage book interactively.","code":""},{"path":"index.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who Should Read This Book?","text":"book anyone interested learning data science, particularly new field. designed :Business professionals want leverage data decision-making,Students researchers looking apply data analysis work,Beginners prior programming experience,Anyone interested data science machine learning using R.","code":""},{"path":"index.html","id":"what-you-will-learn","chapter":"Preface","heading":"What You Will Learn","text":"primary goal book introduce data science concepts using R tool data analysis machine learning. R open-source language environment statistical computing graphics, offering vast collection packages data mining, visualization, modeling.hands-examples real-world datasets, learn:basics R set environment,core principles data science Data Science Methodology,clean, transform, explore data,fundamentals statistical analysis, machine learning, data visualization,build evaluate machine learning models, including classification, regression, clustering, neural networks,apply techniques real-world datasets.","code":""},{"path":"index.html","id":"the-data-science-process","chapter":"Preface","heading":"The Data Science Process","text":"Data science follows iterative structured methodology analyzing extracting insights data. book follows framework:Problem Understanding – Defining objective understanding data.Data Preparation – Preparing raw data analysis.Exploratory Data Analysis (EDA) – Identifying patterns relationships data.Preparing Data Modeling – Transforming data machine learning models.Modeling – Building predictive models using machine learning algorithms.Evaluation – Assessing model performance using various metrics.Deployment – Applying trained model real-world scenarios.end book, solid understanding phases able apply effectively.","code":""},{"path":"index.html","id":"how-this-book-is-structured","chapter":"Preface","heading":"How This Book Is Structured","text":"book structured hands-guide, designed take beginner practitioner R data science. chapters follow logical progression, starting foundational concepts gradually introducing advanced techniques.use real-world datasets (see Table 0.1) throughout book illustrate key concepts. datasets available liver package can accessed easily. brief overview book’s chapters:Chapter 1 – Introduction R, including installation basic operations.Chapter 2 – Introduction Data Science methodology.Chapter 3 – Data preparation techniques.Chapter 4 – Exploratory Data Analysis (EDA) using visualization summary statistics.Chapter 5 – Basics statistical analysis, including descriptive statistics hypothesis testing.Chapter 6 – Overview machine learning models.Chapter 7 – k-Nearest Neighbors (k-NN) algorithm.Chapter 8 – Model evaluation metrics techniques.Chapter 9 – Naïve Bayes classifier probabilistic modeling.Chapter 10 – Linear regression predictive modeling.Chapter 11 – Decision trees Random Forests.Chapter 12 – Neural networks deep learning basics.Chapter 13 – Clustering techniques, including k-means.end chapter, find practical exercises labs reinforce learning. exercises use real-world datasets provide step--step guidance ensure hands-experience.","code":""},{"path":"index.html","id":"how-to-use-this-book","chapter":"Preface","heading":"How to Use This Book","text":"book designed self-study classroom use. can read cover cover jump chapters interest . chapter builds previous ones, beginners encouraged follow sequence smooth learning experience.get book:Run code examples – code snippets designed executed interactively R.Complete exercises – Practical exercises reinforce key concepts improve problem-solving skills.Modify experiment – Try changing code explore different scenarios.Use reference – ’re familiar basics, use book guide working real-world data.book also used data science courses University Amsterdam. can serve textbook similar courses supplementary resource advanced analytics training.","code":""},{"path":"index.html","id":"datasets-used-in-this-book","chapter":"Preface","heading":"Datasets Used in This Book","text":"Table 0.1 lists datasets used book. real-world datasets used illustrate key concepts available liver package, can downloaded CRAN.\nTable 0.1: List datasets used case studies different chapters. Available R package liver.\n","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"prior programming experience required, basic understanding numbers logic helpful. run code book, need install R, RStudio, several R packages.","code":""},{"path":"chapter-into-R.html","id":"chapter-into-R","chapter":"1 The Basics for R","heading":"1 The Basics for R","text":"can analyze data, need way communicate computer. ’s programming languages like R Python come . Many data science teams use mix languages, R great starting point designed specifically data analysis statistical computing.","code":""},{"path":"chapter-into-R.html","id":"why-choose-r-for-data-science","chapter":"1 The Basics for R","heading":"Why Choose R for Data Science?","text":"R widely used statistics, data analysis, visualization due rich ecosystem libraries tools tailored data science. Unlike general-purpose programming languages, R built statistical analysis, allowing data scientists perform everything basic calculations advanced machine learning just lines code.Python another popular language data science, R particularly well-suited :\n- Statistical Computing – R built-statistical functions methods hypothesis testing, regression modeling, machine learning.\n- Data Visualization – Packages like ggplot2 provide powerful tools creating high-quality plots graphs minimal effort.\n- Reproducible Research – R Markdown Shiny make easy generate reports interactive dashboards directly R code.\n- Bioinformatics & Finance – Many researchers analysts fields use R due robust statistical libraries domain-specific packages.Beyond capabilities, R :Free & Open Source – Available everyone, vibrant community contributors.Cross-Platform – Runs Windows, macOS, Linux.Flexible & Powerful – Supports interactive data exploration, visualization, machine learning.R language, RStudio tool makes working R easier. RStudio integrated development environment (IDE) provides:console running R commands,script editor syntax highlighting auto-completion,Built-tools data visualization, debugging, package management.chapter, learn fundamental skills needed work R, installation running first commands. Let’s begin! 🚀","code":""},{"path":"chapter-into-R.html","id":"how-to-install-r","chapter":"1 The Basics for R","heading":"1.1 How to Install R","text":"get started R, first need install computer. Follow steps:Go CRAN website – Comprehensive R Archive Network.Select operating system – Click link Windows, macOS, Linux.Download install R – Follow -screen instructions complete installation.","code":""},{"path":"chapter-into-R.html","id":"keeping-r-up-to-date","chapter":"1 The Basics for R","heading":"Keeping R Up to Date","text":"R receives major update year, along 2-3 minor updates annually. updating R—especially major versions—requires reinstalling packages, staying date ensures :✅ Access latest features improvements,\n✅ Maintain compatibility new packages,\n✅ Benefit security patches performance enhancements.Keeping R updated might feel like hassle, postponing updates can make process cumbersome later. ’s best update regularly ensure smooth performance compatibility.","code":""},{"path":"chapter-into-R.html","id":"how-to-install-rstudio","chapter":"1 The Basics for R","heading":"1.2 How to Install RStudio","text":"RStudio open-source integrated development environment (IDE) makes working R easier, interactive, efficient. provides user-friendly interface, advanced script editor, various tools plotting, debugging, workspace management—significantly enhance R programming experience.","code":""},{"path":"chapter-into-R.html","id":"installing-rstudio","chapter":"1 The Basics for R","heading":"Installing RStudio","text":"Follow steps install RStudio:Go RStudio website.Download latest version RStudio Desktop (free, open-source edition).Run installer follow -screen instructions.Launch RStudio, ’re ready start coding R!RStudio updated several times year, notify new version available. Keeping RStudio date recommended take advantage new features performance improvements.","code":""},{"path":"chapter-into-R.html","id":"exploring-the-rstudio-interface","chapter":"1 The Basics for R","heading":"Exploring the RStudio Interface","text":"open RStudio, see window similar Figure 1.1.\nFigure 1.1: RStudio window first launch program.\nsee three panels, add fourth selecting File > New File > R Script. opens script editor can write save R code. ’s quick overview RStudio’s panels:Top-left: Script Editor – Write save R code.Bottom-left: Console – Run R commands see output.Top-right: Environment & History – View variables, datasets, past commands.Bottom-right: Plots, Help, & Files – Display graphs, access documentation, manage files.now, just know can type R code console press Enter run . progress book, ’ll become familiar RStudio’s features learn efficiently write, run, debug R code.","code":""},{"path":"chapter-into-R.html","id":"customizing-rstudio","chapter":"1 The Basics for R","heading":"Customizing RStudio","text":"RStudio highly customizable, allowing tailor workflow. adjust settings, go :Tools > Global Options – Access general settings.Appearance > Editor Theme – Change editor’s theme (e.g., “Tomorrow Night 80” dark mode).Font & Layout Settings – Modify font size, panel positions, interface options.\ncomfortable coding environment enhances productivity—feel free explore tweak settings suit preferences!","code":""},{"path":"chapter-into-R.html","id":"how-to-learn-r","chapter":"1 The Basics for R","heading":"1.3 How to Learn R","text":"Learning R exciting rewarding journey opens doors data science, statistics, machine learning. Fortunately, numerous resources—books, online courses, tutorials, forums—can help get started advance skills.","code":""},{"path":"chapter-into-R.html","id":"video-tutorials","chapter":"1 The Basics for R","heading":"1. Video Tutorials","text":"prefer learning watching, YouTube offers wealth R tutorials, ranging beginner advanced levels:R Programming – Covers R basics data science concepts.Data School – Focuses data analysis, machine learning, practical R applications.","code":""},{"path":"chapter-into-R.html","id":"books","chapter":"1 The Basics for R","heading":"2. Books","text":"Books great way build deep understanding R. top recommendations:Absolute Beginners: Hands-Programming R Garrett Grolemund1 – practical introduction new programming.Data Science R: R Data Science Hadley Wickham Garrett Grolemund2 – Covers data visualization, wrangling, modeling.Machine Learning: Machine Learning R Brett Lantz3 – comprehensive guide machine learning techniques using R.","code":""},{"path":"chapter-into-R.html","id":"online-courses","chapter":"1 The Basics for R","heading":"3. Online Courses","text":"prefer structured learning hands-exercises, online courses offer interactive experiences:DataCamp – Features beginner-friendly courses like Introduction R.Coursera – Offers courses R Programming Data Science Specialization.","code":""},{"path":"chapter-into-R.html","id":"r-communities-forums","chapter":"1 The Basics for R","heading":"4. R Communities & Forums","text":"Engaging online communities great way learn others, ask questions, get support:Stack Overflow – Find answers R-related coding questions.RStudio Community – Connect R users participate discussions.","code":""},{"path":"chapter-into-R.html","id":"practice-regularly","chapter":"1 The Basics for R","heading":"5. Practice Regularly","text":"best way learn R consistent practice. Start simple exercises, explore real-world datasets, experiment R code. combining structured learning hands-experience, ’ll quickly develop confidence proficiency R.🚀 Start today! Choose one resources begin R learning journey.","code":""},{"path":"chapter-into-R.html","id":"getting-help-and-learning-more","chapter":"1 The Basics for R","heading":"1.4 Getting Help and Learning More","text":"begin journey R, ’ll likely encounter challenges questions along way. Fortunately, many resources available help troubleshoot problems, deepen understanding, continue learning. Whether ’re stuck error message, exploring new function, looking best practices, combination built-documentation, online communities, external learning materials can guide .R comes extensive built-documentation provides details functions, packages, programming techniques. quickly look function, type ? followed function name R console. bring official documentation, including usage examples, argument details, additional references. can also use help() example() get context function works.Beyond R’s internal help system, R community invaluable resource. question, chances someone already asked (answered) . Platforms like Stack Overflow, RStudio Community, R-help mailing list contain thousands discussions common advanced topics R programming, data science, machine learning. Searching forums can often lead quick reliable solutions. don’t find existing answer, posting question clear explanation reproducible example increase chances getting helpful responses.simple Google search often fastest way troubleshoot issues. Searching error message function name usually direct blog posts, documentation, forum discussions relevant explanations. Additionally, AI tools like ChatGPT can assist R programming questions, debugging, conceptual explanations. AI-generated solutions aren’t always perfect, can provide useful insights, suggest alternative approaches, help clarify difficult concepts.Ultimately, best way master R hands-experience. Don’t afraid experiment—write code, test different functions, explore new datasets. Mistakes natural part learning, one helps reinforce understanding. practice, confident proficient ’ll become R. Keep coding, keep exploring, enjoy journey!","code":""},{"path":"chapter-into-R.html","id":"data-science-with-r","chapter":"1 The Basics for R","heading":"1.5 Data Science with R","text":"R provides strong foundation data science, real power comes extensive ecosystem packages—collections functions, datasets, documentation extend R’s capabilities. base version R includes many essential tools, come preloaded statistical machine learning algorithms may need. Instead, algorithms developed shared large community researchers practitioners free open-source R packages.package modular, reusable library enhances R’s functionality. Packages include well-documented functions, usage instructions, often sample datasets testing learning. book, frequently use liver package, developed specifically accompany book. contains datasets functions designed illustrate key data science concepts techniques. Additionally, machine learning algorithm covered book, introduce use appropriate R packages implement methods.interested exploring , Comprehensive R Archive Network (CRAN) hosts thousands packages statistical computing, data visualization, machine learning. full list available packages can browsed CRAN website, providing access tools tailored various domains data science beyond.","code":""},{"path":"chapter-into-R.html","id":"install-packages","chapter":"1 The Basics for R","heading":"1.6 How to Install R Packages","text":"two ways install R packages. first method RStudio’s graphical interface. Click “Tools” tab select “Install Packages…”. dialog box appears, enter name package(s) wish install “Packages” field click “Install” button. Make sure check “Install dependencies” option ensure necessary supporting packages installed well. See Figure 1.2 visual guide.\nFigure 1.2: visual guide installing R packages using ‘Tools’ tab RStudio.\nsecond method install packages directly using install.packages() function. example, install liver package, provides datasets functions used throughout book, enter following command R console:Press “Enter” execute command. R connect CRAN download package correct format operating system. encounter issues installation, ensure connected internet proxy firewall blocking access CRAN. first time install package, R may ask select CRAN mirror. Choose one geographically close faster downloads.install.packages() function also allows customization, installing package local file specific repository. learn , type following command R console:Packages need installed . installation, must loaded new R session using library() function. cover load packages next section.","code":"\ninstall.packages(\"liver\")\n?install.packages()"},{"path":"chapter-into-R.html","id":"how-to-load-r-packages","chapter":"1 The Basics for R","heading":"1.7 How to Load R Packages","text":"optimize memory usage, R automatically load installed packages. Instead, must explicitly load necessary packages new R session. ensures relevant functions datasets available, minimizing resource consumption.\nload package, use library() require() function. functions locate package system make functions, datasets, documentation accessible. example, load liver package, enter following command R console:Press Enter execute command. error message appears stating package found (e.g., \"package called 'liver'\"), indicates package installed. cases, refer previous section installing packages.Beyond liver, book utilizes several R packages, introduced progressively throughout chapters needed. However, R packages contain functions identical names. instance, liver* dplyr** packages include select() function. multiple packages loaded, R defaults using function recently loaded package.explicitly specify package function sourced , use :: operator. ensures clarity prevents conflicts. example, use select() function liver package, enter:approach particularly useful complex projects multiple packages required, preventing unintended overwrites functions name.","code":"\nlibrary(liver)\nliver::select()"},{"path":"chapter-into-R.html","id":"running-r-code","chapter":"1 The Basics for R","heading":"1.8 Running R Code","text":"R interactive language, allowing type commands directly console see results immediately. example, can perform basic arithmetic operations addition, subtraction, multiplication, division. add two numbers, type following R console:Press Enter execute command. R compute sum display result. can also store result variable later use:, <- assignment operator R, used assign values variables. users prefer = operator (result = 2 + 3), also works cases, <- remains recommended convention R programming.Variables R store values later use, allowing perform calculations efficiently. example, can multiply result 4:R retrieve stored value result compute multiplication.","code":"2 + 3\n   [1] 5\nresult <- 2 + 3result * 4\n   [1] 20"},{"path":"chapter-into-R.html","id":"using-comments-in-r","chapter":"1 The Basics for R","heading":"Using Comments in R","text":"Comments used explain code make easier understand. R, comment starts #, everything following line ignored interpreter.Comments affect execution code essential documentation, especially working complex projects collaborating others.","code":"\n# Store the sum of 2 and 3 in the variable `result`\nresult <- 2 + 3"},{"path":"chapter-into-R.html","id":"functions-in-r","chapter":"1 The Basics for R","heading":"1.8.1 Functions in R","text":"R provides rich set built-functions perform specific tasks. function takes input(s) (arguments), processes , returns output. example, c() function creates vectors:can apply functions vector. example, compute average numbers x, use mean() function:Functions R follow simple structure:functions require arguments, others optional. learn function, use ? followed function name:open R’s help documentation, providing details function’s purpose, usage, arguments, examples.Functions essential R programming, helping simplify complex operations making code reusable efficient. progress, also learn write functions automate tasks improve workflow.","code":"\nx <- c(1, 2, 3, 4, 5)  # Create a vectormean(x)  # Calculate the mean of x\n   [1] 3\nfunction_name(arguments)\n?mean  # or help(mean)"},{"path":"chapter-into-R.html","id":"how-to-import-data-into-r","chapter":"1 The Basics for R","heading":"1.9 How to Import Data into R","text":"performing analysis, first need load data R. R can read data multiple sources, including text files, Excel files, online datasets. Depending file format data source, can choose several methods importing data R.","code":""},{"path":"chapter-into-R.html","id":"using-rstudios-graphical-interface","chapter":"1 The Basics for R","heading":"Using RStudio’s Graphical Interface","text":"easiest way import data R RStudio’s graphical interface. Click Import Dataset button top-right panel RStudio (see Figure 1.3 visual guide). open dialog box can choose file type:\n- Text (base) – CSV tab-delimited files.\n- Excel – Microsoft Excel files.\n- formats available, depending installed packages.selecting file, RStudio display import settings window (see Figure 1.4). , can adjust column names, data types, options. first row contains column names, select Yes Heading option. Click Import, dataset appear RStudio’s Environment panel, ready analysis.\nFigure 1.3: visual guide loading dataset R using ‘Import Dataset’ tab RStudio.\n\nFigure 1.4: visual guide customizing import settings loading dataset R using ‘Import Dataset’ tab RStudio.\n","code":""},{"path":"chapter-into-R.html","id":"using-read.csv","chapter":"1 The Basics for R","heading":"Using read.csv()","text":"can also import data directly using read.csv() function, reads tabular data (CSV files) R data frame. data file stored locally, can load follows:Replace \"path///file.csv\" actual file path. file contain column names, use:","code":"\ndata <- read.csv(\"path/to/your/file.csv\")\ndata <- read.csv(\"path/to/your/file.csv\", header = FALSE)"},{"path":"chapter-into-R.html","id":"setting-the-working-directory","chapter":"1 The Basics for R","heading":"Setting the Working Directory","text":"default, R looks files current working directory. data located elsewhere, can specify full path read.csv() set working directory.check current working directory:set new working directory:Alternatively, RStudio, go Session > Set Working Directory > Choose Directory… select desired folder.","code":"\ngetwd()\nsetwd(\"~/Documents\")  # Adjust the path based on your system"},{"path":"chapter-into-R.html","id":"using-file.choose-with-read.csv","chapter":"1 The Basics for R","heading":"Using file.choose() with read.csv()","text":"interactively select file instead typing path manually, use file.choose():open file selection dialog, making convenient option working multiple datasets.","code":"\ndata <- read.csv(file.choose())"},{"path":"chapter-into-R.html","id":"loading-data-from-online-sources","chapter":"1 The Basics for R","heading":"Loading Data from Online Sources","text":"R also allows direct import datasets web sources. example, load publicly available COVID-19 dataset:approach useful accessing open datasets research institutions government agencies.","code":"\ncorona_data <- read.csv(\"https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\", na.strings = \"\", fileEncoding = \"UTF-8-BOM\")"},{"path":"chapter-into-R.html","id":"using-read_excel-for-excel-files","chapter":"1 The Basics for R","heading":"Using read_excel() for Excel Files","text":"import Excel files, use read_excel() function readxl package. First, install load package:, import Excel file:Unlike read.csv(), read_excel() supports multiple sheets within Excel file, can specified using sheet argument.\n### Loading Data R Packages {-}datasets available directly R packages require importing external file. example, liver package, developed book, contains multiple datasets. access churn dataset:Since many datasets used book included liver package (see Table 0.1), frequently use package examples demonstrations.section well-structured clearly explains fundamental data types R. concise informative, making accessible beginners maintaining professional tone suitable Springer publication. minor refinements improve clarity, consistency, readability.","code":"\ninstall.packages(\"readxl\")\n\nlibrary(readxl)\ndata <- read_excel(\"path/to/your/file.xlsx\")\nlibrary(liver)\ndata(churn)"},{"path":"chapter-into-R.html","id":"data-types-in-r","chapter":"1 The Basics for R","heading":"1.10 Data Types in R","text":"Data R can take various forms, correctly identifying types essential effective data manipulation, visualization, analysis. data type specific properties determine R processes , understanding helps avoid errors ensures accurate results.common data types R:Numeric: Represents real numbers, 3.14 -5.67. type used continuous numerical values, like heights, weights, temperatures.Integer: Represents whole numbers without decimals, 1, 42, -10. type useful count-based data, number customers items sold.Character: Represents text string data, \"Data Science\" \"R Programming\". type commonly used categorical labels, names, descriptive values.Logical: Represents Boolean values: TRUE FALSE. Logical data often used conditional statements filtering operations.Factor: Represents categorical data predefined levels. Factors commonly used storing variables \"Male\" \"Female\" dataset particularly useful statistical modeling.check data type variable, use class() function. example, determine type variable result, type:Press Enter, R display variable’s data type.Recognizing different data types essential choosing right analytical visualization techniques. explore later chapters (e.g., Chapters 4 5), numerical categorical variables require different approaches performing descriptive statistics, hypothesis testing, data visualization.","code":"class(result)\n   [1] \"numeric\""},{"path":"chapter-into-R.html","id":"data-structures-in-r","chapter":"1 The Basics for R","heading":"1.11 Data Structures in R","text":"Data structures fundamental working data R. define data stored manipulated, directly impacts efficiency accuracy data analysis. commonly used data structures R vectors, matrices, data frames, lists, illustrated Figure 1.4.\nFigure 1.5: visual guide different types data structures R.\n","code":""},{"path":"chapter-into-R.html","id":"vectors-in-r","chapter":"1 The Basics for R","heading":"Vectors in R","text":"vector simplest data structure R. one-dimensional array holds elements type (numeric, character, logical). Vectors building blocks data structures. can create vector using c() function:, x numeric vector containing five elements. .vector() function confirms x indeed vector, length(x) returns number elements vector.","code":"# Create a numeric vector\nx <- c(1, 2, 0, -3, 5)\n\n# Display the vector\nx\n   [1]  1  2  0 -3  5\n\n# Check if x is a vector\nis.vector(x)\n   [1] TRUE\n\n# Check the length of the vector\nlength(x)\n   [1] 5"},{"path":"chapter-into-R.html","id":"matrices-in-r","chapter":"1 The Basics for R","heading":"Matrices in R","text":"matrix two-dimensional array elements must type. Matrices useful mathematical operations structured numerical data. can create matrix using matrix() function:matrix m consists two rows three columns, filled row-wise. dim() function returns dimensions matrix. fill matrix column-wise, set byrow = FALSE.","code":"# Create a matrix with 2 rows and 3 columns\nm <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Display the matrix\nm\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n# Check if m is a matrix\nis.matrix(m)\n   [1] TRUE\n\n# Check the dimensions of the matrix\ndim(m)\n   [1] 2 3"},{"path":"chapter-into-R.html","id":"data-frames-in-r","chapter":"1 The Basics for R","heading":"Data Frames in R","text":"data frame two-dimensional table column can contain different data type (numeric, character, logical). makes data frames ideal storing tabular data, similar spreadsheets. can create data frame using data.frame() function:data frame students_df consists four columns: student_id, name, age, grade. class() function confirms object data frame, .data.frame() checks structure.inspect first rows data frame, use head() function. example, display first six rows churn dataset liver package:code loads liver package, retrieves churn dataset, provides overview structure. str() function particularly useful summarizing data frames, displays data types column values.","code":"# Create vectors for student data\nstudent_id <- c(101, 102, 103, 104)\nname       <- c(\"Emma\", \"Bob\", \"Alice\", \"Noah\")\nage        <- c(20, 21, 19, 22)\ngrade      <- c(\"A\", \"B\", \"A\", \"C\")\n\n# Create a data frame from the vectors\nstudents_df <- data.frame(student_id, name, age, grade)\n\n# Display the data frame\nstudents_df\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     A\n   4        104  Noah  22     Clibrary(liver)  # Load the liver package\ndata(churn)     # Load the churn dataset\n\n# Check the structure of the dataset\nstr(churn)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\n# Display the first six rows\nhead(churn)\n     state     area.code account.length voice.plan voice.messages intl.plan\n   1    KS area_code_415            128        yes             25        no\n   2    OH area_code_415            107        yes             26        no\n   3    NJ area_code_415            137         no              0        no\n   4    OH area_code_408             84         no              0       yes\n   5    OK area_code_415             75         no              0       yes\n   6    AL area_code_510            118         no              0       yes\n     intl.mins intl.calls intl.charge day.mins day.calls day.charge eve.mins\n   1      10.0          3        2.70    265.1       110      45.07    197.4\n   2      13.7          3        3.70    161.6       123      27.47    195.5\n   3      12.2          5        3.29    243.4       114      41.38    121.2\n   4       6.6          7        1.78    299.4        71      50.90     61.9\n   5      10.1          3        2.73    166.7       113      28.34    148.3\n   6       6.3          6        1.70    223.4        98      37.98    220.6\n     eve.calls eve.charge night.mins night.calls night.charge customer.calls churn\n   1        99      16.78      244.7          91        11.01              1    no\n   2       103      16.62      254.4         103        11.45              1    no\n   3       110      10.30      162.6         104         7.32              0    no\n   4        88       5.26      196.9          89         8.86              2    no\n   5       122      12.61      186.9         121         8.41              3    no\n   6       101      18.75      203.9         118         9.18              0    no"},{"path":"chapter-into-R.html","id":"lists-in-r","chapter":"1 The Basics for R","heading":"Lists in R","text":"list flexible data structure can contain elements different types, including vectors, matrices, data frames, even lists. Lists useful storing complex objects structured way. can create list using list() function:list my_list stores vector, matrix, data frame within single object. Lists allow efficient organization heterogeneous data. explore structure list, use str() function:Lists powerful tools R, especially handling nested hierarchical data. exploration, use ?list access documentation additional examples.","code":"# Create a list containing a vector, matrix, and data frame\nmy_list <- list(vector = x, matrix = m, data_frame = students_df)\n\n# Display the list\nmy_list\n   $vector\n   [1]  1  2  0 -3  5\n   \n   $matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n   \n   $data_frame\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     A\n   4        104  Noah  22     Cstr(my_list)\n   List of 3\n    $ vector    : num [1:5] 1 2 0 -3 5\n    $ matrix    : num [1:2, 1:3] 1 4 2 5 3 6\n    $ data_frame:'data.frame':  4 obs. of  4 variables:\n     ..$ student_id: num [1:4] 101 102 103 104\n     ..$ name      : chr [1:4] \"Emma\" \"Bob\" \"Alice\" \"Noah\"\n     ..$ age       : num [1:4] 20 21 19 22\n     ..$ grade     : chr [1:4] \"A\" \"B\" \"A\" \"C\""},{"path":"chapter-into-R.html","id":"accessing-records-or-variables-in-r","chapter":"1 The Basics for R","heading":"1.12 Accessing Records or Variables in R","text":"’ve imported data R, can easily access specific records variables using $ [] operators. tools essential extracting data data frames lists.$ operator allows extract specific column data frame specific element list. example, access name column students_df data frame, use:command retrieves displays name column students_df data frame.Similarly, can use $ operator access elements within list. example, access vector element my_list list:command retrieves displays vector element my_list list. $ operator straightforward powerful way access specific variables elements within data frames lists.Another method accessing specific records variables [] operator, allows subset data frames, matrices, lists based specific conditions. example, extract first three rows students_df data frame, can use:command display first three rows students_df data frame.can also use [] operator extract specific columns. instance, select name grade columns students_df data frame:command retrieves displays name grade columns students_df data frame.[] operator versatile, enabling subset data frames, matrices, lists precision. $ [] operators fundamental tools data manipulation R, allowing efficiently access manage data need.","code":"students_df$name\n   [1] \"Emma\"  \"Bob\"   \"Alice\" \"Noah\"my_list$vector\n   [1]  1  2  0 -3  5students_df[1:3, ]\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     Astudents_df[, c(\"name\", \"grade\")]\n      name grade\n   1  Emma     A\n   2   Bob     B\n   3 Alice     A\n   4  Noah     C"},{"path":"chapter-into-R.html","id":"visualizing-data-in-r","chapter":"1 The Basics for R","heading":"1.13 Visualizing Data in R","text":"Data visualization powerful tool exploring communicating insights data. plays crucial role exploratory data analysis (EDA), delve Chapter 4. saying goes, “picture worth thousand words,” data science, especially true. R provides broad array tools creating high-quality plots visualizations, allowing effectively present findings.R, two primary ways create plots: using base R graphics using ggplot2 package. Base R graphics offer simple direct way generate plots, ggplot2 provides greater flexibility customization. book primarily uses ggplot2, follows structured approach based grammar graphics, breaks plots three key components:Data: dataset visualized, data frame format using ggplot2.Aesthetics: visual properties data points, color, shape, size.Geometries: type plot created, scatter plots, bar plots, line plots.create plot using ggplot2, first install load package. Instructions installing packages provided Section 1.6. load ggplot2, use following command:Next, define data, aesthetics, geometries plot. example, create scatter plot miles per gallon (mpg) versus horsepower (hp) using built-mtcars dataset:code initializes plot ggplot() function, specifying dataset (mtcars). geom_point() function adds points plot, aes() function maps mpg x-axis hp y-axis.general template creating plots ggplot2 follows structure:Using template, variety visualizations can created.","code":"\nlibrary(ggplot2)\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp))ggplot(data = <DATA>) +\n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))"},{"path":"chapter-into-R.html","id":"geom-functions-in-ggplot2","chapter":"1 The Basics for R","heading":"Geom Functions in ggplot2","text":"Geom functions determine type plot created ggplot2. commonly used geom functions include:geom_point() scatter plotsgeom_bar() bar plotsgeom_line() line plotsgeom_boxplot() box plotsgeom_histogram() histogramsgeom_density() density plotsgeom_smooth() adding smoothed conditional means plotsFor example, create smoothed line plot mpg versus hp:Multiple geom functions can combined single plot. overlay scatter plot smoothed line:Alternatively, aes() function can placed inside ggplot() streamline code:Additional visualization examples can found Chapter 4. complete list geom functions, refer ggplot2 documentation.","code":"\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp))\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp)) + \n  geom_point(mapping = aes(x = mpg, y = hp))\nggplot(data = mtcars, mapping = aes(x = mpg, y = hp)) +\n  geom_smooth() + \n  geom_point()"},{"path":"chapter-into-R.html","id":"aesthetics-in-ggplot2","chapter":"1 The Basics for R","heading":"Aesthetics in ggplot2","text":"Aesthetics control visual properties data points, color, size, shape. properties specified within aes() function. example:, color = cyl maps color points number cylinders (cyl) mtcars dataset. ggplot2 automatically assigns unique color category adds corresponding legend.addition color, aesthetics size alpha (transparency) can used:Aesthetics can also set directly inside geom functions. example, make points blue triangles size 3:section introduced fundamentals data visualization R using ggplot2. next chapters explore visualization plays crucial role exploratory data analysis (Chapter 4) refine plots communication reporting. details visualization techniques, see ggplot2 documentation. interactive graphics, consider exploring plotly package Shiny web applications.","code":"\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, color = cyl))\n# Left plot: using the size aesthetic\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, size = cyl))\n\n# Right plot: using the alpha aesthetic\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, alpha = cyl))\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp), \n             color = \"blue\", size = 3, shape = 2)"},{"path":"chapter-into-R.html","id":"sec-formula-in-R","chapter":"1 The Basics for R","heading":"1.14 Formula in R","text":"Formulas R provide concise intuitive way specify relationships variables statistical modeling. widely used functions regression, classification, machine learning define response variable depends one predictors.R, formulas use tilde symbol ~ express relationships variables, response variable appears left-hand side predictor variables right-hand side. example, formula y ~ x specifies y modeled function x. multiple predictors, separated +.\ninstance, using diamonds dataset, formula:models price diamond based carat, cut, color.include variables dataset predictors, can use shorthand notation:approach particularly useful large datasets listing predictors manually impractical.formula R acts quoting operator, instructing R interpret variables symbolically rather evaluating immediately. variable left-hand side ~ dependent variable (response variable), variables right-hand side independent variables (predictor variables).Example 1.1  illustrate, suppose want predict price diamond using linear regression model. can pass formula lm() function:, formula price ~ carat + cut + color defines relationship, data argument specifies dataset use.defined, formulas can used various R functions statistical modeling machine learning. progress later chapters, encounter formulas functions regression, classification, (e.g., Chapters 7, 9, 10). Mastering formula syntax enable efficiently build, customize, interpret models throughout book.","code":"\nprice ~ carat + cut + color\nprice ~ .\nmodel <- lm(price ~ carat + cut + color, data = diamonds)"},{"path":"chapter-into-R.html","id":"reporting-with-r-markdown","chapter":"1 The Basics for R","heading":"1.15 Reporting with R Markdown","text":"Thus far, book covered interact R RStudio data analysis. section focuses equally important aspect: effectively communicating analytical findings. Data scientists must present results clearly teams, stakeholders, clients. Regardless depth analysis, impact limited communicated effectively. R Markdown facilitates process enabling seamless integration code, text, output dynamic, reproducible reports.R Markdown allows users write execute R code within document, producing reports, presentations, dashboards. Unlike traditional notebooks word processors, R Markdown ensures text, code, results remain synchronized data changes. book entirely written using R Markdown generated bookdown package, ensuring fully reproducible dynamic workflow.R Markdown documents can exported multiple formats, including HTML, PDF, Word, PowerPoint, making adaptable various audiences reporting needs. Furthermore, supports creation interactive documents using Shiny, allowing users build web applications facilitate exploratory data analysis.get started, following resources provide useful references:R Markdown Cheat Sheet: R Markdown Cheat Sheet offers concise reference creating documents, including syntax, formatting, output options. available RStudio Help > Cheatsheets > R Markdown Cheat Sheet.R Markdown Reference Guide: R Markdown Reference Guide provides detailed overview R Markdown’s features, including document structure customization.","code":""},{"path":"chapter-into-R.html","id":"r-markdown-basics","chapter":"1 The Basics for R","heading":"R Markdown Basics","text":"R Markdown follows literate programming approach, combining text executable code single document. Unlike word processors formatting visible writing, R Markdown requires compilation generate final report. approach ensures automation, plots figures generated dynamically inserted document. Since code embedded, analyses fully reproducible.create R Markdown document RStudio:\nFile > New File > R Markdown\ndialog box appear, allowing selection document type. standard report, choose “Document.” options include “Presentation” slides, “Shiny” interactive applications, “Template” predefined formats. selecting document type, enter title author name. output format can set HTML, PDF, Word; HTML often recommended debugging.R Markdown files use .Rmd extension, distinguishing .R script files. newly created file contains template can modified custom text, code, formatting.","code":""},{"path":"chapter-into-R.html","id":"the-header","chapter":"1 The Basics for R","heading":"The Header","text":"header defines metadata document’s title, author, date, output format. enclosed within three dashes (---).Title: document’s title.Author: name author.Date: date creation.Output format: format final document (html_document, pdf_document, word_document).Additional metadata can included customization, table contents options formatting preferences.","code":"---\ntitle: \"An Analysis of Customer Churn\"\nauthor: \"Reza Mohammadi\"\ndate: \"Aug 12, 2024\"\noutput: html_document\n---"},{"path":"chapter-into-R.html","id":"code-chunks-and-inline-code","chapter":"1 The Basics for R","heading":"Code Chunks and Inline Code","text":"R Markdown integrates R code within documents using code chunks, enclosed triple backticks (```{r}) followed code. example:compiled, R executes code displays output within document. Code chunks used analysis, visualizations, modeling. “Run” button RStudio allows individual execution chunks. See Figure 1.6 visual guide.\nFigure 1.6: Executing code chunk R Markdown using ‘Run’ button RStudio.\nCommon chunk options include:echo = FALSE: Displays output hides code.eval = FALSE: Shows code execute .message = FALSE: Suppresses messages.warning = FALSE: Suppresses warnings.error = FALSE: Hides error messages.include = FALSE: Omits code output.inline calculations, use backticks r keyword:renders dynamically :","code":"\n```r\n2 + 3\n   [1] 5\n```The factorial of 5 is 120.The factorial of 5 is 120."},{"path":"chapter-into-R.html","id":"styling-text","chapter":"1 The Basics for R","heading":"Styling Text","text":"R Markdown supports various text formatting options:Headings: Use # section titles.Bold: Enclose text double asterisks (**bold**).Italic: Use single asterisks (*italic*).Lists: Use * bullet points.Links: [R Markdown website](https://rmarkdown.rstudio.com)Images: ![Alt text](path//image.png)mathematical notation, use LaTeX-style equations:","code":"Inline: $y = \\beta_0 + \\beta_1 x$  \nBlock: $$ y = \\beta_0 + \\beta_1 x $$"},{"path":"chapter-into-R.html","id":"mastering-r-markdown","chapter":"1 The Basics for R","heading":"Mastering R Markdown","text":"learning:Books: R Markdown: Definitive Guide.Tutorials: R Markdown website.Courses: DataCamp R Markdown course.Forums: RStudio Community.leveraging R Markdown, data scientists can produce high-quality, reproducible reports enhance collaboration communication.","code":""},{"path":"chapter-into-R.html","id":"intro-R-exercises","chapter":"1 The Basics for R","heading":"1.16 Exercises","text":"section provides hands-exercises reinforce understanding fundamental concepts covered chapter.","code":""},{"path":"chapter-into-R.html","id":"basic-exercises","chapter":"1 The Basics for R","heading":"Basic Exercises","text":"Install R RStudio computer.Use getwd() function check current working directory. , change new directory using setwd().Create numeric vector named numbers containing values 5, 10, 15, 20, 25. , calculate mean standard deviation vector.Create matrix 3 rows 4 columns, filled numbers 1 12.Create data frame containing following variables:student_id (integer)name (character)score (numeric)passed (logical, TRUE means student passed FALSE means failed)\nPrint first rows data frame using head().Install load liver ggplot2 packages R. encounter errors, check internet connection ensure CRAN accessible.Load churn dataset liver package display first rows using head() function.Report data types variables churn dataset using str() function.Report dimensions churn dataset using dim() function.Report summary statistics variables churn dataset using summary() function.Create scatter plot using ggplot2 visualizes relationship day.mins eve.mins churn dataset. Hint: See code Section 4.6.Create histogram day.calls variable churn dataset.Create boxplot day.mins variable churn dataset.Create boxplot day.mins variable churn dataset, grouped churn variable. Hint: See code Section 4.5.Use mean() function compute mean customer.calls variable churn dataset. , calculate mean customer.calls churner churn == yes.Create R Markdown document includes title, author, small analysis churn dataset. Generate HTML report.","code":""},{"path":"chapter-into-R.html","id":"more-challenges-exercise","chapter":"1 The Basics for R","heading":"More Challenges Exercise","text":"following R code generates simulated dataset 200 observations. use simulated dataset simple toy example Chapter 7 explain k-nearest neighbors algorithm works. simulated data patients three variables:Age: Age patients numeric variable range 15 75 years old.Ratio: Sodium/Potassium ratio patient’s blood numeric variable. ratio generated based Type variable.Type: factor three levels: \"\", \"B\", \"C\" representing type drug patient taking.Run code report summary statistics data.Visualize data using following ggplot2 code:Extend dataset drug_data adding new variable named Outcome, factor two levels (\"Good\" \"Bad\").Patients Type == \"\" higher probability \"Good\" outcomes.Patients Type == \"B\" Type == \"C\" lower probability \"Good\" outcomes.Use sample() appropriate probabilities generate Outcome variable.Create new scatter plot using ggplot2 visualizes relationship Age Ratio, colored Outcome variable.Create new variable Age_group drug_data dataset categorizes patients three groups:“Young” (\\(\\leq 30\\) years old)“Middle-aged” (31-50 years old)“Senior” (>50 years old).Calculate mean Ratio Age_group category drug_data dataset.Create bar chart using ggplot2 displays average Ratio Age_group.Modify drug_data dataset adding Risk_factor variable, calculated Ratio * Age / 10. Analyze Risk_factor differs Type.Create histogram Risk_factor variable, grouped Type.Generate boxplot visualize distribution Risk_factor across different Outcome categories.","code":"\n# Simulate data for kNN\nset.seed(10)\n\nn  = 200         # Number of patients\nn1 = 90          # Number of patients with drug A\nn2 = 60          # Number of patients with drug B \nn3 = n - n1 - n2 # Number of patients with drug C\n\n# Generate Age variable between 15 and 75\nAge = sample(x = 15:75, size = n, replace = TRUE)\n\n# Generate Drug Type variable with three levels\nType = sample(x = c(\"A\", \"B\", \"C\"), size = n, replace = TRUE, prob = c(n1, n2, n3))\n\n# Generate Sodium/Potassium Ratio based on Drug Type\nRatio = numeric(n)\n\nRatio[Type == \"A\"] = sample(x = 10:40, size = sum(Type == \"A\"), replace = TRUE)\nRatio[Type == \"B\"] = sample(x =  5:15, size = sum(Type == \"B\"), replace = TRUE)\nRatio[Type == \"C\"] = sample(x =  5:15, size = sum(Type == \"C\"), replace = TRUE)\n\n# Create a data frame with the generated variables\ndrug_data = data.frame(Age = Age, Ratio = Ratio, Type = Type)\nggplot(data = drug_data, aes(x = Age, y = Ratio)) +\n  geom_point(aes(color = Type, shape = Type)) + \n  labs(title = \"Age vs. Sodium/Potassium Ratio\", \n       x = \"Age\", y = \"Sodium/Potassium Ratio\")"},{"path":"chapter-intro-DS.html","id":"chapter-intro-DS","chapter":"2 Introduction to Data Science","heading":"2 Introduction to Data Science","text":"Data Science rapidly evolving field transforming industries leveraging computational, statistical, analytical techniques. 21st century, data become one valuable resources, often called “new oil” due potential drive innovation reshape future.Data science key unlocking potential. applying computational, statistical, analytical techniques, data scientists extract insights vast amounts data, enabling organizations make informed decisions, optimize processes, predict trends, develop intelligent systems. led groundbreaking advancements fields healthcare, finance, marketing, artificial intelligence (AI), beyond.Given rapid growth increasing demand, data science critical ever. chapter, ’ll explore fundamentals data science, discuss significance modern society, introduce Data Science Workflow—structured approach data scientists use transform raw data actionable insights.section well-structured provides clear introduction data science. effectively conveys interdisciplinary nature field highlights core components. However, areas clarity, consistency, flow can improved. suggestions:","code":""},{"path":"chapter-intro-DS.html","id":"what-is-data-science","chapter":"2 Introduction to Data Science","heading":"2.1 What is Data Science?","text":"Data science interdisciplinary field integrates computer science, statistics, domain expertise extract insights data. involves using analytical computational techniques process vast amounts raw data, transforming meaningful information supports decision-making strategic planning.\nFigure 2.1: Data science multidisciplinary field applies computational statistical methods extract insights data.\nAlthough term “data science” relatively new, foundations lie well-established disciplines statistics, data analysis, machine learning. exponential growth digital data, advancements computational power, increasing demand data-driven decision-making, data science emerged distinct essential field.core, data science concerned extracting knowledge data using combination statistical techniques, machine learning algorithms, domain-specific methodologies. helps organizations manage understand vast amounts information generated digital age.","code":""},{"path":"chapter-intro-DS.html","id":"key-components-of-data-science","chapter":"2 Introduction to Data Science","heading":"Key Components of Data Science","text":"field data science encompasses three main components:Data Engineering: foundation data science, responsible collecting, storing, structuring large datasets. includes development data pipelines infrastructure enable efficient analysis. crucial, data engineering beyond scope book.Data Analysis Statistics: application statistical methods explore analyze data. includes data visualization, hypothesis testing, predictive modeling. details topic covered Statistical Inference Hypothesis Testing Exploratory Data Analysis chapters.Machine Learning Artificial Intelligence: use algorithms identify patterns, make predictions, extract deeper insights. includes supervised unsupervised learning, deep learning, natural language processing. concepts discussed Modeling Process chapter.","code":""},{"path":"chapter-intro-DS.html","id":"why-data-science-matters","chapter":"2 Introduction to Data Science","heading":"2.2 Why Data Science Matters","text":"digital age, data become one valuable resources often referred “new oil” 21st century. comparison makes sense, world’s valuable companies today—including OpenAI, Google, Apple—driven artificial intelligence data science. Just wealthiest companies 20th century controlled oil energy, today’s leading enterprises leverage data key asset innovation competitive advantage.Across industries, data-driven decision-making become essential. Organizations generate vast amounts data every day, without right tools techniques, much data remain untapped. Data science helps organizations uncover patterns, detect trends, make informed decisions enhance efficiency, reduce costs, improve customer experiences.\nData science plays crucial role wide range sectors, including:Finance: Financial institutions leverage data science risk assessment, fraud detection, algorithmic trading. Machine learning models identify anomalies transaction patterns, improving fraud detection regulatory compliance.Marketing: Businesses use data science analyze customer behavior, segment audiences, create targeted marketing campaigns. Platforms Facebook Google Ads leverage sophisticated algorithms match advertisements relevant audiences, improving engagement conversion rates.Retail E-commerce: Companies like Amazon Walmart use data science optimize inventory management, predict demand, personalize recommendations. analyzing purchase history browsing behavior, retailers can offer tailored promotions enhance customer satisfaction.Healthcare: Hospitals medical researchers use data science disease diagnosis, patient risk prediction, personalized treatment plans. analyzing large datasets medical records, institutions can identify high-risk patients take preventative measures improve health outcomes.example, Netflix applies data science analyze viewing patterns recommend personalized content users, supply chain optimization Amazon ensures faster deliveries leveraging predictive analytics.","code":""},{"path":"chapter-intro-DS.html","id":"the-data-science-workflow","chapter":"2 Introduction to Data Science","heading":"2.3 The Data Science Workflow","text":"data science workflow follows iterative cyclical approach, insights gained stage inform refine subsequent steps. Unlike strictly linear process, data science involves continuous refinement enhance accuracy efficiency. structured approach ensures data-driven projects conducted systematically, balancing exploratory analysis, model building, evaluation derive meaningful conclusions.data science workflow follows phased, adaptive approach within scientific framework, transforming raw data actionable knowledge. transformation often conceptualized using DIKW Pyramid (Data → Information → Knowledge → Wisdom), illustrated Figure 2.2.\nFigure 2.2: DIKW Pyramid illustrates transformation raw data actionable insights, progressing data information, knowledge, ultimately wisdom.\nspecifics may vary across projects, data science workflows follow common structure. book, adopt Data Science Workflow guiding framework structuring data science projects. workflow inspired Cross-Industry Standard Process Data Mining (CRISP-DM) model, widely recognized methodology data-driven projects. cyclic framework guides data scientists following key stages (see Figure 2.3):Problem Understanding – Defining business research question outlining objectives.Data Preparation – Collecting, cleaning, transforming, organizing data ensure suitable analysis. step includes handling missing values, addressing inconsistencies, detecting outliers, preparing features scaling, encoding, transformation.Exploratory Data Analysis (EDA) – Identifying patterns, distributions, relationships within data.Preparing Data Modeling – Engineering relevant features, normalizing data, selecting meaningful variables.Modeling – Applying machine learning statistical techniques develop predictive descriptive models.Evaluation – Assessing model performance using appropriate metrics validation techniques.Deployment – Integrating model production environment monitoring performance time.\nFigure 2.3: Data Science Workflow iterative framework structuring data science machine learning projects. Inspired CRISP-DM model, ensures systematic problem-solving continuous refinement.\ndata science inherently iterative, steps often revisited multiple times within single project. feedback loops stages allow continuous refinement—adjusting data preprocessing, modifying features, retraining models new insights emerge. following structured workflow, data scientists can ensure rigor, accuracy, efficiency transforming data valuable insights.","code":""},{"path":"chapter-intro-DS.html","id":"problem-understanding","chapter":"2 Introduction to Data Science","heading":"2.4 Problem Understanding","text":"first step data science project clearly define problem—whether business challenge research question. phase crucial data science just building models; solving real-world problems using data-driven approaches. well-defined problem ensures efforts aligned meaningful objectives, improving likelihood delivering actionable insights.stage, data scientists work closely stakeholders understand goals, clarify expectations, define success criteria. following questions help frame problem:research business question important?desired outcome impact?can data science techniques contribute addressing question?Focusing diving essential. Simon Sinek emphasizes TED talk “Great Leaders Inspire Action”, “People don’t buy ; buy .” concept applies data science well—understanding deeper motivation behind project provides clarity direction.example, data science team business analytics department may approached client wants predictive model lacks clarity specific problem trying solve. Without clear , becomes difficult develop solution delivers real value. Similarly, students working research projects often focus want build rather needed.Suppose company aims reduce customer churn. well-defined objective might develop predictive model identifies customers risk leaving targeted retention strategies can implemented. initial understanding helps frame problem guides selection relevant data, modeling techniques, evaluation metrics.Problem understanding analytical creative process. data science provides tools methodologies, defining right problem requires domain expertise critical thinking. following steps help ensure structured approach:Clearly articulate project objectives requirements terms overall goals business research entity.Break objectives outline specific expectations desired outcomes.Translate objectives data science problem can addressed using analytical techniques.Draft preliminary strategy achieve objectives, considering potential approaches methodologies.thoroughly defining problem, data scientists set stage effective workflow, ensuring subsequent analysis modeling efforts remain aligned meaningful outcomes.","code":""},{"path":"chapter-intro-DS.html","id":"data-preparation","chapter":"2 Introduction to Data Science","heading":"2.5 Data Preparation","text":"problem well-defined, next step data preparation, ensuring data accurate, complete, well-structured. Raw data often contains missing values, inconsistencies, outliers, making phase critical reliable analysis. Poorly prepared data can lead misleading insights, even sophisticated models.Data can originate various sources, including databases, spreadsheets, APIs, web scraping. may structured (e.g., numerical data databases) unstructured (e.g., text, images). Preprocessing essential analysis.Key steps data preparation include:Data Collection Integration: Merging data multiple sources ensuring consistency.Handling Missing Values: Removing, imputing, flagging incomplete data.Outlier Detection: Identifying managing extreme values using visualization.Resolving Inconsistencies: Standardizing formats, correcting errors, aligning categorical values.Feature Engineering: Transforming data encoding, scaling, normalization model compatibility.Data Summarization: Checking variable types, computing summary statistics, detecting duplicates.Though time-consuming, data preparation essential accurate modeling meaningful analysis. Chapter 3, explore techniques real-world examples.","code":""},{"path":"chapter-intro-DS.html","id":"exploratory-data-analysis-eda","chapter":"2 Introduction to Data Science","heading":"2.6 Exploratory Data Analysis (EDA)","text":"Exploratory Data Analysis (EDA) fundamental step data science workflow, providing initial understanding dataset formal modeling. primary objective EDA uncover patterns, relationships, anomalies data, helping data scientists refine hypotheses validate assumptions. systematically examining data, EDA ensures subsequent modeling process informed solid understanding dataset’s structure characteristics.Several key techniques commonly used EDA:Summary statistics – Measures mean, median, standard deviation, interquartile range provide insights distribution central tendencies numerical variables.Data visualization – Graphical techniques, including histograms, scatter plots, box plots, reveal data distributions, trends, potential outliers.Correlation analysis – Examining relationships numerical features using correlation coefficients helps identify dependencies may influence modeling decisions.EDA serves diagnostic exploratory functions. helps detect data quality issues, missing values inconsistencies, also guiding feature selection engineering. instance, strong correlation exists certain features target variable, features may prioritized modeling phase.thorough EDA process improves quality dataset also enhances interpretability reliability analytical results. Chapter 4, explore EDA techniques greater detail, applying real-world datasets illustrate practical applications.","code":""},{"path":"chapter-intro-DS.html","id":"preparing-data-for-modeling","chapter":"2 Introduction to Data Science","heading":"2.7 Preparing Data for Modeling","text":"insights EDA, next step prepare data modeling. stage involves feature engineering, feature selection, data splitting—crucial building effective models.Feature Engineering: Creating new features transforming existing ones enhance model performance. example, deriving new variables combining existing ones applying transformations can provide additional predictive power.Feature Selection: Identifying selecting relevant features improve model efficiency prevent overfitting. Removing irrelevant redundant features simplifies model enhances interpretability.Data Splitting: Dividing dataset training, validation, testing sets. training set used develop model, validation set helps fine-tune parameters, test set assesses final model performance.end stage, data structured well-prepared format, ensuring models can learn effectively. Chapter 6, explore techniques detail apply real-world datasets.","code":""},{"path":"chapter-intro-DS.html","id":"modeling","chapter":"2 Introduction to Data Science","heading":"2.8 Modeling","text":"Modeling stage data scientists apply machine learning statistical techniques prepared data create predictive descriptive model. goal build model effectively captures relationships within data generalizes well new, unseen data.modeling process typically involves:Choosing Model: Selecting appropriate model based problem type (e.g., regression, classification, clustering) characteristics dataset.Training Model: Fitting model training data learn patterns relationships.Tuning Hyperparameters: Adjusting model parameters optimize performance validation set.Common algorithms include linear regression (Chapter 10), decision trees (Chapter 11), Naïve Bayes classifier (Chapter 9), k-Nearest Neighbors (k-NN) algorithm (Chapter 7), neural networks (Chapter 12). method strengths limitations, selecting suitable model depends nature problem, data quality, computational constraints. Often, multiple models tested compared determine best-performing approach.","code":""},{"path":"chapter-intro-DS.html","id":"evaluation","chapter":"2 Introduction to Data Science","heading":"2.9 Evaluation","text":"model built, must rigorously evaluated ensure accuracy, generalizability, robustness deployment. evaluation process relies well-defined performance metrics, vary depending type problem. classification models, commonly used metrics include accuracy, precision, recall, F1-score, area receiver operating characteristic curve (ROC-AUC). regression tasks, measures mean squared error (MSE), mean absolute error (MAE), coefficient determination (\\(R^2\\)) assess model effectiveness.ensure model overfitting training data, cross-validation techniques, k-fold cross-validation, employed. methods provide reliable estimate model’s performance partitioning data multiple subsets training validation. Beyond numerical evaluation, error analysis plays crucial role diagnosing weaknesses, particularly confusion matrix interpretation classification problems residual analysis regression. careful examination errors often reveals underlying biases, data inconsistencies, model limitations require refinement.model fails meet expectations, adjustments may necessary, feature selection, hyperparameter tuning, exploring alternative modeling approaches. Chapter 8, explore techniques detail apply real-world datasets.","code":""},{"path":"chapter-intro-DS.html","id":"deployment","chapter":"2 Introduction to Data Science","heading":"2.10 Deployment","text":"model evaluated meets project goals, final step deployment, integrated production environment generate real-time insights predictions. phase crucial ensuring model contributes tangible value, whether supporting decision-making processes automating tasks within operational systems. Models can deployed various ways, embedding web applications, integrating enterprise software, automating processes large-scale data pipelines.Beyond initial integration, continuous monitoring essential track model’s performance detect potential issues. real-world data evolves, models may experience concept drift, predictive accuracy deteriorates due changes underlying patterns. mitigate , periodic model updates retraining necessary maintain reliability. Additionally, implementing robust logging performance tracking mechanisms helps ensure discrepancies predicted actual outcomes quickly identified addressed.Deployment one-time event ongoing process. Effective deployment strategies account scalability, interpretability, maintainability, allowing models remain useful dynamic environments. field data science advances, ability manage deployed models effectively continue critical factor transforming analytical insights real-world impact.","code":""},{"path":"chapter-intro-DS.html","id":"machine-learning","chapter":"2 Introduction to Data Science","heading":"2.11 Machine Learning","text":"Data science relies machine learning techniques extract insights data, make predictions, uncover patterns. methods enable data scientists move beyond descriptive analysis explore predictive prescriptive approaches, essential real-world applications. section, provide overview machine learning, including main types—supervised learning unsupervised learning—discuss machine learning differs statistical learning.Machine learning branch artificial intelligence focuses developing algorithms learn data make predictions. Rather explicitly programmed task, machine learning models identify patterns within data use make informed decisions. approach particularly useful complex problems rule-based programming impractical.instance, rather defining fixed set rules detect spam emails, machine learning model can trained labeled dataset emails classified “spam” “spam.” model learns distinguishing patterns can classify new emails high accuracy. ability generalize data makes machine learning invaluable fields finance, healthcare, marketing.","code":""},{"path":"chapter-intro-DS.html","id":"machine-learning-tasks-supervised-vs.-unsupervised-learning","chapter":"2 Introduction to Data Science","heading":"Machine Learning Tasks: Supervised vs. Unsupervised Learning","text":"Machine learning tasks can broadly categorized supervised learning unsupervised learning, differ terms models learn data objectives analysis.Supervised learning involves training model labeled dataset, data point associated known output. goal model learn relationship input features corresponding output, enabling make accurate predictions new data. Common supervised learning tasks include classification numeric prediction. classification, model assigns data points predefined categories, detecting whether email spam identifying whether patient particular disease. book covers classification techniques decision trees (Chapter 11), Naïve Bayes classifier (Chapter 9), k-Nearest Neighbors (k-NN) algorithm (Chapter 7). Numeric prediction, also known regression, focuses estimating continuous values, forecasting house prices based location size. detailed discussion regression techniques provided Chapter 10.Unsupervised learning, hand, applied datasets lack labeled outputs. objective uncover hidden patterns, relationships, structures within data. Clustering, common unsupervised learning technique, groups data points based similarity, segmenting customers according purchasing behavior. Another important unsupervised learning method pattern discovery, also known association rule learning, identifies relationships variables. technique widely used market basket analysis detect frequently co-purchased items. concepts explored detail Chapter 13.summary, supervised learning used labeled data available specific predictive outcome required, unsupervised learning beneficial exploratory data analysis, goal identify underlying structures unlabeled data. distinction two approaches fundamental selecting appropriate machine learning techniques given data science problem.","code":""},{"path":"chapter-intro-DS.html","id":"exercises","chapter":"2 Introduction to Data Science","heading":"2.12 Exercises","text":"following exercises help reinforce key concepts covered chapter. questions range fundamental definitions applied problem-solving related data science, data science workflow, machine learning.data-driven decision-making impact businesses? Give example real-world application.Data Science Workflow inspired CRISP-DM model. CRISP-DM stand , guide data-driven projects? key stages CRISP-DM model?Data Science Workflow CRISP-DM model standard processes data science projects. methodologies used industry?think can skip Problem Understanding phase directly jump Data Preparation data science project? Justify answer.Data Preparation considered one time-consuming steps data science project? common challenges faced phase?extent can Data Science projects automated without human intervention? risks limitations relying solely automated tools?following scenarios, identify appropriate stage data science workflow:\ncompany wants predict customer churn based historical data.\nresearcher exploring relationship air pollution respiratory diseases.\ne-commerce platform analyzing user behavior personalize product recommendations.\nhospital developing predictive model patient readmission rates.\ncompany wants predict customer churn based historical data.researcher exploring relationship air pollution respiratory diseases.e-commerce platform analyzing user behavior personalize product recommendations.hospital developing predictive model patient readmission rates.task, classify supervised unsupervised learning, explain reasoning, identify suitable machine learning algorithm applied.\nIdentifying fraudulent transactions credit card dataset.\nSegmenting customers based purchasing behavior.\nPredicting stock prices based historical data.\nGrouping news articles topics using natural language processing.\nIdentifying fraudulent transactions credit card dataset.Segmenting customers based purchasing behavior.Predicting stock prices based historical data.Grouping news articles topics using natural language processing.Define training dataset test dataset. important? improper splitting datasets affect model performance? Provide example real-world issue caused poor dataset partitioning.Many AI-driven systems criticized biased predictions, hiring algorithms favor certain demographics facial recognition models misidentify certain racial groups.\ncommon sources bias data science projects?\ncan data scientists ensure fairness mitigate biases models?\nGive example real-world case bias AI led negative consequences.\ncommon sources bias data science projects?can data scientists ensure fairness mitigate biases models?Give example real-world case bias AI led negative consequences.Accuracy common metric used evaluate models, always best indicator success. Consider binary classification problem 2% cases positive (e.g., detecting rare diseases fraud).\nmight accuracy misleading case?\nalternative evaluation metrics used?\ndecide whether model truly valuable decision-making?\nmight accuracy misleading case?alternative evaluation metrics used?decide whether model truly valuable decision-making?","code":""},{"path":"chapter-data-prep.html","id":"chapter-data-prep","chapter":"3 Data Preparation","heading":"3 Data Preparation","text":"Data preparation foundational step data science project, ensuring raw data transformed clean structured format suitable analysis. process often time-consuming yet crucial stage, quality data directly influences accuracy insights effectiveness predictive models.chapter explores key data preparation techniques, including handling missing values, detecting outliers, transforming data, feature engineering. end chapter, clear understanding preprocess raw data, enabling robust statistical modeling machine learning applications.illustrate concepts, use diamonds dataset ggplot2 package. dataset contains detailed attributes diamonds, carat, cut, color, clarity, price, making excellent case study data preprocessing. chapter, focus first two steps Data Science Workflow—data cleaning transformation—laying groundwork analysis subsequent chapters.","code":""},{"path":"chapter-data-prep.html","id":"problem-understanding","chapter":"3 Data Preparation","heading":"3.1 Problem Understanding","text":"preparing data analysis, essential define problem establish clear objectives. case, aim analyze diamonds dataset gain insights diamond pricing, critical factor industries jewelry retail, gemology, e-commerce. dataset includes attributes influence diamond value, allowing us explore key factors affecting pricing.","code":""},{"path":"chapter-data-prep.html","id":"objectives-and-key-questions","chapter":"3 Data Preparation","heading":"Objectives and Key Questions","text":"primary objectives diamonds dataset :Examine relationships diamond attributes (e.g., carat, cut, color, clarity) price.Identify patterns improve price estimation.Assess data quality, ensuring consistency detecting missing values outliers may affect analysis.achieve objectives, address key questions :attributes significant influence price?pricing trends based characteristics carat weight cut quality?inconsistencies, errors, missing values need corrected?","code":""},{"path":"chapter-data-prep.html","id":"framing-the-problem-as-a-data-science-task","chapter":"3 Data Preparation","heading":"Framing the Problem as a Data Science Task","text":"business perspective, understanding diamond pricing can provide valuable insights jewelers, e-commerce platforms, gemologists. data science perspective, problem can approached two ways:Predictive modeling: Developing model estimates diamond price based attributes.Exploratory data analysis (EDA): Identifying trends relationships without building predictive model.Clearly defining objectives ensures data preparation efforts align intended analytical approach, whether exploratory insights building robust predictive models generalize well unseen data. structured problem framing guide decisions data cleaning, transformation, feature engineering, ensuring analysis remains focused actionable.","code":""},{"path":"chapter-data-prep.html","id":"Data-pre-diamonds","chapter":"3 Data Preparation","heading":"3.2 diamonds Dataset Overview","text":"diamonds dataset, included ggplot2 package, provides structured information various characteristics diamonds. row represents unique diamond, 54,940 entries total, contains 10 descriptive variables, including price, carat, cut, clarity, color. goal analysis gain deeper insights factors influence diamond pricing, understand distribution data across attributes, explore quantitative qualitative relationships variables.use diamonds dataset R, first ensure ggplot2 package installed. , install using:, load package dataset:inspect dataset structure, use:function reveals dataset 53940 observations 10 variables. summary key attributes:price: price US dollars ($326–$18,823).carat: weight diamond (0.2–5.01).cut: quality cut (Fair, Good, Good, Premium, Ideal).color: diamond color, D (best) J (worst).clarity: measurement clear diamond (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, (best)).x: length mm (0–10.74).y: width mm (0–58.9).z: depth mm (0–31.8).depth: total depth percentage = 2 * z / (x + y).table: width top diamond relative widest point.","code":"\ninstall.packages(\"ggplot2\") \nlibrary(ggplot2)  # Load ggplot2 package\ndata(diamonds)    # Load diamonds datasetstr(diamonds)   \n   tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n    $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n    $ cut    : Ord.factor w/ 5 levels \"Fair\"<\"Good\"<..: 5 4 2 4 2 3 3 3 1 3 ...\n    $ color  : Ord.factor w/ 7 levels \"D\"<\"E\"<\"F\"<\"G\"<..: 2 2 2 6 7 7 6 5 2 5 ...\n    $ clarity: Ord.factor w/ 8 levels \"I1\"<\"SI2\"<\"SI1\"<..: 2 3 5 4 2 6 7 3 4 5 ...\n    $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n    $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n    $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n    $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n    $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n    $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ..."},{"path":"chapter-data-prep.html","id":"types-of-features-in-the-diamonds-dataset","chapter":"3 Data Preparation","heading":"Types of Features in the diamonds Dataset","text":"Understanding types features dataset essential determining appropriate data preparation steps:Quantitative (Numerical) Variables: represented numbers can continuous discrete.\nContinuous Variables: variables can take value within range. dataset, carat, price, x, y, z, depth continuous.\nDiscrete Variables: variables take countable values, often integers. example, count customers number purchases discrete, though dataset doesn’t include variable.\nContinuous Variables: variables can take value within range. dataset, carat, price, x, y, z, depth continuous.Discrete Variables: variables take countable values, often integers. example, count customers number purchases discrete, though dataset doesn’t include variable.Categorical (Qualitative) Variables: describe data fits categories rather numerical value. divided three types:\nOrdinal Variables: Categorical variables meaningful order, intervals categories equal. instance, cut, color, clarity ordinal variables dataset. ordering levels variables (e.g., “Fair” “Ideal” cut) meaning.\nNominal Variables: Categorical variables without intrinsic ordering among categories. datasets, examples might include “gender” “product type,” diamonds dataset contain nominal variables.\nBinary Variables: Variables two levels, often coded 0 1. diamonds dataset doesn’t contain binary variables, example feature like “has_certificate” values “yes” “.”\nOrdinal Variables: Categorical variables meaningful order, intervals categories equal. instance, cut, color, clarity ordinal variables dataset. ordering levels variables (e.g., “Fair” “Ideal” cut) meaning.Nominal Variables: Categorical variables without intrinsic ordering among categories. datasets, examples might include “gender” “product type,” diamonds dataset contain nominal variables.Binary Variables: Variables two levels, often coded 0 1. diamonds dataset doesn’t contain binary variables, example feature like “has_certificate” values “yes” “.”Knowing type feature guides decisions data preparation. instance:\n- Numerical variables can normalized standardized using techniques like Min-Max Scaling Z-score Scaling.\n- Ordinal variables may encoded using ordinal encoding one-hot encoding, depending whether model recognize order.\n- Categorical variables without meaningful order typically one-hot encoded.understanding types variables diamonds dataset, can select appropriate transformations encoding methods prepare data effectively analysis modeling.","code":""},{"path":"chapter-data-prep.html","id":"key-considerations-for-data-preparation","chapter":"3 Data Preparation","heading":"Key Considerations for Data Preparation","text":"objectives mind, main priorities preparing dataset:Data Quality: Ensure data accurate, consistent, free major issues. involves checking missing values, outliers, inconsistencies bias analysis.Feature Engineering: Explore possibility creating new features improve predictive accuracy. instance, calculating volume (using product x, y, z dimensions) provide additional measure diamond’s size.Data Transformation: Ensure features appropriate formats. Categorical variables like cut color may need converted numeric codes dummy variables work machine learning algorithms effectively.","code":""},{"path":"chapter-data-prep.html","id":"Data-pre-outliers","chapter":"3 Data Preparation","heading":"3.3 Outliers","text":"Outliers data points significantly deviate general distribution dataset. can arise due measurement variability, data entry errors, genuinely unique observations. Identifying handling outliers crucial, can skew statistical analyses, affect model performance, lead misleading insights.Outliers play critical role multiple industries:Finance: Outliers transaction data can indicate fraud. Detecting unusually high spending patterns key fraud detection models.Healthcare: Medical records often contain anomalous lab results, may indicate rare diseases measurement errors.Manufacturing: Sensors factories may detect equipment failures unusual temperature spikes.many cases, outliers errors signals important events. Understanding role data analysis ensures don’t remove valuable insights unintentionally.","code":""},{"path":"chapter-data-prep.html","id":"identifying-outliers-using-visualization-techniques","chapter":"3 Data Preparation","heading":"Identifying Outliers Using Visualization Techniques","text":"","code":""},{"path":"chapter-data-prep.html","id":"boxplots-detecting-extreme-values","chapter":"3 Data Preparation","heading":"Boxplots: Detecting Extreme Values","text":"Boxplots visual tool detecting extreme values. boxplot y variable (diamond width) using ggplot() geom_boxplot() functions ggplot2 package:, boxplots highlight values beyond whiskers, may indicate potential outliers. Since diamonds width 0 mm, values like 32 mm 59 mm likely result data entry errors.","code":"\nggplot(data = diamonds) +\n    geom_boxplot(mapping = aes(y = y))"},{"path":"chapter-data-prep.html","id":"histograms-understanding-outlier-distribution","chapter":"3 Data Preparation","heading":"Histograms: Understanding Outlier Distribution","text":"Histograms provide another visual approach detecting outliers displaying frequency distribution values. histogram y variable using ggplot() geom_histogram() functions:enhance visibility, can zoom smaller frequencies using coord_cartesian() function ggplot2 package:useful visualization techniques include:Violin plots – Show outliers density distributions.Density plots – Provide smoother insights rare values multimodal distributions.","code":"\nggplot(data = diamonds) +\n    geom_histogram(aes(x = y), binwidth = 0.5, color = 'blue', fill = \"lightblue\")\nggplot(data = diamonds) +\n    geom_histogram(mapping = aes(x = y), binwidth = 0.5, color = 'blue', fill = \"lightblue\") + \n    coord_cartesian(ylim = c(0, 30))"},{"path":"chapter-data-prep.html","id":"handling-outliers-best-practices","chapter":"3 Data Preparation","heading":"Handling Outliers: Best Practices","text":"outliers identified, several strategies handling :Removing outliers: appropriate outlier clearly error (e.g., negative height, duplicate data entry).Transforming values: Techniques log transformation square root scaling can reduce influence extreme values preserving trends.Winsorization: Instead removing outliers, replace nearest percentile-based value (e.g., capping extreme values 95th percentile).Using robust statistical methods: algorithms, like median-based regression random forests, less sensitive outliers.Treating outliers separate category: fraud detection rare event prediction, outliers may contain valuable insights removed.Choosing right strategy depends context analysis potential impact outlier.","code":""},{"path":"chapter-data-prep.html","id":"expanded-code-example-handling-outliers-in-r","chapter":"3 Data Preparation","heading":"Expanded Code Example: Handling Outliers in R","text":"detecting outliers, can choose either replace NA values remove . , consider using mutate() function dplyr package. ’s example treating outliers missing values using mutate() ifelse():’s verify update:method ensures outliers distort dataset allowing imputation analysis.","code":"\ndiamonds_2 <- mutate(diamonds, y = ifelse(y == 0 | y > 30, NA, y))summary(diamonds_2$y)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     3.680   4.720   5.710   5.734   6.540  10.540       9"},{"path":"chapter-data-prep.html","id":"missing-values","chapter":"3 Data Preparation","heading":"3.4 Missing Values","text":"Missing values pose significant challenges data analysis, can lead biased results, reduce statistical power, impact performance machine learning models. handling missing data, typically consider two approaches:Imputation: Replacing missing values estimated values retain data integrity.Removal: Deleting records missing values, though may lead data loss potential bias.","code":""},{"path":"chapter-data-prep.html","id":"imputation-techniques","chapter":"3 Data Preparation","heading":"Imputation Techniques","text":"several strategies imputing missing values, different use cases:Mean, median, mode imputation: Replaces missing values mean, median, mode corresponding column.Random sampling: Fills missing values random observations drawn existing data distribution.Predictive imputation: Uses machine learning models regression k-nearest neighbors estimate missing values.Multiple imputation: Generates several possible values missing entries averages results reduce uncertainty.\n### Example: Random Sampling Imputation R {-}impute missing values y using random sampling, use impute() function Hmisc package:impute() function replaces missing values randomly sampled values existing distribution y, maintaining overall statistical properties dataset.","code":"\ndiamonds_2$y <- impute(diamonds_2$y, \"random\")"},{"path":"chapter-data-prep.html","id":"best-practices","chapter":"3 Data Preparation","heading":"Best Practices","text":"Use mean median imputation numerical variables missing values missing random (MAR).Use mode imputation categorical variables.Consider predictive models dataset large missing values completely random.Always assess proportion missing data—many values missing, removing variable may better approach imputation.","code":""},{"path":"chapter-data-prep.html","id":"feature-scaling","chapter":"3 Data Preparation","heading":"3.5 Feature Scaling","text":"Feature scaling, also known normalization standardization, crucial step data preprocessing. adjusts range distribution numerical features similar scale. Many machine learning algorithms, especially based distance metrics k-nearest neighbors, benefit significantly scaled input features, prevents variables larger ranges disproportionately influencing model’s outcome.instance, diamonds dataset, carat variable ranges 0.2 5, price ranges 326 18823. Without scaling, variables like price wider range can dominate model’s predictions, potentially leading suboptimal results. address , apply feature scaling techniques bring numeric variables onto comparable scale. section, explore two common scaling methods:Min-Max Scaling: Also known min-max normalization min-max transformation.Z-score Scaling: Also known standardization Z-score normalization.Feature scaling provides several benefits:Improved Model Performance: Ensures features contribute equally model, preventing features larger numerical ranges dominating learning algorithms.Better Model Convergence: Particularly useful gradient-based optimization methods logistic regression neural networks.Effective Distance-Based Learning: Algorithms k-means clustering support vector machines rely distance calculations, making feature scaling essential.Consistent Feature Interpretation: standardizing numerical values, models become easier compare interpret.However, feature scaling also drawbacks:Potential Loss Information: cases, scaling can obscure meaningful differences data points.Impact Outliers: Min-max scaling, particular, sensitive extreme values, can distort scaled representation.Additional Computation: Scaling adds preprocessing overhead, particularly working large datasets.Reduced Interpretability: original units measurement lost, making harder relate scaled values real-world meanings.Selecting right scaling method depends characteristics data requirements model. next sections, explore methods detail apply diamonds dataset.","code":""},{"path":"chapter-data-prep.html","id":"min-max-scaling","chapter":"3 Data Preparation","heading":"3.6 Min-Max Scaling","text":"Min-Max Scaling transforms values feature fixed range, typically \\([0, 1]\\). transformation ensures minimum value feature becomes 0 maximum value becomes 1. especially useful algorithms rely distance metrics, equalizes contributions features, making comparisons balanced.formula Min-Max Scaling :\\[\nx_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}},\n\\]\n\\(x\\) original feature value, \\(x_{\\text{min}}\\) \\(x_{\\text{max}}\\) minimum maximum values feature, \\(x_{\\text{scaled}}\\) scaled value, ranging 0 1.Min-Max Scaling particularly useful models require bounded input values, neural networks algorithms relying gradient-based optimization. However, method sensitive outliers, extreme values significantly affect scaled distribution.Example 3.1  demonstrate Min-Max Scaling, ’ll apply carat variable diamonds dataset, carat values range approximately 0.2 5. Using minmax() function liver package, can scale carat values fit within range [0, 1].first histogram (left) shows distribution carat without scaling, second histogram (right) shows Min-Max Scaling. scaling, carat values compressed range 0 1, allowing comparable features may different original scales. scaling method particularly beneficial distance-based algorithms, prevents features wider ranges undue influence.","code":"\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` without scaling\") + \n  xlab(\"Values for variable `carat`\")\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = minmax(carat)), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` with Min-Max Scaling\") + \n  xlab(\"Values for variable `carat`\")"},{"path":"chapter-data-prep.html","id":"z-score-scaling","chapter":"3 Data Preparation","heading":"3.7 Z-score Scaling","text":"Z-score Scaling, also known standardization, transforms feature values mean 0 standard deviation 1. method particularly useful algorithms assume normally distributed data, linear regression logistic regression, centers data around 0 normalizes spread values.formula Z-score Scaling :\\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)}\n\\]\\(x\\) original feature value, \\(\\text{mean}(x)\\) mean feature, \\(\\text{sd}(x)\\) standard deviation feature, \\(x_{\\text{scaled}}\\) standardized value, now mean 0 standard deviation 1.Z-score Scaling particularly beneficial models assume normality use gradient-based optimization, ensuring numerical features contribute equally. However, since relies mean standard deviation, sensitive outliers, can distort transformation.Example 3.2  Applying Z-score Scaling carat variable diamonds dataset, mean standard deviation carat approximately 0.8 0.47, respectively. use zscore() function liver package standardize values.first histogram (left) displays distribution carat without scaling, second histogram (right) shows distribution Z-score Scaling. transformation makes feature values comparable across different scales ensures feature contributes equally distance-based computations model training.Note: common misconception Z-score Scaling, data follows standard normal distribution. Z-score Scaling centers data mean 0 scales standard deviation 1, alter shape distribution. original distribution skewed, remain skewed scaling, seen histograms .choice Min-Max Scaling Z-score Scaling depends requirements model characteristics data. Min-Max Scaling preferable algorithms require fixed input range, Z-score Scaling better suited models assume normally distributed features. selecting appropriate scaling method, ensure balanced feature contributions improved model performance.","code":"\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` without scaling\") + \n  xlab(\"Values for variable `carat`\")\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = zscore(carat)), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` with Z-score Scaling\") + \n  xlab(\"Values for variable `carat`\")"},{"path":"chapter-data-prep.html","id":"how-to-reexpress-categorical-field-values","chapter":"3 Data Preparation","heading":"3.8 How to Reexpress Categorical Field Values","text":"data science, categorical features often need transformed numeric format can used machine learning models. Algorithms like decision trees, neural networks, linear regression require numeric inputs process data effectively. Converting categorical variables numerical representations ensures features contribute appropriately model, rather ignored treated incorrectly.process reexpressing categorical values crucial part data preparation, enables us leverage full range features dataset. section, explore several methods convert categorical fields numeric representations, focus techniques like one-hot encoding ordinal encoding. demonstrate techniques using diamonds dataset, includes several categorical features cut, color, clarity.","code":""},{"path":"chapter-data-prep.html","id":"why-reexpress-categorical-fields","chapter":"3 Data Preparation","heading":"3.8.1 Why Reexpress Categorical Fields?","text":"Categorical fields, also known nominal ordinal variables, often represent qualitative aspects data, product types, user locations, levels satisfaction. diamonds dataset, example:cut indicates quality diamond’s cut (e.g., “Fair,” “Good,” “Good,” “Premium,” “Ideal”).color represents diamond’s color grade (e.g., “D,” “E,” “F,” “D” colorless thus valuable).clarity describes diamond’s clarity, reflecting absence internal external flaws.fields essential understanding predicting diamond pricing, raw form text labels, suitable machine learning algorithms. Transforming numeric form allows us include valuable insights analysis.","code":""},{"path":"chapter-data-prep.html","id":"techniques-for-reexpressing-categorical-variables","chapter":"3 Data Preparation","heading":"3.8.2 Techniques for Reexpressing Categorical Variables","text":"several approaches converting categorical variables numeric representations. method choose depends type categorical variable nature data.","code":""},{"path":"chapter-data-prep.html","id":"ordinal-encoding","chapter":"3 Data Preparation","heading":"Ordinal Encoding","text":"Ordinal encoding suitable categorical variable meaningful order. example, cut feature diamonds dataset ordinal, natural hierarchy “Fair” “Ideal.” ordinal encoding, category assigned unique integer based rank level importance.example, might assign values follows:“Fair” → 1“Good” → 2“Good” → 3“Premium” → 4“Ideal” → 5This approach preserves order categories, can useful models interpret numeric values relative way, linear regression. However, important apply ordinal encoding order meaningful. non-ordinal variables, methods like one-hot encoding appropriate.","code":""},{"path":"chapter-data-prep.html","id":"one-hot-encoding","chapter":"3 Data Preparation","heading":"One-Hot Encoding","text":"One-hot encoding preferred technique nominal variables—categorical fields without intrinsic order. approach, unique category field transformed new binary (0 1) feature. method particularly useful variables like color clarity diamonds dataset, categories follow clear sequence.example, one-hot encode color feature, create set binary columns, one color grade:color_D: 1 diamond color “D,” 0 otherwise.color_E: 1 diamond color “E,” 0 otherwise.color_F: 1 diamond color “F,” 0 otherwise.One-hot encoding avoids introducing false ordinal relationships, ensuring model treats category independent entity. However, one downside can significantly increase dimensionality dataset categorical field many unique values.Note: Many machine learning libraries automatically drop one binary columns avoid multicollinearity (perfect correlation among features). instance, seven color categories, six binary columns created, missing category implied columns zero. approach, known dummy encoding, helps avoid redundancy keeps model simpler.","code":""},{"path":"chapter-data-prep.html","id":"frequency-encoding","chapter":"3 Data Preparation","heading":"Frequency Encoding","text":"Another useful approach, especially high-cardinality categorical variables (many unique values), frequency encoding. technique replaces category frequency dataset, allowing model capture information common category . Frequency encoding can particularly helpful fields like clarity want give model indication prevalent level .example:“VS2” appears 10,000 times dataset, encoded 10,000.“” appears 500 times, encoded 500.Frequency encoding less commonly used basic machine learning workflows can valuable dealing large datasets, one-hot encoding introduce many columns. However, cautious approach, may inadvertently add implicit weight common categories.","code":""},{"path":"chapter-data-prep.html","id":"choosing-the-right-encoding-technique","chapter":"3 Data Preparation","heading":"3.8.3 Choosing the Right Encoding Technique","text":"Selecting appropriate encoding technique depends nature categorical variable requirements analysis:Ordinal variables (like cut): Use ordinal encoding preserve natural order.Nominal variables unique values (like color clarity): Use one-hot encoding represent category binary column.High-cardinality categorical variables: Consider frequency encoding one-hot encoding introduce many features.Example 3.3  Applying techniques diamonds dataset:example:Ordinal Encoding: encoded cut variable based quality hierarchy.One-Hot Encoding: applied one-hot encoding color, creating binary columns color grade.encoding categorical fields way, transform dataset format compatible machine learning algorithms preserving essential information categorical feature.dataset now cleaned, scaled, encoded, ready move next stage data analysis. upcoming chapter, explore Exploratory Data Analysis (EDA), use visualizations summary statistics gain insights structure relationships within data. combining prepared data EDA techniques, can better understand features may hold predictive value model set stage successful machine learning outcomes.","code":"\n# Example: Ordinal encoding for `cut`\ndiamonds <- diamonds %>%\n  mutate(cut_encoded = as.integer(factor(cut, levels = c(\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"))))\n\n# Example: One-hot encoding for `color`\ndiamonds <- diamonds %>%\n  mutate(\n    color_D = ifelse(color == \"D\", 1, 0),\n    color_E = ifelse(color == \"E\", 1, 0),\n    color_F = ifelse(color == \"F\", 1, 0),\n    color_G = ifelse(color == \"G\", 1, 0),\n    color_H = ifelse(color == \"H\", 1, 0),\n    color_I = ifelse(color == \"I\", 1, 0),\n    color_J = ifelse(color == \"J\", 1, 0)\n  )"},{"path":"chapter-data-prep.html","id":"Data-pre-adult","chapter":"3 Data Preparation","heading":"3.9 Case Study: Who Can Earn More Than $50K Per Year?","text":"case study, explore Adult dataset, sourced US Census Bureau. dataset contains demographic information individuals, including age, education, occupation, income. dataset available liver package. details, refer documentation.goal study predict whether individual earns $50,000 per year based attributes. Section 11.5 Chapter 11, apply decision tree random forest algorithms build predictive model. applying techniques, need preprocess dataset handling missing values, encoding categorical variables, scaling numerical features. Let’s begin loading dataset examining structure.","code":""},{"path":"chapter-data-prep.html","id":"overview-of-the-dataset","chapter":"3 Data Preparation","heading":"Overview of the Dataset","text":"use Adult dataset, first ensure liver package installed. , install using:Next, load package dataset:inspect dataset structure, use:dataset contains 48598 records 15 variables. , 14 predictors, target variable, income, categorical variable two levels: <=50K >50K. features include numerical categorical variables:age: Age years (numerical).workclass: Employment type (categorical, 6 levels).demogweight: Census weighting factor (numerical).education: Highest level education (categorical, 16 levels).education.num: Number years education (numerical).marital.status: Marital status (categorical, 5 levels).occupation: Job category (categorical, 15 levels).relationship: Family relationship status (categorical, 6 levels).race: Racial background (categorical, 5 levels).gender: Gender identity (categorical, Male/Female).capital.gain: Capital gains (numerical).capital.loss: Capital losses (numerical).hours.per.week: Hours worked per week (numerical).native.country: Country origin (categorical, 42 levels).income: Target variable indicating annual income (<=50K >50K).clarity, categorize dataset’s variables:Nominal variables: workclass, marital.status, occupation, relationship, race, native.country, gender.Ordinal variable: education.Numerical variables: age, demogweight, education.num, capital.gain, capital.loss, hours.per.week.better understand dataset, generate summary statistics:summary provides insights distribution numerical variables, missing values, categorical variable levels, guiding us preparing data analysis.","code":"\ninstall.packages(\"liver\")\nlibrary(liver)  # Load liver package\ndata(adult)     # Load Adult datasetstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education.num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital.status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital.gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital.loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours.per.week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native.country: Factor w/ 42 levels \"?\",\"Cambodia\",..: 40 40 40 40 40 40 40 40 40 40 ...\n    $ income        : Factor w/ 2 levels \"<=50K\",\">50K\": 1 1 2 2 1 1 1 2 1 1 ...summary(adult)\n         age              workclass      demogweight             education    \n    Min.   :17.0   ?           : 2794   Min.   :  12285   HS-grad     :15750  \n    1st Qu.:28.0   Gov         : 6536   1st Qu.: 117550   Some-college:10860  \n    Median :37.0   Never-worked:   10   Median : 178215   Bachelors   : 7962  \n    Mean   :38.6   Private     :33780   Mean   : 189685   Masters     : 2627  \n    3rd Qu.:48.0   Self-emp    : 5457   3rd Qu.: 237713   Assoc-voc   : 2058  \n    Max.   :90.0   Without-pay :   21   Max.   :1490400   11th        : 1812  \n                                                          (Other)     : 7529  \n    education.num         marital.status            occupation   \n    Min.   : 1.00   Divorced     : 6613   Craft-repair   : 6096  \n    1st Qu.: 9.00   Married      :22847   Prof-specialty : 6071  \n    Median :10.00   Never-married:16096   Exec-managerial: 6019  \n    Mean   :10.06   Separated    : 1526   Adm-clerical   : 5603  \n    3rd Qu.:12.00   Widowed      : 1516   Sales          : 5470  \n    Max.   :16.00                         Other-service  : 4920  \n                                          (Other)        :14419  \n            relationship                   race          gender     \n    Husband       :19537   Amer-Indian-Eskimo:  470   Female:16156  \n    Not-in-family :12546   Asian-Pac-Islander: 1504   Male  :32442  \n    Other-relative: 1506   Black             : 4675                 \n    Own-child     : 7577   Other             :  403                 \n    Unmarried     : 5118   White             :41546                 \n    Wife          : 2314                                            \n                                                                    \n     capital.gain      capital.loss     hours.per.week        native.country \n    Min.   :    0.0   Min.   :   0.00   Min.   : 1.00   United-States:43613  \n    1st Qu.:    0.0   1st Qu.:   0.00   1st Qu.:40.00   Mexico       :  949  \n    Median :    0.0   Median :   0.00   Median :40.00   ?            :  847  \n    Mean   :  582.4   Mean   :  87.94   Mean   :40.37   Philippines  :  292  \n    3rd Qu.:    0.0   3rd Qu.:   0.00   3rd Qu.:45.00   Germany      :  206  \n    Max.   :41310.0   Max.   :4356.00   Max.   :99.00   Puerto-Rico  :  184  \n                                                        (Other)      : 2507  \n      income     \n    <=50K:37155  \n    >50K :11443  \n                 \n                 \n                 \n                 \n   "},{"path":"chapter-data-prep.html","id":"missing-values-1","chapter":"3 Data Preparation","heading":"3.9.1 Missing Values","text":"summary() function reveals variables workclass native.country contain missing values, represented \"?\" category. Specifically, 2794 records workclass 847 records native.country missing values. Since \"?\" used placeholder missing data, first convert entries NA:replacing \"?\" NA, remove unused factor levels clean dataset:visualize distribution missing values, use gg_miss_var() function naniar package:plot indicates workclass, occupation, native.country contain missing values. percentage missing values variables relatively low, workclass occupation less 0.06 percent missing data, native.country 0.02 percent.","code":"\nadult[adult == \"?\"] = NA\nadult = droplevels(adult)\nlibrary(naniar)  # Load package for visualizing missing values\n\ngg_miss_var(adult, show_pct = TRUE)"},{"path":"chapter-data-prep.html","id":"imputing-missing-values","chapter":"3 Data Preparation","heading":"Imputing Missing Values","text":"Instead removing records missing values, can lead information loss, apply random imputation, missing values filled randomly selected values existing distribution variable. maintains natural proportions category.use impute() function Hmisc package purpose:confirm missing values successfully imputed, generate another missing values plot:updated plot show missing values, indicating successful imputation.","code":"\nlibrary(Hmisc)  # Load package for imputation\n\n# Impute missing values using random sampling from existing categories\nadult$workclass      = impute(adult$workclass,      'random')\nadult$native.country = impute(adult$native.country, 'random')\nadult$occupation     = impute(adult$occupation,     'random')\ngg_miss_var(adult, show_pct = TRUE)"},{"path":"chapter-data-prep.html","id":"alternative-approaches","chapter":"3 Data Preparation","heading":"Alternative Approaches","text":"impute() function allows different statistical methods mean, median, mode imputation. default behavior median imputation. advanced techniques, aregImpute() function Hmisc package offers predictive imputation using additive regression, bootstrapping, predictive mean matching.Although removing records missing values using na.omit() option, generally discouraged unless missing values excessive biased way distort analysis.properly handling missing values, ensure data completeness maintain integrity dataset subsequent preprocessing steps, recoding categorical variables grouping country-level data broader regions.","code":""},{"path":"chapter-data-prep.html","id":"encoding-categorical-variables","chapter":"3 Data Preparation","heading":"3.9.2 Encoding Categorical Variables","text":"Categorical variables often contain large number unique values, making challenging use predictive models. Adult dataset, native.country workclass multiple categories, can introduce complexity redundancy. simplify variables, group similar categories together preserving interpretability.","code":""},{"path":"chapter-data-prep.html","id":"grouping-native.country-by-continent","chapter":"3 Data Preparation","heading":"Grouping native.country by Continent","text":"native.country variable contains 41 distinct countries. make manageable, categorize countries broader geographical regions:Europe: England, France, Germany, Greece, Netherlands, Hungary, Ireland, Italy, Poland, Portugal, Scotland, YugoslaviaAsia: China, Hong Kong, India, Iran, Cambodia, Japan, Laos, Philippines, Vietnam, Taiwan, ThailandNorth America: Canada, United States, Puerto RicoSouth America: Colombia, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Mexico, Nicaragua, Outlying US territories, Peru, Jamaica, Trinidad & TobagoOther: includes ambiguous “South” category, meaning unclear dataset documentation.\nuse fct_collapse() function forcats package reassign categories:confirm changes, display frequency distribution native.country:grouping original 42 countries 5 broader regions, simplify variable maintaining relevance analysis.","code":"\nlibrary(forcats)  # Load package for categorical variable transformation\n\n# To create a new factor variable with fewer levels for `native.country`\nEurope = c(\"England\", \"France\", \"Germany\", \"Greece\", \"Holand-Netherlands\", \"Hungary\", \"Ireland\", \"Italy\", \"Poland\", \"Portugal\", \"Scotland\", \"Yugoslavia\")\n\nAsia = c(\"China\", \"Hong\", \"India\", \"Iran\", \"Cambodia\", \"Japan\", \"Laos\", \"Philippines\", \"Vietnam\", \"Taiwan\", \"Thailand\")\n\nN.America = c(\"Canada\", \"United-States\", \"Puerto-Rico\")\n\nS.America = c(\"Columbia\", \"Cuba\", \"Dominican-Republic\", \"Ecuador\", \"El-Salvador\", \"Guatemala\", \"Haiti\", \"Honduras\", \"Mexico\", \"Nicaragua\", \"Outlying-US(Guam-USVI-etc)\", \"Peru\", \"Jamaica\", \"Trinadad&Tobago\")\n\n# Reclassify native.country into broader regions\nadult$native.country = fct_collapse(adult$native.country, \n                                    \"Europe\"    = Europe,\n                                    \"Asia\"      = Asia,\n                                    \"N.America\" = N.America,\n                                    \"S.America\" = S.America,\n                                    \"Other\"     = c(\"South\") )table(adult$native.country)\n   \n        Asia N.America S.America    Europe     Other \n         993     44747      1946       797       115"},{"path":"chapter-data-prep.html","id":"simplifying-workclass","chapter":"3 Data Preparation","heading":"Simplifying workclass","text":"workclass variable originally contains several employment categories. Since “Never-worked” “Without-pay” represent similar employment statuses, merge single category labeled “Unemployed”:verify updated categories, check frequency distribution:reducing number unique categories workclass native.country, improve model interpretability reduce risk overfitting applying machine learning algorithms.","code":"\nadult$workclass = fct_collapse(adult$workclass, \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))table(adult$workclass)\n   \n          Gov Unemployed    Private   Self-emp \n         6919         32      35851       5796"},{"path":"chapter-data-prep.html","id":"outliers","chapter":"3 Data Preparation","heading":"3.9.3 Outliers","text":"Detecting handling outliers essential step data preprocessing, extreme values can significantly impact statistical analysis model performance. , examine potential outliers capital.loss variable determine whether adjustments necessary.","code":""},{"path":"chapter-data-prep.html","id":"summary-statistics","chapter":"3 Data Preparation","heading":"Summary Statistics","text":"gain initial understanding capital.loss, compute summary statistics:summary output reveals following insights:minimum value 0, maximum 4356.median 0, significantly lower mean, indicating highly skewed distribution.75% observations capital loss 0, confirming strong right-skew.mean capital loss 87.94, influenced small number extreme values.","code":"summary(adult$capital.loss)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0.00    0.00    0.00   87.94    0.00 4356.00"},{"path":"chapter-data-prep.html","id":"visualizing-outliers","chapter":"3 Data Preparation","heading":"Visualizing Outliers","text":"investigate distribution capital.loss, use boxplot histogram:plots, observe:boxplot shows strong positive skew, many extreme values upper whisker.histogram indicates observations zero capital loss, cases around 2,000 4,000.Since large proportion observations report capital loss, examine nonzero cases.","code":"\nggplot(data = adult, aes(y = capital.loss)) +\n     geom_boxplot()\nggplot(data = adult, aes(x = capital.loss)) +\n     geom_histogram(bins = 30, color = \"blue\", fill = \"lightblue\")"},{"path":"chapter-data-prep.html","id":"zooming-into-the-nonzero-distribution","chapter":"3 Data Preparation","heading":"Zooming into the Nonzero Distribution","text":"better visualize spread nonzero values, focus observations capital.loss > 0:Key takeaways refined plots:majority nonzero values 500, small number extending beyond 4,000.distribution nonzero values approximately symmetric, suggesting extreme values, follow structured pattern rather random anomalies.","code":"\nggplot(data = adult, mapping = aes(x = capital.loss)) +\n    geom_histogram(bins = 30, color = \"blue\", fill = \"lightblue\") +\n    coord_cartesian(xlim = c(500, 4000), ylim = c(0, 1000))\nggplot(data = subset(adult, capital.loss > 0)) +\n     geom_boxplot(aes(y = capital.loss)) "},{"path":"chapter-data-prep.html","id":"handling-outliers","chapter":"3 Data Preparation","heading":"Handling Outliers","text":"Although capital.loss contains many high values, appear erroneous. Instead, reflect genuine cases within dataset. Since values provide meaningful information particular individuals, retain rather applying transformations removals.However, model performance significantly affected extreme values, might consider:Winsorization: Capping values reasonable percentile (e.g., 95th percentile).Log Transformation: Applying log transformation reduce skewness.Creating Binary Indicator: Introducing new variable indicating whether capital loss occurred (capital.loss > 0).Next, perform similar outlier analysis capital.gain variable. See exercises guided approach.","code":""},{"path":"chapter-data-prep.html","id":"exercises-1","chapter":"3 Data Preparation","heading":"3.10 Exercises","text":"section provides hands-exercises reinforce key concepts covered chapter. questions include theoretical, exploratory, practical challenges related data types, outliers, encoding techniques, feature engineering.","code":""},{"path":"chapter-data-prep.html","id":"understanding-data-types","chapter":"3 Data Preparation","heading":"Understanding Data Types","text":"difference continuous discrete numerical variables? Provide example real-world data.ordinal categorical variables differ nominal categorical variables? Give example .","code":""},{"path":"chapter-data-prep.html","id":"exploring-the-diamonds-dataset","chapter":"3 Data Preparation","heading":"Exploring the diamonds Dataset","text":"Report summary statistics diamonds dataset using summary() function. insights can derive output?diamonds dataset, variables nominal, ordinal, numerical? List accordingly.","code":""},{"path":"chapter-data-prep.html","id":"detecting-and-handling-outliers","chapter":"3 Data Preparation","heading":"Detecting and Handling Outliers","text":"Identify outliers variable x. exist, handle appropriately. Follow approach Section 3.3 y variable diamonds dataset.Repeat outlier detection process variable z. necessary, apply transformations filtering techniques.Check outliers depth variable. method use detect handle ?","code":""},{"path":"chapter-data-prep.html","id":"encoding-categorical-variables-1","chapter":"3 Data Preparation","heading":"Encoding Categorical Variables","text":"cut variable diamonds dataset ordinal. can encode properly using ordinal encoding?color variable diamonds dataset nominal. can encode using one-hot encoding?","code":""},{"path":"chapter-data-prep.html","id":"analyzing-the-adult-dataset","chapter":"3 Data Preparation","heading":"Analyzing the Adult Dataset","text":"Load Adult dataset liver package examine structure. Identify categorical variables classify nominal ordinal.Compute proportion individuals earn 50K (>50K). distribution tell income levels dataset?Adult dataset, generate summary statistics, boxplot, histogram variable capital.gain. observe?Based visualizations previous question, outliers capital.gain variable? , suggest strategy handle .","code":""},{"path":"chapter-data-prep.html","id":"feature-engineering-challenge","chapter":"3 Data Preparation","heading":"Feature Engineering Challenge","text":"Create new categorical variable Age_Group Adult dataset, grouping ages :Young (≤30 years old)Middle-aged (31-50 years old)Senior (>50 years old)\nUse cut() function implement transformation.Compute mean capital.gain Age_Group. insights gain income levels across different age groups?","code":""},{"path":"chapter-data-prep.html","id":"advanced-data-preparation-challenges","chapter":"3 Data Preparation","heading":"Advanced Data Preparation Challenges","text":"Adult dataset, education variable contains 16 distinct levels. Reduce categories broader groups “Diploma,” “High School Graduate,” “College,” “Postgraduate.” Implement transformation using fct_collapse() function.Adult dataset, education variable contains 16 distinct levels. Reduce categories broader groups “Diploma,” “High School Graduate,” “College,” “Postgraduate.” Implement transformation using fct_collapse() function.capital.gain capital.loss variables represent financial assets. Create new variable net.capital computes difference capital.gain capital.loss. Analyze distribution.capital.gain capital.loss variables represent financial assets. Create new variable net.capital computes difference capital.gain capital.loss. Analyze distribution.Perform Min-Max scaling numerical variables Adult dataset (age, capital.gain, capital.loss, hours.per.week). Use mutate() function apply transformation.Perform Min-Max scaling numerical variables Adult dataset (age, capital.gain, capital.loss, hours.per.week). Use mutate() function apply transformation.Perform Z-score normalization set numerical variables. Compare results Min-Max scaling. scenarios one approach preferable ?Perform Z-score normalization set numerical variables. Compare results Min-Max scaling. scenarios one approach preferable ?Construct logistic regression model predict whether individual earns 50K (>50K) based selected numerical features (age, education.num, hours.per.week). Preprocess data accordingly interpret coefficients model.Construct logistic regression model predict whether individual earns 50K (>50K) based selected numerical features (age, education.num, hours.per.week). Preprocess data accordingly interpret coefficients model.","code":""},{"path":"chapter-EDA.html","id":"chapter-EDA","chapter":"4 Exploratory Data Analysis","heading":"4 Exploratory Data Analysis","text":"Exploratory Data Analysis (EDA) fundamental step data science helps uncover insights, detect patterns, understand relationships within dataset. applying statistical models machine learning algorithms, essential explore visualize data identify inconsistencies, anomalies, potential predictors. EDA provides structured yet flexible approach data exploration, allowing data scientists make informed decisions proceeding modeling.EDA iterative process rather rigid sequence steps. encourages curiosity adaptability, different datasets may present unique challenges insights. exploratory paths may lead inconclusive findings, others uncover valuable patterns shape direction analysis. familiarity data increases, analysts can refine focus, identifying relevant features variables modeling.primary goal EDA explore summarize data rather perform hypothesis testing confirm specific relationships. using summary statistics, visualizations, preliminary correlation analysis, EDA provides foundational understanding dataset. However, insights remain preliminary require validation statistical testing predictive modeling. Recognizing distinction ensures exploratory findings interpreted cautiously lead premature conclusions.Another key consideration EDA balancing statistical significance practical relevance. Large datasets often reveal statistically significant relationships may lack meaningful real-world implications. example, weak correlation customer engagement churn might statistically significant yet offer little actionable insight business decision-making. EDA encourages analysts integrate domain expertise practical considerations interpreting patterns data.EDA also plays crucial role data cleaning preparation. Missing values, inconsistencies, outliers often become apparent exploration, requiring careful handling ensure data quality. data cleaning distinct process, closely linked EDA, identifying resolving data issues early helps establish solid foundation analysis modeling.Selecting appropriate tools techniques EDA depends nature dataset specific analytical questions. Histograms box plots help visualize distributions, scatter plots correlation matrices assess relationships variables. following sections demonstrate techniques practical applications, including -depth analysis churn dataset illustrate EDA can uncover patterns relevant customer retention.key objectives EDA :Understanding structure data – Determine data types, variable ranges, number observations, presence missing values anomalies.Analyzing individual variable distributions – Examine numerical categorical variables assess distribution, central tendency, spread.Exploring relationships variables – Identify correlations, dependencies, interactions may influence predictive modeling.Detecting patterns outliers – Identify unusual data points assess whether result data errors represent meaningful trends.objectives ensure comprehensive understanding dataset, forming strong foundation subsequent modeling analysis.","code":""},{"path":"chapter-EDA.html","id":"guiding-questions-for-eda","chapter":"4 Exploratory Data Analysis","heading":"4.1 Guiding Questions for EDA","text":"Exploratory data analysis effective structured around key questions help uncover meaningful patterns relationships. questions generally fall two broad categories: univariate multivariate analysis.Univariate analysis examines individual variables assess distributions, central tendencies, variability, potential data quality issues missing values outliers. Typical univariate questions include:distribution target variable?numerical variables, income age, distributed?missing values, distributed across dataset?analysis helps detect skewness, irregularities, unexpected patterns may impact later modeling. Common tools univariate analysis include histograms, box plots, summary statistics mean, median, quartiles, standard deviation.Multivariate analysis explores relationships two variables, identifying dependencies, interactions, correlations influence predictive modeling. Key multivariate questions include:target variable relate predictor variables?predictors highly correlated, suggesting potential multicollinearity?categorical numerical variables interact?analyze relationships, data scientists commonly use scatter plots, correlation matrices, pairwise comparisons, help visualize dependencies variables guide feature selection.common challenge EDA choosing appropriate visualization statistical summary given analysis. selection depends type data insight sought. table provides structured guide selecting effective tools various exploratory tasks:Table 4.1: EDA Tool Selection Guide.systematically addressing univariate multivariate questions appropriate EDA techniques, gain deeper understanding dataset’s structure key patterns. process improves data quality also provides valuable insights inform subsequent modeling decision-making.","code":""},{"path":"chapter-EDA.html","id":"eda-as-data-storytelling","chapter":"4 Exploratory Data Analysis","heading":"4.2 EDA as Data Storytelling","text":"Exploratory Data Analysis just technical step; way uncover communicate meaningful insights data. Beyond summarizing numbers visualizing patterns, EDA helps shape narrative hidden within data. Effective data storytelling integrates data, visuals, context, making findings accessible actionable. Whether communicating data scientists, business professionals, decision-makers, presenting insights clearly essential.summary statistics provide overview, visualizations reveal patterns, relationships, anomalies might otherwise go unnoticed. Different types visualizations serve distinct purposes. Scatter plots correlation matrices highlight relationships numerical variables, histograms box plots illustrate distributions potential skewness. Categorical data best explored bar charts stacked visualizations, allowing comparisons across different groups. Choosing right visualization ensures insights accurate intuitive.strong narrative connects data insights real-world significance. Rather simply presenting correlation coefficient distribution plot, well-structured EDA report explains pattern matters informs decision-making. Instead stating customers high daytime phone usage higher churn rate, impactful provide context:“Customers extensive daytime usage significantly likely churn, possibly due pricing concerns dissatisfaction service quality. Targeted retention strategies, customized discounts flexible pricing plans, may help mitigate risk.”approach goes beyond numerical reporting, framing insights actionable strategies.Data storytelling widely used business, scientific research, journalism. Consider following examples illustrate visual storytelling enhances understanding.One example comes climate science. Figure 4.1 presents global mean surface temperature changes Common Era, highlighting long-term warming trends. Adapted Raphael Neukom et al.4, visualization provides historical perspective climate change, illustrating temperature anomalies time.\nFigure 4.1: Global mean surface temperature history Common Era. Temperature anomalies respect 1961–1990 CE. colored lines represent 30-year low-pass-filtered ensemble medians different reconstruction methods.\nAnother example focuses global health demographics. Figure 4.2 illustrates relationship fertility rate life expectancy across world regions 1960 2015. Adapted Hans Rosling’s TED Talk “New insights poverty”, visualization effectively conveys trends population health economic development time.\nFigure 4.2: Animated scatter plot fertility rate life expectancy birth different world regions 1960 2015.\nexamples highlight well-designed visualizations can make complex data accessible engaging.conducting EDA, essential consider broader narrative. Instead just reporting statistical results, think trend reveals, relevant, can inform decision-making. Integrating storytelling techniques ensures EDA serves data exploration step also communication tool connects technical analysis practical application.","code":""},{"path":"chapter-EDA.html","id":"EDA-sec-churn","chapter":"4 Exploratory Data Analysis","heading":"4.3 EDA in Practice: The Churn Dataset","text":"illustrate exploratory data analysis process, use churn dataset, contains information customer behavior, including whether customer churned (.e., left service) various demographic behavioral attributes.EDA help us understand dataset identifying patterns related customer churn, determining features influence retention, establishing structured approach predictive modeling. examining summary statistics visualizations, can extract meaningful insights moving machine learning techniques later chapters.","code":""},{"path":"chapter-EDA.html","id":"problem-understanding-1","chapter":"4 Exploratory Data Analysis","heading":"Problem Understanding","text":"Companies seek minimize customer churn identifying factors influence customer decisions. Key business questions include:customers leaving?primary factors contributing churn?can take action improve retention?EDA helps answer questions uncovering trends patterns customer behavior. Chapter 7, build predictive model identify customers likely churn. , essential explore data understand structure.","code":""},{"path":"chapter-EDA.html","id":"data-understanding","chapter":"4 Exploratory Data Analysis","heading":"Data Understanding","text":"churn dataset comes IBM Sample Data Sets contains 5000 customer records across 20 variables. dataset available liver package. target variable, churn, indicates whether customer left company. dataset includes mix categorical numerical variables:state : Categorical, state U.S. (51 states + D.C.).area.code : Categorical, area code assigned customer.account.length : Numerical (discrete), duration account activity (days).voice.plan : Categorical (binary), subscription voice mail plan (yes/).voice.messages : Numerical (discrete), number voice mail messages.intl.plan : Categorical (binary), subscription international calling plan (yes/).intl.mins : Numerical (continuous), total international call minutes.intl.calls : Numerical (discrete), total international calls made.intl.charge : Numerical (continuous), total international call charges.day.mins : Numerical (continuous), total daytime call minutes.day.calls : Numerical (discrete), total daytime calls made.day.charge : Numerical (continuous), total daytime call charges.eve.mins : Numerical (continuous), total evening call minutes.eve.calls : Numerical (discrete), total evening calls made.eve.charge : Numerical (continuous), total evening call charges.night.mins : Numerical (continuous), total nighttime call minutes.night.calls : Numerical (discrete), total nighttime calls made.night.charge : Numerical (continuous), total nighttime call charges.customer.calls : Numerical (discrete), number customer service calls made.churn : Categorical (binary), indicates whether customer churned (yes/).use dataset, first load liver package import dataset R:examine structure dataset, use:command reveals dataset stored data.frame 5000 observations 20 variables. target variable, churn, categorizes whether customer left service.summarize dataset, use:function provides high-level overview variables, including distributions potential missing values. dataset clean ready exploratory data analysis. next sections, explore structure using visualizations statistical summaries. help us identify key variables influencing churn ensure dataset well-prepared predictive modeling.One notable observation 51 unique states represented dataset, yet 3 unique area codes. suggests area codes correspond directly state locations, worth investigating .","code":"\nlibrary(liver)\ndata(churn)  str(churn)  \n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...summary(churn)\n        state              area.code    account.length  voice.plan\n    WV     : 158   area_code_408:1259   Min.   :  1.0   yes:1323  \n    MN     : 125   area_code_415:2495   1st Qu.: 73.0   no :3677  \n    AL     : 124   area_code_510:1246   Median :100.0             \n    ID     : 119                        Mean   :100.3             \n    VA     : 118                        3rd Qu.:127.0             \n    OH     : 116                        Max.   :243.0             \n    (Other):4240                                                  \n    voice.messages   intl.plan    intl.mins       intl.calls      intl.charge   \n    Min.   : 0.000   yes: 473   Min.   : 0.00   Min.   : 0.000   Min.   :0.000  \n    1st Qu.: 0.000   no :4527   1st Qu.: 8.50   1st Qu.: 3.000   1st Qu.:2.300  \n    Median : 0.000              Median :10.30   Median : 4.000   Median :2.780  \n    Mean   : 7.755              Mean   :10.26   Mean   : 4.435   Mean   :2.771  \n    3rd Qu.:17.000              3rd Qu.:12.00   3rd Qu.: 6.000   3rd Qu.:3.240  \n    Max.   :52.000              Max.   :20.00   Max.   :20.000   Max.   :5.400  \n                                                                                \n       day.mins       day.calls     day.charge       eve.mins       eve.calls    \n    Min.   :  0.0   Min.   :  0   Min.   : 0.00   Min.   :  0.0   Min.   :  0.0  \n    1st Qu.:143.7   1st Qu.: 87   1st Qu.:24.43   1st Qu.:166.4   1st Qu.: 87.0  \n    Median :180.1   Median :100   Median :30.62   Median :201.0   Median :100.0  \n    Mean   :180.3   Mean   :100   Mean   :30.65   Mean   :200.6   Mean   :100.2  \n    3rd Qu.:216.2   3rd Qu.:113   3rd Qu.:36.75   3rd Qu.:234.1   3rd Qu.:114.0  \n    Max.   :351.5   Max.   :165   Max.   :59.76   Max.   :363.7   Max.   :170.0  \n                                                                                 \n      eve.charge      night.mins     night.calls      night.charge   \n    Min.   : 0.00   Min.   :  0.0   Min.   :  0.00   Min.   : 0.000  \n    1st Qu.:14.14   1st Qu.:166.9   1st Qu.: 87.00   1st Qu.: 7.510  \n    Median :17.09   Median :200.4   Median :100.00   Median : 9.020  \n    Mean   :17.05   Mean   :200.4   Mean   : 99.92   Mean   : 9.018  \n    3rd Qu.:19.90   3rd Qu.:234.7   3rd Qu.:113.00   3rd Qu.:10.560  \n    Max.   :30.91   Max.   :395.0   Max.   :175.00   Max.   :17.770  \n                                                                     \n    customer.calls churn     \n    Min.   :0.00   yes: 707  \n    1st Qu.:1.00   no :4293  \n    Median :1.00             \n    Mean   :1.57             \n    3rd Qu.:2.00             \n    Max.   :9.00             \n   "},{"path":"chapter-EDA.html","id":"chapter-EDA-categorical","chapter":"4 Exploratory Data Analysis","heading":"4.4 Investigating Categorical Variables","text":"Categorical variables represent discrete values labels, names, binary indicators. churn dataset, key categorical features include state, area.code, voice.plan, intl.plan. Understanding distributions relationships target variable helps uncover trends may influence customer retention.begin, examine distribution target variable churn determine whether dataset balanced:bar plot reveals dataset imbalanced, customers staying (churn = \"\") leaving (churn = \"yes\"). proportion churners approximately 1.4 percent, proportion non-churners 8.6 percent. Since imbalanced data can impact predictive modeling, understanding churn patterns essential improving retention strategies.","code":"\nggplot(data = churn, aes(x = churn, label = scales::percent(prop.table(stat(count))))) +\n  geom_bar(fill = c(\"palevioletred1\", \"darkseagreen1\")) + \n  geom_text(stat = 'count', vjust = 0.2, size = 6)"},{"path":"chapter-EDA.html","id":"relationship-between-churn-and-subscription-plans","chapter":"4 Exploratory Data Analysis","heading":"Relationship Between Churn and Subscription Plans","text":"first analyze intl.plan, indicates whether customer international calling plan. binary variable, allows straightforward comparison churn rates subscribed non-subscribed customers.first plot (left) compares raw counts churners non-churners among customers without international plan. second plot (right) normalizes proportions, revealing customers international plan significantly higher churn rate.quantify relationship, generate contingency table:results confirm churn prevalent among customers subscribed international plan. suggests international service offerings may meeting customer expectations, leading higher attrition. Companies may need investigate whether pricing, service quality, competition influencing trend.","code":"\nggplot(data = churn) + \n  geom_bar(aes(x = intl.plan, fill = churn)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = churn) + \n  geom_bar(aes(x = intl.plan, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) addmargins(table(churn$churn, churn$intl.plan, \n                 dnn = c(\"Churn\", \"International Plan\")))\n        International Plan\n   Churn  yes   no  Sum\n     yes  199  508  707\n     no   274 4019 4293\n     Sum  473 4527 5000"},{"path":"chapter-EDA.html","id":"relationship-between-churn-and-voice-mail-plan","chapter":"4 Exploratory Data Analysis","heading":"Relationship Between Churn and Voice Mail Plan","text":"Next, examine voice.plan, indicates whether customer subscribed voice mail plan.Customers without voice mail plan appear churn slightly higher rate. confirmed using contingency table:difference less pronounced intl.plan, suggests customers actively use voice mail services may engaged therefore less likely leave.","code":"\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) addmargins(table(churn$churn, churn$voice.plan, dnn = c(\"Churn\", \"Voice Mail Plan\")))\n        Voice Mail Plan\n   Churn  yes   no  Sum\n     yes  102  605  707\n     no  1221 3072 4293\n     Sum 1323 3677 5000"},{"path":"chapter-EDA.html","id":"key-insights","chapter":"4 Exploratory Data Analysis","heading":"Key Insights","text":"Customers subscribed international plan significantly higher churn rate, indicating potential issue service expectations, pricing, customer satisfaction. variable likely important predictor churn models.Customers voice mail plan slightly lower churn rate, suggesting engagement additional services may contribute customer retention.insights highlight importance investigating product-specific factors analyzing churn, different subscription plans may varying impacts customer behavior.exploring categorical variables way, uncover actionable insights can inform predictive modeling business decisions aimed reducing customer churn. next section, examine numerical variables refine understanding customer behavior.","code":""},{"path":"chapter-EDA.html","id":"EDA-sec-numeric","chapter":"4 Exploratory Data Analysis","heading":"4.5 Investigating Numerical Variables","text":"now turn numerical variables churn dataset, examining distributions relationships target variable. Summary statistics provide initial understanding, visualizations histograms, box plots, density plots help reveal patterns potential predictors churn.","code":""},{"path":"chapter-EDA.html","id":"customer-service-calls-and-churn","chapter":"4 Exploratory Data Analysis","heading":"Customer Service Calls and Churn","text":"variable customer.calls represents number calls customer makes customer service. Since discrete numerical variable, use histogram examine distribution:histogram shows customers make service calls, smaller group contacts customer service frequently. right-skewed distribution suggests customers make unusually high number calls, potentially signaling dissatisfaction.investigate, overlay churn status:normalized histogram (right) reveals striking trend: customers making four service calls significantly higher churn rate. suggests frequent service interactions may indicate unresolved issues, leading customer dissatisfaction.Key Insights Business Implications:Customers making frequent service calls higher risk churning.Companies implement proactive retention strategies customer makes multiple calls, escalating issues offering incentives third call.variable likely strong predictor churn models included analysis.","code":"\nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls), \n                 bins = 10, fill = \"skyblue\", color = \"black\")\nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls, fill = churn), position = \"stack\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n  \nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) "},{"path":"chapter-EDA.html","id":"daytime-minutes-and-churn","chapter":"4 Exploratory Data Analysis","heading":"Daytime Minutes and Churn","text":"Next, examine day.mins, represents number minutes customer spends daytime calls. use box plots density plots compare distributions churners non-churners.box plot (left) shows customers churn tend higher daytime call usage. density plot (right) confirms , noticeable peak churners higher day.mins values.Key Insights Business Implications:High day.mins usage associated increased churn.Customers extensive daytime usage may dissatisfied pricing service quality.Targeted retention offers, flexible rate plans heavy users, help mitigate churn.","code":"\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = day.mins), \n                 fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = day.mins, fill = churn), alpha = 0.3)"},{"path":"chapter-EDA.html","id":"evening-and-nighttime-minutes","chapter":"4 Exploratory Data Analysis","heading":"Evening and Nighttime Minutes","text":"investigate whether evening nighttime call patterns also relate churn, plot eve.mins night.mins.slight trend suggests churners higher eve.mins, effect weaker day.mins. Similarly, analysis night.mins reveal clear distinction churners non-churners.similar distributions suggest nighttime call usage strong churn indicator.Key Insights Business Implications:Unlike daytime calls, evening nighttime minutes strongly predict churn.Focusing daytime usage service call patterns may yield better predictive power.statistical testing (e.g., t-tests logistic regression) confirm whether subtle differences exist.subsection well-structured provides clear overview analysis. However, areas improvement terms clarity, conciseness, flow. improved version maintains structure meaning enhancing readability coherence.","code":"\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = eve.mins), fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = eve.mins, fill = churn), alpha = 0.3)\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = night.mins), fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = night.mins, fill = churn), alpha = 0.3)"},{"path":"chapter-EDA.html","id":"international-calls-and-churn","chapter":"4 Exploratory Data Analysis","heading":"International Calls and Churn","text":"now examine intl.calls, represents total number international calls made customers. explore relationship churn, visualize distribution using box plots density plots.box plot (left) indicates churners (churn=yes) tend make slightly fewer international calls non-churners. density plot (right) supports , showing minor difference distribution two groups.Although slight trend suggesting churners make fewer international calls average, difference appear substantial. suggests intl.calls strong predictor churn. confirm whether relationship statistically significant, testing—two-sample t-test logistic regression—required. Section 5.6 next chapter, demonstrate formally test relationship using statistical methods.","code":"\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = intl.calls), \n                 fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = intl.calls, fill = churn), alpha = 0.3)"},{"path":"chapter-EDA.html","id":"final-takeaways","chapter":"4 Exploratory Data Analysis","heading":"Final Takeaways","text":"customer.calls day.mins strongly associated churn key predictors churn models.Customers making four service calls high risk leaving.High daytime minute usage another important churn indicator, possibly due pricing concerns.Evening nighttime call usage shows strong relationship churn, suggesting may essential predictive feature.focusing service calls daytime minutes, companies can take targeted action reduce churn, optimizing customer support escalation processes offering personalized rate plans. findings also guide feature selection process future predictive modeling efforts.","code":""},{"path":"chapter-EDA.html","id":"EDA-sec-multivariate","chapter":"4 Exploratory Data Analysis","heading":"4.6 Investigating Multivariate Relationships","text":"univariate analysis provides insights individual variables, multivariate analysis helps uncover interactions may influence churn. Examining variable relationships can reveal behavioral patterns might evident analyzing feature isolation.useful example relationship day.mins (Day Minutes) eve.mins (Evening Minutes), visualized scatter plot :diagonal line, represented equation:\\[\n\\text{day.mins} = 400 - 0.6 \\times \\text{eve.mins}\n\\]separates dataset two regions. Customers upper-right region, day evening minutes high, exhibit noticeably higher churn rate. pattern apparent univariate analysis eve.mins, demonstrating feature interactions can provide deeper insights.quantify effect, isolate high-churn segment:Within subset, churn rate significantly higher overall dataset, reinforcing importance considering variable interactions. combination high day evening usage may indicate specific customer behavior pattern correlates dissatisfaction.Another key relationship exists customer.calls day.mins:scatter plot reveals interesting high-churn region upper left, customers make frequent customer service calls low day-minute usage. group may represent dissatisfied customers heavy users still experiencing service-related frustrations. contrast, high-minute users also make frequent service calls show lower churn rate, possibly indicating engaged customers tolerant service issues.","code":"\nggplot(data = churn) +\n    geom_point(aes(x = eve.mins, y = day.mins, color = churn), size = 0.7, alpha = 0.8) +\n    scale_color_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) +\n    geom_abline(intercept = 400, slope = -0.6, color = \"blue\", size = 1)\nsub_churn = subset(churn, (day.mins > 400 - 0.6 * eve.mins))\n\nggplot(data = sub_churn, aes(x = churn, label = scales::percent(prop.table(stat(count))))) +\n    geom_bar(fill = c(\"palevioletred1\", \"darkseagreen1\")) + \n    geom_text(stat = 'count', vjust = 0.2, size = 6)\nggplot(data = churn) +\n  geom_point(aes(x = day.mins, y = customer.calls, color = churn), alpha = 0.8) +\n  scale_color_manual(values = c(\"palevioletred1\", \"darkseagreen1\"))"},{"path":"chapter-EDA.html","id":"key-takeaways","chapter":"4 Exploratory Data Analysis","heading":"Key Takeaways","text":"Multivariate analysis reveals customers high day evening call usage much higher churn rate.Customers making frequent customer service calls using daytime minutes also higher risk leaving.interaction frequent customer service calls high call usage suggests dissatisfaction alone always drive churn—usage patterns also play role.Identifying customers high-churn regions scatter plots can help targeted retention efforts.Chapter 5, move exploratory insights formal statistical analysis, applying techniques quantify relationships assess predictive value.","code":""},{"path":"chapter-EDA.html","id":"investigating-correlated-variables","chapter":"4 Exploratory Data Analysis","heading":"4.6.1 Investigating Correlated Variables","text":"Correlation measures degree two variables move together. positive correlation means one variable increases, also increases. negative correlation indicates one variable rises, decreases. two variables correlation, changes one provide information changes .common misconception correlation implies causation. example, analysis finds customers make calls customer service tend higher churn rates, necessarily mean calling customer service causes churn. dissatisfied customers likely call assistance leaving, making number service calls symptom rather cause.strength direction correlation measured correlation coefficient, denoted \\(r\\), ranges -1 1. value 1 indicates perfect positive relationship, -1 represents perfect negative correlation. value near zero suggests linear relationship. large datasets, even small correlations may statistically significant, practical significance must also considered. correlation 0.05 might significant dataset thousands observations, unlikely provide meaningful predictive power., Figure 4.3 shows examples different correlation coefficients.\nFigure 4.3: Example scatterplots showing different correlation coefficients.\nmultiple variables highly correlated, redundancy can become problem. Including variables model may add much new information can lead instability, particularly regression-based models multicollinearity makes difficult determine effect individual predictors. Instead automatically removing correlated variables, thoughtful approach involves assessing practical relevance whether provide distinct information.examine correlations churn dataset, compute correlation matrix numerical variables visualize using heatmap.correlation matrix highlights key relationships. charge variables perfectly correlated corresponding minutes variables charges calculated directly call duration. Including introduce redundancy. avoid , charge variables removed, keeping minutes variables.Another notable observation number calls within time period strongly correlated total minutes. One might expect customers make calls also spend time phone, data support assumption. suggests call frequency call duration may capture different aspects customer behavior, making valuable retain types variables modeling.addressing correlations exploratory data analysis, dataset can refined ensure informative variables used predictive modeling. Removing redundant features reduces complexity retaining meaningful signals, improving interpretability performance models.","code":"\nlibrary(ggcorrplot)  \nvariable_list = c(\"intl.mins\",  \"intl.calls\",  \"intl.charge\", \n                  \"day.mins\",   \"day.calls\",   \"day.charge\",\n                  \"eve.mins\",   \"eve.calls\",   \"eve.charge\",\n                  \"night.mins\", \"night.calls\", \"night.charge\")\n\ncor_matrix = cor(churn[, variable_list])\n\nggcorrplot(cor_matrix, type = \"lower\", lab = TRUE, lab_size = 3)"},{"path":"chapter-EDA.html","id":"key-findings-and-insights","chapter":"4 Exploratory Data Analysis","heading":"4.7 Key Findings and Insights","text":"exploratory data analysis churn dataset provided deeper understanding factors influencing customer attrition. examining individual variables interactions, identified several key trends inform predictive modeling business strategy.One striking findings role customer service interactions churn. Customers made four calls customer service significantly likely leave. suggests frequent interactions may indicate unresolved complaints, leading dissatisfaction. Additionally, customers high daytime evening call usage exhibit churn rates six times higher average, indicating high usage may correlate dissatisfaction—perhaps due service quality concerns pricing issues.international calling plan also appears strong predictor churn. Customers subscribe plan leaving much higher rate, suggesting plan may delivering sufficient value. contrast, customers voice mail plan show lower churn rate, indicating feature may contribute customer retention.Several variables, directly linked churn univariate analysis, still provide value combined features predictive modeling. example, customers relatively low daytime usage frequent customer service calls show higher likelihood leaving, pattern suggests service dissatisfaction among lower-usage customers.modeling perspective, variables introduce redundancy. charge variables (day, evening, night, international) perfectly correlated corresponding minute variables, derived directly . Retaining minute variables avoid multicollinearity preserving relevant information. Similarly, area code state fields may contribute much predictive power, show strong relationships churn dataset.","code":""},{"path":"chapter-EDA.html","id":"strategic-recommendations","chapter":"4 Exploratory Data Analysis","heading":"Strategic Recommendations","text":"findings present opportunities targeted interventions improve customer retention. Given frequent customer service calls strong indicator churn, companies implement proactive escalation strategies. Customers making third service call receive priority attention, potentially issue resolution specialists targeted retention offers.high-usage customers, personalized plans loyalty incentives help reduce churn. Offering flexible pricing additional benefits high daytime evening callers may address concerns drive switch providers. Similarly, review international plan necessary assess whether pricing, service quality, features leading dissatisfaction.variables, night minutes certain demographic features, show strong direct correlations churn, may still contribute combined predictors machine learning model. analysis determine importance predictive framework.identifying churn-related patterns early, businesses can take proactive steps improve customer satisfaction reduce attrition, strengthening overall retention efforts relying predictive modeling. insights serve foundation next stage analysis, machine learning models applied quantify relationships precisely.","code":""},{"path":"chapter-EDA.html","id":"exercises-2","chapter":"4 Exploratory Data Analysis","heading":"4.8 Exercises","text":"","code":""},{"path":"chapter-EDA.html","id":"conceptual-questions","chapter":"4 Exploratory Data Analysis","heading":"Conceptual Questions","text":"important perform exploratory data analysis proceeding modeling phase? potential risks skipping EDA directly applying data mining techniques?important perform exploratory data analysis proceeding modeling phase? potential risks skipping EDA directly applying data mining techniques?predictor exhibit clear relationship target variable exploratory data analysis, omitted modeling stage? Justify answer considering potential interactions, hidden patterns, role feature selection.predictor exhibit clear relationship target variable exploratory data analysis, omitted modeling stage? Justify answer considering potential interactions, hidden patterns, role feature selection.mean two variables correlated? Explain concept correlation, including direction strength, discuss differs causation. Provide example illustrate explanation.mean two variables correlated? Explain concept correlation, including direction strength, discuss differs causation. Provide example illustrate explanation.can identify address correlated variables exploratory data analysis? Describe steps take manage correlated predictors effectively explain benefits approach predictive modeling.can identify address correlated variables exploratory data analysis? Describe steps take manage correlated predictors effectively explain benefits approach predictive modeling.consequences including highly correlated variables predictive model? Discuss impact multicollinearity model performance, interpretability, stability, explain can detected addressed.consequences including highly correlated variables predictive model? Discuss impact multicollinearity model performance, interpretability, stability, explain can detected addressed.always advisable remove one two correlated predictors model? Discuss advantages drawbacks approach, explain circumstances keeping predictors might beneficial.always advisable remove one two correlated predictors model? Discuss advantages drawbacks approach, explain circumstances keeping predictors might beneficial.following descriptive methods, determine whether applies categorical data, continuous numerical data, . Provide brief explanation method used exploratory data analysis.\nHistograms\nBox plots\nDensity plots\nScatter plots\nSummary statistics\nCorrelation analysis\nContingency tables\nBar plots\nHeatmaps\nfollowing descriptive methods, determine whether applies categorical data, continuous numerical data, . Provide brief explanation method used exploratory data analysis.HistogramsBox plotsDensity plotsScatter plotsSummary statisticsCorrelation analysisContingency tablesBar plotsHeatmapsA telecommunications company analyzing customer data identify factors influencing churn. exploratory data analysis, discover customers high day minutes high evening minutes significantly higher churn rate. actionable insights company derive finding, might use information reduce customer attrition?telecommunications company analyzing customer data identify factors influencing churn. exploratory data analysis, discover customers high day minutes high evening minutes significantly higher churn rate. actionable insights company derive finding, might use information reduce customer attrition?Suppose conducting exploratory data analysis dataset 20 predictor variables. examining correlation matrix, find several pairs variables highly correlated (r > 0.9). address correlations ensure reliability interpretability predictive models? Describe steps take manage correlated variables effectively.Suppose conducting exploratory data analysis dataset 20 predictor variables. examining correlation matrix, find several pairs variables highly correlated (r > 0.9). address correlations ensure reliability interpretability predictive models? Describe steps take manage correlated variables effectively.Discuss importance considering statistical practical implications evaluating correlations exploratory data analysis. Provide example statistically significant correlation may real-world significance, explain essential consider aspects data analysis.Discuss importance considering statistical practical implications evaluating correlations exploratory data analysis. Provide example statistically significant correlation may real-world significance, explain essential consider aspects data analysis.important investigate multivariate relationships rather relying univariate analysis? Provide example interaction two variables reveals pattern missed separate univariate analyses.important investigate multivariate relationships rather relying univariate analysis? Provide example interaction two variables reveals pattern missed separate univariate analyses.data visualization aid exploratory data analysis process? Discuss least two specific examples visualizations provide insights summary statistics alone reveal.data visualization aid exploratory data analysis process? Discuss least two specific examples visualizations provide insights summary statistics alone reveal.Suppose discover customers high daytime call usage make frequent customer service calls likely churn. business actions taken based insight?Suppose discover customers high daytime call usage make frequent customer service calls likely churn. business actions taken based insight?Outliers can influential statistical modeling. possible causes outliers dataset? decide whether keep, modify, remove outlier?Outliers can influential statistical modeling. possible causes outliers dataset? decide whether keep, modify, remove outlier?context exploratory data analysis, explain missing values critical issue. different strategies handling missing values, circumstances appropriate?context exploratory data analysis, explain missing values critical issue. different strategies handling missing values, circumstances appropriate?","code":""},{"path":"chapter-EDA.html","id":"hands-on-practice-exploring-the-bank-dataset","chapter":"4 Exploratory Data Analysis","heading":"Hands-On Practice: Exploring the Bank Dataset","text":"hands-practice, explore bank dataset available R package liver. bank dataset related direct marketing campaigns Portuguese banking institution. campaigns conducted via phone calls, multiple contacts sometimes needed determine whether client subscribe term deposit. goal dataset predict whether client subscribe term deposit, explored using classification techniques Chapters 7 12.details dataset can found : https://rdrr.io/cran/liver/man/bank.html.can import dataset R follows:examine dataset’s structure, use:Dataset Overview: Report summary statistics dataset, including types variables. can infer structure data?Dataset Overview: Report summary statistics dataset, including types variables. can infer structure data?Target Variable Analysis: Generate bar plot target variable deposit using ggplot() function ggplot2. proportion clients subscribed term deposit?Target Variable Analysis: Generate bar plot target variable deposit using ggplot() function ggplot2. proportion clients subscribed term deposit?Binary Variable Exploration: Investigate binary categorical variables default, housing, loan. Create contingency tables bar plots visualize distributions. insights can draw variables?Binary Variable Exploration: Investigate binary categorical variables default, housing, loan. Create contingency tables bar plots visualize distributions. insights can draw variables?Exploring Numerical Variables: Analyze numerical variables dataset. Create histograms box plots visualize distributions. Identify patterns, skewness, unusual observations.Exploring Numerical Variables: Analyze numerical variables dataset. Create histograms box plots visualize distributions. Identify patterns, skewness, unusual observations.Outlier Detection: Identify whether numerical variables contain outliers. handle outliers maintain data integrity ensuring robust analysis?Outlier Detection: Identify whether numerical variables contain outliers. handle outliers maintain data integrity ensuring robust analysis?Correlation Analysis: Compute correlation matrix numerical variables. Identify pairs highly correlated variables. strategies use handle correlations avoid redundancy multicollinearity predictive modeling?Correlation Analysis: Compute correlation matrix numerical variables. Identify pairs highly correlated variables. strategies use handle correlations avoid redundancy multicollinearity predictive modeling?Key EDA Findings: Summarize key findings exploratory data analysis based exercises . preparing formal report, highlight notable patterns, relationships, anomalies?Key EDA Findings: Summarize key findings exploratory data analysis based exercises . preparing formal report, highlight notable patterns, relationships, anomalies?Business Implications: Based findings, actionable insights bank derive exploratory analysis? insights help optimizing marketing strategies improving customer targeting?Business Implications: Based findings, actionable insights bank derive exploratory analysis? insights help optimizing marketing strategies improving customer targeting?Multivariate Analysis: Investigate relationship number previous marketing campaign contacts (campaign) term deposit subscriptions (deposit). higher contact frequency correlate increased subscriptions? Use box plot bar chart support findings.Multivariate Analysis: Investigate relationship number previous marketing campaign contacts (campaign) term deposit subscriptions (deposit). higher contact frequency correlate increased subscriptions? Use box plot bar chart support findings.Feature Engineering Insight: Based EDA, propose least one new feature improve predictive power classification model term deposit subscriptions. Justify reasoning.Feature Engineering Insight: Based EDA, propose least one new feature improve predictive power classification model term deposit subscriptions. Justify reasoning.Seasonality Effects: Investigate whether time year influences term deposit subscriptions analyzing month variable. certain months higher success rate? Visualize pattern discuss potential business implications.Seasonality Effects: Investigate whether time year influences term deposit subscriptions analyzing month variable. certain months higher success rate? Visualize pattern discuss potential business implications.Effect Employment Type: Examine job variable relates term deposit subscriptions. job categories higher success rates? Present findings using suitable visualization discuss banks use insight targeted marketing.Effect Employment Type: Examine job variable relates term deposit subscriptions. job categories higher success rates? Present findings using suitable visualization discuss banks use insight targeted marketing.Interaction Effects: Analyze whether interactions different predictors, education job, influence likelihood subscribing term deposit. Use appropriate visualizations statistical summaries support findings.Interaction Effects: Analyze whether interactions different predictors, education job, influence likelihood subscribing term deposit. Use appropriate visualizations statistical summaries support findings.Effect Contact Duration: Investigate whether duration last contact (duration variable) strong relationship term deposit subscription. Visualize distribution discuss whether longer calls associated higher success rates.Effect Contact Duration: Investigate whether duration last contact (duration variable) strong relationship term deposit subscription. Visualize distribution discuss whether longer calls associated higher success rates.Comparison Campaign Outcomes: Compare subscription rates (deposit variable) across different types marketing campaigns (campaign variable). trends emerge, inform future marketing strategies?Comparison Campaign Outcomes: Compare subscription rates (deposit variable) across different types marketing campaigns (campaign variable). trends emerge, inform future marketing strategies?","code":"\nlibrary(liver)\ndata(bank)      str(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-statistics.html","id":"chapter-statistics","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5 Statistical Inference and Hypothesis Testing","text":"Statistical inference bridges gap observe sample want understand population. exploratory data analysis (EDA) helps us identify patterns relationships, statistical inference allows us determine whether patterns hold beyond sample—whether arisen chance. chapter, transition exploring data validating insights estimation, hypothesis testing, quantifying uncertainty.goals statistical inference can summarized three fundamental tasks:Estimating population characteristics, averages proportions, based sample data.Quantifying uncertainty measure confident can results.Testing hypotheses evaluate whether observed patterns statistically meaningful simply due random variation.tasks form foundation data-driven decision-making, enabling us distinguish meaningful insights statistical noise. chapter, explore three pillars—estimation, uncertainty, hypothesis testing—using intuitive explanations practical examples.statistical inference isn’t just applying formulas—’s also critical thinking. end chapter, ’ll develop two essential skills:detect statistical misuses misleading claims, helping critically evaluate data-driven arguments.avoid common pitfalls statistical analysis, ensuring conclusions sound defensible.interested art identifying statistical manipulation, Darrell Huff’s classic book, Lie Statistics, offers timeless lessons statistical skepticism. Understanding techniques valuable just avoiding errors also recognizing data used mislead.Let’s dive learn make statistical inferences confidence, curiosity, healthy dose skepticism.","code":""},{"path":"chapter-statistics.html","id":"estimation-using-data-to-make-predictions","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.1 Estimation: Using Data to Make Predictions","text":"Estimation fundamental aspect statistical inference allows us make informed guesses population based sample. Rather relying entire population, often impractical, use sample data estimate key characteristics averages proportions. instance, churn dataset, might want estimate:average number customer service calls among churners.proportion customers subscribed International Plan.two main types estimation:Point estimation provides single best guess population parameter, using sample mean estimate population mean.Interval estimation gives range plausible values (confidence interval) within true population parameter likely fall.Let’s explore examples:Example 5.1  estimate proportion churners dataset, use sample proportion point estimate population proportion. ’s calculate R:estimated proportion churners dataset 0.14, serving best guess proportion churners population.Example 5.2  Now, let’s estimate average number customer service calls customers churned. sample mean serves point estimate population mean:sample mean 4 calls, best estimate average number customer service calls among churners population.point estimates useful, provide information uncertainty. Confidence intervals help quantify precision estimate, explore next.","code":"\nlibrary(liver)\ndata(churn) \n\nprop.table(table(churn$churn))[\"yes\"]\n      yes \n   0.1414# Filter churners\nchurned_customers <- churn[churn$churn == \"yes\", ]\n\n# Calculate the mean\nmean_calls <- mean(churned_customers$customer.calls)\ncat(\"Point Estimate: Average Customer Service Calls for Churners:\", mean_calls)\n   Point Estimate: Average Customer Service Calls for Churners: 2.254597"},{"path":"chapter-statistics.html","id":"statistics-confidence-interval","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.2 Quantifying Uncertainty: Confidence Intervals","text":"Confidence intervals help quantify uncertainty estimating population parameters. Instead simply stating “average number customer service calls 4,” confidence interval provides range, “95% confident true average 3.8 4.2.” range accounts sampling variability, offering clearer picture reliable estimate .confidence interval consists point estimate, sample mean proportion, margin error, accounts uncertainty. general form confidence interval :\\[\n\\text{Point Estimate}  \\pm \\text{Margin Error}\n\\]population mean, confidence interval calculated :\\[\n\\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\times \\left( \\frac{s}{\\sqrt{n}} \\right),\n\\]\\(\\bar{x}\\) sample mean, \\(z_{\\frac{\\alpha}{2}}\\) critical value standard normal distribution (1.96 95% confidence level), \\(s\\) sample standard deviation, \\(n\\) sample size. concept illustrated Figure 5.1, interval centered around point estimate width depends margin error.\nFigure 5.1: Confidence interval population mean. interval centered around point estimate, width determined margin error. confidence level specifies probability interval contains true population parameter.\nSeveral factors influence width confidence interval. Larger sample sizes generally yield narrower intervals, increasing precision, higher variability data results wider intervals. choice confidence level also affects width; example, 99% confidence level produces wider interval 90% confidence level must capture possible values.Imagine want estimate average height students university. survey 10 students, confidence interval wide little data. survey 1,000 students, estimate becomes much precise, confidence interval shrinks. illustrates larger sample sizes lead reliable estimates—data reduces uncertainty results tighter confidence intervals.illustrate, suppose want estimate average number customer service calls among churners 95% confidence:computed interval [2.12, 2.39], can say 95% confidence true average number service calls churners falls within range.smaller sample sizes, better use t-distribution instead normal distribution, accounts added uncertainty estimating population standard deviation. adjustment applied automatically R using t.test() function:Confidence intervals particularly useful comparing groups. confidence intervals two groups, churners non-churners, overlap significantly, suggests meaningful differences behavior. providing range rather single estimate, confidence intervals help balance precision uncertainty, making valuable tool statistical inference.","code":"# Calculate mean and standard error\nmean_calls <- mean(churned_customers$customer.calls)\nse_calls <- sd(churned_customers$customer.calls) / sqrt(nrow(churned_customers))\n\n# Confidence Interval\nz_score <- 1.96  # For 95% confidence\nci_lower <- mean_calls - z_score * se_calls\nci_upper <- mean_calls + z_score * se_calls\n\ncat(\"95% Confidence Interval: [\", ci_lower, \",\", ci_upper, \"]\")\n   95% Confidence Interval: [ 2.120737 , 2.388457 ]t.test(churned_customers$customer.calls, conf.level = 0.95)$conf.int\n   [1] 2.120509 2.388685\n   attr(,\"conf.level\")\n   [1] 0.95"},{"path":"chapter-statistics.html","id":"hypothesis-testing","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3 Hypothesis Testing","text":"Hypothesis testing provides structured framework evaluating claims population parameters using sample data. helps us assess whether patterns observed exploratory analysis statistically significant simply result random variation. method fundamental data-driven decision-making, enabling us distinguish meaningful insights noise.core, hypothesis testing involves two competing statements population parameter:null hypothesis (\\(H_0\\)) represents default assumption status quo, often stating difference groups, effect treatment, relationship variables.alternative hypothesis (\\(H_a\\)) challenges \\(H_0\\), suggesting difference, effect, relationship exist.Using sample evidence, decide whether :Reject \\(H_0\\) conclude data supports \\(H_a\\).Fail reject \\(H_0\\), meaning evidence insufficient dismiss \\(H_0\\), though prove true.strength evidence \\(H_0\\) quantified using p-value, represents probability obtaining observed data—something extreme—\\(H_0\\) true. smaller p-value suggests stronger evidence \\(H_0\\).\\(p < 0.05\\), reject \\(H_0\\) conclude statistical evidence \\(H_a\\).\\(p > 0.05\\), fail reject \\(H_0\\), meaning evidence strong enough support \\(H_a\\).threshold decision-making called significance level (\\(\\alpha\\)), typically set 0.05 (5%). value represents maximum probability making Type error—incorrectly rejecting \\(H_0\\). fields errors serious consequences, medicine aerospace, stricter thresholds (e.g., \\(\\alpha = 0.01\\)) often used.simple takeaway, often emphasized hypothesis testing, :Reject \\(H_0\\) \\(p\\)-value < \\(\\alpha\\).example:\n- \\(p = 0.03\\) \\(\\alpha = 0.05\\), reject \\(H_0\\) \\(p < \\alpha\\).\n- \\(p = 0.12\\), fail reject \\(H_0\\) \\(p > \\alpha\\).Although p-values provide structured way make decisions, limitations. small p-value necessarily mean result practically important—indicates statistical significance. Large datasets can generate small p-values trivial effects, small datasets may fail detect meaningful differences. Additionally, binary reject/fail--reject approach can sometimes oversimplify interpretation.","code":""},{"path":"chapter-statistics.html","id":"types-of-hypothesis-tests","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.1 Types of Hypothesis Tests","text":"Depending research question, hypothesis tests can take different forms:Left-tailed test: alternative hypothesis states parameter less specified value (\\(H_a: \\theta < \\theta_0\\)). Example: Testing whether average number customer service calls less 3.Right-tailed test: alternative hypothesis states parameter greater specified value (\\(H_a: \\theta > \\theta_0\\)). Example: Testing whether churn rate greater 30%.Two-tailed test: alternative hypothesis states parameter equal specified value (\\(H_a: \\theta \\neq \\theta_0\\)), evaluating deviations either direction. Example: Testing whether mean monthly charges differ $50.useful analogy hypothesis testing criminal trial. null hypothesis (\\(H_0\\)) represents presumption innocence, alternative hypothesis (\\(H_a\\)) represents guilt. jury weighs evidence either rejects \\(H_0\\) (convicts defendant) fails reject \\(H_0\\) (acquits due insufficient evidence). Just juries can make errors, hypothesis tests also two types errors summarized Table 5.1.Table 5.1:  Possible outcomes hypothesis testing two correct decisions two types errors.Type Error (\\(\\alpha\\)) occurs \\(H_0\\) rejected even though true—similar convicting innocent person. Type II Error (\\(\\beta\\)) happens \\(H_0\\) rejected even though false—similar acquitting guilty person. probability Type error controlled chosen significance level (\\(\\alpha\\)), probability Type II error depends factors like sample size test sensitivity.","code":""},{"path":"chapter-statistics.html","id":"common-hypothesis-tests","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"Common Hypothesis Tests","text":"several widely used hypothesis tests, listed Table 5.1, suited different types data.Table 5.2:  Seven commonly used hypothesis tests, null hypotheses (\\(H_0\\)), types variables apply .test serves specific purpose. t-test compares means, Z-test compares proportions, Chi-square test assesses categorical relationships, ANOVA compares means across multiple groups. tests explored following sections practical examples.","code":""},{"path":"chapter-statistics.html","id":"one-sample-t-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.4 One-sample t-test","text":"one-sample t-test evaluates whether mean numerical variable population equal specified value. commonly used compare sample mean benchmark theoretical expectation. term “one-sample” reflects single group tested fixed value, “t-test” refers fact test statistic follows t-distribution, used compute p-value.hypotheses one-sample t-test depend research question can formulated different ways. two-tailed test assesses whether mean differs specified value, regardless direction. left-tailed test evaluates whether mean lower specified value, right-tailed test examines whether mean greater. mathematical formulation :Two-Tailed Test:\n\\[\n\\begin{cases}\nH_0:  \\mu   =  \\mu_0 \\\\\nH_a:  \\mu \\neq \\mu_0\n\\end{cases}\n\\]Left-Tailed Test:\n\\[\n\\begin{cases}\nH_0:  \\mu \\geq \\mu_0 \\\\\nH_a:  \\mu  <   \\mu_0\n\\end{cases}\n\\]Right-Tailed Test:\n\\[\n\\begin{cases}\nH_0:  \\mu \\leq \\mu_0 \\\\\nH_a:  \\mu >   \\mu_0\n\\end{cases}\n\\]p-value represents probability observing sample mean, extreme value, assumption null hypothesis true. smaller p-value provides stronger evidence \\(H_0\\). p-value less significance level (\\(\\alpha = 0.05\\)), null hypothesis rejected, indicating sample mean differs significantly specified value.following example demonstrates apply one-sample t-test R using t.test() function.Example 5.3  company assumes , average, customers make two service calls churning. test whether actual average number customer service calls among churners differs assumed value, conduct one-sample t-test using churn dataset provided liver package.conduct test, set following hypotheses:Null Hypothesis (\\(H_0\\)): \\(H_0: \\mu = 2\\) (average number customer service calls 2.)Alternative Hypothesis (\\(H_a\\)): \\(H_a: \\mu \\neq 2\\) (average number customer service calls 2.)can present hypotheses mathematical form :\n\\[\n\\begin{cases}\n    H_0: \\mu = 2   \\\\\n    H_a: \\mu \\neq 2  \n\\end{cases}\n\\]begin loading churn dataset filtering customers churned:Now, conduct two-tailed one-sample t-test R using t.test() function; want know functionality t.test() function, can find typing ?t.test R console.output includes p-value, test statistic, degrees freedom, confidence interval population mean. Since p-value = 2^{-4} less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). indicate sufficient evidence, 5% significance level, conclude true average number customer service calls differs 2.test also provides 95% confidence interval, [2.12, 2.39], represents range plausible values true population mean. Since 2 outside interval, evidence true average number service calls different assumed value. Additionally, sample mean, 2.25, reported best estimate population mean.Since sample standard deviation used place population standard deviation, test statistic follows t-distribution \\(n - 1\\) degrees freedom. measures far sample mean deviates hypothesized mean terms standard error. larger absolute value indicates stronger evidence \\(H_0\\).one-sample t-test provides structured approach comparing sample mean predefined benchmark. determines statistical significance also offers additional insights confidence interval, sample mean, test statistic. statistical significance important, practical relevance must also evaluated determine whether observed difference meaningful real-world implications. Even statistically significant difference detected, magnitude difference determines whether real-world implications. deviation 0.1 calls may negligible, whereas difference two calls impact customer service strategies.integrating statistical inference domain knowledge, one-sample t-test allows analysts determine whether deviations expectations statistically significant practically meaningful.","code":"\nlibrary(liver)  # Load the liver package\ndata(churn)     # Load the churn dataset\n\n# Filter churned customers\nchurned_customers <- churn[churn$churn == \"yes\", ]t_test <- t.test(churned_customers$customer.calls, mu = 2)\nt_test\n   \n    One Sample t-test\n   \n   data:  churned_customers$customer.calls\n   t = 3.7278, df = 706, p-value = 0.0002086\n   alternative hypothesis: true mean is not equal to 2\n   95 percent confidence interval:\n    2.120509 2.388685\n   sample estimates:\n   mean of x \n    2.254597"},{"path":"chapter-statistics.html","id":"hypothesis-testing-for-proportion","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.5 Hypothesis Testing for Proportion","text":"test proportion evaluates whether proportion (\\(\\pi\\)) specific category population aligns hypothesized value (\\(\\pi_0\\)). particularly useful binary categorical variables, observations fall one two groups, churned vs. churned. test helps determine whether observed sample proportion deviates significantly specified benchmark, making valuable business scientific contexts.instance, company might want assess whether proportion churners population aligns expected value based historical data industry standards. following example demonstrates apply proportion test R using prop.test() function.Example 5.4  company assumes 15% customers churn. test whether actual churn rate churn dataset differs assumption, conduct proportion test. hypotheses :\\[\n\\begin{cases}\nH_0: \\pi  =   0.15 \\\\\nH_a: \\pi \\neq 0.15\n\\end{cases}\n\\]test performed R using prop.test() function. like explore details function, can type ?prop.test R console.output provides key results, including p-value, confidence interval, sample proportion. results interpreted follows:p-value indicates probability obtaining observed sample proportion assumed population proportion true. Since p-value = 0.0923, greater \\(\\alpha = 0.05\\), reject null hypothesis. means insufficient evidence conclude churn rate population differs 15%. case, report:“statistically significant evidence suggest population proportion churners deviates 15%.”\np-value smaller 0.05, reject null hypothesis, concluding churn rate significantly different 15%.test also provides 95% confidence interval, [0.13, 0.15], represents plausible range true population proportion (\\(\\pi\\)). 0.15 lies within interval, supports failing reject \\(H_0\\). 0.15 outside interval, strengthens evidence \\(H_0\\).Additionally, test reports sample proportion, 0.14, observed proportion churners dataset. value serves estimate true population proportion.test helps assess whether observed churn rate aligns company’s expectation. p-value determines statistical significance, confidence interval sample proportion provide additional context interpretation. considering statistical results practical implications, businesses can evaluate whether assumptions churn accurate require adjustment.","code":"prop_test <- prop.test(x = sum(churn$churn == \"yes\"), \n                       n = nrow(churn), \n                       p = 0.15)\nprop_test\n   \n    1-sample proportions test with continuity correction\n   \n   data:  sum(churn$churn == \"yes\") out of nrow(churn), null probability 0.15\n   X-squared = 2.8333, df = 1, p-value = 0.09233\n   alternative hypothesis: true p is not equal to 0.15\n   95 percent confidence interval:\n    0.1319201 0.1514362\n   sample estimates:\n        p \n   0.1414"},{"path":"chapter-statistics.html","id":"two-sample-t-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.6 Two-sample T-test","text":"two-sample t-test, also known Student’s t-test, statistical method used compare means numerical variable two independent groups. assesses whether observed difference group means statistically significant simply due random variation. test named William Sealy Gosset, worked Guinness Brewery Dublin published pseudonym “Student” maintain confidentiality regarding statistical quality control methods.Section 4.5 previous chapter, explored relationship International Calls (intl.calls) churn status using visualizations like box plots density plots. visualizations help identify potential differences, statistical testing quantifies likelihood differences due chance.boxplot (left) density plot (right) illustrate distributions intl.calls churners non-churners. visualizations suggest minor differences, perform two-sample t-test assess whether differences statistically significant.conduct test, first establish hypotheses:Null Hypothesis (\\(H_0\\)): mean number international calls churners non-churners (\\(\\mu_1 = \\mu_2\\)).Alternative Hypothesis (\\(H_a\\)): mean number international calls differs churners non-churners (\\(\\mu_1 \\neq \\mu_2\\)).can also expressed mathematically :\n\\[\n\\begin{cases}\n    H_0: \\mu_1 = \\mu_2   \\\\\n    H_a: \\mu_1 \\neq \\mu_2\n\\end{cases}\n\\]perform test R using t.test() function:function evaluates difference means two groups (churn = \"yes\" vs. churn = \"\") provides p-value, confidence interval, descriptive statistics.p-value = 0.0014. Since value less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis, concluding mean number international calls differs significantly churners non-churners. 95% confidence interval = [-0.53, -0.13] provides range plausible values true difference means. interval include zero, statistical evidence two groups differ significantly average number international calls. test output also provides sample means:\n- Mean churners = 4.15\n- Mean non-churners = 4.48These values allow direct comparison international call usage churners non-churners. instance, churners made average 1.5 international calls non-churners made 2.3 calls, suggests churners tend make fewer international calls.two-sample t-test assumes two groups independent variable interest normally distributed within group sample sizes small. larger samples, Central Limit Theorem ensures validity test even normality strictly met. minor deviations normality generally acceptable, large departures may require alternative tests, Mann-Whitney U test.business perspective, test results suggest international call frequency relevant factor churn. Customers churn tend make fewer international calls. Companies may explore whether international call costs contribute customer churn. higher costs deter international calls, targeted discounts low-usage customers encourage engagement improve retention. However, statistical significance always imply practical significance. Even churners make fewer international calls average, actual impact churn evaluated conjunction variables.Although example uses two-tailed test detect difference means, one-tailed test used research question specifies directional hypothesis. instance, company hypothesizes churners make fewer international calls non-churners, one-tailed test increase test’s sensitivity.two-sample t-test powerful widely used method comparing group means. provides statistical foundation validating insights suggested exploratory data analysis. integrating graphical exploration hypothesis testing, analysts can make well-informed inferences derive actionable business insights.","code":"t_test_calls <- t.test(intl.calls ~ churn, data = churn)\nt_test_calls\n   \n    Welch Two Sample t-test\n   \n   data:  intl.calls by churn\n   t = -3.2138, df = 931.13, p-value = 0.001355\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    -0.5324872 -0.1287201\n   sample estimates:\n   mean in group yes  mean in group no \n            4.151344          4.481947"},{"path":"chapter-statistics.html","id":"two-sample-z-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.7 Two-Sample Z-Test","text":"previous section, applied two-sample t-test compare mean number international calls churners non-churners. t-test useful assessing differences numerical variables, many business scientific questions involve categorical variables, proportions rather means interest. two-sample Z-test designed compare proportions two independent groups, determining whether observed difference proportions statistically significant. test particularly valuable analyzing binary categorical variables, customer churn subscription status.Section 4.4 previous chapter, examined relationship Voice Mail Plan (voice.plan) churn status using bar plots. visualizations suggest potential differences churn rates customers without Voice Mail Plan, statistical testing quantifies whether differences statistically significant.first bar plot (left) shows raw counts churners non-churners across two categories Voice Mail Plan (Yes ), second plot (right) displays proportions, allowing direct comparison churn rates. visualizations suggest customers without Voice Mail Plan may higher churn rate, hypothesis testing needed confirm whether difference statistically meaningful.formally test whether proportion churners Voice Mail Plan differs proportion non-churners plan, establish following hypotheses:Null Hypothesis (\\(H_0\\)): \\(\\pi_1 = \\pi_2\\)\n(proportions customers Voice Mail Plan churners non-churners.)Null Hypothesis (\\(H_0\\)): \\(\\pi_1 = \\pi_2\\)\n(proportions customers Voice Mail Plan churners non-churners.)Alternative Hypothesis (\\(H_a\\)): \\(\\pi_1 \\neq \\pi_2\\)\n(proportions customers Voice Mail Plan differ churners non-churners.)Alternative Hypothesis (\\(H_a\\)): \\(\\pi_1 \\neq \\pi_2\\)\n(proportions customers Voice Mail Plan differ churners non-churners.)can also expressed mathematically :\n\\[\n\\begin{cases}\n    H_0: \\pi_1 = \\pi_2   \\\\\n    H_a: \\pi_1 \\neq \\pi_2\n\\end{cases}\n\\]perform Z-test R, begin creating contingency table summarize counts customers without Voice Mail Plan churner non-churner groups. can done using table() function:table displays count churners non-churners without Voice Mail Plan. conduct Z-test, use prop.test() function:output provides p-value, confidence interval, sample proportions. Since p-value (0) smaller significance level (\\(\\alpha = 0.05\\)), reject null hypothesis. result suggests proportion customers Voice Mail Plan differs significantly churners non-churners.test also provides 95% confidence interval = [-0.1702, -0.1101] difference proportions. Since interval include zero, reinforces conclusion difference statistically significant. Additionally, sample proportions—0.1443 churners 0.2844 non-churners—provide insight magnitude difference.business perspective, finding suggests customers without Voice Mail Plan may likely churn. Companies leverage information encouraging Voice Mail Plan subscriptions among -risk customers investigating whether plan improves customer satisfaction retention. Companies leverage information encouraging Voice Mail Plan subscriptions among -risk customers investigating whether plan improves customer satisfaction retention. Although Z-test shows statistically significant difference, businesses evaluate whether promoting Voice Mail Plan meaningfully reduces churn rates justifies marketing investment.two-sample Z-test provides formal approach comparing proportions groups, complementing exploratory data analysis. integrating statistical testing business insights, companies can validate patterns take targeted actions reduce churn.","code":"table_plan = table(churn$churn, churn$voice.plan, dnn = c(\"churn\", \"voice.plan\"))\ntable_plan\n        voice.plan\n   churn  yes   no\n     yes  102  605\n     no  1221 3072z_test = prop.test(table_plan)\nz_test\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  table_plan\n   X-squared = 60.552, df = 1, p-value = 7.165e-15\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.1701734 -0.1101165\n   sample estimates:\n      prop 1    prop 2 \n   0.1442716 0.2844165"},{"path":"chapter-statistics.html","id":"chi-square-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.8 Chi-square Test","text":"two-sample Z-test effective comparing proportions two groups, many real-world analyses involve categorical variables two levels. Chi-square test allows us assess whether multiple categorical groups associated, providing broader framework understanding categorical relationships. makes particularly useful understanding customer behaviors business outcomes across multiple categories.Unlike Z-test, focuses comparing proportions two groups, Chi-square test evaluates whether distributions across multiple categories differ significantly expected independence. provides formal way test relationships categorical variables widely used marketing analysis, customer segmentation, business decision-making.illustrate, examine whether marital status associated purchasing deposit bank dataset (available liver package). dataset revisited Chapters 7 12 classification modeling. variable marital three categories: “divorced,” “married,” “single,” target variable deposit two categories: “yes” (customers purchased deposit) “” (customers ). goal determine whether marital status influences deposit purchases.begin visualizing relationship marital deposit using bar plots:first bar plot (left) displays raw counts deposit purchases across marital categories, second plot (right) presents relative proportions. Visual inspection suggests differences deposit purchase rates marital status, statistical test needed confirm whether differences significant.summarize observed counts contingency table:formally test independence, define hypotheses:\n\\[\n\\begin{cases}\n    H_0: \\pi_{divorced, \\ yes} = \\pi_{married, \\ yes} = \\pi_{single, \\ yes}  \\\\\n    H_a: \\ least \\ one \\ \\ \\ claims \\ \\ H_0 \\ \\ wrong.\n\\end{cases}\n\\]\nChi-square test applied using chisq.test() function:output includes p-value, Chi-square test statistic, degrees freedom, expected frequencies \\(H_0\\). p-value = 7.3735354^{-5} smaller \\(\\alpha = 0.05\\), reject null hypothesis, concluding marital status deposit purchases independent. means least one marital group differs significantly others deposit purchase rates.Examining expected frequencies can reveal marital groups contribute observed association. particular group much higher lower deposit purchase rate expected, marketing efforts can tailored accordingly.business perspective, findings suggest banks may benefit personalizing marketing strategies based marital status. example, married customers significantly likely purchase deposits, targeted promotional campaigns emphasize financial planning families. Conversely, single customers exhibit lower deposit adoption rates, banks might develop incentive programs tailored financial goals.Chi-square test powerful tool identifying relationships categorical variables. integrating visual analysis, contingency tables, statistical hypothesis testing, businesses can make data-driven decisions optimize customer engagement product offerings.","code":"table_marital <- table(bank$deposit, bank$marital, dnn = c(\"deposit\", \"marital\"))\ntable_marital\n          marital\n   deposit divorced married single\n       no       451    2520   1029\n       yes       77     277    167chisq_test <- chisq.test(table_marital)\nchisq_test\n   \n    Pearson's Chi-squared test\n   \n   data:  table_marital\n   X-squared = 19.03, df = 2, p-value = 7.374e-05"},{"path":"chapter-statistics.html","id":"analysis-of-variance-anova-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.9 Analysis of Variance (ANOVA) Test","text":"previous sections, explored hypothesis tests compare two groups, two-sample t-test Z-test. However, many real-world scenarios, categorical variables two levels. cases, Analysis Variance (ANOVA) provides systematic way determine whether numerical variable differs across multiple groups. evaluates whether least one group mean differs significantly others. ANOVA especially useful analyzing relationship numerical variable categorical variable multiple levels, providing formal way determine categorical variable impacts numerical variable. test relies F-distribution assess whether observed differences means statistically significant.illustrate, let’s analyze relationship variable cut target variable price popular diamonds dataset (available ggplot2 package). See Section X.X overview dataset. variable cut five categories (“Fair,” “Good,” “Good,” “Premium,” “Ideal”), price numerical. objective test whether mean price diamonds differs across five cut categories.begin box plot visualize distribution diamond prices category cut:box plot displays spread median prices diamonds cut category. differences medians ranges suggest cut quality might influence price, statistical testing required confirm whether differences significant. apply ANOVA test formally assess relationship.test whether mean prices differ cut type, set following hypotheses:\\[\n\\begin{cases}\n    H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 \\quad \\text{(group means equal.)} \\\\\n    H_a: \\text{least one group mean differs others.}  \n\\end{cases}\n\\]conduct ANOVA test R, use aov() function:output provides test statistic (F-value), degrees freedom, p-value. Since p-value smaller significance level (\\(\\alpha = 0.05\\)), reject null hypothesis. indicates variable cut significant impact price diamonds.Rejecting \\(H_0\\) ANOVA specify groups differ. identify differences, post-hoc tests Tukey’s Honestly Significant Difference (Tukey HSD) test necessary. tests control multiple comparisons pinpointing significant pairwise differences. example, apply Tukey’s test determine cut categories (e.g., “Ideal” vs. “Good”) drive observed differences.Understanding impact diamond cut price crucial pricing strategies consumer insights. higher-quality cuts command significantly higher prices, retailers may adjust marketing efforts accordingly. Conversely, certain mid-tier cuts show meaningful price differences, companies might reconsider pricing models enhance competitiveness.ANOVA test provides structured approach evaluating whether categorical variable multiple levels influences numerical variable. case, relationship cut price suggests diamond cut type important predictor price, offering valuable insights quality impacts cost. integrating statistical testing business insights, analysts can determine whether categorical variables meaningful effects use knowledge inform data-driven decisions.","code":"\ndata(diamonds)   \n\nggplot(data = diamonds) + \n  geom_boxplot(aes(x = cut, y = price, fill = cut)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\", \"skyblue1\", \"gold1\", \"lightcoral\"))anova_test <- aov(price ~ cut, data = diamonds)\nsummary(anova_test)\n                  Df    Sum Sq   Mean Sq F value Pr(>F)    \n   cut             4 1.104e+10 2.760e+09   175.7 <2e-16 ***\n   Residuals   53935 8.474e+11 1.571e+07                   \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"chapter-statistics.html","id":"correlation-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.10 Correlation Test","text":"previous sections, explored hypothesis tests comparing means proportions across groups. analyzing relationships two numerical variables, correlation testing provides formal method assess whether significant linear association exists. correlation test evaluates strength direction relationship testing null hypothesis population correlation coefficient (\\(\\rho\\)) equal zero. test particularly useful understanding two continuous variables co-vary, can inform business strategies, pricing models, predictive analytics.illustrate, examine whether significant relationship exists carat (diamond weight) price diamonds dataset (available ggplot2 package). Since larger diamonds generally expensive, expect positive correlation variables. scatter plot provides initial visual assessment relationship:scatter plot shows clear upward trend, suggesting carat increases, price. However, visualizations alone confirm statistical significance. formally test relationship, establish following hypotheses:\\[\n\\begin{cases}\n    H_0: \\rho   =  0 \\quad \\text{(linear correlation `carat` `price`.)} \\\\\n    H_a: \\rho \\neq 0 \\quad \\text{(significant linear correlation `carat` `price`.)}\n\\end{cases}\n\\]conduct correlation test R, use cor.test() function:output provides key results, including p-value, correlation coefficient, confidence interval:p-value: p-value = 0 smaller significance level (\\(\\alpha = 0.05\\)), reject \\(H_0\\), confirming correlation statistically significant.Correlation Coefficient: correlation coefficient (\\(r = 0.92\\)) quantifies strength direction relationship. value close 1 indicates strong positive correlation, value near 0 suggests linear association.Confidence Interval: 95% confidence interval [0.92, 0.92] provides plausible range true population correlation (\\(\\rho\\)). interval include 0, supports rejecting \\(H_0\\) confirms meaningful association.correlation coefficient 0.92 suggests strong positive relationship carat price, meaning larger diamonds tend expensive. small p-value confirms pattern unlikely due random variation, confidence interval provides estimate precisely can measure correlation.Beyond statistical significance, relationship practical implications diamond pricing strategies. correlation particularly strong, pricing models rely carat key determinant value. However, variability remains high despite significant correlation, additional factors—diamond clarity, cut, market conditions—may play influential role. analysis involve multivariate regression assess carat interacts attributes predicting price.integrating visualization, statistical inference, business insights, correlation test offers robust framework understanding numerical relationships. approach ensures observed trends statistically sound practically meaningful, laying foundation advanced modeling techniques.","code":"\nggplot(data = diamonds) +\n    geom_point(aes(x = carat, y = price), colour = \"blue\") +\n    labs(x = \"Carat\", y = \"Price\") cor_test <- cor.test(diamonds$carat, diamonds$price)\ncor_test\n   \n    Pearson's product-moment correlation\n   \n   data:  diamonds$carat and diamonds$price\n   t = 551.41, df = 53938, p-value < 2.2e-16\n   alternative hypothesis: true correlation is not equal to 0\n   95 percent confidence interval:\n    0.9203098 0.9228530\n   sample estimates:\n         cor \n   0.9215913"},{"path":"chapter-statistics.html","id":"wrapping-up","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.11 Wrapping Up","text":"chapter provided foundation statistical inference, beginning estimation, explored point estimates confidence intervals help quantify population parameters accounting uncertainty. introduced hypothesis testing, learning formulate null alternative hypotheses, compute test statistics, interpret p-values make informed decisions. practical examples, applied various statistical tests, including t-tests comparing means, proportion tests categorical data, ANOVA assessing differences across multiple groups, Chi-square test correlation analysis uncovering relationships variables. Together, tools form robust framework extracting insights answering key research questions.Statistical inference plays critical role data-driven decision-making, helping analysts distinguish meaningful patterns random variation. methods widely used business research, evaluating marketing strategies predicting customer behavior. However, reliable conclusions require statistical significance. essential check assumptions, contextualize results, integrate domain knowledge ensure findings accurate actionable.\nstatistical inference hypothesis testing essential tools data science, fall outside scope machine learning. interested exploring topics , recommend introductory statistics textbooks Intuitive Introductory Statistics Wolfe Schneider.5In next chapter, transition statistical inference predictive modeling, focusing partition datasets effectively. Just hypothesis testing helps determine whether patterns data real, proper data partitioning ensures machine learning models generalize well unseen data. move forward, ensuring data validity model robustness key building reliable predictive systems.","code":""},{"path":"chapter-statistics.html","id":"exercises-3","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.12 Exercises","text":"","code":""},{"path":"chapter-statistics.html","id":"conceptual-questions-1","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"Conceptual Questions","text":"hypothesis testing important data science? Explain role making data-driven decisions complements exploratory data analysis.hypothesis testing important data science? Explain role making data-driven decisions complements exploratory data analysis.difference confidence interval hypothesis test? provide different ways drawing conclusions population parameters?difference confidence interval hypothesis test? provide different ways drawing conclusions population parameters?p-value represents probability observing sample data, something extreme, assuming null hypothesis true. p-values interpreted, p-value 0.001 two-sample t-test necessarily evidence practical significance?**p-value represents probability observing sample data, something extreme, assuming null hypothesis true. p-values interpreted, p-value 0.001 two-sample t-test necessarily evidence practical significance?**Explain concepts Type Type II errors hypothesis testing. important balance risks errors designing statistical tests?Explain concepts Type Type II errors hypothesis testing. important balance risks errors designing statistical tests?hypothesis test, failing reject null hypothesis imply null hypothesis true. Explain case discuss implications result practice.hypothesis test, failing reject null hypothesis imply null hypothesis true. Explain case discuss implications result practice.working small sample sizes, t-distribution used instead normal distribution? shape t-distribution change sample size increases?working small sample sizes, t-distribution used instead normal distribution? shape t-distribution change sample size increases?One-tailed two-tailed hypothesis tests serve different purposes. one-tailed test appropriate two-tailed test? Provide example type test applicable.One-tailed two-tailed hypothesis tests serve different purposes. one-tailed test appropriate two-tailed test? Provide example type test applicable.two-sample Z-test Chi-square test analyze categorical data serve different purposes. differ, one preferred ?two-sample Z-test Chi-square test analyze categorical data serve different purposes. differ, one preferred ?Analysis Variance (ANOVA) test designed compare means across multiple groups. can’t multiple t-tests used instead? advantage using ANOVA context?Analysis Variance (ANOVA) test designed compare means across multiple groups. can’t multiple t-tests used instead? advantage using ANOVA context?","code":""},{"path":"chapter-statistics.html","id":"hands-on-practice-hypothesis-testing-in-r","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"Hands-On Practice: Hypothesis Testing in R","text":"following exercises, use churn, bank, marketing, diamonds datasets available liver ggplot2 packages. previously used churn, bank, diamonds datasets earlier chapters. Chapter 10, introduce marketing dataset regression analysis.load datasets, use following commands:interested knowing 90% confidence interval population mean variable “night.calls” churn dataset. R, can obtain confidence interval population mean using t.test() function follows:Interpret confidence interval context customer service calls made night. Report 99% confidence interval population mean “night.calls” compare 90% confidence interval. interval wider, indicate precision estimates? increasing confidence level result wider interval, impact decision-making business context?Subgroup analyses help identify behavioral patterns specific customer segments. churn dataset, focus customers International Plan Voice Mail Plan make 220 daytime minutes calls. create subset, use:Next, compute 95% confidence interval proportion churners subset using prop.test():Compare confidence interval overall churn rate dataset (see Section 5.2). insights can drawn customer segment, might inform retention strategies?churn dataset, test whether mean number customer service calls (customer.calls) greater 1.5 significance level 0.01. right-tailed test formulated :\\[\n\\begin{cases}\n  H_0:  \\mu \\leq 1.5 \\\\\n  H_a:  \\mu > 1.5\n\\end{cases}\n\\]Since level significance \\(\\alpha = 0.01\\), confidence level \\(1-\\alpha = 0.99\\). perform test using:Report p-value determine whether reject null hypothesis \\(\\alpha=0.01\\). Explain decision discuss implications context customer service interactions.churn dataset, test whether proportion churners (\\(\\pi\\)) less 0.14 significance level \\(\\alpha=0.01\\). confidence level \\(99\\%\\), corresponding \\(1-\\alpha = 0.99\\). test conducted R using:State null alternative hypotheses. Report p-value determine whether reject null hypothesis \\(\\alpha=0.01\\). Explain conclusion potential impact customer retention strategies.churn dataset, examine whether number customer service calls (customer.calls) differs churners non-churners. test , perform two-sample t-test:State null alternative hypotheses. Determine whether reject null hypothesis significance level \\(\\alpha=0.05\\). Report p-value interpret results, explaining whether evidence relationship churn status customer service call frequency.marketing dataset, test whether positive relationship revenue spend significance level \\(\\alpha = 0.025\\). perform one-tailed correlation test using:State null alternative hypotheses. Report p-value determine whether reject null hypothesis. Explain decision discuss implications understanding relationship marketing spend revenue.churn dataset, variable “day.mins”, test whether mean number “Day Minutes” greater 180. Set level significance 0.05.churn dataset, variable “day.mins”, test whether mean number “Day Minutes” greater 180. Set level significance 0.05.churn dataset, variable “intl.plan” test \\(\\alpha=0.05\\) weather proportion customers international plan less 0.15.churn dataset, variable “intl.plan” test \\(\\alpha=0.05\\) weather proportion customers international plan less 0.15.churn dataset, test whether relationship target variable “churn” variable “intl.charge” \\(\\alpha=0.05\\).churn dataset, test whether relationship target variable “churn” variable “intl.charge” \\(\\alpha=0.05\\).bank dataset, test whether relationship target variable “deposit” variable “education” \\(\\alpha=0.05\\).bank dataset, test whether relationship target variable “deposit” variable “education” \\(\\alpha=0.05\\).Compute proportion customers churn dataset International Plan (intl.plan). Construct 95% confidence interval proportion using R, interpret confidence interval context customer subscriptions.Compute proportion customers churn dataset International Plan (intl.plan). Construct 95% confidence interval proportion using R, interpret confidence interval context customer subscriptions.Using churn dataset, test whether average number daytime minutes (day.mins) churners differs significantly 200 minutes. Conduct one-sample t-test R interpret results relation customer behavior.Using churn dataset, test whether average number daytime minutes (day.mins) churners differs significantly 200 minutes. Conduct one-sample t-test R interpret results relation customer behavior.Compare average number international calls (intl.calls) churners non-churners. Perform two-sample t-test evaluate whether observed differences means statistically significant.Compare average number international calls (intl.calls) churners non-churners. Perform two-sample t-test evaluate whether observed differences means statistically significant.Test whether proportion customers Voice Mail Plan (voice.plan) differs churners non-churners. Use two-sample Z-test R interpret results, considering implications customer retention strategies.Test whether proportion customers Voice Mail Plan (voice.plan) differs churners non-churners. Use two-sample Z-test R interpret results, considering implications customer retention strategies.Investigate whether marital status (marital) associated deposit subscription (deposit) bank dataset. Construct contingency table perform Chi-square test assess whether marital status significant impact deposit purchasing behavior.Investigate whether marital status (marital) associated deposit subscription (deposit) bank dataset. Construct contingency table perform Chi-square test assess whether marital status significant impact deposit purchasing behavior.Using diamonds dataset, test whether mean price diamonds differs across different diamond cuts (cut). Conduct ANOVA test interpret results. test finds significant differences, discuss post-hoc tests used explore findings.Using diamonds dataset, test whether mean price diamonds differs across different diamond cuts (cut). Conduct ANOVA test interpret results. test finds significant differences, discuss post-hoc tests used explore findings.Assess correlation carat price diamonds dataset. Perform correlation test R visualize relationship using scatter plot. Interpret results context diamond pricing.Assess correlation carat price diamonds dataset. Perform correlation test R visualize relationship using scatter plot. Interpret results context diamond pricing.Construct 95% confidence interval mean number customer service calls (customer.calls) among churners. Explain confidence interval helps quantify uncertainty might inform business decisions regarding customer support.Construct 95% confidence interval mean number customer service calls (customer.calls) among churners. Explain confidence interval helps quantify uncertainty might inform business decisions regarding customer support.Take random sample 100 observations churn dataset test whether average eve.mins differs 200. Repeat test using sample 1000 observations. Compare results discuss sample size affects hypothesis testing statistical power.Take random sample 100 observations churn dataset test whether average eve.mins differs 200. Repeat test using sample 1000 observations. Compare results discuss sample size affects hypothesis testing statistical power.Suppose hypothesis test indicates customers Voice Mail Plan significantly less likely churn (p < 0.01). potential business strategies company implement based finding? Beyond statistical significance, additional factors considered making marketing decisions?Suppose hypothesis test indicates customers Voice Mail Plan significantly less likely churn (p < 0.01). potential business strategies company implement based finding? Beyond statistical significance, additional factors considered making marketing decisions?","code":"\nlibrary(liver)\nlibrary(ggplot2)   \n\n# To import the datasets\ndata(churn)  \ndata(bank)  \ndata(marketing, package = \"liver\")  \ndata(diamonds)  t.test(x = churn$night.calls, conf.level = 0.90)$\"conf.int\"\n   [1]  99.45484 100.38356\n   attr(,\"conf.level\")\n   [1] 0.9\nsub_churn = subset(churn, (intl.plan == \"yes\") & (voice.plan == \"yes\") & (day.mins > 220)) prop.test(table(sub_churn$churn), conf.level = 0.95)$\"conf.int\"\n   [1] 0.2595701 0.5911490\n   attr(,\"conf.level\")\n   [1] 0.95t.test(x = churn$customer.calls, \n        mu = 1.5, \n        alternative = \"greater\", \n        conf.level = 0.99)\n   \n    One Sample t-test\n   \n   data:  churn$customer.calls\n   t = 3.8106, df = 4999, p-value = 7.015e-05\n   alternative hypothesis: true mean is greater than 1.5\n   99 percent confidence interval:\n    1.527407      Inf\n   sample estimates:\n   mean of x \n      1.5704prop.test(table(churn$churn), \n           p = 0.14, \n           alternative = \"less\", \n           conf.level = 0.99)\n   \n    1-sample proportions test with continuity correction\n   \n   data:  table(churn$churn), null probability 0.14\n   X-squared = 0.070183, df = 1, p-value = 0.6045\n   alternative hypothesis: true p is less than 0.14\n   99 percent confidence interval:\n    0.0000000 0.1533547\n   sample estimates:\n        p \n   0.1414t.test(customer.calls ~ churn, data = churn)\n   \n    Welch Two Sample t-test\n   \n   data:  customer.calls by churn\n   t = 11.292, df = 804.21, p-value < 2.2e-16\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    0.6583525 0.9353976\n   sample estimates:\n   mean in group yes  mean in group no \n            2.254597          1.457722cor.test(x = marketing$spend, \n         y = marketing$revenue, \n         alternative = \"greater\", \n         conf.level = 0.975)\n   \n    Pearson's product-moment correlation\n   \n   data:  marketing$spend and marketing$revenue\n   t = 7.9284, df = 38, p-value = 7.075e-10\n   alternative hypothesis: true correlation is greater than 0\n   97.5 percent confidence interval:\n    0.6338152 1.0000000\n   sample estimates:\n        cor \n   0.789455"},{"path":"chapter-modeling.html","id":"chapter-modeling","chapter":"6 Preparing Data for Modeling","heading":"6 Preparing Data for Modeling","text":"can build reliable machine learning models, must ensure data well-prepared. previous chapters established foundation addressing key steps Data Science Workflow (Figure 2.3). Now, focus transitioning data exploration model building.Chapter 2.4, discussed defining problem aligning objectives data-driven strategies. Chapter 3 addressed handling missing values, outliers, data transformations create clean dataset. Chapter 4, visualized data uncover patterns, Chapter 5 introduced statistical inference, including hypothesis testing feature selection—tools help us validate data partitioning.diving machine learning, must complete Setup Phase, ensures dataset structured robust model development. phase involves three essential steps:Partitioning Data: Splitting dataset training testing sets create clear separation model learning evaluation.Validating Partition: Ensuring split representative unbiased insights training generalize new data.Balancing Training Dataset: Addressing potential class imbalances categorical targets prevent biased models.Although often overlooked, steps critical ensuring modeling process rigorous, fair, effective. Students often ask, “necessary partition data?” “need follow specific steps?” important questions, address throughout chapter. , ’s useful briefly examine data science process aligns diverges statistical inference. Understanding similarities differences helps bridge traditional statistics practical demands modern machine learning.","code":""},{"path":"chapter-modeling.html","id":"statistical-inference-in-the-context-of-data-science","chapter":"6 Preparing Data for Modeling","heading":"6.1 Statistical Inference in the Context of Data Science","text":"Although statistical inference remains fundamental tool data science, role shifts preparing data modeling, goals applications differ. traditional inference focuses drawing conclusions populations sample data, machine learning prioritizes predictive accuracy generalization.Statistical inference data science diverge two key ways applied modeling tasks:Significance Practicality: large datasets containing thousands even millions observations, nearly detected relationship can become statistically significant. However, statistical significance always translate practical importance. machine learning model might identify minute effect size statistically valid negligible impact decision-making. modeling, focus shifts statistical significance assessing whether effect strong enough meaningfully influence predictions.Significance Practicality: large datasets containing thousands even millions observations, nearly detected relationship can become statistically significant. However, statistical significance always translate practical importance. machine learning model might identify minute effect size statistically valid negligible impact decision-making. modeling, focus shifts statistical significance assessing whether effect strong enough meaningfully influence predictions.Exploration vs. Hypothesis Testing: Traditional statistical inference begins predefined hypothesis, testing whether new treatment improves outcomes compared control. contrast, data science often adopts exploratory approach, using data uncover patterns, relationships, predictive features without rigid hypotheses. Rather testing predefined relationships, machine learning practitioners iteratively refine datasets evaluate features contribute predictive accuracy.Exploration vs. Hypothesis Testing: Traditional statistical inference begins predefined hypothesis, testing whether new treatment improves outcomes compared control. contrast, data science often adopts exploratory approach, using data uncover patterns, relationships, predictive features without rigid hypotheses. Rather testing predefined relationships, machine learning practitioners iteratively refine datasets evaluate features contribute predictive accuracy.Despite differences, statistical inference remains crucial key stages data preparation, particularly :Partition Validation: dividing data training testing sets, statistical tests help ensure subsets representative original dataset.Feature Selection: Hypothesis testing can aid selecting features strong relationships target variable, enhancing model performance.understanding differences applying statistical inference strategically, can ensure data preparation supports building robust, interpretable, generalizable models. Throughout chapter, see inference techniques continue play role refining datasets machine learning.","code":""},{"path":"chapter-modeling.html","id":"why-is-it-necessary-to-partition-the-data","chapter":"6 Preparing Data for Modeling","heading":"6.2 Why Is It Necessary to Partition the Data?","text":"Partitioning dataset crucial step preparing data modeling. common question students ask , need partition data? answer lies generalization—ability model perform well unseen data. Without proper partitioning, models may fit training data exceptionally well fail make accurate predictions real-world scenarios. Partitioning ensures performance evaluated data model seen , providing unbiased measure ability generalize effectively.Partitioning involves dividing dataset two subsets: training set, used build model, testing set, used evaluate performance. separation simulates real-world conditions, model must make predictions new data. helps detect address two common pitfalls machine learning: overfitting underfitting. trade-offs illustrated Figure 6.1, highlights balance model complexity performance training testing datasets.\nFigure 6.1: trade-model complexity accuracy training test sets. highlights optimal model complexity (sweet spot), test set accuracy reaches highest value unseen data.\nOverfitting occurs model memorizes training data, including noise random fluctuations, instead capturing general patterns. models achieve high accuracy training set perform poorly unseen data. instance, churn prediction model might memorize specific customer IDs rather recognizing broader behavioral trends, making ineffective new customers.Underfitting, contrast, occurs model simplistic capture underlying patterns. might happen model lacks complexity preprocessing removes much useful information. underfitted churn model, example, might predict constant churn rate customers without considering individual differences, leading poor performance.Partitioning mitigates risks allowing us evaluate performance unseen data. Comparing accuracy training testing sets helps determine whether model overfitting (high training accuracy low testing accuracy) underfitting (low accuracy ). evaluation enables iterative refinements strike right balance complexity generalization.Partitioning also prevents data leakage, critical issue information testing set inadvertently influences training. Data leakage inflates performance metrics, creating false sense confidence model’s ability generalize. Strictly separating testing set training process ensures realistic assessment model performance.Beyond simple train-test split, cross-validation enhances robustness. cross-validation, dataset divided multiple subsets (folds). model trained one subset tested another, repeating process across folds. results averaged provide reliable estimate model performance. Cross-validation particularly useful working small datasets tuning hyperparameters, minimizes bias introduced single train-test split.Partitioning isn’t just technical step—’s fundamental building models generalize well. addressing overfitting, underfitting, data leakage, leveraging techniques like cross-validation, ensure models accurate reliable real-world applications.summarize, general strategy supervised machine learning consists three key steps, illustrated Figure 6.2:Partitioning dataset training testing sets, followed validating partition.Building machine learning models training data.Evaluating performance models testing data select effective approach.\nFigure 6.2: general predictive machine learning process building evaluating models. 80-20 split ratio example may vary based dataset task.\nfollowing structured process, build models robust capable making accurate predictions unseen data. chapter focuses first step: partitioning data effectively, validating partition, preparing balanced training dataset—key steps developing reliable interpretable machine learning models.","code":""},{"path":"chapter-modeling.html","id":"sec-partitioning","chapter":"6 Preparing Data for Modeling","heading":"6.3 Partitioning the Data","text":"Partitioning dataset fundamental step preparing data machine learning. common approach train-test split, also known holdout method, dataset divided two subsets: training set used build model testing set reserved evaluating performance. separation ensures model assessed unseen data, providing unbiased estimate well generalizes.typical train-test split ratio 70-30, 80-20, 90-10, depending dataset size modeling needs. training set contains available features, including target variable, used teach model patterns data. testing set, however, target variable temporarily hidden simulate real-world conditions. trained model applied testing set predict hidden values, predictions compared actual target values assess performance.","code":""},{"path":"chapter-modeling.html","id":"example-train-test-split-in-r","chapter":"6 Preparing Data for Modeling","heading":"Example: Train-Test Split in R","text":"illustrate, revisit churn dataset Section 4.3, goal predict customer churn. first load dataset:split churn dataset training testing subsets using partition() function liver package. following code demonstrates create subsets R:code begins setting seed using set.seed(43), ensures reproducibility making random split consistent across different runs. particularly important sharing results collaborating project. Next, partition() function used divide dataset two subsets: 80% data assigned train_set model training, remaining 20% stored test_set evaluation. Finally, actual_test created store true target values test set, used later assess model’s predictive accuracy.Reproducibility essential machine learning. Setting seed guarantees results remain consistent, allowing others replicate findings exactly. seed value arbitrary, using one ensures partitioning process stable across different runs.","code":"\nlibrary(liver)\ndata(churn) \nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test = test_set$churn"},{"path":"chapter-modeling.html","id":"why-partitioning-matters","chapter":"6 Preparing Data for Modeling","heading":"Why Partitioning Matters","text":"primary reason partitioning prevent data leakage—situation information testing set influences training, leading overly optimistic performance estimates. strictly separating sets, ensure performance metrics reflect model’s ability generalize new data rather just memorizing training patterns.Beyond simple train-test split, cross-validation can enhance robustness training testing model multiple subsets data. method particularly useful working small datasets tuning hyperparameters.Partitioning lays groundwork reliable machine learning models. However, well-executed split alone guarantee training testing sets representative original dataset. next section, validate partition confirm subsets retain key statistical properties, ensuring fair unbiased model evaluation.","code":""},{"path":"chapter-modeling.html","id":"sec-validate-partition","chapter":"6 Preparing Data for Modeling","heading":"6.4 Validating the Partition","text":"success entire modeling process depends quality data partition. Validating partition ensures training testing sets representative original dataset, enabling model learn diverse examples generalize effectively unseen data. Without validation, modeling process risks bias—either model fails generalize training set isn’t representative, testing set doesn’t provide accurate evaluation real-world performance.Validation involves comparing training testing sets confirm distributions statistically similar, particularly key variables. Since datasets often include many variables, step typically focuses small set randomly selected features features particular importance, target variable. choice statistical test depends type variable compared, shown Table 6.1.Table 6.1:  Suggested hypothesis tests validating partitions, based type target variable.Validating partition procedural step—safeguard biased modeling. training testing sets differ significantly, model’s performance compromised. training set representative original dataset, model may fail generalize effectively. Conversely, testing set reflect population, model evaluation misleading. Ensuring split retains characteristics original dataset allows fair reliable model assessment.","code":""},{"path":"chapter-modeling.html","id":"example-validating-the-target-variable-churn","chapter":"6 Preparing Data for Modeling","heading":"Example: Validating the Target Variable churn","text":"Let’s consider churn dataset introduced previous section. target variable, churn (whether customer churned ), binary. According Table 6.1, appropriate statistical test validate partition variable Two-Sample Z-Test, compares proportion churned customers training testing sets. Thus, hypotheses test :\\[\n\\begin{cases}\nH_0:  \\pi_{\\text{churn, train}} = \\pi_{\\text{churn, test}} \\quad \\text{(Proportions equal)} \\\\\nH_a:  \\pi_{\\text{churn, train}} \\neq \\pi_{\\text{churn, test}} \\quad \\text{(Proportions equal)}\n\\end{cases}\n\\]’s can implemented R:, \\(x_1\\) \\(x_2\\) represent number churned customers training testing sets, respectively, \\(n_1\\) \\(n_2\\) denote total number observations set. prop.test() function used compare proportions churned customers two subsets.test result provides p-value = 0.69. Since p-value greater significance level (\\(\\alpha = 0.05\\)), fail reject null hypothesis (\\(H_0\\)). indicates statistically significant difference proportions churned customers training testing sets. failing reject \\(H_0\\), confirm partition valid respect target variable churn. proportions churned customers consistent across subsets, ensuring model trained tested representative data.validating target variable crucial, extending process key predictors customer.calls day.mins ensures subsets remain representative across important features. example, numerical features can validated using two-sample t-test, categorical features multiple levels can assessed using Chi-square test. broader validation ensures important variables retain statistical properties across training testing sets.","code":"x1 <- sum(train_set$churn == \"yes\")\nx2 <- sum(test_set$churn == \"yes\")\n\nn1 <- nrow(train_set)\nn2 <- nrow(test_set)\n\ntest_churn <- prop.test(x = c(x1, x2), n = c(n1, n2))\ntest_churn\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.1566, df = 1, p-value = 0.6923\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.0190317  0.0300317\n   sample estimates:\n   prop 1 prop 2 \n   0.1425 0.1370"},{"path":"chapter-modeling.html","id":"what-if-the-partition-is-invalid","chapter":"6 Preparing Data for Modeling","heading":"What If the Partition Is Invalid?","text":"statistical tests reveal significant differences training testing sets, adjustments necessary ensure partition remains representative. Several strategies can applied:Revisiting partitioning process: Changing random seed adjusting split ratio can sometimes lead balanced split.Stratified sampling: Ensuring key categorical variables, target variable, proportionally represented subsets.Cross-validation: Using k-fold cross-validation instead single train-test split provide robust evaluation model performance.Additionally, dataset small highly variable, minor differences training testing sets might inevitable. cases, alternative approaches like bootstrapping can help validate model performance effectively.Validating partition critical step data preparation process. ensures modeling process fair, reliable, capable producing generalizable results. addressing potential discrepancies early, set stage robust machine learning models perform effectively real-world, unseen data.","code":""},{"path":"chapter-modeling.html","id":"balancing-the-training-dataset","chapter":"6 Preparing Data for Modeling","heading":"6.5 Balancing the Training Dataset","text":"many real-world classification problems, one class target variable significantly underrepresented. imbalance can lead biased models perform well majority class fail predict minority class accurately. example, fraud detection, fraudulent transactions rare compared legitimate ones, churn prediction, majority customers may churn. Without addressing issue, models may appear perform well based accuracy alone fail identify rare yet important events.Imbalanced datasets pose challenge machine learning algorithms optimize overall accuracy, can favor majority class. churn prediction model trained imbalanced dataset, example, might classify nearly customers non-churners, leading high accuracy failing detect actual churners. problematic minority class (e.g., fraud cases, churners, patients rare disease) key focus analysis.","code":""},{"path":"chapter-modeling.html","id":"techniques-for-addressing-class-imbalance","chapter":"6 Preparing Data for Modeling","heading":"Techniques for Addressing Class Imbalance","text":"Balancing training dataset ensures classes adequately represented model training, improving model’s ability generalize. Several techniques can used address class imbalance:Oversampling: Increasing number minority class examples duplicating existing observations generating synthetic samples. Synthetic Minority -sampling Technique (SMOTE) popular approach generates synthetic examples instead simple duplication.Undersampling: Reducing number majority class examples randomly removing observations.Hybrid Methods: Combining oversampling undersampling achieve balanced dataset.Class Weights: Modifying algorithm penalize misclassifications minority class heavily.choice technique depends factors dataset size, severity imbalance, specific machine learning algorithm used.","code":""},{"path":"chapter-modeling.html","id":"example-balancing-the-churn-dataset","chapter":"6 Preparing Data for Modeling","heading":"Example: Balancing the Churn Dataset","text":"First, examine distribution target variable (churn) training dataset:Suppose output shows churners (churn = \"yes\") constitute 0.14, non-churners (churn = \"\") make 0.86. significant imbalance suggests balancing may necessary, particularly churn prediction business priority.address , use ROSE package R oversample minority class (churn = \"yes\") constitutes 30% training dataset:example, ovun.sample() function increases proportion churners 30% training dataset. formula notation churn ~ . specifies balancing applied based target variable (churn). oversampling, new class distribution checked ensure desired balance.","code":"# Check the class distribution\ntable(train_set$churn)\n   \n    yes   no \n    570 3430\nprop.table(table(train_set$churn))\n   \n      yes     no \n   0.1425 0.8575# Load the ROSE package\nlibrary(ROSE)\n\n# Oversample the training set to balance the classes with 30% churners\nbalanced_train_set <- ovun.sample(churn ~ ., data = train_set, method = \"over\", p = 0.3)$data\n\n# Check the new class distribution\ntable(balanced_train_set$churn)\n   \n     no  yes \n   3430 1444\nprop.table(table(balanced_train_set$churn))\n   \n          no       yes \n   0.7037341 0.2962659"},{"path":"chapter-modeling.html","id":"key-considerations-for-balancing","chapter":"6 Preparing Data for Modeling","heading":"Key Considerations for Balancing","text":"Balancing performed training dataset, test dataset. test dataset remain representative original class distribution provide unbiased evaluation model performance. Modifying test set introduce bias make model’s performance appear artificially better real-world scenarios.Furthermore, balancing must applied partitioning dataset. balancing done splitting, information test set may influence training process (data leakage), leading misleadingly high performance.said, balancing always necessary. Many modern machine learning algorithms, random forests gradient boosting, incorporate class weighting ensemble learning handle imbalanced datasets effectively. Additionally, alternative evaluation metrics precision, recall, F1-score, AUC-ROC can provide better insights model performance dealing imbalanced classes.summary, balancing training dataset can improve model performance, especially minority class primary focus. However, always required used selectively. balancing necessary, must applied partitioning maintain validity model evaluation. ensuring classes adequately represented training, help machine learning models make accurate fair predictions.","code":""},{"path":"chapter-modeling.html","id":"exercises-4","chapter":"6 Preparing Data for Modeling","heading":"6.6 Exercises","text":"","code":""},{"path":"chapter-modeling.html","id":"conceptual-questions-2","chapter":"6 Preparing Data for Modeling","heading":"Conceptual Questions","text":"partitioning dataset crucial training machine learning model? Explain role ensuring generalization.partitioning dataset crucial training machine learning model? Explain role ensuring generalization.main risk training model without separating dataset training testing subsets? Provide example lead misleading results.main risk training model without separating dataset training testing subsets? Provide example lead misleading results.Explain difference overfitting underfitting. proper partitioning help address issues?Explain difference overfitting underfitting. proper partitioning help address issues?Describe role training set testing set machine learning. test set remain unseen model training?Describe role training set testing set machine learning. test set remain unseen model training?data leakage, can occur data partitioning? Provide example scenario data leakage lead overly optimistic model performance.data leakage, can occur data partitioning? Provide example scenario data leakage lead overly optimistic model performance.Compare contrast random partitioning stratified partitioning. stratified partitioning preferred?Compare contrast random partitioning stratified partitioning. stratified partitioning preferred?necessary validate partition splitting dataset? go wrong training test sets significantly different?necessary validate partition splitting dataset? go wrong training test sets significantly different?validate numerical variables, customer.calls churn dataset, similar distributions training testing sets?validate numerical variables, customer.calls churn dataset, similar distributions training testing sets?dataset highly imbalanced, might model trained fail generalize well? Provide example real-world domain class imbalance serious issue.dataset highly imbalanced, might model trained fail generalize well? Provide example real-world domain class imbalance serious issue.Compare oversampling, undersampling, hybrid methods handling imbalanced datasets. advantages disadvantages ?Compare oversampling, undersampling, hybrid methods handling imbalanced datasets. advantages disadvantages ?balancing techniques applied training dataset test dataset?balancing techniques applied training dataset test dataset?machine learning algorithms robust class imbalance, others require explicit handling imbalance. types models typically require class balancing, can handle imbalance naturally?machine learning algorithms robust class imbalance, others require explicit handling imbalance. types models typically require class balancing, can handle imbalance naturally?dealing class imbalance, accuracy always best metric evaluate model performance? alternative metrics considered?dealing class imbalance, accuracy always best metric evaluate model performance? alternative metrics considered?Suppose dataset rare critical class (e.g., fraud detection). steps taken data partitioning balancing phase ensure effective model?Suppose dataset rare critical class (e.g., fraud detection). steps taken data partitioning balancing phase ensure effective model?","code":""},{"path":"chapter-modeling.html","id":"hands-on-practice","chapter":"6 Preparing Data for Modeling","heading":"Hands-On Practice","text":"following exercises, use churn, bank, risk datasets available liver package. previously used churn bank datasets earlier chapters. Chapter 9, introduce risk dataset. Load datasets using:","code":"\nlibrary(liver)\n\n# Load datasets\ndata(churn)\ndata(bank)\ndata(risk)"},{"path":"chapter-modeling.html","id":"partitioning-the-data","chapter":"6 Preparing Data for Modeling","heading":"Partitioning the Data","text":"Using partition() function, split churn dataset 75% training 25% testing. Ensure reproducibility setting seed value partitioning.Using partition() function, split churn dataset 75% training 25% testing. Ensure reproducibility setting seed value partitioning.Perform 90-10 train-test split bank dataset. Report number observations subset.Perform 90-10 train-test split bank dataset. Report number observations subset.Apply stratified sampling partition churn dataset, ensuring proportion churners (churn == \"yes\") remains training test sets.Apply stratified sampling partition churn dataset, ensuring proportion churners (churn == \"yes\") remains training test sets.risk dataset, partition data using 60-40 split store training test sets train_risk test_risk.risk dataset, partition data using 60-40 split store training test sets train_risk test_risk.Compare distribution income training test sets bank dataset using density plots. appear similar?Compare distribution income training test sets bank dataset using density plots. appear similar?","code":""},{"path":"chapter-modeling.html","id":"validating-the-partition","chapter":"6 Preparing Data for Modeling","heading":"Validating the Partition","text":"churn dataset, test whether proportion churners statistically different training test sets. Use two-sample Z-test.churn dataset, test whether proportion churners statistically different training test sets. Use two-sample Z-test.bank dataset, test whether average age customers differs significantly training test sets using two-sample t-test.bank dataset, test whether average age customers differs significantly training test sets using two-sample t-test.Perform Chi-square test validate whether distribution marital status (marital) bank dataset similar training test sets.Perform Chi-square test validate whether distribution marital status (marital) bank dataset similar training test sets.Suppose churn dataset partitioned incorrectly, resulting training set 30% churners test set 15% churners. statistical test confirm issue, corrected?Suppose churn dataset partitioned incorrectly, resulting training set 30% churners test set 15% churners. statistical test confirm issue, corrected?Select three numerical variables risk dataset validate whether distributions differ training test sets using appropriate statistical tests.Select three numerical variables risk dataset validate whether distributions differ training test sets using appropriate statistical tests.","code":""},{"path":"chapter-modeling.html","id":"balancing-the-training-dataset-1","chapter":"6 Preparing Data for Modeling","heading":"Balancing the Training Dataset","text":"churn dataset, check whether churners (churn = \"yes\") underrepresented training dataset. Report class proportions.churn dataset, check whether churners (churn = \"yes\") underrepresented training dataset. Report class proportions.Use random oversampling increase number churners (churn = \"yes\") training set 40% dataset using ROSE package.Use random oversampling increase number churners (churn = \"yes\") training set 40% dataset using ROSE package.Apply undersampling bank dataset proportion customers deposit = \"yes\" deposit = \"\" equal training set.Apply undersampling bank dataset proportion customers deposit = \"yes\" deposit = \"\" equal training set.Compare class distributions balancing churn dataset. Use bar plots visualize change.Compare class distributions balancing churn dataset. Use bar plots visualize change.","code":""},{"path":"chapter-knn.html","id":"chapter-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7 Classification using k-Nearest Neighbors","text":"Classification one fundamental tasks machine learning, enabling models categorize data predefined groups. detecting spam emails predicting customer churn, classification algorithms widely used across various domains. chapter, first explore concept classification, discussing applications, key principles, commonly used algorithms.solid understanding classification, introduce k-Nearest Neighbors (kNN), simple yet effective algorithm based idea similarity data points. kNN widely used classification due intuitive approach ease implementation. delve details kNN works, demonstrate implementation R, discuss strengths, limitations, real-world applications.illustrate kNN practice, apply real-world dataset: churn dataset. goal build classification model predicts whether customer churn based service usage account features. hands-example, demonstrate data preprocessing, selecting optimal \\(k\\), evaluating model performance, interpreting results.","code":""},{"path":"chapter-knn.html","id":"classification","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.1 Classification","text":"ever wondered email app effortlessly filters spam, streaming service seems know exactly want watch next, banks detect fraudulent credit card transactions real-time? seemingly magical predictions made possible classification, fundamental task machine learning.core, classification involves assigning label category observation based features. example, given customer data, classification can predict whether likely churn stay loyal. Unlike regression, predicts continuous numerical values (e.g., house prices), classification deals discrete outcomes. target variable, often called class label, can either :Binary: Two possible categories (e.g., spam vs. spam).Multi-class: two categories (e.g., car, bicycle, pedestrian image recognition).diagnosing diseases identifying fraudulent activities, classification versatile tool used across countless domains solve practical problems.","code":""},{"path":"chapter-knn.html","id":"where-is-classification-used","chapter":"7 Classification using k-Nearest Neighbors","heading":"Where Is Classification Used?","text":"Classification algorithms power many everyday applications cutting-edge technologies. examples:\n- Email filtering: Sorting spam non-spam messages.\n- Fraud detection: Identifying suspicious credit card transactions.\n- Customer retention: Predicting whether customer churn.\n- Medical diagnosis: Diagnosing diseases based patient records.\n- Object recognition: Detecting pedestrians vehicles self-driving cars.\n- Recommendation systems: Suggesting movies, songs, products based user preferences.Every time interact technology “predicts” something , chances , classification model working behind scenes.","code":""},{"path":"chapter-knn.html","id":"how-does-classification-work","chapter":"7 Classification using k-Nearest Neighbors","heading":"How Does Classification Work?","text":"Classification involves two critical phases:Training Phase: algorithm learns patterns labeled dataset, contains predictor variables (features) target class labels. instance, fraud detection system, algorithm might learn transactions involving unusually high amounts originating foreign locations likely fraudulent.Prediction Phase: model trained, applies learned patterns classify new, unseen data. example, given new transaction, model predicts whether fraudulent legitimate.good classification model just memorize training data—generalizes well, meaning performs accurately new, unseen data. instance, model trained historical medical records able diagnose patient never encountered , rather simply repeating past diagnoses.","code":""},{"path":"chapter-knn.html","id":"which-classification-algorithm-should-you-use","chapter":"7 Classification using k-Nearest Neighbors","heading":"Which Classification Algorithm Should You Use?","text":"Different classification algorithms designed different kinds problems datasets. commonly used algorithms include:\n- k-Nearest Neighbors (kNN): simple, distance-based algorithm (introduced chapter).\n- Naive Bayes: Particularly useful text classification, like spam filtering (covered Chapter 9).\n- Logistic Regression: popular method binary classification tasks, predicting customer churn (covered Chapter 10).\n- Decision Trees Random Forests: Versatile, interpretable methods handling complex problems (covered Chapter 11).\n- Neural Networks: Effective handling high-dimensional complex data, images natural language (covered Chapter 12).choice algorithm depends factors dataset size, feature relationships, trade-interpretability performance. instance, ’re working small dataset need easy--interpret solution, kNN Decision Trees might ideal. Conversely, high-dimensional data like images speech recognition, Neural Networks effective.illustrate classification action, consider bank dataset goal predict whether customer make deposit (deposit = yes) (deposit = ). features might include customer details like age, education, job, marital status. training classification model data, bank can identify target potential customers likely invest, improving marketing strategy.","code":""},{"path":"chapter-knn.html","id":"why-is-classification-important","chapter":"7 Classification using k-Nearest Neighbors","heading":"Why Is Classification Important?","text":"Classification forms backbone countless machine learning applications drive smarter decisions actionable insights industries like finance, healthcare, retail, technology. Understanding works critical step mastering machine learning applying solve real-world problems.Among many classification techniques, k-Nearest Neighbors (kNN) stands simplicity effectiveness. easy understand requires minimal assumptions data, kNN often used baseline model exploring advanced techniques. rest chapter, explore kNN works, widely used, implement R.","code":""},{"path":"chapter-knn.html","id":"how-k-nearest-neighbors-works","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.2 How k-Nearest Neighbors Works","text":"ever sought advice trusted friends making decision? k-Nearest Neighbors (kNN) algorithm follows similar principle—“consults” closest data points determine category new observation. simple yet effective idea makes kNN one intuitive classification methods machine learning.Unlike many machine learning algorithms require explicit training phase, kNN lazy learning instance-based method. Instead constructing complex model, stores entire training dataset makes predictions demand. given new observation, kNN identifies k closest data points using predefined distance metric. class label assigned based majority vote among nearest neighbors. choice \\(k\\), number neighbors considered, plays crucial role balancing sensitivity local patterns generalization broader trends.","code":""},{"path":"chapter-knn.html","id":"how-does-knn-classify-a-new-observation","chapter":"7 Classification using k-Nearest Neighbors","heading":"How Does kNN Classify a New Observation?","text":"classify new observation, kNN calculates distance every data point training set using specified metric, Euclidean distance, instance. identifying \\(k\\)-nearest neighbors, algorithm assigns frequent class among predicted category.Figure 7.1 illustrates concept two classes: Class (red circles) Class B (blue squares). new data point, represented dark star, needs classified. figure compares predictions two different values \\(k\\):\\(k = 3\\): algorithm considers 3 closest neighbors—two blue squares one red circle. Since majority class Class B (blue squares), new point classified Class B.\\(k = 6\\): algorithm expands neighborhood include 6 nearest neighbors. larger set consists four red circles two blue squares, shifting majority class Class (red circles). result, new point classified Class .\nFigure 7.1: two-dimensional toy dataset two classes (Class Class B) new data point (dark star), illustrating k-Nearest Neighbors algorithm k = 3 k = 6.\nexamples illustrate choice \\(k\\) affects classification. smaller \\(k\\) (e.g., 3) makes predictions highly sensitive local patterns, capturing finer details also increasing risk misclassification due noise. contrast, larger \\(k\\) (e.g., 6) smooths predictions incorporating neighbors, reducing sensitivity individual data points potentially overlooking localized structures data. Selecting appropriate \\(k\\) ensures kNN generalizes well without becoming overly complex overly simplistic.","code":""},{"path":"chapter-knn.html","id":"strengths-and-limitations-of-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"Strengths and Limitations of kNN","text":"kNN algorithm widely used due simplicity intuitive nature, making excellent starting point classification problems. relying distance metrics majority voting, avoids complexity training explicit models. However, simplicity comes trade-offs, particularly handling large datasets noisy features.One kNN’s key strengths ease implementation interpretability. Since require model training, can applied directly datasets minimal preprocessing. performs well small datasets patterns well-defined feature relationships strong. However, kNN highly sensitive irrelevant noisy features, distance calculations may become less meaningful unnecessary attributes included. Additionally, can computationally expensive large datasets, since must calculate distances every training point prediction. choice \\(k\\) also plays crucial role—small \\(k\\) makes algorithm overly sensitive noise, large \\(k\\) may oversimplify patterns, leading reduced accuracy.","code":""},{"path":"chapter-knn.html","id":"knn-in-action-a-toy-example-for-drug-classification","chapter":"7 Classification using k-Nearest Neighbors","heading":"kNN in Action: A Toy Example for Drug Classification","text":"illustrate kNN, consider real-world scenario involving drug prescription classification. dataset 200 patients includes age, sodium--potassium (Na/K) ratio, drug type prescribed. dataset synthetically generated reflect real-world scenario. details dataset generated, refer Section 1.16. Figure 7.2 visualizes dataset, different drug types represented :Red circles Drug ,Green triangles Drug B, andBlue squares Drug C.\nFigure 7.2: Scatter plot Age vs. Sodium/Potassium Ratio 200 patients, drug type indicated color shape.\nSuppose three new patients arrive clinic, need determine drug suitable based age sodium--potassium ratio. details follows:Patient 1: 40 years old Na/K ratio 30.5.Patient 2: 28 years old Na/K ratio 9.6.Patient 3: 61 years old Na/K ratio 10.5.patients represented orange circles Figure 7.3. Using kNN, classify drug type patient.\nFigure 7.3: Scatter plot Age vs. Sodium/Potassium Ratio 200 patients, drug type indicated color shape. three new patients represented large orange circles.\nPatient 1, located deep within cluster red-circle points (Drug ), classification straightforward: Drug . nearest neighbors belong Drug , making easy decision.Patient 2, situation nuanced. \\(k = 1\\), nearest neighbor blue square, resulting classification Drug C. \\(k = 2\\), tie Drug B Drug C, leading clear majority. \\(k = 3\\), two three nearest neighbors blue squares, classification remains Drug C.Patient 3, classification becomes even ambiguous. \\(k = 1\\), closest neighbor blue square, classifying patient Drug C. However, \\(k = 2\\) \\(k = 3\\), nearest neighbors belong multiple classes, creating uncertainty classification.\nFigure 7.4: Zoom-plots three new patients nearest neighbors. left plot Patient 1, middle plot Patient 2, right plot Patient 3.\nexamples highlight several key aspects kNN. choice \\(k\\) significantly influences classification—small values \\(k\\) make algorithm highly sensitive local patterns, larger values introduce smoothing considering broader neighborhoods. Additionally, selection distance metrics, Euclidean distance, affects neighbors determined. Finally, proper feature scaling ensures variables contribute fairly distance calculations, preventing dominance features larger numeric ranges.example demonstrates kNN assigns labels based proximity, reinforcing importance thoughtful parameter selection preprocessing techniques. applying kNN real-world datasets, essential understand similarity measured—leads next discussion distance metrics.","code":""},{"path":"chapter-knn.html","id":"distance-metrics","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.3 Distance Metrics","text":"kNN algorithm, classification new data point determined identifying similar records training dataset. define measure similarity? similarity might seem intuitive, applying machine learning requires precise distance metrics. metrics quantify “closeness” two data points multidimensional space, directly influencing neighbors selected classification.Imagine ’re shopping online looking recommendations. ’re 50-year-old married female—’s similar : 40-year-old single female 30-year-old married male? answer depends measure distance person. kNN, distance computed using numerical features age categorical features marital status. smaller distance, “similar” two individuals , influence determining predictions. Since kNN assumes closer points (lower distance) belong class, choosing right distance metric crucial accurate classification.","code":""},{"path":"chapter-knn.html","id":"euclidean-distance","chapter":"7 Classification using k-Nearest Neighbors","heading":"Euclidean Distance","text":"widely used distance metric kNN Euclidean distance, measures straight-line distance two points. Think “--crow-flies” distance, similar shortest path two locations map. metric intuitive aligns often perceive distance real world.Mathematically, Euclidean distance two points, \\(x\\) \\(y\\), \\(n\\)-dimensional space given :\\[\n\\text{dist}(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\ldots + (x_n - y_n)^2},\n\\]\\(x = (x_1, x_2, \\ldots, x_n)\\) \\(y = (y_1, y_2, \\ldots, y_n)\\) represent feature vectors two points. differences corresponding features (\\(x_i - y_i\\)) squared, summed, square-rooted calculate distance.Example 7.1  Let’s calculate Euclidean distance two patients based age sodium--potassium (Na/K) ratio:Patient 1: \\(x = (40, 30.5)\\)Patient 2: \\(y = (28, 9.6)\\)Using formula:\\[\n\\text{dist}(x, y) = \\sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \\sqrt{(12)^2 + (20.9)^2} = 24.11\n\\]result quantifies dissimilarity two patients. kNN, distance helps determine similar Patient 1 Patient 2 whether classified drug class.","code":""},{"path":"chapter-knn.html","id":"choosing-the-right-distance-metric","chapter":"7 Classification using k-Nearest Neighbors","heading":"Choosing the Right Distance Metric","text":"Euclidean distance widely used kNN, always best choice. distance metrics can suitable depending dataset’s characteristics:Manhattan Distance: Measures distance summing absolute differences coordinates. useful movement restricted grid-like paths, city blocks.Hamming Distance: Used categorical variables, distance number positions two feature vectors differ.Cosine Similarity: Measures angle two vectors rather absolute distance. useful high-dimensional spaces, text classification.choice distance metric depends data type problem domain. dataset contains categorical high-dimensional features, exploring alternative metrics—Manhattan Cosine Similarity—might necessary. details, refer dist() function R.","code":""},{"path":"chapter-knn.html","id":"how-to-choose-an-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.4 How to Choose an Optimal \\(k\\)","text":"many opinions seek making important decision? might lead biased perspective, many might dilute relevance advice. Similarly, k-Nearest Neighbors (kNN) algorithm, choice \\(k\\)—number neighbors considered classification—directly impacts model’s performance. determine right \\(k\\)?universally “correct” value \\(k\\). optimal choice depends specific dataset classification problem, requiring careful consideration trade-offs involved.","code":""},{"path":"chapter-knn.html","id":"balancing-overfitting-and-underfitting","chapter":"7 Classification using k-Nearest Neighbors","heading":"Balancing Overfitting and Underfitting","text":"\\(k\\) small, \\(k = 1\\), algorithm becomes highly sensitive individual training points. new observation classified based single closest neighbor, making model highly reactive noise outliers. can lead overfitting, model memorizes training data fails generalize unseen data. example, small cluster mislabeled data points disproportionately influence predictions, reducing model’s reliability.Conversely, \\(k\\) increases, algorithm incorporates neighbors classification decision. Larger \\(k\\) values smooth decision boundary, reducing impact noise outliers. However, \\(k\\) large, model may oversimplify, averaging meaningful patterns data. \\(k\\) comparable size training set, majority class dominates predictions, leading underfitting, model fails capture important distinctions.Choosing appropriate \\(k\\) requires balancing extremes. Smaller values \\(k\\) capture fine-grained local structures risk overfitting, larger values provide stability expense detail.","code":""},{"path":"chapter-knn.html","id":"choosing-k-through-validation","chapter":"7 Classification using k-Nearest Neighbors","heading":"Choosing \\(k\\) Through Validation","text":"Since optimal \\(k\\) depends dataset, common approach evaluate multiple values \\(k\\) using validation set cross-validation. Performance metrics accuracy, precision, recall, F1-score help identify best \\(k\\) given problem.illustrate, use churn dataset evaluate accuracy kNN algorithm across different \\(k\\) values (ranging 1 30). Figure 7.5 shows accuracy fluctuates \\(k\\) increases. plot generated using kNN.plot() function liver package R.\nFigure 7.5: Accuracy k-Nearest Neighbors algorithm different values k range 1 30.\nplot, observe kNN accuracy fluctuates \\(k\\) increases. highest accuracy achieved \\(k = 5\\), algorithm balances sensitivity local patterns robustness noise. value, kNN delivers accuracy 0.932 error rate 0.068.Choosing optimal \\(k\\) much art science. ’s universal rule selecting \\(k\\), experimentation validation key. Start range plausible \\(k\\) values, test model’s performance, select one provides best results based chosen metric.Keep mind optimal \\(k\\) may vary across datasets. Whenever applying kNN new problem, repeating process ensures model remains accurate generalizable. carefully tuning \\(k\\), strike right balance overfitting underfitting, improving model’s predictive power.","code":""},{"path":"chapter-knn.html","id":"preparing-data-for-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5 Preparing Data for kNN","text":"effectiveness kNN algorithm relies heavily dataset prepared. Since kNN uses distance metrics evaluate similarity data points, proper preprocessing crucial ensure accurate meaningful results. Two essential steps process feature scaling one-hot encoding, enable algorithm handle numerical categorical features effectively. steps part Preparing Data Modeling stage Data Science Workflow (Figure 2.3).","code":""},{"path":"chapter-knn.html","id":"feature-scaling-1","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.1 Feature Scaling","text":"datasets, numerical features often vastly different ranges. instance, age may range 20 70, income range 20,000 150,000. Without proper scaling, features larger ranges, income, dominate distance calculations, leading biased predictions. address , numerical features must transformed comparable scales. See Section 3.5 details scaling methods.widely used method min-max scaling, transforms feature specified range, typically \\([0, 1]\\), using formula:\n\\[\nx_{\\text{scaled}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)},\n\\]\n\\(x\\) represents original feature value, \\(\\min(x)\\) \\(\\max(x)\\) minimum maximum values feature, respectively. formula rescales feature \\([0,1]\\) range, ensuring single feature dominates distance calculation.Another common method z-score standardization, rescales features mean 0 standard deviation 1:\n\\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)}\n\\]\nmethod particularly useful features contain outliers follow different distributions. Unlike min-max scaling, z-score standardization constrain values within fixed range ensures follow standard normal distribution, making robust extreme values.Choosing Right Scaling Method:\nMin-max scaling preferable feature values bounded within known range, pixel values images percentages. ensures features contribute equally distance metric maintaining relative proportions. hand, z-score standardization suitable data contains extreme values follows different distributions across features. transforms values standard normal distribution, making particularly effective datasets outliers varying units measurement.Avoiding Data Leakage:\nScaling must always performed partitioning dataset training test sets. scaling parameters, minimum maximum min-max scaling mean standard deviation z-score standardization, computed training set applied consistently training test sets. Performing scaling partitioning can introduce data leakage, information test set inadvertently influences training process. can lead misleadingly high accuracy evaluation, model indirectly gains access test data making predictions.","code":""},{"path":"chapter-knn.html","id":"scaling-training-and-test-data-the-same-way","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.2 Scaling Training and Test Data the Same Way","text":"illustrate importance consistent scaling, consider patient drug classification problem, involves two features: age sodium/potassium (Na/K) ratio. Figure 7.3 shows dataset 200 patients training set, three additional patients test set. Using minmax() function liver package, demonstrate correct incorrect ways scale data:difference illustrated Figure 7.6. middle panel shows results proper scaling, test set scaled using parameters derived training set. ensures consistency distance calculations across datasets. contrast, right panel shows improper scaling, test set scaled independently. leads distorted relationships training test data, can cause unreliable predictions.\nFigure 7.6: Visualization illustrating difference proper scaling improper scaling. left panel shows original data without scaling. middle panel shows results proper scaling. right panel shows results improper scaling.\nKey Insight: Proper scaling ensures distance metrics remain valid, improper scaling creates inconsistencies undermine kNN algorithm’s performance. Scaling parameters always derived training set applied consistently test set. Neglecting principle introduces data leakage, distorts model evaluation leads overly optimistic performance estimates.","code":"\n# Load the liver package\nlibrary(liver)\n\n# A proper way to scale the data\ntrain_scaled = minmax(train_data, col = c(\"Age\", \"Ratio\"))\n\ntest_scaled = minmax(test_data, col = c(\"Age\", \"Ratio\"), min = c(min(train_data$Age), min(train_data$Ratio)), max = c(max(train_data$Age), max(train_data$Ratio)))\n\n# An incorrect way to scale the data\ntrain_scaled_wrongly = minmax(train_data, col = c(\"Age\", \"Ratio\"))\ntest_scaled_wrongly  = minmax(test_data , col = c(\"Age\", \"Ratio\"))"},{"path":"chapter-knn.html","id":"one-hot-encoding-1","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.3 One-Hot Encoding","text":"Categorical features, marital status subscription type, directly used distance calculations distance metrics like Euclidean distance work numerical data. overcome , use one-hot encoding, converts categorical variables binary (dummy) variables.example, categorical variable voice.plan, levels yes , can encoded :\\[\n\\text{voice.plan-yes} =\n\\begin{cases}\n1 \\quad \\text{voice plan = yes}  \\\\\n0 \\quad \\text{voice plan = }\n\\end{cases}\n\\]categorical variables two categories, one-hot encoding creates multiple binary columns—one category except one, avoid redundancy. approach ensures categorical variable fully represented without introducing unnecessary correlations.liver package R provides one.hot() function perform one-hot encoding automatically. identifies categorical variables encodes binary columns, leaving numerical features unchanged. Applying one-hot encoding marital variable bank dataset, instance, adds binary columns encoded categories:Setting dropCols = FALSE retains original categorical column dataset, may useful reference debugging. However, cases, recommended remove original column encoding avoid redundancy.Note: One-hot encoding unnecessary ordinal features, categories natural order (e.g., low, medium, high). Ordinal variables instead assigned numerical values preserve order (e.g., low = 1, medium = 2, high = 3), enabling kNN algorithm treat numerical features. instance, education.level values {low, medium, high}, one-hot encoding lose natural progression categories. Instead, assigning numerical values (low = 1, medium = 2, high = 3) allows algorithm recognize ordinal nature feature, preserving relationship distance calculations.","code":"data(bank)\n\n# To perform one-hot encoding on the \"marital\" variable\nbank_encoded <- one.hot(bank, cols = c(\"marital\"), dropCols = FALSE)\n\nstr(bank_encoded)\n   'data.frame':    4521 obs. of  20 variables:\n    $ age             : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job             : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital         : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ marital_divorced: int  0 0 0 0 0 0 0 0 0 0 ...\n    $ marital_married : int  1 1 0 1 1 0 1 1 1 1 ...\n    $ marital_single  : int  0 0 1 0 0 1 0 0 0 0 ...\n    $ education       : Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance         : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing         : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan            : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact         : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day             : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month           : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration        : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign        : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays           : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous        : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome        : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-knn.html","id":"sec-kNN-churn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6 Applying kNN Algorithm in Practice","text":"Applying kNN algorithm involves several key steps, preparing data training model, making predictions, evaluating performance. section, demonstrate entire workflow using churn dataset liver package R. target variable, churn, indicates whether customer churned (yes) (), predictors include customer characteristics account length, international plan status, call details. details exploratory data analysis, problem understanding, data preparation dataset, refer Section 4.3.dataset data.frame R 5000 observations 19 predictor variables. target variable, churn, indicates whether customer churned (yes) ().Based insights gained Section 4.3, select following features building kNN model:account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, customer.calls.next steps involve preparing data feature scaling one-hot encoding, followed selecting optimal \\(k\\), training kNN model, evaluating performance.","code":"str(churn)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-knn.html","id":"step-1-preparing-the-data","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.1 Step 1: Preparing the Data","text":"first step applying kNN partition dataset training test sets, followed preprocessing tasks like feature scaling one-hot encoding. Since dataset already cleaned free missing values, can proceed directly partitioning applying transformations.split dataset 80% training set 20% test set using partition() function liver package:partition() function randomly splits dataset maintaining class distribution target variable, ensuring representative training test set. validated partition Section 6.4, can now proceed feature scaling one-hot encoding ensure compatibility kNN algorithm.","code":"\nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$churn"},{"path":"chapter-knn.html","id":"one-hot-encoding-2","chapter":"7 Classification using k-Nearest Neighbors","heading":"One-Hot Encoding","text":"Since kNN relies distance calculations, categorical variables like voice.plan intl.plan must converted numerical representations. One-hot encoding achieves creating binary (dummy) variables category. apply one.hot() function liver package transform categorical features numerical format suitable kNN:binary categorical variables, one-hot encoding produces two columns (e.g., voice.plan_yes voice.plan_no). Since one variable always complement , retain one (e.g., voice.plan_yes) avoid redundancy.","code":"categorical_vars = c(\"voice.plan\", \"intl.plan\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_vars)\ntest_onehot  = one.hot(test_set,  cols = categorical_vars)\n\nstr(test_onehot)\n   'data.frame':    1000 obs. of  22 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 2 50 14 46 10 4 25 15 11 32 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 3 2 1 3 2 2 2 2 2 1 ...\n    $ account.length: int  118 141 85 76 147 130 20 142 72 149 ...\n    $ voice.plan_yes: int  0 1 1 1 0 0 0 0 1 0 ...\n    $ voice.plan_no : int  1 0 0 0 1 1 1 1 0 1 ...\n    $ voice.messages: int  0 37 27 33 0 0 0 0 37 0 ...\n    $ intl.plan_yes : int  1 1 0 0 0 0 0 0 0 0 ...\n    $ intl.plan_no  : int  0 0 1 1 1 1 1 1 1 1 ...\n    $ intl.mins     : num  6.3 11.2 13.8 10 10.6 9.5 6.3 14.2 14.7 11.1 ...\n    $ intl.calls    : int  6 5 4 5 4 19 6 6 6 9 ...\n    $ intl.charge   : num  1.7 3.02 3.73 2.7 2.86 2.57 1.7 3.83 3.97 3 ...\n    $ day.mins      : num  223 259 196 190 155 ...\n    $ day.calls     : int  98 84 139 66 117 112 109 95 80 94 ...\n    $ day.charge    : num  38 44 33.4 32.2 26.4 ...\n    $ eve.mins      : num  221 222 281 213 240 ...\n    $ eve.calls     : int  101 111 90 65 93 99 84 63 102 92 ...\n    $ eve.charge    : num  18.8 18.9 23.9 18.1 20.4 ...\n    $ night.mins    : num  203.9 326.4 89.3 165.7 208.8 ...\n    $ night.calls   : int  118 97 75 108 133 78 102 148 71 108 ...\n    $ night.charge  : num  9.18 14.69 4.02 7.46 9.4 ...\n    $ customer.calls: int  0 0 1 1 0 0 0 2 3 1 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-knn.html","id":"feature-scaling-2","chapter":"7 Classification using k-Nearest Neighbors","heading":"Feature Scaling","text":"Since kNN calculates distances data points, features larger numerical ranges can disproportionately influence results. Scaling ensures features contribute equally distance calculations, preventing dominance high-magnitude features.standardize numerical variables, apply min-max scaling using minmax() function liver package. Scaling parameters (minimum maximum values) must computed training set applied consistently training test sets. prevents data leakage, occurs test data influences training process, leading misleadingly high performance estimates.minmax() function normalizes numerical features range \\([0, 1]\\), ensuring comparable scales preserving relative differences. transformation prevents single feature dominating kNN distance calculations, leading balanced accurate predictions.","code":"\nnumeric_vars = c(\"account.length\", \"voice.messages\", \"intl.mins\", \"intl.calls\", \n                 \"day.mins\", \"day.calls\", \"eve.mins\", \"eve.calls\", \n                 \"night.mins\", \"night.calls\", \"customer.calls\")\n\nmin_train = sapply(train_set[, numeric_vars], min)\nmax_train = sapply(train_set[, numeric_vars], max)\n\ntrain_scaled = minmax(train_onehot, col = numeric_vars, min = min_train, max = max_train)\ntest_scaled  = minmax(test_onehot,  col = numeric_vars, min = min_train, max = max_train)"},{"path":"chapter-knn.html","id":"step-2-choosing-an-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.2 Step 2: Choosing an Optimal \\(k\\)","text":"choice \\(k\\) determines trade-capturing local patterns generalizing well unseen data. Selecting inappropriate \\(k\\) may result overfitting (\\(k\\) small) oversmoothing (\\(k\\) large). identify optimal \\(k\\), evaluate model’s accuracy different values \\(k\\) using kNN.plot() function:kNN.plot() function visualizes relationship \\(k\\) model accuracy, helping us determine value \\(k\\) balances model complexity generalization. examining plot, observe highest accuracy achieved \\(k = 5\\). choice maintains sufficient flexibility capture meaningful patterns avoiding excessive sensitivity outliers.","code":"formula = churn ~ account.length + voice.plan_yes + voice.messages + \n                  intl.plan_yes + intl.mins + intl.calls + \n                  day.mins + day.calls + eve.mins + eve.calls + \n                  night.mins + night.calls + customer.calls\n\nkNN.plot(formula = formula, train = train_scaled, test = test_scaled, \n         k.max = 30, set.seed = 43)\n   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-knn.html","id":"step-3-training-the-model-and-making-predictions","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.3 Step 3: Training the Model and Making Predictions","text":"Since identified \\(k = 5\\) optimal value Step 2, now proceed train kNN model make predictions test set. apply kNN algorithm R, use kNN() function liver package follows:kNN() function automates kNN classification process computing distances test observation training data points. selects 5 closest neighbors based chosen distance metric assigns frequently occurring class among predicted label. Since test data used training, predictions provide unbiased estimate well model generalizes new observations.","code":"\nkNN_predict = kNN(formula = formula, train = train_scaled, test = test_scaled, k = 5)"},{"path":"chapter-knn.html","id":"step-4-evaluating-the-model","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.4 Step 4: Evaluating the Model","text":"Evaluating model performance crucial ensure kNN algorithm generalizes well unseen data makes reliable predictions. confusion matrix provides summary correct incorrect predictions comparing predicted labels actual labels test set. compute using conf.mat() function liver package:confusion matrix, see model correctly classified 910 instances, 90 instances misclassified. summary helps assess model performance identify areas improvement.","code":"conf.mat(kNN_predict, test_labels, reference = \"yes\")\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856"},{"path":"chapter-knn.html","id":"final-remarks","chapter":"7 Classification using k-Nearest Neighbors","heading":"Final Remarks","text":"step--step implementation kNN highlighted crucial role data preprocessing, parameter tuning, model evaluation achieving reliable predictions. Key factors choice \\(k\\), feature scaling, encoding categorical data significantly influence accuracy generalization kNN models.confusion matrix provides initial assessment model performance, additional evaluation metrics accuracy, precision, recall, F1-score offer deeper insights. aspects explored detail next chapter (Chapter 8).","code":""},{"path":"chapter-knn.html","id":"key-takeaways-from-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.7 Key Takeaways from kNN","text":"chapter, explored k-Nearest Neighbors (kNN) algorithm, simple yet effective method solving classification problems. began revisiting concept classification real-world applications, highlighting difference binary multi-class problems. examined mechanics kNN, emphasizing reliance distance metrics identify similar data points. Essential preprocessing steps, feature scaling one-hot encoding, discussed ensure accurate meaningful distance calculations. also covered importance selecting optimal \\(k\\) value demonstrated implementation kNN using liver package R churn dataset. practical examples, reinforced significance proper data preparation parameter tuning building reliable classification models.simplicity interpretability kNN make excellent starting point understanding classification exploring dataset structures. However, algorithm notable limitations, including sensitivity noise, computational inefficiency large datasets, necessity proper scaling feature selection. challenges make kNN less practical large-scale applications, remains valuable tool small medium-sized datasets serves benchmark evaluating advanced algorithms.kNN intuitive easy implement, prediction speed scalability constraints often limit use modern, large-scale datasets. Nonetheless, useful baseline method stepping stone sophisticated techniques. upcoming chapters, explore advanced classification algorithms, Decision Trees, Random Forests, Logistic Regression, address limitations kNN provide enhanced performance scalability wide range applications.","code":""},{"path":"chapter-knn.html","id":"exercises-5","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.8 Exercises","text":"","code":""},{"path":"chapter-knn.html","id":"conceptual-questions-3","chapter":"7 Classification using k-Nearest Neighbors","heading":"Conceptual Questions","text":"Explain fundamental difference classification regression. Provide example .key steps applying kNN algorithm?choice \\(k\\) important kNN, happens \\(k\\) small large?Describe role distance metrics kNN classification. Euclidean distance commonly used?limitations kNN compared classification algorithms?feature scaling impact performance kNN? necessary?Describe one-hot encoding used kNN. necessary categorical variables?kNN handle missing values? strategies can used deal missing data?Explain difference lazy learning (kNN) eager learning (decision trees logistic regression).kNN considered non-parametric algorithm? advantages disadvantages bring?","code":""},{"path":"chapter-knn.html","id":"hands-on-practice-applying-knn-to-the-bank-dataset","chapter":"7 Classification using k-Nearest Neighbors","heading":"Hands-On Practice: Applying kNN to the Bank Dataset","text":", want apply concepts covered chapter using bank dataset liver package. bank dataset contains customer information, including demographics financial details, target variable deposit indicates whether customer subscribed term deposit. dataset well-suited classification problems provides opportunity practice kNN real-world scenarios.begin, load necessary package dataset:","code":"library(liver)\n\n# Load the dataset\ndata(bank)\n\n# View the structure of the dataset\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-knn.html","id":"data-exploration-and-preparation","chapter":"7 Classification using k-Nearest Neighbors","heading":"Data Exploration and Preparation","text":"Load bank dataset display structure. Identify target variable predictor variables.Count number instances customer subscribed term deposit (deposit = “yes”) versus (deposit = “”). tell dataset?Identify nominal variables dataset. Convert numerical features using one-hot encoding one.hot() function.Partition dataset 80% training 20% testing sets using partition() function. Ensure target variable remains proportionally distributed sets.Validate partitioning comparing class distribution target variable training test sets.Apply min-max scaling numerical variables training test sets. Ensure scaling parameters derived training set .","code":""},{"path":"chapter-knn.html","id":"choosing-the-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"Choosing the Optimal \\(k\\)","text":"Use kNN.plot() function determine optimal \\(k\\) value classifying deposit bank dataset.best \\(k\\) value based accuracy? accuracy change \\(k\\) increases?Interpret meaning accuracy curve generated kNN.plot(). patterns observe?","code":""},{"path":"chapter-knn.html","id":"building-and-evaluating-the-knn-model","chapter":"7 Classification using k-Nearest Neighbors","heading":"Building and Evaluating the kNN Model","text":"Train kNN model using optimal \\(k\\) make predictions test set.Generate confusion matrix kNN model predictions using conf.mat() function. Interpret results.Calculate accuracy kNN model. well perform predicting deposit?Besides accuracy, evaluation metrics (e.g., precision, recall, F1-score) useful assessing kNN performance bank dataset? Compute interpret metrics.Compare performance kNN different values \\(k\\) (e.g., \\(k = 1, 5, 15, 25\\)). changing \\(k\\) affect classification results?Train kNN model using subset features: age, balance, duration, campaign. Compare accuracy full-feature model. tell feature selection?Compare accuracy kNN using min-max scaling versus z-score standardization. choice scaling method impact model performance?","code":""},{"path":"chapter-knn.html","id":"critical-thinking-and-real-world-applications","chapter":"7 Classification using k-Nearest Neighbors","heading":"Critical Thinking and Real-World Applications","text":"Suppose building fraud detection system bank. kNN suitable algorithm? advantages limitations context?handle imbalanced classes bank dataset? strategies improve classification performance?high-dimensional dataset hundreds features, kNN still effective approach? ?Imagine working dataset new data points arrive real-time. challenges kNN face, addressed?financial institution wants classify customers different risk categories loan approval, preprocessing steps essential applying kNN?dataset features irrelevant redundant, improve kNN’s performance? feature selection methods use?computation time concern, strategies apply make kNN efficient large datasets?Suppose kNN performing poorly bank dataset. possible reasons explain , troubleshoot issue?","code":""},{"path":"chapter-evaluation.html","id":"chapter-evaluation","chapter":"8 Model Evaluation","heading":"8 Model Evaluation","text":"progress Data Science Workflow, introduced Chapter 2 illustrated Figure 2.3, already completed first five phases:Problem Understanding: Defining problem aim solve.Data Preparation: Cleaning, transforming, organizing data analysis.Exploratory Data Analysis (EDA): Gaining insights uncovering patterns data.Preparing Data Modeling: Setting data modeling scaling, encoding, partitioning.Modeling: Applying algorithms make predictions extract insights—kNN classification method explored previous chapter.Now, arrive Model Evaluation phase, pivotal step Data Science Workflow. phase answers critical question: well model perform?Building model just beginning. Without evaluation, way knowing whether model generalizes well new data simply memorizing patterns training set. model performs well training fails real-world applications little practical value. Model evaluation ensures predictions reliable model effectively captures underlying patterns rather just noise.chapter introduce key evaluation techniques metrics assess performance classification regression models, helping us make informed decisions model selection improvement.","code":""},{"path":"chapter-evaluation.html","id":"why-is-model-evaluation-important","chapter":"8 Model Evaluation","heading":"Why Is Model Evaluation Important?","text":"Building model just beginning. real test effectiveness lies ability generalize new, unseen data. Without proper evaluation, model may seem successful development fail applied real-world scenarios.Consider example:\ndevelop model detect fraudulent credit card transactions, achieves 95% accuracy. Sounds impressive, right? 1% transactions actually fraudulent, model might simply classify every transaction legitimate achieve high accuracy—completely ignoring fraud cases. highlights crucial point: accuracy alone can misleading, especially imbalanced datasets.Model evaluation provides comprehensive understanding model’s performance assessing:Strengths: model well (e.g., correctly detecting fraud).Weaknesses: falls short (e.g., missing fraudulent cases flagging many legitimate transactions fraud).Trade-offs: balance competing priorities, sensitivity vs. specificity precision vs. recall.well-evaluated model aligns real-world objectives. helps answer key questions :well model handle imbalanced datasets?good identifying true positives (e.g., detecting cancer medical diagnoses)?minimize false positives (e.g., avoiding mistakenly flagging legitimate emails spam)?George Box, renowned statistician, famously said, “models wrong, useful.” model always simplification reality—capture every nuance complexity. However, proper evaluation, can determine whether model useful enough make reliable predictions guide decision-making.chapter, explore evaluation classification models, starting binary classification, target variable two categories (e.g., spam vs. spam). discuss evaluation metrics multi-class classification, two categories (e.g., types vehicles: car, truck, bike). Finally, introduce evaluation metrics regression models, target variable continuous (e.g., predicting house prices).goal establish strong foundation model evaluation, enabling assess model performance effectively make data-driven decisions. Let’s begin one fundamental tools classification evaluation: Confusion Matrix.","code":""},{"path":"chapter-evaluation.html","id":"confusion-matrix","chapter":"8 Model Evaluation","heading":"8.1 Confusion Matrix","text":"confusion matrix fundamental tool evaluating classification models. provides detailed breakdown model’s predictions categorizing four distinct groups based actual versus predicted values. binary classification problems, confusion matrix structured shown Table 8.1.classification tasks, one class typically designated positive class (class interest), negative class. instance, fraud detection, fraudulent transactions might considered positive class, legitimate transactions negative class.Table 8.1:  Confusion matrix summarizing correct incorrect predictions binary classification problems. positive class refers class interest, negative class represents category.element confusion matrix corresponds one four possible prediction outcomes:True Positives (TP): model correctly predicts positive class (e.g., fraud detected fraud).False Positives (FP): model incorrectly predicts positive class (e.g., legitimate transactions falsely flagged fraud).True Negatives (TN): model correctly predicts negative class (e.g., legitimate transactions classified correctly).False Negatives (FN): model incorrectly predicts negative class (e.g., fraudulent transactions classified legitimate).structure feels familiar, mirrors concept Type Type II errors introduced Chapter 5 hypothesis testing. diagonal elements (TP TN) represent correct predictions, -diagonal elements (FP FN) represent incorrect ones.","code":""},{"path":"chapter-evaluation.html","id":"calculating-key-metrics","chapter":"8 Model Evaluation","heading":"Calculating Key Metrics","text":"Using values confusion matrix, can derive key performance metrics model, accuracy (also known success rate) error rate:\n\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total Predictions}} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}\n\\]\n\\[\n\\text{Error Rate} = 1 - \\text{Accuracy} = \\frac{\\text{FP} + \\text{FN}}{\\text{Total Predictions}}\n\\]Accuracy represents proportion correct predictions (TP TN) among predictions, providing overall assessment model performance. Conversely, error rate measures proportion incorrect predictions (FP FN) among predictions.accuracy provides high-level assessment performance, distinguish different types errors, false positives false negatives. example, imbalanced dataset one class significantly outnumbers , accuracy may appear high even model performs poorly detecting minority class. need additional metrics, sensitivity, specificity, precision, recall, explore later sections.Example 8.1  Let’s revisit k-Nearest Neighbors (kNN) model Chapter 7, built classifier predict customer churn using churn dataset. now evaluate performance using confusion matrix.First, apply kNN model generate predictions:details kNN model built, refer Section 7.6. Now, generate confusion matrix predictions using conf.mat() function liver package:, set reference = \"yes\" specify churn = yes positive class, aligning confusion matrix problem focus—correctly identifying customers actually churned. confusion matrix summarizes model’s performance follows:True Positives (TP): 54 correctly predicted churn cases.True Negatives (TN): 856 correctly predicted non-churn cases.False Positives (FP): 83 incorrectly predicted churn customers churn.False Negatives (FN): 7 missed churn cases, predicting non-churn.can also visualize confusion matrix using conf.mat.plot() function:confusion matrix, compute accuracy error rate:\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total Predictions}} = \\frac{54 + 856}{1000} = 0.91\n\\]\\[\n\\text{Error Rate} = \\frac{\\text{FP} + \\text{FN}}{\\text{Total Predictions}} = \\frac{83 + 7}{1000} = 0.09\n\\]accuracy indicates model correctly classified 91% test set, 9% predictions incorrect.accuracy provides useful summary overall performance, account imbalanced datasets misclassification costs. example, customer churn prediction, false negatives (missed churners) might costly false positives (incorrectly predicted churners). Therefore, additional evaluation metrics necessary provide deeper understanding model performance.gain deeper insights model performance, now turn sensitivity, specificity, precision, recall—metrics provide detailed evaluation classification outcomes.","code":"\nlibrary(liver)  \n# Load the churn dataset\ndata(churn)\n\n# Partition the data into training and testing sets\nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\nactual_test = test_set$churn\n\n# Build and predict using the kNN model\nformula = churn ~ account.length + voice.plan + voice.messages + \n                  intl.plan + intl.mins + intl.calls + \n                  day.mins + day.calls + eve.mins + eve.calls + \n                  night.mins + night.calls + customer.calls\n\nkNN_predict = kNN(formula = formula, train = train_set, \n                  test = test_set, k = 5, scaler = \"minmax\")conf.mat(kNN_predict, actual_test, reference = \"yes\")\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856conf.mat.plot(kNN_predict, actual_test)\n   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-evaluation.html","id":"sensitivity-and-specificity","chapter":"8 Model Evaluation","heading":"8.2 Sensitivity and Specificity","text":"classification, ’s important evaluate just many predictions correct overall, well model identifies specific classes. Sensitivity Specificity two complementary metrics focus model’s ability distinguish positive negative classes.metrics particularly valuable cases class distribution imbalanced. example, fraud detection rare disease diagnosis, majority cases belong negative class, can lead misleadingly high accuracy. Sensitivity specificity allow us separately assess well model detects class.","code":""},{"path":"chapter-evaluation.html","id":"sensitivity","chapter":"8 Model Evaluation","heading":"Sensitivity","text":"Sensitivity (also called Recall fields, like information retrieval) measures model’s ability correctly identify positive cases. answers question:“actual positives, many model correctly predict?”Mathematically, sensitivity defined :\\[\n\\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n\\]Let’s compute sensitivity k-Nearest Neighbors (kNN) model built Chapter 7, predicted whether customers churned (churn = yes). Sensitivity case reflects percentage churners correctly identified model. Using confusion matrix Example 8.1:\\[\n\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{54}{54 + 7} = 0.885\n\\]means model correctly identified 88.5% actual churners.perfect model achieve sensitivity 1.0 (100%), meaning correctly identifies positive cases. However, ’s important note even naïve model classifies customers churners also achieve 100% sensitivity. illustrates sensitivity alone isn’t enough evaluate model’s performance—must paired metrics capture full picture.","code":""},{"path":"chapter-evaluation.html","id":"specificity","chapter":"8 Model Evaluation","heading":"Specificity","text":"sensitivity focuses positive class, Specificity measures model’s ability correctly identify negative cases. answers question:“actual negatives, many model correctly predict?”Specificity particularly important situations avoiding false positives critical. example, spam detection, incorrectly marking legitimate email spam (false positive) can severe consequences missing spam messages. Mathematically, specificity defined :\\[\n\\text{Specificity} = \\frac{\\text{True Negatives (TN)}}{\\text{True Negatives (TN)} + \\text{False Positives (FP)}}\n\\]Using kNN model confusion matrix Example 8.1, let’s calculate specificity identifying non-churners (churn = ):\\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{856}{856 + 83} = 0.912\n\\]means model correctly classified 91.2% actual non-churners leaving company.good classification model ideally achieve high sensitivity high specificity, relative importance metrics depends problem domain. example, medical diagnostics, sensitivity often prioritized ensure disease cases missed, credit scoring, specificity might take precedence avoid mistakenly classifying reliable customers risks.kNN model Example 8.1, sensitivity 0.885, specificity 0.912. trade-may acceptable instance, identifying churners (sensitivity) might critical avoiding false positives (specificity).","code":""},{"path":"chapter-evaluation.html","id":"sensitivity-vs.-specificity-a-balancing-act","chapter":"8 Model Evaluation","heading":"Sensitivity vs. Specificity: A Balancing Act","text":"trade-sensitivity specificity often essential consideration model evaluation. many cases, improving one comes cost :Increasing sensitivity (recall) often leads false positives, lowering specificity.Increasing specificity reduces false positives can increase false negatives, lowering sensitivity.example, medical screening, missing serious disease (false negative) can severe consequences, model high sensitivity preferred—even results false positives (low specificity). contrast, email spam filtering, high false positive rate (flagging important emails spam) can frustrating users. Therefore, model high specificity preferable, even occasionally misses spam emails.balance one core challenges classification. optimal trade-depends business domain priorities.next section, refine evaluation introducing precision recall. metrics extend sensitivity specificity focusing reliability positive predictions ability capture relevant positive cases.","code":""},{"path":"chapter-evaluation.html","id":"precision-recall-and-f1-score","chapter":"8 Model Evaluation","heading":"8.3 Precision, Recall, and F1-Score","text":"addition sensitivity specificity, precision, recall, F1-score offer deeper insights classification model’s performance. metrics particularly valuable scenarios imbalanced datasets, accuracy alone can misleading.\nPrecision, also known positive predictive value, measures many model’s predicted positives actually positive. answers question:“model predicts positive, often correct?”\nMathematically, precision defined :\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\nPrecision especially important applications false positives costly. example, fraud detection, flagging legitimate transactions fraudulent can lead customer dissatisfaction unnecessary investigations.Recall (equivalent sensitivity) measures model’s ability identify positive cases. answers question:“actual positives, many model correctly predict?”\nMathematically, recall defined :\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\nRecall particularly useful missing positive cases (false negatives) serious consequences, failing diagnose disease missing fraudulent transactions. recall often used interchangeably sensitivity medical diagnostics, commonly referred recall areas like information retrieval, spam detection, text classification.","code":""},{"path":"chapter-evaluation.html","id":"precision-vs.-recall-a-trade-off","chapter":"8 Model Evaluation","heading":"Precision vs. Recall: A Trade-Off","text":"inherent trade-precision recall:Increasing precision makes model selective, reducing false positives potentially missing true positives (lower recall).Increasing recall allows model capture positive cases, reducing false negatives potentially misclassifying negatives positives (lower precision).example, medical test cancer screening prioritize high recall ensure patient cancer missed. However, email spam detection, precision might important avoid mistakenly classifying important emails spam.","code":""},{"path":"chapter-evaluation.html","id":"the-f1-score-balancing-precision-and-recall","chapter":"8 Model Evaluation","heading":"The F1-Score: Balancing Precision and Recall","text":"balance trade-, F1-score combines precision recall single metric. harmonic mean precision recall, emphasizing balance:\n\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n   = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}\n\\]\nF1-score particularly useful imbalanced datasets, one class significantly outnumbers . Unlike accuracy, considers false positives false negatives, providing informative evaluation model’s predictive performance.Now, let’s apply concepts k-Nearest Neighbors (kNN) model Example 8.1, predicts customer churn (churn = yes).First, precision quantifies often model’s predicted churners actual churners:\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{54}{54 + 83} = 0.394\n\\]\nmeans model predicts churn, correct 39.4% cases.Next, recall measures many actual churners correctly identified model:\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{54}{54 + 7} = 0.885\n\\]\nshows model successfully identifies 88.5% actual churners.Finally, F1-score provides single measure balances precision recall:\n\\[\nF1 = \\frac{2 \\cdot 54}{2 \\cdot 54 + 83 + 7} = 0.545\n\\]\nF1-score provides summary measure model’s ability correctly identify churners minimizing false predictions.","code":""},{"path":"chapter-evaluation.html","id":"choosing-the-right-metric","chapter":"8 Model Evaluation","heading":"Choosing the Right Metric","text":"F1-score valuable metric, assumes precision recall equally important, may always align priorities particular problem. medical diagnostics, recall (ensuring cases missed) might critical precision. spam filtering, precision (avoiding false positives) might take precedence prevent misclassifying important emails.comprehensive evaluation, now turn metrics assess performance across different classification thresholds. Instead relying fixed decision threshold, metrics analyze model behaves classification cutoff changes. leads us Receiver Operating Characteristic (ROC) curve Area Curve (AUC), provide insights well model distinguishes positive negative cases.","code":""},{"path":"chapter-evaluation.html","id":"taking-uncertainty-into-account","chapter":"8 Model Evaluation","heading":"8.4 Taking Uncertainty into Account","text":"evaluating classification model, metrics precision, recall, F1-score provide valuable insights performance. However, metrics based discrete predictions, observation classified either positive negative. approach overlooks important factor: uncertainty. Many classification models, including k-Nearest Neighbors (kNN), can output probability scores instead fixed labels, offering measure confidence prediction.probability scores allow us fine-tune decisions made adjusting classification threshold. default, threshold 0.5 commonly used, meaning model assigns probability 50% higher positive class, instance classified positive; otherwise, classified negative. However, default may always ideal. Adjusting threshold can significantly impact model performance, allowing better align business goals domain-specific needs. example, applications, missing true positives (false negatives) far costlier misclassifying negatives positives—vice versa. experimenting different thresholds, can explore trade-offs sensitivity, specificity, precision, recall optimize model decisions.Example 8.2  Let’s revisit kNN model Example 8.1, predicts customer churn (churn = yes). time, instead making discrete predictions, obtain probability scores positive class (churn = yes) setting type parameter \"prob\" kNN() function:output displays first 10 probability scores class: first column corresponds churn = yes, second column corresponds churn = . instance, first row probability 0.4, model 40% confident customer churn, probability 0.6 suggests 60% confidence customer churn.demonstrate impact threshold selection, compute confusion matrices two different thresholds: default 0.5 stricter 0.7 threshold.threshold 0.5, model classifies customer churner probability churn least 50%. confusion matrix aligns one Example 8.1. However, raise threshold 0.7, model becomes conservative, requiring least 70% confidence classifying instance churn. shifts balance true positives, false positives, false negatives:Lowering threshold increases sensitivity, catching true positives potentially leading false positives.Raising threshold increases specificity, reducing false positives risk missing true positives.Adjusting threshold particularly useful cases cost false positives false negatives equal. example, fraud detection, false negatives (missing fraudulent transactions) can costly, lowering threshold prioritize recall (sensitivity) might preferable. Conversely, spam detection, false positives (flagging legitimate emails spam) undesirable, higher threshold might used prioritize precision.","code":"kNN_prob = kNN(formula = formula, train = train_set, \n               test = test_set, k = 5, scaler = \"minmax\",\n               type = \"prob\")\nkNN_prob[1:10, ]\n      yes  no\n   6  0.4 0.6\n   10 0.2 0.8\n   17 0.0 1.0\n   19 0.0 1.0\n   21 0.0 1.0\n   23 0.2 0.8\n   29 0.0 1.0\n   31 0.0 1.0\n   36 0.0 1.0\n   40 0.0 1.0conf.mat(kNN_prob[, 1], actual_test, reference = \"yes\", cutoff = 0.5)\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856\nconf.mat(kNN_prob[, 1], actual_test, reference = \"yes\", cutoff = 0.7)\n          Actual\n   Predict yes  no\n       yes  22   1\n       no  115 862"},{"path":"chapter-evaluation.html","id":"choosing-an-optimal-threshold","chapter":"8 Model Evaluation","heading":"Choosing an Optimal Threshold","text":"Fine-tuning threshold allows us align model behavior business domain-specific requirements. Suppose need sensitivity least 90% ensure churners detected. iteratively adjusting threshold recalculating sensitivity, can determine cutoff achieves goal. process known setting operating point model.However, threshold adjustments always involve trade-offs. lower threshold improves recall may reduce precision increasing false positives. Conversely, higher threshold increases precision may lower recall missing true positives. instance, setting threshold 0.9 might achieve near-perfect specificity miss actual churners.manually tuning threshold can helpful, systematic approach needed evaluate model performance across possible thresholds. leads us Receiver Operating Characteristic (ROC) curve Area Curve (AUC), provide comprehensive way assess model’s ability distinguish classes.","code":""},{"path":"chapter-evaluation.html","id":"roc-curve-and-auc","chapter":"8 Model Evaluation","heading":"8.5 ROC Curve and AUC","text":"adjusting classification thresholds provides valuable insights, often impractical systematically comparing models. Additionally, sensitivity, specificity, precision, recall evaluate model fixed threshold, offering snapshot performance. Instead, need way assess performance across range thresholds, revealing broader trends model behavior. Models similar overall accuracy may perform differently—one might excel detecting positive cases misclassify many negatives, another might opposite. systematically evaluate model’s ability distinguish positive negative cases across thresholds, use Receiver Operating Characteristic (ROC) curve associated metric, Area Curve (AUC).ROC curve visually represents trade-sensitivity (true positive rate) specificity (true negative rate) various classification thresholds. plots True Positive Rate (Sensitivity) False Positive Rate (1 - Specificity). Originally developed radar signal detection World War II, ROC curve now widely used machine learning assess classifier effectiveness.Figure 8.1 illustrates key ROC curve characteristics. vertical axis represents True Positive Rate (Sensitivity), horizontal axis represents False Positive Rate (1 - Specificity). Three scenarios highlighted:Optimal Performance (Green Curve): model near-perfect performance passes top-left corner, achieving high sensitivity high specificity.Good Performance (Blue Curve): well-performing imperfect model remains closer top-left corner diagonal line.Random Classifier (Diagonal Line): gray dashed diagonal represents model predictive power, classifying randomly. model close line provides little practical utility.\nFigure 8.1: ROC curve illustrates trade-sensitivity specificity different thresholds. diagonal line represents classifier predictive value (gray dashed line), curves represent varying levels performance: green optimal blue good.\npoint ROC curve corresponds specific threshold. threshold varies, True Positive Rate (Sensitivity) False Positive Rate (1 - Specificity) change, tracing curve. curve remains close top-left corner indicates better performance, model achieves high sensitivity minimizing false positives. However, moving along curve reflects different trade-offs sensitivity specificity. Choosing optimal threshold depends application:medical diagnostics, maximizing sensitivity ensures cases missed, even results false positives.fraud detection, prioritizing specificity prevents legitimate transactions falsely flagged.construct ROC curve, classifier’s predictions sorted estimated probabilities positive class. Starting origin, prediction’s impact sensitivity specificity plotted. Correct predictions (true positives) result vertical movements, incorrect predictions (false positives) lead horizontal shifts.Example 8.3  Let’s apply concept k-Nearest Neighbors (kNN) model Example 8.2, obtained probabilities positive class (churn = yes). ’ll use probabilities generate ROC curve model. pROC package R simplifies process. Ensure package installed using install.packages(\"pROC\") proceeding.create ROC curve, two inputs needed: estimated probabilities positive class actual class labels. Using roc() function pROC package, generate ROC curve object:can visualize ROC curve using ggroc() function pROC package plot() function basic display. ’s ROC curve kNN model:\nFigure 8.2: ROC curve KNN k = 5, based churn data.\nROC curve visually demonstrates model’s performance across different thresholds. curve closer top-left corner indicates better performance, achieves high sensitivity specificity. diagonal line represents random classifier, serving baseline comparison. case, kNN model’s ROC curve much closer top-left corner, suggesting strong performance distinguishing churners non-churners.","code":"\nlibrary(pROC)\n\nroc_knn <- roc(response = actual_test, predictor = kNN_prob[, 1])\nggroc(roc_knn, colour = \"blue\") +\n    ggtitle(\"ROC curve for KNN with k = 5, based on churn data\")"},{"path":"chapter-evaluation.html","id":"area-under-the-curve-auc","chapter":"8 Model Evaluation","heading":"Area Under the Curve (AUC)","text":"Another critical metric derived ROC curve Area Curve (AUC), quantifies overall performance model. AUC represents probability randomly chosen positive instance higher predicted score randomly chosen negative instance. Mathematically, AUC computed :\\[\n\\text{AUC} = \\int_{0}^{1} \\text{TPR}(t) \\, d\\text{FPR}(t)\n\\]\\(t\\) represents threshold, reinforcing AUC measures model’s ability rank positive cases negative ones across possible thresholds.\nFigure 8.3: AUC summarizes ROC curve single number, representing model’s ability rank positive cases higher negative ones. AUC = 1: Perfect model. AUC = 0.5: better random guessing.\nAUC values range 0 1, value 1 indicates perfect classifier ideal discrimination classes, value 0.5 suggests better performance random guessing. AUC values 0.5 1 represent varying levels model performance, higher values reflecting better separation positive negative cases.kNN model, compute AUC using auc() function pROC package:AUC value model 0.849, meaning model ranks positive cases higher negative ones probability 0.849.summary, ROC curve AUC provide comprehensive way evaluate classification models, enabling comparisons across multiple models identifying optimal thresholds specific tasks. tools particularly valuable imbalanced datasets, capture trade-offs sensitivity specificity across classification thresholds. combining insights metrics like precision, recall, F1-score, can develop deeper understanding model performance select best approach given problem.next section, extend discussion multi-class classification, target variable two possible categories, requiring modifications standard evaluation metrics.","code":"auc(roc_knn)\n   Area under the curve: 0.8494"},{"path":"chapter-evaluation.html","id":"metrics-for-multi-class-classification","chapter":"8 Model Evaluation","heading":"8.6 Metrics for Multi-Class Classification","text":"far, focused binary classification, target variable two categories. However, many real-world problems involve multi-class classification, target variable can belong three categories. Examples include classifying species ecological studies identifying different types vehicles. Evaluating models requires extending performance metrics handle multiple categories effectively.multi-class classification, confusion matrix expands include classes, row representing actual class column representing predicted class. Correct predictions appear along diagonal, -diagonal elements indicate misclassifications. structure highlights classes model struggles distinguish.Metrics accuracy, precision, recall, F1-score can adapted multi-class problems. Instead evaluating single positive class, assess class positive class treating classes negative. one-vs-(also known one-vs-rest) approach allows calculation precision, recall, F1-score class separately. summarize overall performance, different averaging techniques used:Macro-average: Computes unweighted mean metric across classes, treating class equally. useful classes equal importance, regardless frequency dataset.Micro-average: Aggregates predictions across classes computing metric, giving weight larger classes. particularly useful dataset uneven class distribution, provides representative measure overall model performance.Weighted-average: Similar macro-averaging weights class’s metric frequency dataset. ensures larger classes contribute proportionally preventing minority classes overshadowed.averaging methods ensure fair evaluation, particularly imbalanced datasets classes may significantly fewer samples others.metrics ROC curve AUC primarily designed binary classification, can extended multi-class problems using strategies like one-vs-, ROC curve generated class others. However, practical applications, macro-averaged weighted-averaged F1-score provides concise meaningful summary multi-class model performance.applying metrics, can assess well model performs across categories, identify weaknesses specific classes, ensure evaluation aligns problem’s objectives. next section explores evaluation metrics regression models, target variable continuous rather categorical.","code":""},{"path":"chapter-evaluation.html","id":"evaluation-metrics-for-continuous-targets","chapter":"8 Model Evaluation","heading":"8.7 Evaluation Metrics for Continuous Targets","text":"far, focused evaluating classification models, predict discrete categories. However, many real-world problems involve predicting continuous target variables, house prices, stock market trends, weather forecasts. tasks require regression models, assessed using metrics specifically designed continuous data.widely used evaluation metric regression models Mean Squared Error (MSE):\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n\\(y_i\\) represents actual value, \\(\\hat{y}_i\\) predicted value, \\(n\\) number observations. MSE calculates average squared difference predicted actual values, larger errors contributing disproportionately due squaring. result, MSE particularly sensitive outliers. Lower values indicate better model performance, zero representing perfect fit.MSE useful, sensitivity large errors may always desirable. robust alternative Mean Absolute Error (MAE), measures average absolute difference predicted actual values:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^{n} |y_i - \\hat{y}_i|\n\\]\nUnlike MSE, MAE treats errors equally, making less sensitive extreme values easier interpret. particularly useful target variable skewed distribution outliers present.Another key metric evaluating regression models \\(R^2\\) score, coefficient determination. \\(R^2\\) score measures proportion variance target variable model explains. defined :\n\\[\nR^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n\\]\n\\(\\bar{y}\\) mean actual values. \\(R^2\\) score ranges 0 1, higher values indicate better fit. \\(R^2\\) value 1 means model perfectly predicts target variable, value 0 suggests model performs better simply predicting mean target variable.metrics provide different perspectives model performance. MSE useful penalizing larger errors important, MAE preferable interpretability robustness outliers, \\(R^2\\) helps quantify well model explains variability data. choice metric depends specific problem goals. Chapter 10, explore evaluation metrics greater depth, alongside various regression modeling techniques.","code":""},{"path":"chapter-evaluation.html","id":"key-takeaways-from-model-evaluation","chapter":"8 Model Evaluation","heading":"8.8 Key Takeaways from Model Evaluation","text":"chapter, explored critical step model evaluation, determines well model performs whether meets requirements problem hand. Starting foundational concepts, examined metrics evaluating classification models, including binary, multi-class, regression models.","code":""},{"path":"chapter-evaluation.html","id":"key-takeaways-1","chapter":"8 Model Evaluation","heading":"Key Takeaways","text":"Binary Classification Metrics:\nbegan understanding confusion matrix, categorizes predictions true positives, true negatives, false positives, false negatives. , derived key metrics accuracy, sensitivity (recall), specificity, precision, F1-score, offering different perspectives model performance.Binary Classification Metrics:\nbegan understanding confusion matrix, categorizes predictions true positives, true negatives, false positives, false negatives. , derived key metrics accuracy, sensitivity (recall), specificity, precision, F1-score, offering different perspectives model performance.Threshold Tuning:\nRecognizing impact probability thresholds model predictions, discussed adjusting thresholds can help align model specific goals, maximizing sensitivity critical applications prioritizing specificity avoid false positives.Threshold Tuning:\nRecognizing impact probability thresholds model predictions, discussed adjusting thresholds can help align model specific goals, maximizing sensitivity critical applications prioritizing specificity avoid false positives.ROC Curve AUC:\nevaluate model performance across possible thresholds, introduced Receiver Operating Characteristic (ROC) curve Area Curve (AUC). tools provide systematic visual way assess model’s ability distinguish classes, making particularly useful comparing multiple models.ROC Curve AUC:\nevaluate model performance across possible thresholds, introduced Receiver Operating Characteristic (ROC) curve Area Curve (AUC). tools provide systematic visual way assess model’s ability distinguish classes, making particularly useful comparing multiple models.Multi-Class Classification:\nclassification problems involving two classes, extended metrics precision, recall, F1-score calculating per-class metrics aggregating using methods macro-average, micro-average, weighted-average. approaches ensure balanced evaluation, especially dealing imbalanced datasets.Multi-Class Classification:\nclassification problems involving two classes, extended metrics precision, recall, F1-score calculating per-class metrics aggregating using methods macro-average, micro-average, weighted-average. approaches ensure balanced evaluation, especially dealing imbalanced datasets.Regression Metrics:\nproblems involving continuous target variables, introduced evaluation metrics mean squared error (MSE), mean absolute error (MAE), \\(R^2\\) score. metrics allow assessing prediction accuracy accounting trade-offs penalizing large errors (MSE) ensuring interpretability (MAE).Regression Metrics:\nproblems involving continuous target variables, introduced evaluation metrics mean squared error (MSE), mean absolute error (MAE), \\(R^2\\) score. metrics allow assessing prediction accuracy accounting trade-offs penalizing large errors (MSE) ensuring interpretability (MAE).","code":""},{"path":"chapter-evaluation.html","id":"closing-thoughts","chapter":"8 Model Evaluation","heading":"Closing Thoughts","text":"chapter emphasized single metric can fully capture model’s performance. Instead, evaluation guided specific goals constraints problem, balancing trade-offs accuracy versus interpretability false positives versus false negatives. Proper evaluation ensures model accurate also actionable reliable real-world applications.mastering evaluation techniques, now equipped critically assess model performance, optimize thresholds, select right model task hand. following chapters, build foundation explore advanced modeling techniques evaluation greater detail.","code":""},{"path":"chapter-evaluation.html","id":"exercises-6","chapter":"8 Model Evaluation","heading":"8.9 Exercises","text":"","code":""},{"path":"chapter-evaluation.html","id":"conceptual-questions-4","chapter":"8 Model Evaluation","heading":"Conceptual Questions","text":"model evaluation important machine learning?Explain difference training accuracy test accuracy.confusion matrix, useful?choice positive class impact evaluation metrics?difference sensitivity specificity?prioritize sensitivity specificity? Provide example.precision, differ recall?use F1-score instead relying solely accuracy?Explain trade-precision recall. changing classification threshold impact ?ROC curve, help compare different models?Area Curve (AUC) represent? interpret different AUC values?can adjusting classification thresholds optimize model performance specific business need?accuracy often misleading imbalanced datasets? alternative metrics can used?macro-average micro-average F1-scores, used?Explain multi-class classification evaluation differs binary classification.Mean Squared Error (MSE), used regression models?Mean Absolute Error (MAE) compare MSE? prefer one ?\\(R^2\\) score regression, indicate?Can \\(R^2\\) score negative? mean happens?important evaluate models using multiple metrics instead relying single one?","code":""},{"path":"chapter-evaluation.html","id":"hands-on-practice-evaluating-models-with-the-bank-dataset","chapter":"8 Model Evaluation","heading":"Hands-On Practice: Evaluating Models with the Bank Dataset","text":"exercises, use bank dataset liver package. dataset contains information customer demographics financial details, target variable deposit indicating whether customer subscribed term deposit.Load necessary package dataset:","code":"library(liver)\n\n# Load the dataset\ndata(bank)\n\n# View the structure of the dataset\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-evaluation.html","id":"data-preparation-1","chapter":"8 Model Evaluation","heading":"Data Preparation","text":"Load bank dataset identify target variable predictor variables.Check class imbalance target variable (deposit). many customers subscribed term deposit versus ?Apply one-hot encoding categorical variables using one.hot().Partition dataset 80% training 20% test sets using partition().Validate partitioning comparing class distribution deposit training test sets.Apply min-max scaling numerical variables ensure fair distance calculations kNN models.","code":""},{"path":"chapter-evaluation.html","id":"model-training-and-evaluation","chapter":"8 Model Evaluation","heading":"Model Training and Evaluation","text":"Train kNN model using training set predict deposit test set.Generate confusion matrix test set predictions using conf.mat(). Interpret results.Compute accuracy, sensitivity, specificity kNN model.Calculate precision, recall, F1-score model.Use conf.mat.plot() visualize confusion matrix.Experiment different values \\(k\\) (e.g., 3, 7, 15) compare evaluation metrics.Plot ROC curve kNN model using pROC package.Compute AUC model. value indicate performance?Adjust classification threshold (e.g., 0.5 0.7) analyze impacts sensitivity specificity.","code":""},{"path":"chapter-evaluation.html","id":"critical-thinking-and-real-world-applications-1","chapter":"8 Model Evaluation","heading":"Critical Thinking and Real-World Applications","text":"Suppose bank wants minimize false positives (incorrectly predicting customer subscribe). classification threshold adjusted?detecting potential subscribers priority, model prioritize precision recall? ?dataset highly imbalanced, strategies used improve model evaluation?Consider fraud detection system false negatives (missed fraud cases) extremely costly. adjust evaluation approach?Imagine comparing two models: one high accuracy low recall, slightly lower accuracy high recall. decide use?new marketing campaign resulted large increase term deposit subscriptions, might affect evaluation metrics?Given evaluation results model, business recommendations make financial institution?","code":""},{"path":"chapter-bayes.html","id":"chapter-bayes","chapter":"9 Naive Bayes Classifier","heading":"9 Naive Bayes Classifier","text":"can make highly accurate predictions minimal data computation? Imagine bank deciding whether approve loan based factors—customer’s income, age, mortgage status. Naive Bayes classifier offers remarkably simple yet effective approach problems, relying probability theory make rapid, informed decisions.Naive Bayes probabilistic classification algorithm balances simplicity effectiveness, making widely used approach machine learning. belongs family classifiers based Bayes’ theorem operates key simplifying assumption: features conditionally independent given target class. assumption rarely true real-world data, allows fast computation efficient probability estimation, making algorithm highly scalable practical.Despite simplicity, Naive Bayes delivers strong performance variety applications, particularly text classification, spam detection, sentiment analysis, financial risk assessment. domains, feature dependencies often weak enough independence assumption significantly impact accuracy.Beyond theoretical foundations, Naive Bayes computationally efficient, making well-suited large-scale datasets high-dimensional feature spaces. instance, risk prediction, multiple financial indicators must analyzed, Naive Bayes can assess customer’s likelihood default milliseconds. intuitive probabilistic reasoning ease implementation make valuable tool beginners experienced practitioners.power Naive Bayes comes foundation Bayesian probability theory, specifically Bayes’ Theorem, introduced 18th-century mathematician Thomas Bayes.6 theorem provides mathematical framework updating probability estimates new data becomes available. combining prior knowledge new evidence, Bayes’ theorem serves basis many Bayesian methods statistics machine learning.","code":""},{"path":"chapter-bayes.html","id":"strengths-and-limitations","chapter":"9 Naive Bayes Classifier","heading":"Strengths and Limitations","text":"Naive Bayes classifier widely valued simplicity efficiency. offers several advantages:performs well high-dimensional datasets, text classification problems thousands features.computationally efficient, making ideal real-time applications like spam filtering risk prediction.remains effective even independence assumption violated, long feature dependencies strong.However, Naive Bayes also limitations:assumption features conditionally independent rarely true real-world datasets, especially features exhibit strong correlations.struggles continuous data unless Gaussian distribution assumed, may always appropriate.complex models, decision trees gradient boosting, often outperform Naive Bayes datasets intricate relationships features.Despite limitations, Naive Bayes remains essential tool machine learning. ease implementation, interpretability, strong baseline performance make valuable first-choice model many classification tasks.","code":""},{"path":"chapter-bayes.html","id":"what-this-chapter-covers","chapter":"9 Naive Bayes Classifier","heading":"What This Chapter Covers","text":"chapter provides comprehensive exploration Naive Bayes classifier. Specifically, :Explain mathematical foundations Naive Bayes, focusing Bayes’ theorem role probabilistic classification.Walk mechanics Naive Bayes step--step examples.Introduce different variants algorithm—Gaussian Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes—discuss appropriate use cases.Examine practical considerations, including strengths, limitations, real-world applications.Implement Naive Bayes R using risk dataset liver package demonstrate effectiveness.end chapter, thorough understanding Naive Bayes classifier, equipping apply confidently real-world classification problems.","code":""},{"path":"chapter-bayes.html","id":"bayes-theorem-and-probabilistic-foundations","chapter":"9 Naive Bayes Classifier","heading":"9.1 Bayes’ Theorem and Probabilistic Foundations","text":"evaluating financial risk, update beliefs borrower’s likelihood defaulting new information—income, debt, mortgage status—becomes available? ability quantify uncertainty refine predictions new evidence arises essential decision-making, precisely Bayes’ Theorem provides.theorem forms foundation probabilistic learning, helping us make data-driven decisions across diverse fields, including finance, medicine, machine learning. determining whether loan applicant poses financial risk, often start general expectations based population statistics (prior knowledge). However, additional details—mortgage status outstanding loans—become available, new evidence refines estimate (posterior probability), leading informed decisions.foundation method laid Thomas Bayes, 18th-century Presbyterian minister self-taught mathematician. pioneering work introduced systematic approach updating probabilities new data emerges, forming basis now known Bayesian inference. interested exploring concept may find book “Everything Predictable: Bayesian Statistics Explain World” insightful. author argues Bayesian statistics help predict future also shape rational decision-making everyday life.","code":""},{"path":"chapter-bayes.html","id":"the-essence-of-bayes-theorem","chapter":"9 Naive Bayes Classifier","heading":"The Essence of Bayes’ Theorem","text":"Bayes’ Theorem provides systematic way update probabilities light new evidence. answers question: Given already know, belief hypothesis change observe new data?Mathematically, Bayes’ Theorem expressed :\\[\\begin{equation}\nP(|B) = P() \\cdot \\frac{P(B|)}{P(B)}\n\\tag{9.1}\n\\end{equation}\\]:\\(P(|B)\\) posterior probability, representing probability event \\(\\) (hypothesis) given event \\(B\\) (evidence) occurred.\\(P()\\) prior probability, reflects initial belief \\(\\) considering \\(B\\).\\(P(B|)\\) likelihood, representing probability observing \\(B\\) assuming \\(\\) true.\\(P(B)\\) evidence, accounts total probability observing \\(B\\).Bayes’ Theorem provides structured way refine understanding uncertainty combining prior knowledge new observations. principle underpins many probabilistic learning techniques, including Naive Bayes classifier.illustrate application, consider financial risk assessment scenario risk dataset liver package. Suppose want estimate probability customer good risk profile (\\(\\)) given mortgage (\\(B\\)). Financial institutions often use risk models assess creditworthiness based various factors, including mortgage status.Example 9.1  Let’s use risk dataset calculate probability customer classified good risk, given mortgage. start loading dataset inspecting relevant data:improve readability, add row column totals contingency table:Now, define relevant events:\\(\\): customer good risk profile.\\(B\\): customer mortgage (mortgage = yes).prior probability customer good risk given :\\[\nP() = \\frac{\\text{Total Good Risk Cases}}{\\text{Total Cases}} = \\frac{123}{246} = 0.5\n\\]Using Bayes’ Theorem, compute probability customer classified good risk given mortgage:\\[\\begin{equation}\n\\label{eq1}\n\\begin{split}\nP(\\text{Good Risk} | \\text{Mortgage = Yes}) & = \\frac{P(\\text{Good Risk} \\cap \\text{Mortgage = Yes})}{P(\\text{Mortgage = Yes})} \\\\\n& = \\frac{\\text{Good Risk Mortgage Cases}}{\\text{Total Mortgage Cases}} \\\\\n& = \\frac{81}{175} \\\\\n& = 0.463\n\\end{split}\n\\end{equation}\\]result indicates among customers mortgages, probability good risk profile lower general population. insights help financial institutions refine credit risk models incorporating new evidence systematically.","code":"library(liver)         \n\ndata(risk)\n\nxtabs(~ risk + mortgage, data = risk)\n              mortgage\n   risk        yes no\n     good risk  81 42\n     bad risk   94 29addmargins(xtabs(~ risk + mortgage, data = risk))\n              mortgage\n   risk        yes  no Sum\n     good risk  81  42 123\n     bad risk   94  29 123\n     Sum       175  71 246"},{"path":"chapter-bayes.html","id":"how-does-bayes-theorem-work","chapter":"9 Naive Bayes Classifier","heading":"How Does Bayes’ Theorem Work?","text":"Bayes’ Theorem provides structured way update understanding uncertainty based new information. many real-world scenarios, start initial belief event’s likelihood, gather data, refine belief make better-informed decisions.instance, financial risk assessment, banks initially estimate borrower’s risk level based general population statistics. However, collect details—income, credit history, mortgage status—Bayes’ Theorem allows update probability borrower classified high low risk. enables precise lending decisions.Beyond finance, Bayes’ Theorem widely applied domains:medical diagnostics, helps estimate probability disease (\\(\\)) given positive test result (\\(B\\)), incorporating test’s reliability disease’s prevalence.spam detection, computes probability email spam (\\(\\)) based presence certain keywords (\\(B\\)), refining predictions new messages analyzed.Probability theory provides rigorous mathematical structure reasoning uncertainty. Bayes’ Theorem extends enabling systematic approach learning data improving decision-making fields ranging healthcare finance beyond.","code":""},{"path":"chapter-bayes.html","id":"a-gateway-to-naive-bayes","chapter":"9 Naive Bayes Classifier","heading":"A Gateway to Naive Bayes","text":"Bayes’ Theorem provides mathematical foundation updating probabilities new evidence emerges. However, practical classification tasks, computing probabilities directly can computationally expensive, particularly datasets many features. Naive Bayes Classifier comes .Naive Bayes builds directly Bayes’ Theorem introducing key simplification: assumes features conditionally independent given target class. assumption rarely true real-world data, drastically reduces computational complexity, making algorithm highly efficient large-scale problems.Despite simplification, Naive Bayes performs remarkably well many applications. example, financial risk prediction, bank may assess borrower’s creditworthiness using features like income, loan history, mortgage status. factors may correlated, Naive Bayes assumes independent given borrower’s risk category, allowing rapid probability estimation classification.efficiency makes Naive Bayes particularly effective domains text classification, spam filtering, sentiment analysis, feature independence reasonable approximation. following sections, explore assumption enables fast, interpretable, scalable classification maintaining competitive performance.","code":""},{"path":"chapter-bayes.html","id":"why-is-it-called-naive","chapter":"9 Naive Bayes Classifier","heading":"9.2 Why is it Called “Naive”?","text":"Imagine assessing borrower’s financial risk based income, mortgage status, number loans. Intuitively, factors related—individuals higher income may better loan repayment histories, loans might higher probability financial distress. However, Naive Bayes assumes features independent know risk category (good risk bad risk).assumption makes algorithm “naive.” reality, features often correlated, income age, treating independent, Naive Bayes significantly simplifies probability calculations, making efficient scalable.illustrate, consider risk dataset liver package:dataset includes financial indicators age, income, marital status, mortgage, number loans. Naive Bayes assumes given person’s risk classification (good risk bad risk), features influence one another. Mathematically, probability customer good risk category given attributes expressed :\\[\nP(Y = y_1 | X_1, X_2, \\dots, X_5) = \\frac{P(Y = y_1) \\cdot P(X_1, X_2, \\dots, X_5 | Y = y_1)}{P(X_1, X_2, \\dots, X_5)}\n\\]However, directly computing \\(P(X_1, X_2, \\dots, X_5 | Y = y_1)\\) computationally expensive, especially number features grows. instance, datasets hundreds thousands features, storing calculating joint probabilities possible feature combinations becomes impractical.naive assumption conditional independence simplifies problem expressing joint probability product individual probabilities:\\[\nP(X_1, X_2, \\dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \\cdot P(X_2 | Y = y_1) \\cdots P(X_5 | Y = y_1)\n\\]transformation eliminates need compute complex joint probabilities, making algorithm scalable even high-dimensional data. Instead handling exponential number feature combinations, Naive Bayes requires computing simple conditional probabilities feature given class label.practice, independence assumption rarely true—features often exhibit degree correlation. However, Naive Bayes frequently performs well despite limitation. remains widely used domains :Feature dependencies weak enough assumption significantly impact accuracy.focus speed interpretability rather capturing complex relationships.Slight violations independence assumption severely affect predictive performance.example, risk prediction, income mortgage status likely correlated, treating independent still allows Naive Bayes classify borrowers effectively. Similarly, spam detection text classification, features (words email) often independent enough, algorithm delivers fast accurate predictions.balancing computational efficiency predictive power, Naive Bayes remains foundational algorithm machine learning, particularly applications demand scalability interpretability.","code":"str(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr.loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-bayes.html","id":"the-laplace-smoothing-technique","chapter":"9 Naive Bayes Classifier","heading":"9.3 The Laplace Smoothing Technique","text":"One challenges Naive Bayes classification handling feature categories appear test data absent training data. Suppose train model dataset borrowers classified “bad risk” married. later encounter married borrower test set, Naive Bayes compute \\(P(\\text{bad risk} | \\text{married})\\) zero. algorithm multiplies probabilities making predictions, even single zero probability results overall probability zero class, making impossible model predict class.issue arises Naive Bayes estimates probabilities frequency counts training data. feature value never appears given class, estimated probability zero, can lead misclassification errors. address , Laplace smoothing (also known add-one smoothing) used. Named mathematician Pierre-Simon Laplace, technique ensures every feature-category combination small, non-zero probability, even missing training data.illustrate, consider marital variable risk dataset. Suppose category married entirely absent customers labeled bad risk. scenario can visualized follows:Without smoothing, probability bad risk given married :\\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married})}{\\text{count}(\\text{married})} = \\frac{0}{\\text{count}(\\text{married})} = 0\n\\]means married borrower always classified good risk, regardless characteristics.Laplace smoothing resolves modifying probability calculation. Instead assigning strict zero probability, small constant \\(k\\) (usually \\(k = 1\\)) added count frequency table. adjusted probability given :\\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married}) + k}{\\text{count}(\\text{bad risk}) + k \\times \\text{number categories } \\text{marital}}\n\\]adjustment ensures :\n- Every category receives small positive count, avoiding zero probabilities.\n- total probability distribution remains valid.R, Laplace smoothing can applied using laplace argument naivebayes package. default, laplace = 0, meaning smoothing applied. apply smoothing, simply set laplace = 1:ensures category assigned probability zero, improving model’s robustness—particularly cases training data limited imbalanced.Laplace smoothing simple yet effective technique prevents Naive Bayes overly sensitive missing categories training data. \\(k = 1\\) common approach, value \\(k\\) can adjusted based specific domain knowledge. ensuring probabilities remain well-defined, Laplace smoothing enhances reliability Naive Bayes classifiers real-world applications.","code":"            risk\n   marital   good risk bad risk\n     single         21       11\n     married        51        0\n     other           8       10\nlibrary(naivebayes)\n\n# Fit Naive Bayes with Laplace smoothing\nformula_nb = risk ~ age + income + marital + mortgage + nr.loans\n\nmodel <- naive_bayes(formula = formula_nb, data = risk, laplace = 1)"},{"path":"chapter-bayes.html","id":"types-of-naive-bayes-classifiers","chapter":"9 Naive Bayes Classifier","heading":"9.4 Types of Naive Bayes Classifiers","text":"Naive Bayes versatile algorithm different variants designed specific data types distributions. choice variant use depends nature features assumptions made underlying distribution. three common types :Multinomial Naive Bayes: Best suited categorical count-based features, word frequencies text data. variant commonly used text classification, features represent discrete counts (e.g., number times word appears document). risk dataset, marital variable, takes categorical values single, married, , aligns well variant.Multinomial Naive Bayes: Best suited categorical count-based features, word frequencies text data. variant commonly used text classification, features represent discrete counts (e.g., number times word appears document). risk dataset, marital variable, takes categorical values single, married, , aligns well variant.Bernoulli Naive Bayes: Designed binary features, variable represents presence absence characteristic. variant particularly useful applications data represented set binary indicators, whether email contains specific keyword spam detection. risk dataset, mortgage variable, two possible values (yes ), example binary feature suitable approach.Bernoulli Naive Bayes: Designed binary features, variable represents presence absence characteristic. variant particularly useful applications data represented set binary indicators, whether email contains specific keyword spam detection. risk dataset, mortgage variable, two possible values (yes ), example binary feature suitable approach.Gaussian Naive Bayes: Applied continuous data features assumed follow normal (Gaussian) distribution. variant estimates likelihood feature using normal distribution, making ideal datasets numerical attributes age, income, credit scores. risk dataset, variables like age income continuous thus well suited variant.Gaussian Naive Bayes: Applied continuous data features assumed follow normal (Gaussian) distribution. variant estimates likelihood feature using normal distribution, making ideal datasets numerical attributes age, income, credit scores. risk dataset, variables like age income continuous thus well suited variant.Naive Bayes classifiers optimized different data types, making essential select one best fits dataset’s characteristics. Understanding distinctions allows better model selection improved performance. following sections, explore variant greater detail, examining assumptions, strengths, use cases.","code":""},{"path":"chapter-bayes.html","id":"case-study-predicting-financial-risk-with-naive-bayes","chapter":"9 Naive Bayes Classifier","heading":"9.5 Case Study: Predicting Financial Risk with Naive Bayes","text":"Financial institutions must assess loan applicants carefully balance profitability risk management. Lending decisions rely estimating likelihood default, depends various financial demographic factors. robust risk classification model helps institutions make informed decisions, reducing financial losses ensuring fair lending practices.case study, apply Naive Bayes classifier predict whether customer good risk bad risk based financial demographic attributes. Using risk dataset liver package R, train evaluate probabilistic classification model. case study demonstrates Naive Bayes can leveraged financial decision-making, providing insights customer risk profiles supporting effective credit evaluation.","code":""},{"path":"chapter-bayes.html","id":"problem-understanding-2","chapter":"9 Naive Bayes Classifier","heading":"Problem Understanding","text":"key challenge financial risk assessment distinguishing customers likely repay loans higher risk default. Predictive modeling enables financial institutions anticipate risk, optimize credit policies, reduce non-performing loans. Key business questions include:financial demographic factors contribute customer’s risk profile?can predict whether customer good bad risk approving loan?insights can gained refine lending policies mitigate financial losses?analyzing risk dataset, aim develop model classifies customers based risk level. allow lenders make data-driven decisions, improve credit scoring, enhance loan approval strategies.","code":""},{"path":"chapter-bayes.html","id":"data-understanding-1","chapter":"9 Naive Bayes Classifier","heading":"Data Understanding","text":"risk dataset contains financial demographic attributes help assess customer’s likelihood classified either good risk bad risk. dataset, included liver package, consists 246 observations 6 variables. provides structured way analyze customer characteristics predict financial risk levels.dataset includes 5 predictors binary target variable, risk, distinguishes customers less likely default. key variables :age: Customer’s age years.marital: Marital status (single, married, ).income: Annual income.mortgage: Indicates whether customer mortgage (yes, ).nr_loans: Number loans held customer.risk: target variable (good risk, bad risk).additional details dataset, refer documentation.begin analysis, load dataset examine structure understand variables data types:gain insights, summarize dataset’s key statistics:summary provides overview variable distributions identifies missing values potential anomalies. Since dataset appears clean well-structured, can proceed data preparation training Naive Bayes classifier.","code":"data(risk)\n\nstr(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr.loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ...summary(risk)\n         age           marital        income      mortgage     nr.loans    \n    Min.   :17.00   single :111   Min.   :15301   yes:175   Min.   :0.000  \n    1st Qu.:32.00   married: 78   1st Qu.:26882   no : 71   1st Qu.:1.000  \n    Median :41.00   other  : 57   Median :37662             Median :1.000  \n    Mean   :40.64                 Mean   :38790             Mean   :1.309  \n    3rd Qu.:50.00                 3rd Qu.:49398             3rd Qu.:2.000  \n    Max.   :66.00                 Max.   :78399             Max.   :3.000  \n           risk    \n    good risk:123  \n    bad risk :123  \n                   \n                   \n                   \n   "},{"path":"chapter-bayes.html","id":"preparing-data-for-modeling-1","chapter":"9 Naive Bayes Classifier","heading":"Preparing Data for Modeling","text":"training Naive Bayes classifier, need split dataset training testing sets. step ensures can evaluate well model generalizes unseen data. use 80/20 split, allocating 80% data training 20% testing. maintain consistency previous chapters, apply partition() function liver package:Setting set.seed(5) ensures reproducibility partitioning achieved time code run. train_set used train Naive Bayes classifier, test_set serve unseen data evaluate model’s predictions. test_labels vector contains true class labels test set, compare model’s predictions.verify training test sets representative original dataset, compare proportions marital variable across sets. chi-squared test used check whether distribution marital statuses (single, married, ) statistically similar training test sets:statistical test evaluates whether proportions marital categories differ significantly training test sets. hypotheses test :\\[\n\\begin{cases}\nH_0:  \\text{proportions marital categories sets.}\\\\\nH_a:  \\text{least one proportions different.}\n\\end{cases}\n\\]\nSince p-value exceeds \\(\\alpha = 0.05\\), fail reject \\(H_0\\), meaning marital status distribution remains statistically similar training test sets. confirms partitioning process maintains dataset’s characteristics, allowing reliable model evaluation.well-structured dataset validated partitioning process, now ready train Naive Bayes classifier assess predictive capabilities.","code":"\nset.seed(5)\n\ndata_sets = partition(data = risk, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$riskchisq.test(x = table(train_set$marital), y = table(test_set$marital))\n   \n    Pearson's Chi-squared test\n   \n   data:  table(train_set$marital) and table(test_set$marital)\n   X-squared = 6, df = 4, p-value = 0.1991"},{"path":"chapter-bayes.html","id":"applying-the-naive-bayes-classifier","chapter":"9 Naive Bayes Classifier","heading":"Applying the Naive Bayes Classifier","text":"dataset partitioned validated, now proceed train evaluate Naive Bayes classifier. objective build model using training set assess ability classify customers good risk bad risk test set.Several R packages provide implementations Naive Bayes, two commonly used options naivebayes e1071. case study, use naivebayes package, offers efficient implementation classifier. naive_bayes() function package supports various probability distributions depending nature features:Categorical distribution discrete variables marital mortgage.Bernoulli distribution binary features, special case categorical distribution.Poisson distribution count-based variables, number loans.Gaussian distribution continuous features, age income.Non-parametric density estimation continuous features specific distribution assumed.Unlike k-NN algorithm previous chapter, classifies new data without explicit training phase, Naive Bayes follows two-step process:Training phase – model learns probability distributions training data.Prediction phase – trained model used classify new data points based learned probabilities.train model, define formula risk target variable, features serve predictors:apply naive_bayes() function naivebayes package train classifier training dataset:naive_bayes() function estimates probability distributions feature, conditioned target class. Specifically:Categorical features (e.g., marital, mortgage) – function computes class-conditional probabilities.Continuous features (e.g., age, income, nr.loans) – function assumes Gaussian distribution calculates mean standard deviation class.inspect model’s learned probability distributions, summarize trained model:summary output provides useful insights classifier models feature’s probability distribution. forms basis making predictions new data points, explore next section.","code":"\nformula = risk ~ age + income + mortgage + nr.loans + maritallibrary(naivebayes)\n\nnaive_bayes = naive_bayes(formula, data = train_set)\n\nnaive_bayes\n   \n   ================================= Naive Bayes ==================================\n   \n   Call:\n   naive_bayes.formula(formula = formula, data = train_set)\n   \n   -------------------------------------------------------------------------------- \n    \n   Laplace smoothing: 0\n   \n   -------------------------------------------------------------------------------- \n    \n   A priori probabilities: \n   \n   good risk  bad risk \n   0.4923858 0.5076142 \n   \n   -------------------------------------------------------------------------------- \n    \n   Tables: \n   \n   -------------------------------------------------------------------------------- \n   :: age (Gaussian) \n   -------------------------------------------------------------------------------- \n         \n   age    good risk  bad risk\n     mean 46.453608 35.470000\n     sd    8.563513  9.542520\n   \n   -------------------------------------------------------------------------------- \n   :: income (Gaussian) \n   -------------------------------------------------------------------------------- \n         \n   income good risk  bad risk\n     mean 48888.987 27309.560\n     sd    9986.962  7534.639\n   \n   -------------------------------------------------------------------------------- \n   :: mortgage (Bernoulli) \n   -------------------------------------------------------------------------------- \n           \n   mortgage good risk  bad risk\n        yes 0.6804124 0.7400000\n        no  0.3195876 0.2600000\n   \n   -------------------------------------------------------------------------------- \n   :: nr.loans (Gaussian) \n   -------------------------------------------------------------------------------- \n           \n   nr.loans good risk  bad risk\n       mean 1.0309278 1.6600000\n       sd   0.7282057 0.7550503\n   \n   -------------------------------------------------------------------------------- \n   :: marital (Categorical) \n   -------------------------------------------------------------------------------- \n            \n   marital    good risk   bad risk\n     single  0.38144330 0.49000000\n     married 0.52577320 0.11000000\n     other   0.09278351 0.40000000\n   \n   --------------------------------------------------------------------------------summary(naive_bayes)\n   \n   ================================= Naive Bayes ================================== \n    \n   - Call: naive_bayes.formula(formula = formula, data = train_set) \n   - Laplace: 0 \n   - Classes: 2 \n   - Samples: 197 \n   - Features: 5 \n   - Conditional distributions: \n       - Bernoulli: 1\n       - Categorical: 1\n       - Gaussian: 3\n   - Prior probabilities: \n       - good risk: 0.4924\n       - bad risk: 0.5076\n   \n   --------------------------------------------------------------------------------"},{"path":"chapter-bayes.html","id":"prediction-and-model-evaluation","chapter":"9 Naive Bayes Classifier","heading":"Prediction and Model Evaluation","text":"training Naive Bayes classifier, evaluate performance applying test set, contains customers unseen training. goal compare predicted probabilities actual class labels stored test_labels.obtain predicted class probabilities, use predict() function naivebayes package:specifying type = \"prob\", function returns posterior probabilities class instead discrete predictions.inspect model’s predictions, display first 10 probability estimates:output contains two columns:first column represents probability customer classified “good risk.”second column represents probability customer classified “bad risk.”example, second row probability 0.987 “bad risk,” indicates second customer test set predicted belong “bad risk” category probability 0.987.probability-based output provides flexibility decision-making. Instead using fixed threshold 0.5, financial institutions can adjust cutoff based business objectives. instance, minimizing loan defaults priority, conservative threshold may set. next section, convert probabilities class predictions evaluate model using confusion matrix performance metrics.","code":"\nprob_naive_bayes = predict(naive_bayes, test_set, type = \"prob\")# Display the first 10 predictions\nround(head(prob_naive_bayes, n = 10), 3)\n         good risk bad risk\n    [1,]     0.001    0.999\n    [2,]     0.013    0.987\n    [3,]     0.000    1.000\n    [4,]     0.184    0.816\n    [5,]     0.614    0.386\n    [6,]     0.193    0.807\n    [7,]     0.002    0.998\n    [8,]     0.002    0.998\n    [9,]     0.378    0.622\n   [10,]     0.283    0.717"},{"path":"chapter-bayes.html","id":"confusion-matrix-1","chapter":"9 Naive Bayes Classifier","heading":"Confusion Matrix","text":"assess classification performance Naive Bayes model, compute confusion matrix using conf.mat() conf.mat.plot() functions liver package. confusion matrix compares predicted class probabilities actual class labels, allowing us measure model’s accuracy analyze different types errors.evaluation, apply classification threshold 0.5, meaning customer’s predicted probability “good risk” least 50%, model classifies “good risk”; otherwise, classified “bad risk.” Additionally, specify “good risk” reference class, meaning performance metrics sensitivity precision calculated respect category.confusion matrix provides following breakdown model predictions:True Positives (TP): Customers correctly classified “good risk.”True Negatives (TN): Customers correctly classified “bad risk.”False Positives (FP): Customers incorrectly classified “good risk” actually “bad risk.”False Negatives (FN): Customers incorrectly classified “bad risk” actually “good risk.”values confusion matrix quantify model’s classification accuracy error rates cutoff 0.5. Specifically, model correctly predicts “24 + 20” cases misclassifies “3 + 2” cases.matrix offers structured way assess classification performance, helping us understand well model differentiates high- low-risk customers. next section, analyze performance using additional evaluation metrics.","code":"# Extract probability of \"good risk\"\nprob_naive_bayes = prob_naive_bayes[, 1] \n\nconf.mat(prob_naive_bayes, test_labels, cutoff = 0.5, reference = \"good risk\")\n              Actual\n   Predict     good risk bad risk\n     good risk        24        3\n     bad risk          2       20\n\nconf.mat.plot(prob_naive_bayes, test_labels, cutoff = 0.5, reference = \"good risk\")"},{"path":"chapter-bayes.html","id":"roc-curve-and-auc-1","chapter":"9 Naive Bayes Classifier","heading":"ROC Curve and AUC","text":"evaluate model, compute Receiver Operating Characteristic (ROC) curve Area Curve (AUC) value. metrics provide comprehensive assessment model’s ability distinguish “good risk” “bad risk” customers across different classification thresholds. pROC package R facilitates calculations.ROC curve plots true positive rate (sensitivity) false positive rate (1 - specificity) various threshold values. curve remains closer top-left corner indicates well-performing model, curve near diagonal suggests performance close random guessing.Next, compute AUC value, summarizes ROC curve single number:AUC value, 0.957, represents probability randomly selected “good risk” customer receive higher predicted probability randomly selected “bad risk” customer. AUC closer 1 indicates strong predictive performance, AUC 0.5 suggests better performance random guessing.analyzing ROC curve AUC, financial institutions can adjust decision threshold align business objectives. minimizing false negatives (misclassifying high-risk customers low-risk) priority, threshold can lowered increase sensitivity. Conversely, false positives (denying loans eligible customers) concern, higher threshold can set improve specificity.case study, demonstrated Naive Bayes can applied financial risk assessment. evaluating model performance using confusion matrix, ROC curve, AUC, identified strengths limitations. highlights efficiency interpretability Naive Bayes, making valuable tool probabilistic classification financial decision-making.","code":"\nlibrary(pROC)          \n\nroc_naive_bayes = roc(test_labels, prob_naive_bayes)\n\nggroc(roc_naive_bayes)round(auc(roc_naive_bayes), 3)\n   [1] 0.957"},{"path":"chapter-bayes.html","id":"takeaways-from-the-case-study","chapter":"9 Naive Bayes Classifier","heading":"Takeaways from the Case Study","text":"case study demonstrated Naive Bayes can applied financial risk assessment classifying customers either good risk bad risk based demographic financial attributes. key evaluation metrics confusion matrix, ROC curve, AUC, analyzed model’s predictive power identified strengths limitations.results highlight efficiency, simplicity, interpretability Naive Bayes, making valuable tool probabilistic classification financial decision-making. model’s ability provide probability estimates allows institutions adjust decision thresholds based business priorities—whether prioritizing sensitivity minimize high-risk approvals improving specificity reduce false rejections.Naive Bayes performs well scenario, relies assumption feature independence, may always hold real-world financial data. Future improvements include using ensemble models integrating additional financial indicators refine predictions .applying Naive Bayes financial risk assessment, demonstrated probabilistic classification methods can support data-driven lending decisions, helping financial institutions manage risk effectively optimizing credit policies.","code":""},{"path":"chapter-bayes.html","id":"exercises-7","chapter":"9 Naive Bayes Classifier","heading":"9.6 Exercises","text":"","code":""},{"path":"chapter-bayes.html","id":"conceptual-questions-5","chapter":"9 Naive Bayes Classifier","heading":"Conceptual questions","text":"Naive Bayes considered probabilistic classification model?difference prior probability, likelihood, posterior probability Bayes’ theorem?mean say Naive Bayes assumes feature independence?situations feature independence assumption become problematic? Provide example.key strengths Naive Bayes? widely used text classification spam filtering?major limitations Naive Bayes, impact performance?Laplace smoothing help handling missing feature values Naive Bayes?use multinomial Naive Bayes, Bernoulli Naive Bayes, Gaussian Naive Bayes?Compare Naive Bayes classifier k-Nearest Neighbors algorithm (Chapter 7). assumptions outputs differ?choice probability threshold affect model predictions?Naive Bayes remain effective even independence assumption violated?type dataset characteristics make Naive Bayes perform poorly compared classifiers?Gaussian Naive Bayes classifier handle continuous data?can domain knowledge help improve Naive Bayes classification results?Naive Bayes handle imbalanced datasets? preprocessing techniques help?Explain prior probabilities can adjusted based business objectives classification problem.","code":""},{"path":"chapter-bayes.html","id":"hands-on-implementation-with-the-churn-dataset","chapter":"9 Naive Bayes Classifier","heading":"Hands-on implementation with the churn dataset","text":"following exercises, use churn dataset liver package. dataset contains information customer subscriptions, goal predict whether customer churn (churn = yes/) using Naive Bayes classifier. Section 4.3, performed exploratory data analysis dataset understand structure key features.","code":""},{"path":"chapter-bayes.html","id":"data-preparation-2","chapter":"9 Naive Bayes Classifier","heading":"Data preparation","text":"Load liver package churn dataset:Display structure summary statistics dataset examine variables distributions.Display structure summary statistics dataset examine variables distributions.Split dataset 80% training set 20% test set using partition() function liver package.Split dataset 80% training set 20% test set using partition() function liver package.Verify partitioning maintains distribution churn variable comparing proportions training test sets.Verify partitioning maintains distribution churn variable comparing proportions training test sets.","code":"\nlibrary(liver)\ndata(churn)"},{"path":"chapter-bayes.html","id":"training-and-evaluating-the-naive-bayes-classifier","chapter":"9 Naive Bayes Classifier","heading":"Training and evaluating the Naive Bayes classifier","text":"Based exploratory data analysis Section 4.3, select following predictors Naive Bayes model: account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, customer.calls. Define model formula:Train Naive Bayes classifier training set using naivebayes package.Train Naive Bayes classifier training set using naivebayes package.Summarize trained model. insights can gain estimated class-conditional probabilities?Summarize trained model. insights can gain estimated class-conditional probabilities?Use trained model predict class probabilities test set using predict() function naivebayes package.Use trained model predict class probabilities test set using predict() function naivebayes package.Extract examine first 10 probability predictions. Interpret values indicate likelihood customer churn.Extract examine first 10 probability predictions. Interpret values indicate likelihood customer churn.Compute confusion matrix using conf.mat() function liver package classification threshold 0.5.Compute confusion matrix using conf.mat() function liver package classification threshold 0.5.Visualize confusion matrix using conf.mat.plot() function liver package.Visualize confusion matrix using conf.mat.plot() function liver package.Compute key evaluation metrics, including accuracy, precision, recall, F1-score, based confusion matrix.Compute key evaluation metrics, including accuracy, precision, recall, F1-score, based confusion matrix.Lower classification threshold 0.5 0.3 recompute confusion matrix. adjusting threshold affect model performance?Lower classification threshold 0.5 0.3 recompute confusion matrix. adjusting threshold affect model performance?Plot ROC curve compute AUC value model. Interpret results terms model’s ability distinguish churn non-churn customers.Plot ROC curve compute AUC value model. Interpret results terms model’s ability distinguish churn non-churn customers.Interpret AUC value. indicate model’s ability distinguish churn non-churn customers?Interpret AUC value. indicate model’s ability distinguish churn non-churn customers?Train Naive Bayes model Laplace smoothing (laplace = 1) compare results model without smoothing. smoothing affect predictions?Train Naive Bayes model Laplace smoothing (laplace = 1) compare results model without smoothing. smoothing affect predictions?Compare Naive Bayes classifier k-Nearest Neighbors algorithm (Chapter 7) trained dataset. Evaluate performance using accuracy, precision, recall, F1-score, AUC. model performs better, factors might explain differences performance?Compare Naive Bayes classifier k-Nearest Neighbors algorithm (Chapter 7) trained dataset. Evaluate performance using accuracy, precision, recall, F1-score, AUC. model performs better, factors might explain differences performance?Experiment removing one predictor variable time retraining model. impact accuracy evaluation metrics?Experiment removing one predictor variable time retraining model. impact accuracy evaluation metrics?","code":"\nformula = churn ~ account.length + voice.plan + voice.messages + \n                 intl.plan + intl.mins + day.mins + eve.mins + \n                 night.mins + customer.calls"},{"path":"chapter-bayes.html","id":"real-world-application-and-critical-thinking","chapter":"9 Naive Bayes Classifier","heading":"Real-world application and critical thinking","text":"Suppose telecommunications company wants use model reduce customer churn. business decisions made based model’s predictions?Suppose telecommunications company wants use model reduce customer churn. business decisions made based model’s predictions?incorrectly predicting false negative (missed churner) costly false positive, decision threshold adjusted?incorrectly predicting false negative (missed churner) costly false positive, decision threshold adjusted?marketing team wants offer promotional discounts customers predicted churn. use model target right customers?marketing team wants offer promotional discounts customers predicted churn. use model target right customers?Suppose dataset included new feature: customer satisfaction score (scale 1 10). feature improve model?Suppose dataset included new feature: customer satisfaction score (scale 1 10). feature improve model?steps take model performed poorly new customer data?steps take model performed poorly new customer data?Explain feature independence may may hold dataset. feature correlation impact model’s reliability?Explain feature independence may may hold dataset. feature correlation impact model’s reliability?Naive Bayes suitable multi-class classification problems? , extend model predict multiple churn reasons instead just yes/?Naive Bayes suitable multi-class classification problems? , extend model predict multiple churn reasons instead just yes/?given time-series data customer interactions months, Naive Bayes still appropriate? ?given time-series data customer interactions months, Naive Bayes still appropriate? ?","code":""},{"path":"chapter-regression.html","id":"chapter-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10 Regression Analysis: Foundations and Applications","text":"Regression analysis fundamental tool statistical modeling centuries remains one versatile techniques data science. mathematical foundations established early statisticians Legendre Gauss, developed least squares method. Since , regression analysis evolved widely used framework examining relationships variables. advancements computing programming languages R, now accessible scalable addressing complex real-world problems.Regression models provide systematic approach quantifying relationships, uncovering patterns, making predictions. models applied across diverse fields, including economics, medicine, engineering, estimate effects, forecast outcomes, support data-driven decision-making. Whether predicting impact advertising expenditure sales, modeling housing prices, identifying risk factors disease, regression analysis serves cornerstone statistical modeling.\nCharles Wheelan describes Naked Statistics7, “Regression modeling hydrogen bomb statistics arsenal.” analogy highlights method’s immense power—used correctly, provides formidable tool making informed decisions, misuse can lead misleading conclusions.chapter provides structured introduction regression techniques, beginning simple linear regression extending multiple regression, generalized linear models (GLMs), non-linear regression approaches. Throughout, apply techniques real-world datasets, including online marketing dataset modeling impact digital advertising revenue housing price dataset explore relationship property attributes market value. end, readers solid foundation theoretical principles practical applications regression modeling R, enabling analyze interpret real-world data effectively.","code":""},{"path":"chapter-regression.html","id":"sec-simple-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.1 Simple Linear Regression","text":"Simple linear regression fundamental regression model, allowing us quantify relationship single predictor response variable. provides straightforward approach estimating changes one variable influence another. focusing single predictor, establish clear understanding regression mechanics extending model multiple predictors.illustrate simple linear regression, use marketing dataset liver package. dataset captures daily digital marketing activities impact revenue generation, making ideal real-world example regression analysis. includes key performance indicators online advertising campaigns, expenditure, user engagement metrics, daily revenue.dataset consists 40 observations 8 variables:spend: Daily expenditure pay-per-click (PPC) advertising.clicks: Number clicks advertisements.impressions: Number times ads displayed users.transactions: Number completed transactions per day.click.rate: Click-rate (CTR), calculated proportion impressions resulting clicks.conversion.rate: Conversion rate, representing proportion clicks leading transactions.display: Whether display campaign active (yes ).revenue: Total daily revenue (response variable).begin loading dataset examining structure:dataset contains 8 variables 40 observations. response variable, revenue, continuous, remaining 7 variables serve potential predictors.","code":"library(liver)\n\ndata(marketing, package = \"liver\")\n\nstr(marketing)\n   'data.frame':    40 obs. of  8 variables:\n    $ spend          : num  22.6 37.3 55.6 45.4 50.2 ...\n    $ clicks         : int  165 228 291 247 290 172 68 112 306 300 ...\n    $ impressions    : int  8672 11875 14631 11709 14768 8698 2924 5919 14789 14818 ...\n    $ display        : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ transactions   : int  2 2 3 2 3 2 1 1 3 3 ...\n    $ click.rate     : num  1.9 1.92 1.99 2.11 1.96 1.98 2.33 1.89 2.07 2.02 ...\n    $ conversion.rate: num  1.21 0.88 1.03 0.81 1.03 1.16 1.47 0.89 0.98 1 ...\n    $ revenue        : num  58.9 44.9 141.6 209.8 197.7 ..."},{"path":"chapter-regression.html","id":"exploring-relationships-in-the-data","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Exploring Relationships in the Data","text":"constructing regression model, first explore relationships variables ensure assumptions hold identify strong predictors. useful tool pairs.panels() function psych package, provides comprehensive overview pairwise relationships:visualization includes:Scatter plots (lower triangle), showing predictor relates response variable.Histograms (diagonal), illustrating distribution variable.Correlation coefficients (upper triangle), quantifying strength direction linear associations.correlation matrix, observe spend revenue exhibit strong positive correlation 0.79. suggests higher advertising expenditure associated higher revenue, making spend strong candidate predicting revenue.next section, formalize relationship using simple linear regression model.","code":"\nlibrary(psych)\n\npairs.panels(marketing)"},{"path":"chapter-regression.html","id":"fitting-a-simple-linear-regression-model","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Fitting a Simple Linear Regression Model","text":"logical starting point regression analysis examining relationship single predictor response variable. allows clearer understanding one variable influences another incorporating additional predictors complex models. , investigate advertising expenditure (spend) affects daily revenue (revenue) using simple linear regression model.fitting model, essential visualize relationship variables assess whether linear assumption reasonable. scatter plot fitted least-squares regression line provides insight strength direction relationship:\nFigure 10.1: Scatter plot daily revenue (€) versus daily spend (€) 40 observations, fitted least-squares regression line (blue) showing linear relationship.\nFigure 10.1 illustrates relationship spend revenue marketing dataset. scatter plot suggests positive association, indicating increased advertising expenditure generally linked higher revenue.simple linear regression model mathematically expressed :\\[\n\\hat{y} = b_0 + b_1x\n\\]:\\(\\hat{y}\\) represents predicted value response variable (revenue).\\(x\\) denotes predictor variable (spend).\\(b_0\\) intercept, indicating estimated revenue advertising expenditure made.\\(b_1\\) slope, representing expected change revenue one-unit increase spend.formulation provides framework estimating relationship advertising expenditure revenue, now proceed quantify.","code":""},{"path":"chapter-regression.html","id":"estimating-the-model-in-r","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Estimating the Model in R","text":"estimate regression coefficients, use lm() function R, fits linear model using least squares method. syntax follows format:analysis, model revenue function spend:fitting model, summarize results using summary() function:summary() output provides key insights estimated model. regression equation based estimated coefficients :\\[\n\\hat{\\text{revenue}} = 15.71 + 5.25 \\cdot \\text{spend}\n\\]:intercept (\\(b_0\\)) 15.71, representing estimated daily revenue money spent advertising (spend = 0).slope (\\(b_1\\)) 5.25, meaning additional €1 spent advertising, daily revenue expected increase approximately €5.25.Beyond estimated coefficients, summary() output provides several key metrics evaluating regression model:Estimate: estimated values intercept slope.Standard error: Measures variability coefficient estimate. Smaller standard errors indicate precise estimates.t-value p-value: t-value quantifies many standard errors coefficient zero, p-value assesses statistical significance. small p-value (typically < 0.05) suggests predictor significant impact response variable.Multiple R-squared (\\(R^2\\)): Measures proportion variance revenue explained spend. , \\(R^2 = 0.623\\), meaning 62.3% variation revenue explained advertising spend.Residual standard error (RSE): Provides estimate typical prediction error. case, \\(RSE = 93.82\\), indicating , average, predictions deviate actual revenue values approximately €93.82.results confirm statistically significant relationship advertising spend revenue, supporting use regression analysis business decision-making. next section, explore model can applied prediction residual analysis helps validate model assumptions.","code":"\nlm(response_variable ~ predictor_variable, data = dataset)\nsimple_reg = lm(revenue ~ spend, data = marketing)summary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09"},{"path":"chapter-regression.html","id":"interpreting-the-regression-line","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Interpreting the Regression Line","text":"regression line provides mathematical approximation relationship advertising spend revenue. model estimated, can used prediction. Suppose company wants estimate expected revenue day €25 spent pay-per-click (PPC) advertising. Using regression equation:\\[\\begin{equation}\n\\begin{split}\n\\hat{\\text{revenue}} & = 15.71 + 5.25 \\cdot 25 \\\\\n& = 147\n\\end{split}\n\\end{equation}\\]\nThus, predicted daily revenue approximately €147.predictive capability particularly valuable marketing teams planning advertising budgets. example, goal maximize returns controlling costs, model provides evidence-based estimate revenue responds different levels spending. Decision-makers can use information determine optimal advertising expenditures, set performance targets, allocate marketing resources efficiently.","code":""},{"path":"chapter-regression.html","id":"residuals-and-model-fit","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Residuals and Model Fit","text":"Residuals measure difference observed predicted values, providing insight well regression model fits data. residual observation calculated :\\[\n\\text{Residual} = y - \\hat{y}\n\\]\\(y\\) actual observed value, \\(\\hat{y}\\) predicted value regression model. example, suppose day dataset marketing spend €25 actual revenue 185.36. residual observation :\\[\\begin{equation}\n\\begin{split}\n\\text{Residual} & = 185.36 - 147 \\\\\n& = 38.36\n\\end{split}\n\\end{equation}\\]Residuals play crucial role assessing model adequacy. Ideally, randomly distributed around zero, indicating model captures relationship variables well. However, residuals exhibit systematic patterns—curvature increasing variance—suggests model fully capture relationship may require adjustments, incorporating additional predictors using non-linear model.regression line estimated using least squares method, finds line minimizes sum squared residuals, also known sum squared errors (SSE):\\[\\begin{equation}\n\\text{SSE} = \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2\n\\tag{10.1}\n\\end{equation}\\]\\(y_i\\) represents observed revenue, \\(\\hat{y}_i\\) predicted revenue, \\(n\\) number observations. Minimizing SSE ensures estimated regression line optimally represents relationship predictor response variable, leading accurate predictions.Monitoring residuals essential step regression analysis. residuals exhibit discernible pattern evenly spread around zero, linear model likely appropriate. However, residuals show trends increasing variability, refinement—adding interaction terms, transforming variables, considering different modeling approach—may necessary.summary, simple linear regression provides effective way model interpret relationship two variables. analyzing marketing dataset, demonstrated estimate, interpret, apply regression model make predictions. foundational understanding simple linear regression sets stage evaluating model quality extending framework multiple predictors following sections.","code":""},{"path":"chapter-regression.html","id":"hypothesis-testing-in-simple-linear-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Hypothesis Testing in Simple Linear Regression","text":"Hypothesis testing regression analysis helps determine whether predictor variable statistically significant relationship response variable. Specifically, test whether estimated slope \\(b_1\\) sample regression model provides evidence true linear relationship population, unknown slope denoted \\(\\beta_1\\).population regression equation models relationship predictor \\(x\\) response \\(y\\) entire population expressed :\\[\ny = \\beta_0 + \\beta_1x + \\epsilon\n\\]:\\(\\beta_0\\) represents population intercept, expected value \\(y\\) \\(x = 0\\).\\(\\beta_1\\) represents population slope, indicating \\(y\\) changes one-unit increase \\(x\\).\\(\\epsilon\\) random error term accounting variability \\(y\\) explained linear model.primary objective hypothesis testing regression determine whether slope \\(\\beta_1\\) significantly different zero. \\(\\beta_1 = 0\\), regression equation simplifies :\\[\ny = \\beta_0 + \\epsilon\n\\]suggests predictor \\(x\\) linear relationship response variable \\(y\\). Conversely, \\(\\beta_1 \\neq 0\\), statistical evidence association \\(x\\) \\(y\\). formally test , set following hypotheses:\\[\n\\begin{cases}\n  H_0: \\beta_1 =  0, \\quad \\text{(linear relationship \\( x \\) \\( y \\))}  \\\\\n  H_a: \\beta_1 \\neq 0, \\quad \\text{(linear relationship exists \\( x \\) \\( y \\))}\n\\end{cases}\n\\]estimated slope \\(b_1\\) sample data provides approximation \\(\\beta_1\\). assess significance, rely following key statistical measures:Standard error slope: Measures variability estimate \\(b_1\\).t-statistic: Determines many standard errors estimated slope zero. computed :\\[\nt = \\frac{b_1}{SE(b_1)}\n\\]p-value: Represents probability observing t-statistic extreme one calculated, assuming null hypothesis true. small p-value (typically less 0.05) provides strong evidence reject \\(H_0\\), indicating predictor significantly associated response variable.illustrate hypothesis testing simple linear regression, examine results model predicts revenue (daily revenue) based spend (advertising expenditure) using marketing dataset. estimated slope \\(b_1\\) spend :output:t-statistic slope 7.93.p-value 1.4150362^{-9}, close zero.Since p-value significantly smaller commonly used significance level (\\(\\alpha = 0.05\\)), reject null hypothesis \\(H_0\\). confirms predictor spend statistically significant effect revenue. Specifically:slope estimate \\(b_1 = 5.25\\) suggests additional €1 spent advertising, daily revenue expected increase approximately 5.25.strong statistical significance spend validates role important predictor revenue, supporting inclusion model.Hypothesis testing simple linear regression provides structured approach determining whether predictor variable meaningful impact response variable. statistically significant slope (\\(\\beta_1 \\neq 0\\)) indicates changes predictor \\(x\\) associated changes response \\(y\\), allowing data-driven decision-making.statistical significance establishes presence relationship, imply causation. Additional factors, potential confounders, omitted variables, model assumptions, considered interpreting regression results.next sections, explore techniques evaluating regression model quality, including measures goodness--fit model diagnostics. also extend concepts multiple predictors, enabling comprehensive analyses better predictions.","code":"summary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09"},{"path":"chapter-regression.html","id":"measuring-the-quality-of-a-regression-model","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Measuring the Quality of a Regression Model","text":"Evaluating effectiveness regression model goes beyond determining whether predictor statistically significant. hypothesis testing confirms whether predictor meaningful relationship response variable, assess well model fits data. measure model quality, rely additional metrics quantify predictive accuracy explanatory power. Two key statistics purpose Residual Standard Error (RSE) \\(R^2\\) (R-squared) statistic.","code":""},{"path":"chapter-regression.html","id":"residual-standard-error-rse","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Residual Standard Error (RSE)","text":"Residual Standard Error (RSE) provides estimate typical prediction error model. measures much observed values deviate predicted values average. formula RSE :\\[\nRSE = \\sqrt{\\frac{1}{n-p-1} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2},\n\\]\n\\(y_i\\) represents observed values response variable, \\(\\hat{y}_i\\) represents predicted values, \\(n\\) number observations, \\(p\\) number predictors model.smaller RSE indicates model precise predictions. example, simple linear regression model marketing dataset, RSE :value represents average deviation predicted revenue actual revenue. lower RSE suggests better-fitting model, though always interpreted context response variable’s scale.","code":"rse_value = sqrt(sum(simple_reg$residuals^2) / summary(simple_reg)$df[2])\nround(rse_value, 2)\n   [1] 93.82"},{"path":"chapter-regression.html","id":"r-squared-r2","chapter":"10 Regression Analysis: Foundations and Applications","heading":"R-squared (\\(R^2\\))","text":"\\(R^2\\) statistic measures well regression model explains variability response variable. defined :\\[\nR^2 = 1 - \\frac{SSE}{SST}\n\\]:\\(SST\\) (Total Sum Squares) represents total variability response variable fitting model.\\(SSE\\) (Sum Squared Errors) represents variability remains unexplained fitting model.\\(R^2\\) ranges 0 1, higher values indicate model explains greater proportion variability response variable. example, marketing dataset, \\(R^2\\) value :means 62.3% variation revenue explained spend. higher \\(R^2\\) values suggest better fit, guarantee model generalizes well new data. important supplement \\(R^2\\) additional model diagnostics.","code":"round(summary(simple_reg)$r.squared, 3)\n   [1] 0.623"},{"path":"chapter-regression.html","id":"relationship-between-r2-and-the-correlation-coefficient","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Relationship Between \\(R^2\\) and the Correlation Coefficient","text":"simple linear regression, \\(R^2\\) directly related correlation coefficient \\(r\\) predictor response variable:\\[\nR^2 = r^2\n\\]example, marketing dataset, correlation spend revenue :Squaring value gives:matches \\(R^2\\) value, reinforcing \\(R^2\\) quantifies strength linear relationship.","code":"round(cor(marketing$spend, marketing$revenue), 2)\n   [1] 0.79round(cor(marketing$spend, marketing$revenue)^2, 2)\n   [1] 0.62"},{"path":"chapter-regression.html","id":"adjusted-r-squared","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Adjusted R-squared","text":"\\(R^2\\) measures proportion variance explained model, Adjusted \\(R^2\\) accounts number predictors, ensuring adding unnecessary variables artificially inflate statistic. calculated :\\[\n\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\cdot \\frac{n-1}{n-p-1},\n\\]\n\\(n\\) number observations \\(p\\) number predictors.Adjusted \\(R^2\\) penalizes inclusion irrelevant predictors, making particularly useful multiple regression settings. simple linear regression (\\(p = 1\\)), \\(R^2\\) Adjusted \\(R^2\\) equal, multiple regression, Adjusted \\(R^2\\) often lower provides better measure model performance.","code":""},{"path":"chapter-regression.html","id":"interpreting-model-quality","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Interpreting Model Quality","text":"good regression model :low RSE, indicating predictions close observed values.high \\(R^2\\), suggesting model explains variability response variable.high Adjusted \\(R^2\\), ensuring additional predictors improve model rather introducing noise.However, model judged metrics alone. Even high \\(R^2\\) model may fail violates regression assumptions overfits data. Additional diagnostics, residual analysis cross-validation, essential ensure model reliability.understanding measures model quality, gain deeper insight effectiveness regression models prepare extending concepts multiple predictors next sections.","code":""},{"path":"chapter-regression.html","id":"sec-multiple-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.2 Multiple Linear Regression","text":"Simple linear regression useful modeling relationships two variables, many real-world applications, multiple factors influence response variable. Multiple linear regression extends simple regression incorporating multiple predictors, improving estimation accuracy predictive performance.illustrate, expand previous model, included spend predictor, adding display, indicator whether display advertising campaign active. additional predictor allows us assess impact revenue. general equation multiple regression model \\(p\\) predictors :\\[\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\\(\\beta_0\\) intercept, \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) represent estimated effects predictor response variable.case, equation two predictors, spend display, :\\[\n\\hat{\\text{revenue}} = \\beta_0 + \\beta_1 \\cdot \\text{spend} + \\beta_2 \\cdot \\text{display}\n\\]spend represents daily advertising expenditure display categorical variable (yes/), R automatically converts binary indicator. , display = 1 indicates active display campaign, display = 0 means display campaign.","code":""},{"path":"chapter-regression.html","id":"fitting-the-multiple-regression-model","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Fitting the Multiple Regression Model","text":"fit multiple regression model using lm() function R:estimated regression equation :\\[\n\\hat{\\text{revenue}} = -41.44 + 5.36 \\cdot \\text{spend} + 104.29 \\cdot \\text{display}\n\\]:\n- intercept (\\(\\beta_0\\)) -41.44, representing estimated revenue spend zero display campaign running.\n- coefficient spend (\\(\\beta_1\\)) 5.36, indicating additional €1 spent, revenue increases approximately 5.36, assuming display remains unchanged.\n- coefficient display (\\(\\beta_2\\)) 104.29, meaning display campaign active (display = 1), revenue increases approximately 104.29, holding spend constant.","code":"multiple_reg = lm(revenue ~ spend + display, data = marketing)\n\nsummary(multiple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend + display, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -189.420  -45.527    5.566   54.943  154.340 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) -41.4377    32.2789  -1.284 0.207214    \n   spend         5.3556     0.5523   9.698 1.05e-11 ***\n   display     104.2878    24.7353   4.216 0.000154 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 78.14 on 37 degrees of freedom\n   Multiple R-squared:  0.7455, Adjusted R-squared:  0.7317 \n   F-statistic: 54.19 on 2 and 37 DF,  p-value: 1.012e-11"},{"path":"chapter-regression.html","id":"making-predictions","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Making Predictions","text":"Consider scenario company spends €25 advertising running display campaign (display = 1). Using regression equation, predicted revenue :\\[\n\\hat{\\text{revenue}} = -41.44 + 5.36 \\cdot 25 + 104.29 \\cdot 1 = 196.74\n\\]Thus, predicted revenue day approximately €196.74.residual (prediction error) specific observation calculated difference actual predicted revenue:\\[\n\\text{Residual} = y - \\hat{y} = 185.36 - 196.74 = -11.49\n\\]prediction error smaller simple regression model, confirming including display improves predictive accuracy.","code":""},{"path":"chapter-regression.html","id":"evaluating-model-performance","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Evaluating Model Performance","text":"Adding display enhances regression model reducing prediction errors improving model fit. compare key performance metrics simple multiple regression models:Residual Standard Error (RSE): simple regression model, \\(RSE = 93.82\\), whereas multiple regression model, \\(RSE = 78.14\\). reduction RSE indicates improved prediction accuracy.Residual Standard Error (RSE): simple regression model, \\(RSE = 93.82\\), whereas multiple regression model, \\(RSE = 78.14\\). reduction RSE indicates improved prediction accuracy.\\(R^2\\) (R-squared): simple regression model \\(R^2 = 62\\%\\), whereas multiple regression model increased \\(R^2 = 75\\%\\), demonstrating improved explanatory power.\\(R^2\\) (R-squared): simple regression model \\(R^2 = 62\\%\\), whereas multiple regression model increased \\(R^2 = 75\\%\\), demonstrating improved explanatory power.Adjusted \\(R^2\\): Unlike \\(R^2\\), Adjusted \\(R^2\\) accounts number predictors. simple regression model, Adjusted \\(R^2 = 61\\%\\), multiple regression model, Adjusted \\(R^2 = 73\\%\\), confirming additional predictor contributes meaningfully model performance.Adjusted \\(R^2\\): Unlike \\(R^2\\), Adjusted \\(R^2\\) accounts number predictors. simple regression model, Adjusted \\(R^2 = 61\\%\\), multiple regression model, Adjusted \\(R^2 = 73\\%\\), confirming additional predictor contributes meaningfully model performance.","code":""},{"path":"chapter-regression.html","id":"key-takeaways-2","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Key Takeaways","text":"multiple regression model improves upon simple regression providing better fit, reducing prediction errors, enabling accurate estimation revenue drivers. Including display alongside spend strengthens model’s ability explain revenue variation. However, models grow complex, careful evaluation necessary prevent issues multicollinearity (high correlation predictors) overfitting (adding unnecessary predictors reduce generalizability).next sections, examine model assumptions, conduct diagnostics, refine regression models ensure validity reliability.","code":""},{"path":"chapter-regression.html","id":"generalized-linear-models-glms","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.3 Generalized Linear Models (GLMs)","text":"Linear regression provides useful framework modeling continuous outcomes, suitable response variable binary, count-based, follows distribution normal. Generalized Linear Models (GLMs) extend traditional linear regression introducing link function, transforms relationship predictors response variable, variance function, accounts non-constant variability response. extensions allow GLMs accommodate broader range response variable distributions, making widely applicable fields finance, healthcare, marketing.GLMs retain fundamental principles linear regression introduce three key components:\n1. Random component: Specifies probability distribution response variable, can belong exponential family (e.g., normal, binomial, Poisson distributions).\n2. Systematic component: Represents linear combination predictor variables.\n3. Link function: Transforms expected value response variable can modeled linear function predictors.following sections, introduce two commonly used GLMs:Logistic regression, models binary outcomes.Poisson regression, suited modeling count data.extending regression beyond continuous responses, models provide flexible interpretable framework analyzing data variety applications. next sections discuss theoretical foundations implementation R.","code":""},{"path":"chapter-regression.html","id":"logistic-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.4 Logistic Regression","text":"Logistic regression generalized linear model designed binary classification, response variable takes two values, 0/1 yes/. Instead predicting continuous outcome, logistic regression estimates probability observation belongs particular category. ensure predicted probabilities remain within range \\([0,1]\\), model applies logit function, transforms linear combination predictors probability scale:\\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\], \\(p\\) represents probability outcome 1, logit transformation ensures linear relationship predictors log-odds response variable.","code":""},{"path":"chapter-regression.html","id":"logistic-regression-in-r","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Logistic Regression in R","text":"illustrate logistic regression, use churn dataset, contains information customer behavior. objective predict whether customer churn (leave service) based customer characteristics service usage patterns. selected predictors include variables account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, customer.calls, capture aspects user engagement service utilization.R, logistic regression implemented using glm() function, fits generalized linear models. function follows syntax:response_variable binary outcome, predictor_variables independent variables, family = binomial specifies logistic regression model.churn dataset, fit logistic regression model follows:model estimates relationship predictors probability churn. examine model’s coefficients significance levels, use:output provides key information, including estimated coefficients, standard errors, z-statistics, p-values. small p-value (typically less 0.05) suggests corresponding predictor statistically significant effect probability churn. variable account.length large p-value, suggests predictor contribute significantly explaining churn may removed model. Refining model removing non-significant predictors re-evaluating improves interpretability predictive performance.","code":"\nglm(response_variable ~ predictor_variables, data = dataset, family = binomial)\ndata(churn)\n\nlogreg_1 = glm(churn ~ account.length + voice.messages + day.mins + eve.mins + \n                         night.mins + intl.mins + customer.calls + intl.plan + voice.plan, \n               data = churn, family = binomial)summary(logreg_1)\n   \n   Call:\n   glm(formula = churn ~ account.length + voice.messages + day.mins + \n       eve.mins + night.mins + intl.mins + customer.calls + intl.plan + \n       voice.plan, family = binomial, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n   (Intercept)     8.8917584  0.6582188  13.509  < 2e-16 ***\n   account.length -0.0013811  0.0011453  -1.206   0.2279    \n   voice.messages -0.0355317  0.0150397  -2.363   0.0182 *  \n   day.mins       -0.0136547  0.0009103 -15.000  < 2e-16 ***\n   eve.mins       -0.0071210  0.0009419  -7.561 4.02e-14 ***\n   night.mins     -0.0040518  0.0009048  -4.478 7.53e-06 ***\n   intl.mins      -0.0882514  0.0170578  -5.174 2.30e-07 ***\n   customer.calls -0.5183958  0.0328652 -15.773  < 2e-16 ***\n   intl.planno     2.0958198  0.1214476  17.257  < 2e-16 ***\n   voice.planno   -2.1637477  0.4836735  -4.474 7.69e-06 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for binomial family taken to be 1)\n   \n       Null deviance: 4075.0  on 4999  degrees of freedom\n   Residual deviance: 3174.3  on 4990  degrees of freedom\n   AIC: 3194.3\n   \n   Number of Fisher Scoring iterations: 6"},{"path":"chapter-regression.html","id":"poisson-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.5 Poisson Regression","text":"Poisson regression generalized linear model designed modeling count data, response variable represents number occurrences event within fixed interval. Examples include number customer service calls received daily, website visits per hour, purchases made per customer. Unlike linear regression, assumes normally distributed residuals, Poisson regression assumes response variable follows Poisson distribution mean equals variance. assumption makes Poisson regression particularly useful data non-negative integer counts.model formulated :\\[\n\\ln(\\lambda) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\\(\\lambda\\) represents expected count (mean) response variable, predictors \\(x_1, x_2, \\dots, x_p\\) influence log \\(\\lambda\\). logarithmic transformation ensures predicted values remain positive, preventing model producing negative counts.","code":""},{"path":"chapter-regression.html","id":"poisson-regression-in-r","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Poisson Regression in R","text":"illustrate Poisson regression, analyze customer service call frequency using churn dataset. objective model number customer service calls (customer.calls) based customer attributes service usage. Since customer.calls integer-valued response variable, Poisson regression appropriate linear regression.R, Poisson regression implemented using glm() function, similar logistic regression. syntax follows:example, fit Poisson regression model follows:, customer.calls response variable, predictors churn, intl.plan, day.mins help explain variations call frequency. family = poisson argument specifies model follows Poisson distribution.model fitted, examine results:summary output provides estimated coefficients, standard errors, z-statistics, p-values. small p-value (typically < 0.05) suggests predictor significantly influences expected number customer calls. predictors voice.messages night.mins large p-values, may contribute meaningfully can removed subsequent model refinements.Interpreting coefficients Poisson regression model differs linear regression. coefficient represents expected percentage change response variable one-unit increase predictor. instance, coefficient intl.plan 0.3, implies customers international plan make approximately \\(e^{0.3} - 1 \\approx 35\\%\\) service calls without one, holding predictors constant.summary, Poisson regression extends linear regression framework count data, making valuable tool event frequency modeling. Like logistic regression, belongs broader family generalized linear models, enabling flexible modeling beyond continuous response variables. iteratively refining model excluding non-significant predictors, ensure interpretable effective model practical applications.next sections, explore techniques validating improving regression models enhance predictive reliability.","code":"\nglm(response_variable ~ predictor_variables, data = dataset, family = poisson)\nformula = customer.calls ~ churn + voice.messages + day.mins + eve.mins + \n                           night.mins + intl.mins + intl.plan + voice.plan\n\nreg_pois = glm(formula, data = churn, family = poisson)summary(reg_pois)\n   \n   Call:\n   glm(formula = formula, family = poisson, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n   (Intercept)     0.9957186  0.1323004   7.526 5.22e-14 ***\n   churnno        -0.5160641  0.0304013 -16.975  < 2e-16 ***\n   voice.messages  0.0034062  0.0028294   1.204 0.228646    \n   day.mins       -0.0006875  0.0002078  -3.309 0.000938 ***\n   eve.mins       -0.0005649  0.0002237  -2.525 0.011554 *  \n   night.mins     -0.0003602  0.0002245  -1.604 0.108704    \n   intl.mins      -0.0075034  0.0040886  -1.835 0.066475 .  \n   intl.planno     0.2085330  0.0407760   5.114 3.15e-07 ***\n   voice.planno    0.0735515  0.0878175   0.838 0.402284    \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for poisson family taken to be 1)\n   \n       Null deviance: 5991.1  on 4999  degrees of freedom\n   Residual deviance: 5719.5  on 4991  degrees of freedom\n   AIC: 15592\n   \n   Number of Fisher Scoring iterations: 5"},{"path":"chapter-regression.html","id":"sec-stepwise-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.6 Model Selection Using Stepwise Regression","text":"Selecting right predictors essential building regression model accurate interpretable. process, known model specification, helps model retain essential relationships preventing overfitting excluding irrelevant predictors. Proper model specification enhances predictive accuracy ensures insights derived model remain meaningful.practice, datasets—especially business data science applications—often contain numerous potential predictors. Managing complexity requires systematic methods identifying relevant variables. One approach stepwise regression, iterative algorithm evaluates predictors based statistical contribution model. Stepwise regression iteratively adds removes predictors based statistical significance, ensuring relevant variables retained.Due structured approach, stepwise regression particularly useful small medium-sized datasets automated predictor selection improves model interpretability without excessive computational burden.","code":""},{"path":"chapter-regression.html","id":"the-role-of-aic-in-model-selection","chapter":"10 Regression Analysis: Foundations and Applications","heading":"The Role of AIC in Model Selection","text":"evaluate model quality selection process, use criteria Akaike Information Criterion (AIC). AIC provides trade-model complexity goodness fit, lower values indicate optimal balance explanatory power parsimony. defined :\\[\nAIC = 2p + n \\log\\left(\\frac{SSE}{n}\\right),\n\\]\n\\(p\\) represents number estimated parameters model, \\(n\\) number observations, \\(SSE\\) sum squared errors, representing total unexplained variability response variable measuring extent model fails account observed data.Unlike \\(R^2\\), always increases additional predictors included, AIC accounts overfitting introducing penalty model complexity. prevents overly complex models fit training data well fail generalize new observations. prioritizing models lower AIC, select achieve best balance simplicity predictive accuracy.","code":""},{"path":"chapter-regression.html","id":"implementing-stepwise-regression-in-r","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Implementing Stepwise Regression in R","text":"Stepwise regression implemented R using step() function, automates selection predictors find optimal model. function iteratively evaluates variables makes inclusion exclusion decisions based statistical criteria. Three approaches can specified using direction argument: \"forward\", starts predictors adds incrementally; \"backward\", begins predictors removes least significant ones; \"\", combines forward selection backward elimination refine model iterative process.Example 10.1  illustrate stepwise regression, apply marketing dataset, contains seven predictors. objective identify best regression model predicting revenue ensuring balance model complexity interpretability.begin fitting regression model includes available predictors:initial model includes predictors, may contribute meaningfully explaining revenue. Evaluating model fit using Akaike Information Criterion (AIC) helps balance predictive accuracy model simplicity.Next, apply stepwise regression using step() function, setting direction = \"\" allow forward selection backward elimination:algorithm iteratively assesses predictor’s contribution, removing improve model performance adding enhance , based AIC. example, spend removed first iteration significantly enhance model. stepwise process continues improvements can made, terminating 6 iterations.Tracking AIC values throughout selection process allows us quantify model improvements. initial full model, includes predictors, AIC value 355.21. multiple iterations, final model achieves lower AIC value 345.34, indicating efficient model improved fit.examine final selected model, use:Stepwise regression results parsimonious model two predictors: clicks display. refined regression equation :\\[\n\\hat{\\text{revenue}} = -33.63 + 0.9 \\cdot \\text{clicks} + 95.51 \\cdot \\text{display}\n\\]final model demonstrates improved fit compared initial full model. Residual Standard Error (RSE), measures typical prediction error, decreased approximately 93.82 72.29, indicating improved accuracy. R-squared (\\(R^2\\)) value increased 62% 77%, suggesting greater proportion variability revenue now explained selected predictors.","code":"ml_all = lm(revenue ~ ., data = marketing)\n\nsummary(ml_all)\n   \n   Call:\n   lm(formula = revenue ~ ., data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -138.00  -59.12   15.16   54.58  106.99 \n   \n   Coefficients:\n                     Estimate Std. Error t value Pr(>|t|)\n   (Intercept)     -25.260020 246.988978  -0.102    0.919\n   spend            -0.025807   2.605645  -0.010    0.992\n   clicks            1.211912   1.630953   0.743    0.463\n   impressions      -0.005308   0.021588  -0.246    0.807\n   display          79.835729 117.558849   0.679    0.502\n   transactions     -7.012069  66.383251  -0.106    0.917\n   click.rate      -10.951493 106.833894  -0.103    0.919\n   conversion.rate  19.926588 135.746632   0.147    0.884\n   \n   Residual standard error: 77.61 on 32 degrees of freedom\n   Multiple R-squared:  0.7829, Adjusted R-squared:  0.7354 \n   F-statistic: 16.48 on 7 and 32 DF,  p-value: 5.498e-09ml_stepwise = step(ml_all, direction = \"both\")\n   Start:  AIC=355.21\n   revenue ~ spend + clicks + impressions + display + transactions + \n       click.rate + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - spend            1       0.6 192760 353.21\n   - click.rate       1      63.3 192822 353.23\n   - transactions     1      67.2 192826 353.23\n   - conversion.rate  1     129.8 192889 353.24\n   - impressions      1     364.2 193123 353.29\n   - display          1    2778.1 195537 353.79\n   - clicks           1    3326.0 196085 353.90\n   <none>                         192759 355.21\n   \n   Step:  AIC=353.21\n   revenue ~ clicks + impressions + display + transactions + click.rate + \n       conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - click.rate       1      67.9 192828 351.23\n   - transactions     1      75.1 192835 351.23\n   - conversion.rate  1     151.5 192911 351.24\n   - impressions      1     380.8 193141 351.29\n   - display          1    2787.2 195547 351.79\n   - clicks           1    3325.6 196085 351.90\n   <none>                         192760 353.21\n   + spend            1       0.6 192759 355.21\n   \n   Step:  AIC=351.23\n   revenue ~ clicks + impressions + display + transactions + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - transactions     1      47.4 192875 349.24\n   - conversion.rate  1     129.0 192957 349.25\n   - impressions      1     312.9 193141 349.29\n   - clicks           1    3425.7 196253 349.93\n   - display          1    3747.1 196575 350.00\n   <none>                         192828 351.23\n   + click.rate       1      67.9 192760 353.21\n   + spend            1       5.2 192822 353.23\n   \n   Step:  AIC=349.24\n   revenue ~ clicks + impressions + display + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - conversion.rate  1      89.6 192965 347.26\n   - impressions      1     480.9 193356 347.34\n   - display          1    5437.2 198312 348.35\n   <none>                         192875 349.24\n   + transactions     1      47.4 192828 351.23\n   + click.rate       1      40.2 192835 351.23\n   + spend            1      13.6 192861 351.23\n   - clicks           1   30863.2 223738 353.17\n   \n   Step:  AIC=347.26\n   revenue ~ clicks + impressions + display\n   \n                     Df Sum of Sq    RSS    AIC\n   - impressions      1       399 193364 345.34\n   <none>                         192965 347.26\n   - display          1     14392 207357 348.13\n   + conversion.rate  1        90 192875 349.24\n   + click.rate       1        52 192913 349.24\n   + spend            1        33 192932 349.25\n   + transactions     1         8 192957 349.25\n   - clicks           1     35038 228002 351.93\n   \n   Step:  AIC=345.34\n   revenue ~ clicks + display\n   \n                     Df Sum of Sq    RSS    AIC\n   <none>                         193364 345.34\n   + impressions      1       399 192965 347.26\n   + transactions     1       215 193149 347.29\n   + conversion.rate  1         8 193356 347.34\n   + click.rate       1         6 193358 347.34\n   + spend            1         2 193362 347.34\n   - display          1     91225 284589 358.80\n   - clicks           1    606800 800164 400.15summary(ml_stepwise)\n   \n   Call:\n   lm(formula = revenue ~ clicks + display, data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -141.89  -55.92   16.44   52.70  115.46 \n   \n   Coefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) -33.63248   28.68893  -1.172 0.248564    \n   clicks        0.89517    0.08308  10.775 5.76e-13 ***\n   display      95.51462   22.86126   4.178 0.000172 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 72.29 on 37 degrees of freedom\n   Multiple R-squared:  0.7822, Adjusted R-squared:  0.7704 \n   F-statistic: 66.44 on 2 and 37 DF,  p-value: 5.682e-13"},{"path":"chapter-regression.html","id":"strengths-limitations-and-considerations-for-stepwise-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Strengths, Limitations, and Considerations for Stepwise Regression","text":"Stepwise regression offers systematic approach model selection, balancing interpretability efficiency. iteratively refining set predictors, helps identify optimal model without manually testing every possible combination. However, stepwise regression also important limitations considered.One key limitation algorithm evaluates predictors sequentially rather exhaustively considering possible subsets variables. can sometimes result suboptimal models, especially strong predictor interactions ignored. Additionally, stepwise regression prone overfitting, particularly small datasets many predictors. Overfitting occurs model captures random noise rather meaningful relationships, reducing generalizability new data. Furthermore, presence multicollinearity among predictors can distort coefficient estimates p-values, leading misleading conclusions.high-dimensional datasets cases predictor selection must robust, alternative methods LASSO (Least Absolute Shrinkage Selection Operator) Ridge Regression often preferred. techniques introduce regularization, helps stabilize model estimates improve predictive accuracy penalizing overly complex models. exploration, refer Introduction Statistical Learning Applications R.8Careful model specification crucial step regression analysis. selecting predictors systematically evaluating model performance appropriate criteria, can construct models accurate interpretable. stepwise regression limitations, remains widely used method predictor selection datasets moderate size. ability enhance predictive performance maintaining simplicity makes valuable tool data-driven decision-making.","code":""},{"path":"chapter-regression.html","id":"extending-linear-models-to-capture-non-linear-relationships","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.7 Extending Linear Models to Capture Non-Linear Relationships","text":"Thus far, focused linear regression models, simple, interpretable, easy implement. models work well relationships predictors response variables approximately linear, predictive power limited relationships exhibit curvature forms non-linearity. cases, assuming strictly linear relationship can lead poor model performance inaccurate predictions.Earlier, explored techniques stepwise regression (Section 10.6) refine model selection reducing complexity addressing multicollinearity. However, methods account non-linearity relationships predictors response variable. address limitation maintaining model interpretability, turn polynomial regression, extension linear regression introduces non-linear terms.","code":""},{"path":"chapter-regression.html","id":"the-need-for-non-linear-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"The Need for Non-Linear Regression","text":"Linear regression assumes constant rate change predictors response variable, resulting straight-line relationship. However, many real-world datasets exhibit complex patterns. Consider scatter plot Figure 10.2, depicts relationship unit.price (house price per unit area) house.age (age house) house dataset. orange line represents simple linear regression fit, adequately capture curvature data.better model relationship, can introduce non-linear terms regression equation. data suggests quadratic trend, model can expressed :\\[\nunit.price = b_0 + b_1 \\cdot house.age + b_2 \\cdot house.age^2\n\\]equation incorporates house.age squared term (house.age^2), allowing curved relationship predictor response variable. Although polynomial regression introduces non-linear predictors, model remains linear regression model coefficients (\\(b_0, b_1, b_2\\)) estimated using standard least squares methods. blue curve Figure 10.2 illustrates improved fit quadratic regression model, captures pattern data effectively simple linear model.\nFigure 10.2: Scatter plot house price ($) versus house age (years) house dataset, fitted simple linear regression line orange quadratic regression curve blue.\nexample highlights need non-linear regression techniques assumption linearity hold. incorporating polynomial terms, can improve model accuracy retaining interpretability, ensuring predictions align closely real-world data patterns.","code":""},{"path":"chapter-regression.html","id":"polynomial-regression","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.8 Polynomial Regression","text":"Polynomial regression extends linear regression incorporating higher-degree terms predictor variable, squared (\\(x^2\\)) cubic (\\(x^3\\)) terms. allows model capture non-linear relationships remaining linear coefficients, meaning can still estimated using least squares. general polynomial regression model given :\\[\n\\hat{y} = b_0 + b_1 \\cdot x + b_2 \\cdot x^2 + \\dots + b_d \\cdot x^d\n\\]\\(d\\) represents degree polynomial. polynomial regression provides flexibility, higher-degree polynomials (\\(d > 3\\)) can lead overfitting, capturing noise rather meaningful patterns, particularly boundaries predictor range.Example 10.2  illustrate polynomial regression, use house dataset liver package. dataset includes housing prices features age, proximity public transport, local amenities. goal model unit.price (house price per unit area) function house.age compare performance simple linear regression polynomial regression.First, load dataset examine structure:dataset consists 414 observations 6 variables. target variable unit.price, predictors include house.age (years), distance..MRT (distance nearest MRT station), stores.number (number nearby convenience stores), latitude, longitude.begin fitting simple linear regression model:R-squared (\\(R^2\\)) value model 0.04, indicating 4.43% variability house prices explained house.age. suggests linear model fully capture relationship.Next, fit quadratic polynomial regression model introduce curvature:\\[\nunit.price = b_0 + b_1 \\cdot house.age + b_2 \\cdot house.age^2\n\\]can implemented R using poly() function:quadratic model achieves significantly higher R-squared (\\(R^2\\)) value 0.2, compared simple regression model. Additionally, Residual Standard Error (RSE) lower, indicating smaller prediction errors. improvements confirm incorporating quadratic term better captures non-linear relationship house age price.Polynomial regression effectively extends linear regression allowing curvature data. However, selecting appropriate polynomial degree crucial avoid overfitting. advanced techniques, splines generalized additive models, provide additional flexibility addressing limitations polynomial regression. techniques discussed Chapter 7 Introduction Statistical Learning Applications R.9","code":"data(house)\n\nstr(house)\n   'data.frame':    414 obs. of  6 variables:\n    $ house.age      : num  32 19.5 13.3 13.3 5 7.1 34.5 20.3 31.7 17.9 ...\n    $ distance.to.MRT: num  84.9 306.6 562 562 390.6 ...\n    $ stores.number  : int  10 9 5 5 5 3 7 6 1 3 ...\n    $ latitude       : num  25 25 25 25 25 ...\n    $ longitude      : num  122 122 122 122 122 ...\n    $ unit.price     : num  37.9 42.2 47.3 54.8 43.1 32.1 40.3 46.7 18.8 22.1 ...simple_reg_house = lm(unit.price ~ house.age, data = house)\n\nsummary(simple_reg_house)\n   \n   Call:\n   lm(formula = unit.price ~ house.age, data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -31.113 -10.738   1.626   8.199  77.781 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) 42.43470    1.21098  35.042  < 2e-16 ***\n   house.age   -0.25149    0.05752  -4.372 1.56e-05 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 13.32 on 412 degrees of freedom\n   Multiple R-squared:  0.04434,    Adjusted R-squared:  0.04202 \n   F-statistic: 19.11 on 1 and 412 DF,  p-value: 1.56e-05reg_nonlinear_house = lm(unit.price ~ poly(house.age, 2), data = house)\n\nsummary(reg_nonlinear_house)\n   \n   Call:\n   lm(formula = unit.price ~ poly(house.age, 2), data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -26.542  -9.085  -0.445   8.260  79.961 \n   \n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           37.980      0.599  63.406  < 2e-16 ***\n   poly(house.age, 2)1  -58.225     12.188  -4.777 2.48e-06 ***\n   poly(house.age, 2)2  109.635     12.188   8.995  < 2e-16 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 12.19 on 411 degrees of freedom\n   Multiple R-squared:  0.2015, Adjusted R-squared:  0.1977 \n   F-statistic: 51.87 on 2 and 411 DF,  p-value: < 2.2e-16"},{"path":"chapter-regression.html","id":"diagnosing-and-validating-regression-models","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.9 Diagnosing and Validating Regression Models","text":"deploying regression model, essential validate assumptions. Ignoring assumptions akin constructing house unstable foundation—predictions based invalid model can lead misleading conclusions costly mistakes. Model diagnostics ensure model robust, reliable, appropriate making predictions.Linear regression models rely several key assumptions:Independence: Observations independent, meaning response one observation depend another.Linearity: relationship predictor(s) response variable approximately linear. Scatter plots predictors response variable help assess assumption.Normality: residuals (errors) follow normal distribution, can assessed visually using Q-Q plot.Constant Variance (Homoscedasticity): residuals exhibit constant variance across levels predictor(s). residuals vs. fitted values plot typically used check assumption.Violations assumptions can undermine validity statistical inferences, leading unreliable predictions inaccurate parameter estimates.Example 10.3  demonstrate model diagnostics, evaluate assumptions multiple regression model constructed Example 10.1 using marketing dataset. fitted model predicts daily revenue (revenue) based clicks display.generate diagnostic plots model follows:\nFigure 10.3: Diagnostic plots assessing regression model assumptions.\ndiagnostic plots provide insights validity model’s assumptions.Normal Q-Q plot (upper-right) assesses whether residuals follow normal distribution. points lie approximately along straight line, assumption normality satisfied. case, residuals closely follow theoretical normal distribution, supporting assumption.Residuals vs. Fitted plot (upper-left) checks linearity homoscedasticity. random scatter pattern without discernible structure supports assumption linearity, even vertical spread across fitted values confirms constant variance. , residuals appear randomly distributed, suggesting assumptions hold.Independence assumption explicitly tested diagnostic plots depends dataset structure. marketing dataset, daily revenue unlikely influenced prior days’ revenue, making independence assumption reasonable.Based diagnostics, regression model satisfies required assumptions, confirming suitability inference prediction. Failing check assumptions result unreliable results, underscoring importance model validation.assumptions violated, alternative approaches may necessary. Robust regression methods can employed normality homoscedasticity assumptions hold. Non-linear regression techniques, including polynomial regression splines, can address cases relationships deviate linearity. Transformations variables, logarithmic square root transformations, can also help stabilize variance improve model fit.Beyond assumption checks, cross-validation --sample testing provide additional validation assessing well model generalizes new data. techniques prevent overfitting ensure model performance driven noise training data.Validating regression models fundamental producing reliable, interpretable, actionable results. following best practices model diagnostics, strengthen statistical foundation analyses enhance trustworthiness predictions.","code":"\nml_stepwise = lm(revenue ~ clicks + display, data = marketing)\n\nplot(ml_stepwise)  "},{"path":"chapter-regression.html","id":"regression-exercises","chapter":"10 Regression Analysis: Foundations and Applications","heading":"10.10 Exercises","text":"exercises structured test theoretical understanding, interpretation regression outputs, practical implementation R using datasets liver package.","code":""},{"path":"chapter-regression.html","id":"simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Simple and Multiple Linear Regression (House, Insurance, and Cereal Datasets)","text":"","code":""},{"path":"chapter-regression.html","id":"conceptual-questions-6","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Conceptual Questions","text":"Explain difference simple linear regression multiple linear regression.key assumptions linear regression? assumptions impact model performance?Define interpret R-squared (\\(R^2\\)) value regression model.Explain purpose Residual Standard Error (RSE) differs \\(R^2\\).multicollinearity affect multiple regression model? can detected?difference Adjusted \\(R^2\\) \\(R^2\\)? Adjusted \\(R^2\\) preferred multiple regression?advantages using categorical variables regression model? R handle categorical variables?","code":""},{"path":"chapter-regression.html","id":"practical-exercises-using-the-house-dataset","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Practical Exercises Using the House Dataset","text":"Load house dataset:Fit simple linear regression model predict unit.price based house.age. Display interpret summary model.Extend model fitting multiple linear regression model using house.age, distance..MRT, stores.number predictors. Interpret coefficient estimates.Use predict() function estimate house prices properties age 10, 20, 30 years.Assess whether latitude longitude improve model’s predictive ability.Evaluate Residual Standard Error (RSE) \\(R^2\\) model. values tell model performance?Create residual plot model analyze whether residuals appear randomly distributed.Generate Q-Q plot residuals. reveal normality assumption?","code":"\ndata(house, package = \"liver\")"},{"path":"chapter-regression.html","id":"practical-exercises-using-the-insurance-dataset","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Practical Exercises Using the Insurance Dataset","text":"Load insurance dataset:Fit multiple linear regression model predicting charges based age, bmi, children, smoker.Interpret coefficient smoker. suggest impact smoking insurance charges?Assess whether interaction effects exist age bmi.Evaluate model’s Adjusted \\(R^2\\). adding region predictor improve model?Perform stepwise regression determine best subset predictors.","code":"\ndata(insurance, package = \"liver\")"},{"path":"chapter-regression.html","id":"practical-exercises-using-the-cereal-dataset","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Practical Exercises Using the Cereal Dataset","text":"Load cereal dataset:Fit multiple linear regression model predicting rating based calories, protein, sugars, fiber.Based model summary, predictor strongest impact rating?sodium significantly affect rating? included model?Compare effects fiber sugars. larger impact rating?Apply stepwise regression refine model identify relevant predictors.","code":"\ndata(cereal, package = \"liver\")"},{"path":"chapter-regression.html","id":"polynomial-regression-house-dataset","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Polynomial Regression (House Dataset)","text":"","code":""},{"path":"chapter-regression.html","id":"conceptual-questions-7","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Conceptual Questions","text":"polynomial regression, differ multiple linear regression?polynomial regression remain linear model even though includes non-linear terms?risk using high-degree polynomial regression?determine optimal degree polynomial regression model?can overfitting detected polynomial regression?","code":""},{"path":"chapter-regression.html","id":"practical-exercises-using-the-house-dataset-1","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Practical Exercises Using the House Dataset","text":"Fit quadratic polynomial regression model predicting unit.price using house.age. Compare simple linear regression model.Fit cubic polynomial regression model. perform better quadratic model?Plot simple, quadratic, cubic regression fits graph.Use cross-validation determine best polynomial degree.Interpret coefficients quadratic regression model.","code":""},{"path":"chapter-regression.html","id":"logistic-regression-bank-dataset","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Logistic Regression (Bank Dataset)","text":"","code":""},{"path":"chapter-regression.html","id":"conceptual-questions-8","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Conceptual Questions","text":"difference linear regression logistic regression?logistic regression use logit function instead fitting linear model directly?interpret odds ratio logistic regression model?confusion matrix, used evaluate logistic regression?difference precision recall classification models?","code":""},{"path":"chapter-regression.html","id":"practical-exercises-using-the-bank-dataset","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Practical Exercises Using the Bank Dataset","text":"Load bank dataset:Fit logistic regression model predicting whether customer subscribed term deposit (y) based age, balance, duration.Interpret coefficients terms odds ratios.Use predict() function estimate probability subscription new customer.Create confusion matrix evaluate model’s performance.Compute accuracy, precision, recall, F1-score model.Use stepwise regression refine logistic model.Evaluate receiver operating characteristic (ROC) curve model.","code":"\ndata(bank, package = \"liver\")"},{"path":"chapter-regression.html","id":"stepwise-regression-house-dataset","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Stepwise Regression (House Dataset)","text":"Apply stepwise regression house dataset identify relevant predictors unit.price.Compare stepwise regression model full multiple regression model. perform better?Assess whether interaction terms improve stepwise regression model.","code":""},{"path":"chapter-regression.html","id":"model-diagnostics-and-validation","chapter":"10 Regression Analysis: Foundations and Applications","heading":"Model Diagnostics and Validation","text":"Check assumptions linear regression multiple regression model house dataset.Generate diagnostic plots (residuals vs. fitted, Q-Q plot, scale-location plot).Use cross-validation assess model performance.Compare mean squared error (MSE) different models.Assess whether log-transformation improves model performance.","code":""},{"path":"chapter-tree.html","id":"chapter-tree","chapter":"11 Decision Trees and Random Forests","heading":"11 Decision Trees and Random Forests","text":"Imagine bank evaluating loan applications. Given details income, age, credit history, debt--income ratio, bank decide whether approve reject loan? Similarly, online retailers recommend products based customer preferences? decisions, mimic human reasoning, often powered decision trees—simple yet powerful machine learning technique classifies data following series logical rules.Decision trees widely used various domains, medical diagnosis fraud detection customer segmentation automated decision-making. intuitive nature makes highly interpretable, enabling data-driven decision-making without requiring deep mathematical expertise. However, individual trees easy understand, prone overfitting, capturing noise data rather general patterns. Random forests address limitation combining multiple decision trees produce accurate stable model.see decision trees action, consider example Figure 11.1, predicts whether customer’s credit risk classified “good” “bad” based features age income. tree trained risk dataset, introduced Chapter 9, consists decision nodes representing yes/questions, whether yearly income €36,000 (income < 36e+3) whether age greater 29. final classification determined terminal nodes, also known leaves.\nFigure 11.1: Decision tree predicting credit risk based age income.\nDecision trees highly interpretable, making especially valuable domains finance, healthcare, marketing, understanding model decisions important accuracy. structured form allows easy visualization decision pathways, helping businesses customer segmentation, risk assessment, process optimization.chapter, explore decision trees random forests work, strengths limitations, can applied solve real-world problems. end chapter, learn:mechanics behind decision trees random forests.build, evaluate, fine-tune decision trees using algorithms CART C5.0.random forests improve predictive accuracy generalization ensemble learning.begin examining core principles decision trees, including make predictions performance can optimized.","code":""},{"path":"chapter-tree.html","id":"how-decision-trees-work","chapter":"11 Decision Trees and Random Forests","heading":"11.1 How Decision Trees Work","text":"decision tree classifies predicts outcomes systematically dividing dataset smaller, uniform groups based feature values. split refines classification prediction, creating structured, tree-like model. divide--conquer approach widely used classification regression due intuitive nature ability model complex decision-making processes.step, algorithm selects feature threshold best separate data. decision based metrics Gini Index, Entropy, Variance Reduction, depending problem type. tree continues growing meets stopping criterion, reaching predefined maximum depth, forming perfectly homogeneous subsets, splits longer improve performance.see process action, consider simple dataset two features (\\(x_1\\) \\(x_2\\)) two classes (Class Class B), shown Figure 11.2. dataset consists 50 data points, goal classify respective categories.\nFigure 11.2: two-dimensional toy dataset (50 observations) two classes (Class Class B), used illustrate build Decision Trees.\nprocess begins identifying feature threshold best separate two classes. algorithm evaluates possible splits selects one maximizes homogeneity resulting subsets. dataset, optimal split occurs \\(x_1 = 10\\), dividing dataset two regions:left region contains data points \\(x_1 < 10\\), 80% belonging Class 20% Class B.right region contains data points \\(x_1 \\geq 10\\), 28% Class 72% Class B.first split illustrated Figure 11.3, decision boundary drawn \\(x_1 = 10\\).\nFigure 11.3: Left: Decision boundary tree depth 1. Right: corresponding Decision Tree.\nAlthough split improves class separation, overlap remains, suggesting refinement needed. tree-building process continues introducing additional splits based \\(x_2\\), creating smaller, homogeneous groups.Figure 11.4, algorithm identifies new thresholds: \\(x_2 = 6\\) left region \\(x_2 = 8\\) right region. additional splits refine classification process, improving model’s ability distinguish two classes.\nFigure 11.4: Left: Decision boundary tree depth 2. Right: corresponding Decision Tree.\nrecursive process continues tree reaches stopping criterion. Figure 11.5 shows fully grown tree depth 5, demonstrating decision trees create increasingly refined decision boundaries.\nFigure 11.5: Left: Decision boundary tree depth 5. Right: corresponding Decision Tree.\ndepth, tree created highly specific decision boundaries closely match training data. deep tree perfectly classifies training data, may generalize well new observations. model likely captured just meaningful patterns also noise, problem known overfitting. Overfitted trees perform well training data struggle make accurate predictions unseen data.","code":""},{"path":"chapter-tree.html","id":"making-predictions-with-a-decision-tree","chapter":"11 Decision Trees and Random Forests","heading":"Making Predictions with a Decision Tree","text":"decision tree built, making predictions involves following decision rules root node leaf. split refines prediction final classification numerical estimate reached.classification tasks, tree assigns new observation common class leaf ends . regression tasks, predicted outcome average target value data points leaf.illustrate, consider new data point \\(x_1 = 8\\) \\(x_2 = 4\\) Figure 11.4. tree classifies following steps:Since \\(x_1 = 8\\), point moves left branch (\\(x_1 < 10\\)).Since \\(x_2 = 4\\), point moves lower-left region (\\(x_2 < 6\\)).final leaf node assigns point Class 80% confidence.step--step traversal tree ensures predictions remain interpretable, making decision trees particularly useful applications understanding prediction made important accuracy.","code":""},{"path":"chapter-tree.html","id":"controlling-tree-complexity","chapter":"11 Decision Trees and Random Forests","heading":"Controlling Tree Complexity","text":"decision trees powerful, can easily grow complex, capturing noise rather meaningful patterns. improve generalization, various techniques help regulate tree complexity prevent overfitting.One approach pre-pruning, restricts tree growth training enforcing stopping criteria. may include setting maximum tree depth, requiring minimum number samples per node, enforcing minimum improvement information gain split. stopping early, pre-pruning prevents tree fitting data closely, reducing risk overfitting.Alternatively, post-pruning allows tree grow fully simplifying . tree built, unnecessary nodes contribute little predictive accuracy removed merged. approach often improves interpretability maintaining performance.choice pre-pruning post-pruning depends dataset problem hand. Additionally, way splits chosen—using criteria Gini Index, Entropy, Variance Reduction—plays crucial role determining tree performance. explored later sections.","code":""},{"path":"chapter-tree.html","id":"classification-and-regression-trees-cart","chapter":"11 Decision Trees and Random Forests","heading":"11.2 Classification and Regression Trees (CART)","text":"classification regression trees (CART) algorithm, introduced Breiman et al. 1984,10 one widely used methods constructing decision trees. CART generates binary trees, meaning decision node splits data exactly two branches. recursively partitions training dataset subsets records share similar values target variable. partitioning guided splitting criterion designed minimize impurity resulting subsets. classification tasks, CART employs measures Gini index entropy evaluate splits, regression tasks, minimizes variance target variable.Gini index commonly used measure impurity classification tasks. calculated :\\[\nGini = 1 - \\sum_{=1}^k p_i^2\n\\]\\(p_i\\) represents proportion samples node belong class \\(\\), \\(k\\) total number classes. node considered pure data points belong single class, resulting Gini index zero. tree construction, CART selects feature threshold result largest reduction impurity, splitting data create two homogeneous child nodes.recursive nature CART can lead highly detailed trees fit training data perfectly. minimizes error rate training set, often results overfitting, tree becomes overly complex fails generalize unseen data. mitigate , CART employs pruning techniques simplify tree.Pruning involves trimming branches contribute meaningfully predictive accuracy validation set. achieved finding adjusted error rate penalizes overly complex trees many leaf nodes. goal pruning balance accuracy simplicity, enhancing tree’s ability generalize new data. pruning process discussed detail Breiman et al..11Despite simplicity, CART widely used practice due interpretability, versatility, ability handle classification regression tasks. tree structure provides intuitive way visualize decision-making, making highly explainable. Additionally, CART works well numerical categorical data, making applicable across range domains.However, CART limitations. algorithm tends produce deep trees may overfit training data, particularly dataset small noisy. reliance greedy splitting can also result suboptimal splits, evaluates one feature time rather considering possible combinations.address shortcomings, advanced algorithms developed, C5.0, incorporates improvements splitting pruning techniques, random forests, combine multiple decision trees create robust models. approaches build foundations CART, improving performance reducing susceptibility overfitting. following sections explore methods detail.","code":""},{"path":"chapter-tree.html","id":"the-c5.0-algorithm-for-building-decision-trees","chapter":"11 Decision Trees and Random Forests","heading":"11.3 The C5.0 Algorithm for Building Decision Trees","text":"C5.0 algorithm, developed J. Ross Quinlan, advanced iteration earlier decision tree models, including C4.5 ID3 (Iterative Dichotomiser 3). introduces improvements efficiency, flexibility, accuracy, making widely used approach academic research practical applications. commercial version available RuleQuest, open-source implementation integrated R machine learning tools.C5.0 differs decision tree algorithms, CART, several ways. Unlike CART, constructs strictly binary trees, C5.0 allows multi-way splits, particularly categorical attributes. can lead compact interpretable trees dealing variables many distinct categories. Another key distinction lies algorithm evaluates node purity. CART uses measures Gini index variance reduction, C5.0 relies entropy information gain, concepts derived information theory.Entropy measures degree disorder dataset. Higher entropy indicates greater diversity among classes, lower entropy suggests homogeneous groups. goal C5.0 identify feature splits reduce entropy, leading purer subsets step tree construction. entropy variable \\(x\\) \\(k\\) classes defined :\\[\nEntropy(x) = - \\sum_{=1}^k p_i \\log_2(p_i)\n\\]\\(p_i\\) represents proportion samples class \\(\\). dataset equal distribution among classes maximum entropy, whereas dataset samples belonging class entropy equal zero. Using measure, algorithm calculates information gain, quantifies reduction entropy resulting particular split. Given candidate split \\(S\\) divides dataset \\(T\\) subsets \\(T_1, T_2, \\dots, T_c\\), entropy splitting computed :\\[\nH_S(T) = \\sum_{=1}^c \\frac{|T_i|}{|T|} \\cdot Entropy(T_i)\n\\]information gain split :\\[\ngain(S) = H(T) - H_S(T)\n\\]\\(H(T)\\) represents entropy split. algorithm evaluates potential splits selects one maximizes information gain, ensuring decision step results purer subsets.illustrate C5.0 constructs decision trees, consider application risk dataset, classifies customer’s credit risk good bad based features age income. Figure 11.6 shows decision tree trained using C5.0 function C50 package R.\nFigure 11.6: C5.0 Decision Tree predicting credit risk based age income.\ntree demonstrates C5.0 selects splits separate classes. Unlike CART, allows binary splits, C5.0 enables multi-way splits dealing categorical features. flexibility can lead trees concise easier interpret, particularly datasets categorical variables play significant role.C5.0 several advantages decision tree algorithms. computationally efficient, making well-suited large datasets, ability handle multi-way splits enables nuanced decision-making. Additionally, incorporates feature weighting, prioritizing informative predictors, can improve model accuracy.However, C5.0 without limitations. trees generates can become overly complex, particularly categorical attributes contain many unique values, increasing risk overfitting. mitigate , pruning techniques can applied simplify tree preserving accuracy. Another challenge computational cost evaluating multiple splits categorical variables, can increase processing time large datasets, though C5.0’s optimizations help reduce impact.summary, C5.0 builds upon earlier decision tree models leveraging entropy information gain construct accurate interpretable decision rules. ability create multi-way splits makes particularly effective categorical data, efficiency allows scale well. next section, explore random forests, ensemble learning technique enhances decision tree models combining multiple trees improved accuracy robustness.","code":""},{"path":"chapter-tree.html","id":"random-forests-an-ensemble-approach","chapter":"11 Decision Trees and Random Forests","heading":"11.4 Random forests: an ensemble approach","text":"Decision trees effective models, tend overfit, particularly grown full depth. Random forests address limitation combining multiple decision trees ensemble, producing accurate robust model. Instead relying single tree, random forests aggregate predictions many trees, reducing overfitting improving performance complex datasets.algorithm introduces two key elements randomness enhance model diversity:Bootstrap aggregation (bagging): tree trained random subset training data, created sampling replacement. means observations appear multiple times tree’s training data, others may excluded. diversity ensures tree learns slightly different patterns.Random feature selection: split, algorithm considers random subset features instead evaluating features. decorrelates trees, forcing rely different combinations features.forest built, predictions trees aggregated produce final output:classification, final prediction determined majority voting, tree votes class, common class selected.regression, final output average predictions trees.strength random forests lies ability leverage diversity. Individually, tree trained limited subset data features, making weak learner. However, combined, collective predictions form stronger model. aggregating multiple trees, random forests reduce risk errors single tree dominating overall prediction.Additionally, randomness introduced feature selection ensures single feature dominates model, making random forests particularly effective datasets correlated redundant features. feature-level decorrelation enhances ability generalize unseen data.","code":""},{"path":"chapter-tree.html","id":"advantages-and-limitations-of-random-forests","chapter":"11 Decision Trees and Random Forests","heading":"Advantages and limitations of random forests","text":"AdvantagesReduced overfitting: averaging predictions multiple trees, random forests smooth noise variance, leading better generalization.High accuracy: perform well classification regression tasks, particularly datasets non-linear relationships high-dimensional feature spaces.Feature importance ranking: algorithm provides feature importance scores, helping identify influential predictors.Robustness: resilient noise outliers, ensemble effect reduces impact anomalies final prediction.Flexibility: Random forests can handle numerical categorical data adapt well different types problems.LimitationsComputational complexity: Training hundreds thousands trees can computationally intensive, especially large datasets. However, can mitigated parallel processing, tree built independently.Reduced interpretability: individual decision trees easy interpret, ensemble nature random forests makes difficult understand individual features contribute predictions.Potential loss fine details: Although random forests reduce variance, may smooth intricate relationships well-tuned single decision tree capture.Random forests balance accuracy robustness, addressing many weaknesses individual decision trees retaining strengths. particularly effective scenarios noisy high-dimensional data. ability compute feature importance scores also provides valuable insights drivers model predictions, making useful predictive modeling exploratory data analysis.Random forests become one widely used machine learning algorithms due versatility, reliability, strong performance across variety applications. next section, apply random forests, along decision trees, case study predicting income levels. practical example demonstrates models work can evaluated real-world scenarios.","code":""},{"path":"chapter-tree.html","id":"tree-case-study","chapter":"11 Decision Trees and Random Forests","heading":"11.5 Case Study: Who Can Earn More Than $50K Per Year?","text":"Predicting income levels important task fields finance, marketing, public policy. Banks use income models assess creditworthiness, employers analyze salary trends compensation planning, governments rely income predictions taxation social welfare policies. case study, explore decision trees random forests can applied classify individuals based likelihood earning $50,000 per year.analysis, use adult dataset, well-known benchmark dataset sourced US Census Bureau available liver package. dataset previously introduced Section 3.9 part data preparation chapter (3). contains demographic employment-related attributes, education, working hours, marital status, occupation, influence earning potential. goal build classification model predicts whether individual belongs one two income groups: <=50K >50K, treating income target variable.","code":""},{"path":"chapter-tree.html","id":"overview-of-the-dataset-1","chapter":"11 Decision Trees and Random Forests","heading":"Overview of the Dataset","text":"adult dataset, available liver package, provides demographic employment-related attributes predict income levels. can load directly R examine summary using following commands:dataset consists 48598 records 15 variables. target variable, income, binary, two categories: <=50K >50K. remaining 14 variables serve predictors, encompassing demographic, occupational, financial characteristics.predictors can categorized follows:Demographic attributes\nage: Age years (numerical).\ngender: Gender (categorical, Male/Female).\nrace: Race (categorical, 5 levels).\nnative.country: Country origin (categorical, 42 levels).\nage: Age years (numerical).gender: Gender (categorical, Male/Female).race: Race (categorical, 5 levels).native.country: Country origin (categorical, 42 levels).Education employment details\neducation: Highest education level attained (categorical, 16 levels).\neducation.num: Years education (numerical).\nworkclass: Type employment (categorical, 6 levels).\noccupation: Job category (categorical, 15 levels).\nhours.per.week: Weekly hours worked (numerical).\neducation: Highest education level attained (categorical, 16 levels).education.num: Years education (numerical).workclass: Type employment (categorical, 6 levels).occupation: Job category (categorical, 15 levels).hours.per.week: Weekly hours worked (numerical).Financial attributes\ncapital.gain: Income capital gains (numerical).\ncapital.loss: Losses investments (numerical).\ncapital.gain: Income capital gains (numerical).capital.loss: Losses investments (numerical).Household relationship details\nmarital.status: Marital status (categorical, 5 levels).\nrelationship: Family role (categorical, 6 levels).\nmarital.status: Marital status (categorical, 5 levels).relationship: Family role (categorical, 6 levels).dataset provides diverse set features influence income levels, making suitable building predictive models. details, refer dataset documentation.","code":"library(liver)\n\ndata(adult)\n\nsummary(adult)\n         age              workclass      demogweight             education    \n    Min.   :17.0   ?           : 2794   Min.   :  12285   HS-grad     :15750  \n    1st Qu.:28.0   Gov         : 6536   1st Qu.: 117550   Some-college:10860  \n    Median :37.0   Never-worked:   10   Median : 178215   Bachelors   : 7962  \n    Mean   :38.6   Private     :33780   Mean   : 189685   Masters     : 2627  \n    3rd Qu.:48.0   Self-emp    : 5457   3rd Qu.: 237713   Assoc-voc   : 2058  \n    Max.   :90.0   Without-pay :   21   Max.   :1490400   11th        : 1812  \n                                                          (Other)     : 7529  \n    education.num         marital.status            occupation   \n    Min.   : 1.00   Divorced     : 6613   Craft-repair   : 6096  \n    1st Qu.: 9.00   Married      :22847   Prof-specialty : 6071  \n    Median :10.00   Never-married:16096   Exec-managerial: 6019  \n    Mean   :10.06   Separated    : 1526   Adm-clerical   : 5603  \n    3rd Qu.:12.00   Widowed      : 1516   Sales          : 5470  \n    Max.   :16.00                         Other-service  : 4920  \n                                          (Other)        :14419  \n            relationship                   race          gender     \n    Husband       :19537   Amer-Indian-Eskimo:  470   Female:16156  \n    Not-in-family :12546   Asian-Pac-Islander: 1504   Male  :32442  \n    Other-relative: 1506   Black             : 4675                 \n    Own-child     : 7577   Other             :  403                 \n    Unmarried     : 5118   White             :41546                 \n    Wife          : 2314                                            \n                                                                    \n     capital.gain      capital.loss     hours.per.week        native.country \n    Min.   :    0.0   Min.   :   0.00   Min.   : 1.00   United-States:43613  \n    1st Qu.:    0.0   1st Qu.:   0.00   1st Qu.:40.00   Mexico       :  949  \n    Median :    0.0   Median :   0.00   Median :40.00   ?            :  847  \n    Mean   :  582.4   Mean   :  87.94   Mean   :40.37   Philippines  :  292  \n    3rd Qu.:    0.0   3rd Qu.:   0.00   3rd Qu.:45.00   Germany      :  206  \n    Max.   :41310.0   Max.   :4356.00   Max.   :99.00   Puerto-Rico  :  184  \n                                                        (Other)      : 2507  \n      income     \n    <=50K:37155  \n    >50K :11443  \n                 \n                 \n                 \n                 \n   "},{"path":"chapter-tree.html","id":"data-preparation-3","chapter":"11 Decision Trees and Random Forests","heading":"Data Preparation","text":"building models, must clean preprocess dataset handle missing values transform categorical variables. adult dataset includes missing values represented \"?\", need addressed. Additionally, categorical variables native.country, workclass, race multiple levels can grouped improve interpretability.Since data preparation covered detail Section 3.9 part data preparation chapter (3), summarize necessary preprocessing steps .","code":""},{"path":"chapter-tree.html","id":"handling-missing-values","chapter":"11 Decision Trees and Random Forests","heading":"Handling Missing Values","text":"dataset encodes missing values \"?\". first replace NA, remove unused factor levels, apply imputation categorical variables using random sampling existing categories.","code":"\nlibrary(Hmisc)    # For handling missing values\n\n# Replace \"?\" with NA\nadult[adult == \"?\"] = NA\n\n# Remove unused factor levels\nadult = droplevels(adult)\n\n# Impute missing values using random sampling from existing categories\nadult$workclass      = impute(factor(adult$workclass), 'random')\nadult$native.country = impute(factor(adult$native.country), 'random')\nadult$occupation     = impute(factor(adult$occupation), 'random')"},{"path":"chapter-tree.html","id":"transforming-categorical-variables","chapter":"11 Decision Trees and Random Forests","heading":"Transforming Categorical Variables","text":"categorical variables contain many levels, making difficult interpret model efficiently. simplify variables, group related categories.dataset originally contains 42 unique country values native.country. reduce dimensionality categorizing broader regions.workclass variable contains categories indicate lack formal employment. consolidate \"Never-worked\" \"Without-pay\" \"Unemployed\".maintain consistency, simplify race variable.preprocessing steps ensure dataset clean ready modeling. next section, partition dataset training testing sets model evaluation.","code":"\nlibrary(forcats)  # For categorical variable transformation\n\nEurope = c(\"England\", \"France\", \"Germany\", \"Greece\", \"Holand-Netherlands\", \"Hungary\", \n           \"Ireland\", \"Italy\", \"Poland\", \"Portugal\", \"Scotland\", \"Yugoslavia\")\n\nAsia = c(\"China\", \"Hong\", \"India\", \"Iran\", \"Cambodia\", \"Japan\", \"Laos\", \n         \"Philippines\", \"Vietnam\", \"Taiwan\", \"Thailand\")\n\nN.America = c(\"Canada\", \"United-States\", \"Puerto-Rico\")\n\nS.America = c(\"Columbia\", \"Cuba\", \"Dominican-Republic\", \"Ecuador\", \"El-Salvador\", \n              \"Guatemala\", \"Haiti\", \"Honduras\", \"Mexico\", \"Nicaragua\", \n              \"Outlying-US(Guam-USVI-etc)\", \"Peru\", \"Jamaica\", \"Trinadad&Tobago\")\n\n# Reclassify `native.country` into broader regions\nadult$native.country = fct_collapse(adult$native.country, \n                                    \"Europe\"    = Europe,\n                                    \"Asia\"      = Asia,\n                                    \"N.America\" = N.America,\n                                    \"S.America\" = S.America,\n                                    \"Other\"     = c(\"South\"))\nadult$workclass = fct_collapse(adult$workclass, \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))\nadult$race = fct_recode(adult$race, \"Amer-Indian\" = \"Amer-Indian-Eskimo\", \n                                    \"Asian\" = \"Asian-Pac-Islander\")"},{"path":"chapter-tree.html","id":"preparing-data-for-modeling-2","chapter":"11 Decision Trees and Random Forests","heading":"Preparing Data for Modeling","text":"training tree-based models, need split dataset training testing sets. step ensures can evaluate well models generalize unseen data. use 80/20 split, allocating 80% data training 20% testing. maintain consistency previous chapters, apply partition() function liver package:set.seed() function ensures reproducibility fixing random seed. use train_set train classification models, test_set serves unseen data evaluation. test_labels vector contains true class labels test_set, compare model’s predictions. allows us assess performance CART, C5.0, Random Forest models.practice, important verify training test sets representative original dataset. One way examining distribution income sets. performed validation found partition valid. report refer Section 6.4 details validate partitions.predict whether individual’s income exceeds $50K, use following predictors:age, workclass, education.num, marital.status, occupation, race, gender, capital.gain, capital.loss, hours.per.week, native.country.exclude demogweight, education, relationship following reasons:demogweight treated ID variable provide meaningful predictive information.education removed education.num represents information numerical format.relationship highly correlated marital.status provide additional independent information.selected predictors used following formula:formula used train decision tree models using CART C5.0 algorithms, well Random Forest algorithm. next section, demonstrate build, evaluate, compare models using adult dataset.","code":"\nset.seed(6)\n\ndata_sets = partition(data = adult, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$income\nformula = income ~ age + workclass + education.num + marital.status + occupation + race + gender + capital.gain + capital.loss + hours.per.week + native.country"},{"path":"chapter-tree.html","id":"decision-tree-with-cart","chapter":"11 Decision Trees and Random Forests","heading":"Decision Tree with CART","text":"fit decision tree using CART algorithm R, use rpart package (Recursive Partitioning Regression Trees), provides widely used implementation CART. package includes functions building, visualizing, evaluating decision trees.rpart package installed, can install using install.packages(\"rpart\") command. , can load R session:decision tree built using rpart() function, constructs classification tree method = \"class\" specified:formula argument defines relationship target variable (income) predictors.data argument specifies training dataset.method = \"class\" argument ensures model performs classification rather regression.","code":"\nlibrary(rpart)\ntree_cart = rpart(formula = formula, data = train_set, method = \"class\")"},{"path":"chapter-tree.html","id":"visualizing-the-decision-tree","chapter":"11 Decision Trees and Random Forests","heading":"Visualizing the Decision Tree","text":"visualize tree, use rpart.plot package, provides tools graphical representation rpart models. installed, can added install.packages(\"rpart.plot\") command. , loaded follows:tree displayed using:\n- type = 4 argument specifies split-labeling style decision rules appear inside nodes.\n- extra = 104 argument displays predicted class probability probable class terminal node.tree large fit within single plot, structure can also examined using:prints text-based version tree, showing nodes, splits, predictions scrollable format.","code":"\nlibrary(rpart.plot)\nrpart.plot(tree_cart, type = 4, extra = 104)print(tree_cart)\n   n= 38878 \n   \n   node), split, n, loss, yval, (yprob)\n         * denotes terminal node\n   \n    1) root 38878 9217 <=50K (0.76292505 0.23707495)  \n      2) marital.status=Divorced,Never-married,Separated,Widowed 20580 1282 <=50K (0.93770651 0.06229349)  \n        4) capital.gain< 7055.5 20261  978 <=50K (0.95172992 0.04827008) *\n        5) capital.gain>=7055.5 319   15 >50K (0.04702194 0.95297806) *\n      3) marital.status=Married 18298 7935 <=50K (0.56634605 0.43365395)  \n        6) education.num< 12.5 12944 4163 <=50K (0.67838381 0.32161619)  \n         12) capital.gain< 5095.5 12350 3582 <=50K (0.70995951 0.29004049) *\n         13) capital.gain>=5095.5 594   13 >50K (0.02188552 0.97811448) *\n        7) education.num>=12.5 5354 1582 >50K (0.29548001 0.70451999) *"},{"path":"chapter-tree.html","id":"interpreting-the-decision-tree","chapter":"11 Decision Trees and Random Forests","heading":"Interpreting the Decision Tree","text":"CART model produces binary tree four decision nodes five leaves. Among 12 predictors, algorithm selects three—marital.status, capital.gain, education.num—relevant predicting income. influential predictor, marital.status, appears root node, meaning marital status first split tree.model categorizes individuals five distinct groups, represented terminal leaf. blue leaves indicate predicted earn less $50,000 (income <= 50K), green leaves represent earning $50,000 (income > 50K).example, rightmost leaf corresponds individuals married least 13 years education (education.num >= 13). group represents 14% dataset, 70% earning $50,000 annually. error rate leaf 0.30, calculated \\(1 - 0.70\\).","code":""},{"path":"chapter-tree.html","id":"decision-tree-with-c5.0","chapter":"11 Decision Trees and Random Forests","heading":"Decision Tree with C5.0","text":"fit decision tree using C5.0 algorithm R, use C50 package. C50 package installed, can install using install.packages(\"C50\") command. , can load R session:tree constructed using C5.0() function:plot() function can used visualize tree, summary() function provides detailed description model. Since tree output large display , print summary model using:output includes key information model, function call used generate tree, number predictors, number observations used training. also reports tree size 74, indicating tree consists 74 decision nodes—substantially larger tree produced CART case.","code":"\nlibrary(C50)\ntree_C50 = C5.0(formula, data = train_set) print(tree_C50)\n   \n   Call:\n   C5.0.formula(formula = formula, data = train_set)\n   \n   Classification Tree\n   Number of samples: 38878 \n   Number of predictors: 11 \n   \n   Tree size: 120 \n   \n   Non-standard options: attempt to group attributes"},{"path":"chapter-tree.html","id":"random-forest","chapter":"11 Decision Trees and Random Forests","heading":"Random Forest","text":"Random forests implemented R using randomForest package, builds classification regression models based ensemble decision trees randomly selected inputs. multiple packages R support random forest modeling, randomForest one widely used implementations due reliability compatibility caret package automated tuning.randomForest package installed, can install using install.packages(\"randomForest\") command. , load R session:Using predictors previous models, construct random forest model 100 decision trees:formula argument specifies relationship target variable (income) predictors.data argument defines training dataset.ntree = 100 argument sets number decision trees forest. higher number trees generally improves accuracy increases computation time.can evaluate importance predictors using varImpPlot() function:\nplot ranks predictors based contribution model accuracy. case, marital.status appears important predictor, followed capital.gain education.num.assess error rate changes number trees increases, use:Random forests provide robust alternative single decision trees reducing overfitting improving predictive performance aggregation. next section compares performance CART, C5.0, Random Forest models using evaluation metrics.","code":"\nlibrary(randomForest)\nrandom_forest = randomForest(formula = formula, data = train_set, ntree = 100)\nvarImpPlot(random_forest)\nplot(random_forest)"},{"path":"chapter-tree.html","id":"prediction-and-model-evaluation-1","chapter":"11 Decision Trees and Random Forests","heading":"Prediction and Model Evaluation","text":"training models (CART, C5.0, Random Forest), evaluate performance test set, contains individuals unseen training. objective compare predicted probabilities actual class labels stored test_labels.obtain predicted class probabilities, use predict() function model. three algorithms, specify type = \"prob\" extract probabilities instead discrete class labels:[ , 1 ] index extracts probability “<=50K” class, class labels stored alphabetical order.","code":"\nprob_cart = predict(tree_cart, test_set, type = \"prob\")[, 1]\nprob_C50 = predict(tree_C50, test_set, type = \"prob\")[, 1]\nprob_random_forest = predict(random_forest, test_set, type = \"prob\")[, 1]"},{"path":"chapter-tree.html","id":"confusion-matrix-and-classification-errors","chapter":"11 Decision Trees and Random Forests","heading":"Confusion Matrix and Classification Errors","text":"confusion matrix summarizes model performance displaying number true positives, true negatives, false positives, false negatives. generate confusion matrices model using conf.mat.plot() function liver package, provides graphical representation. conf.mat() function can also used display numeric values:confusion matrices indicate number correctly classified instances model:CART: “7091 + 1111 = 8202” correct classifications.C5.0: “7084 + 1360 = 8444” correct classifications.Random Forest: “7053 + 1335 = 8388” correct classifications.Among three models, C5.0 highest accuracy, making fewest misclassifications.","code":"\nconf.mat.plot(prob_cart, test_labels, cutoff = 0.5, reference = \"<=50K\", main = \"CART Prediction\")\nconf.mat.plot(prob_C50, test_labels, cutoff = 0.5, reference = \"<=50K\", main = \"C5.0 Prediction\")\nconf.mat.plot(prob_random_forest, test_labels, cutoff = 0.5, reference = \"<=50K\", main = \"Random Forest Prediction\")"},{"path":"chapter-tree.html","id":"roc-curve-and-auc-2","chapter":"11 Decision Trees and Random Forests","heading":"ROC Curve and AUC","text":"Receiver Operating Characteristic (ROC) curve Area Curve (AUC) value provide comprehensive assessment model’s ability distinguish income classes across different classification thresholds. metrics computed using pROC package. pROC package installed, can added using install.packages(\"pROC\"). , can loaded R session:generate ROC curve, compute true positive rate false positive rate different threshold values using roc() function:visualize ROC curves three models using ggroc():\nROC plot, black curve represents CART, red curve represents C5.0, green curve represents Random Forest. ROC curves suggest C5.0 Random Forest outperform CART. However, distinguishing C5.0 Random Forest based ROC curve alone challenging. Note ROC plots also report AUC values model, using auc() function pROC package. AUC values provide insight:CART: AUC = 0.841,C5.0: AUC = 0.903,Random Forest: AUC = 0.899,Based AUC values, C5.0 performs slightly better two models, three demonstrate comparable accuracy, making reliable classification task.","code":"\nlibrary(pROC)\nroc_cart = roc(test_labels, prob_cart)\nroc_C50 = roc(test_labels, prob_C50)\nroc_random_forest = roc(test_labels, prob_random_forest)\nggroc(list(roc_cart, roc_C50, roc_random_forest), size = 0.8) + \n    theme_minimal() + ggtitle(\"ROC Curves with AUC for Three Models\") +\n  scale_color_manual(values = 1:3, \n    labels = c(paste(\"CART; AUC=\", round(auc(roc_cart), 3)), \n                paste(\"C5.0; AUC=\", round(auc(roc_C50), 3)), \n                paste(\"Random Forest; AUC=\", round(auc(roc_random_forest), 3)))) +\n  theme(legend.title = element_blank()) +\n  theme(legend.position = c(.7, .3), text = element_text(size = 17))"},{"path":"chapter-tree.html","id":"tree-exercises","chapter":"11 Decision Trees and Random Forests","heading":"11.6 Exercises","text":"exercises test theoretical understanding, interpretation Decision Trees Random Forests outputs, practical implementation R using datasets liver package.","code":""},{"path":"chapter-tree.html","id":"decision-trees-conceptual-questions","chapter":"11 Decision Trees and Random Forests","heading":"Decision Trees: Conceptual Questions","text":"Explain basic structure Decision Tree makes predictions.key differences classification trees regression trees?purpose splitting criteria Decision Trees? Describe Gini Index, Entropy, Variance Reduction.Decision Trees prone overfitting? techniques can used prevent overfitting?Define pre-pruning post-pruning Decision Trees. differ?Explain bias-variance tradeoff Decision Trees.advantages disadvantages Decision Trees compared logistic regression classification problems?role maximum depth parameter Decision Tree? affect model performance?might Decision Tree favor continuous variables categorical variables constructing splits?Explain difference CART (Classification Regression Trees) C5.0 Decision Trees.","code":""},{"path":"chapter-tree.html","id":"practical-exercises-using-the-churn-dataset-classification-tasks","chapter":"11 Decision Trees and Random Forests","heading":"Practical Exercises Using the Churn Dataset (Classification Tasks)","text":"churn dataset contains information customer churn behavior telecommunications company. goal predict whether customer churn based various attributes.","code":""},{"path":"chapter-tree.html","id":"data-preparation-and-partitioning","chapter":"11 Decision Trees and Random Forests","heading":"11.6.0.1 Data Preparation and Partitioning","text":"Load dataset partition training set (80%) test set (20%) using partition() function liver package.Fit Decision Tree using churn response variable day.mins, eve.mins, intl.mins, customer.calls, voice.plan predictors.Visualize fitted Decision Tree using rpart.plot(). Interpret tree structure identify key decision rules.Identify important predictors Decision Tree.Compute confusion matrix evaluate model performance.Generate ROC curve compute AUC Decision Tree model.Evaluate effect pruning Decision Tree adjusting complexity parameter (cp).Fit C5.0 Decision Tree data compare performance CART model.Use predict() function estimate probability churn new customer following attributes:\nday.mins = 200\neve.mins = 150\nintl.mins = 10\ncustomer.calls = 3\nvoice.plan = \"yes\"\nday.mins = 200eve.mins = 150intl.mins = 10customer.calls = 3voice.plan = \"yes\"Compare confusion matrix classification accuracy CART C5.0 models.Implement cross-validation assess model performance.","code":"\ndata(churn, package = \"liver\")\n\nset.seed(6)\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$churn"},{"path":"chapter-tree.html","id":"random-forests-conceptual-questions","chapter":"11 Decision Trees and Random Forests","heading":"Random Forests: Conceptual Questions","text":"fundamental difference Decision Trees Random Forests?bagging (Bootstrap Aggregation) improve Random Forest models?Explain majority voting works Random Forest classification model.Random Forest tend outperform single Decision Tree?can determine feature importance Random Forest model?Explain limitations Random Forests.increasing number trees (ntree) Random Forest affect model performance?","code":""},{"path":"chapter-tree.html","id":"practical-exercises-using-the-churn-dataset-random-forests-for-classification-tasks","chapter":"11 Decision Trees and Random Forests","heading":"Practical Exercises Using the Churn Dataset (Random Forests for Classification Tasks)","text":"","code":""},{"path":"chapter-tree.html","id":"fitting-and-evaluating-a-random-forest-model","chapter":"11 Decision Trees and Random Forests","heading":"Fitting and Evaluating a Random Forest Model","text":"Fit Random Forest model using churn response day.mins, eve.mins, intl.mins, customer.calls, voice.plan predictors.Identify important variables using varImpPlot().Compare accuracy Random Forest Decision Tree models (CART C5.0).Compute confusion matrix Random Forest model.Compute ROC curve AUC Random Forest model.Adjust number trees (ntree = 200) evaluate whether increasing number trees improves model accuracy.Use tuneRF() function find optimal value mtry (number predictors consider split).Predict churn probabilities new customer using Random Forest model.Perform feature selection training Random Forest model top 3 important features.Evaluate whether simplified model performs comparably full model.","code":""},{"path":"chapter-tree.html","id":"regression-trees-and-random-forests-redwines-dataset","chapter":"11 Decision Trees and Random Forests","heading":"Regression Trees and Random Forests (redWines Dataset)","text":"redWines dataset contains wine quality scores (quantity, score 0 10) 11 chemical attributes. goal predict quantity using regression trees Random Forests.","code":""},{"path":"chapter-tree.html","id":"data-preparation-and-partitioning-1","chapter":"11 Decision Trees and Random Forests","heading":"Data Preparation and Partitioning","text":"Load dataset partition training set (80%) test set (20%) using partition() function liver package.","code":"\ndata(redWines, package = \"liver\")\n\nset.seed(6)\ndata_sets = partition(data = redWines, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$quantity"},{"path":"chapter-tree.html","id":"conceptual-questions-9","chapter":"11 Decision Trees and Random Forests","heading":"Conceptual Questions","text":"regression tree differ classification tree?Mean Squared Error (MSE) used evaluate regression trees?Explain Random Forest regression generally preferred single regression tree.","code":""},{"path":"chapter-tree.html","id":"practical-exercises-using-the-redwines-dataset","chapter":"11 Decision Trees and Random Forests","heading":"Practical Exercises Using the redWines Dataset","text":"Fit regression tree predicting quantity based 11 predictors.Visualize tree interpret splits. important variables?Compute Mean Squared Error (MSE) regression tree.Fit Random Forest regression model predict quantity compare performance single regression tree.Compare MSE Random Forest model regression tree.Identify top 3 important features Random Forest model.Use trained Random Forest model predict wine quality new observation following attributes:\nfixed.acidity = 8.5\nvolatile.acidity = 0.4\ncitric.acid = 0.3\nresidual.sugar = 2.0\nchlorides = 0.08\nfree.sulfur.dioxide = 30\ntotal.sulfur.dioxide = 100\ndensity = 0.995\npH = 3.2\nsulphates = 0.6\nalcohol = 10.5\nfixed.acidity = 8.5volatile.acidity = 0.4citric.acid = 0.3residual.sugar = 2.0chlorides = 0.08free.sulfur.dioxide = 30total.sulfur.dioxide = 100density = 0.995pH = 3.2sulphates = 0.6alcohol = 10.5Use cross-validation compare Random Forest model regression tree.Interpret whether Random Forest significantly improves prediction accuracy compared single Decision Tree.","code":""},{"path":"chapter-nn.html","id":"chapter-nn","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12 Neural Networks: The Building Blocks of Artificial Intelligence","text":"centuries, humans captivated idea creating machines can think learn like us. Philosophers, inventors, storytellers explored vision, mechanical automata ancient Greece artificial beings imagined science fiction. Early inventors like Hero Alexandria designed self-operating machines, myths artificial life—golem automatons—reflected humanity’s enduring fascination intelligent systems. realm imagination now become reality form Artificial Intelligence (AI), transformative force reshaping industries, societies, daily life.AI longer futuristic concept—powers everything recommendation systems fraud detection autonomous vehicles generative AI (GenAI) models capable producing text, images, music. advancements fueled exponential growth computational power, availability vast datasets, breakthroughs machine learning algorithms. heart revolution lies class models known neural networks, driving force behind deep learning.\nNeural networks computational models inspired structure function human brain. Just biological neurons connect form intricate networks process information, artificial neural networks consist layers interconnected nodes learn data. design enables recognize patterns, extract meaningful insights, make predictions, making particularly suited problems involving complex, high-dimensional, unstructured data—images, speech, natural language. Unlike traditional machine learning models, rely handcrafted features, neural networks automatically discover representations data, often outperforming classical approaches.deep learning—powered advanced neural architectures—made groundbreaking strides areas like computer vision language modeling, foundation rests simpler architectures. chapter, focus feed-forward neural networks, also known multilayer perceptrons (MLPs). fundamental models serve building blocks sophisticated deep learning systems.","code":""},{"path":"chapter-nn.html","id":"why-neural-networks-are-powerful","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Why neural networks are powerful","text":"Neural networks excel tackling complex nonlinear problems, making indispensable modern AI applications. strengths stem three key capabilities:Pattern recognition complex data: Neural networks exceptionally effective identifying patterns unstructured data, detecting objects images, recognizing speech, generating human-like text. tasks often pose significant challenges traditional algorithms.Pattern recognition complex data: Neural networks exceptionally effective identifying patterns unstructured data, detecting objects images, recognizing speech, generating human-like text. tasks often pose significant challenges traditional algorithms.Robustness noise: Due dense connectivity adaptive learning, neural networks can extract meaningful information even noisy incomplete datasets, filtering irrelevant variations.Robustness noise: Due dense connectivity adaptive learning, neural networks can extract meaningful information even noisy incomplete datasets, filtering irrelevant variations.Scalability adaptability: Neural networks can handle massive datasets model highly nonlinear relationships adjusting architecture—adding layers neurons capture increasing complexity.Scalability adaptability: Neural networks can handle massive datasets model highly nonlinear relationships adjusting architecture—adding layers neurons capture increasing complexity.Despite advantages, neural networks come challenges. Unlike interpretable models decision trees, often seen “black boxes” decision-making process distributed across many parameters, making difficult explain individual predictions. Additionally, training neural networks requires significant computational power, often necessitating specialized hardware like GPUs TPUs process large volumes data efficiently.power neural networks lies ability emulate biological learning. Just neurons brain collaborate recognize patterns, artificial neurons network combine outputs solve problems traditional algorithms struggle . ability adaptively model nonlinear relationships placed neural networks forefront research real-world applications.","code":""},{"path":"chapter-nn.html","id":"whats-ahead","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"What’s ahead","text":"chapter, explore key principles neural networks transformative applications following topics:Biological inspiration: structure function human brain inspired artificial neural networks.Core algorithmic principles: fundamental mechanics neural networks, including layers, nodes, weights.Activation functions: role non-linearity enabling neural networks learn complex patterns.Training neural networks: iterative optimization process used adjust neural network parameters.Case study: Applying neural networks predict customer subscription behavior using bank marketing dataset.Neural networks revolutionized computing, enabling machines tackle problems considered unsolvable. driving autonomous vehicles powering medical diagnostics, models shaping future AI. understand function, begin exploring biological origins inspiration drawn human brain.","code":""},{"path":"chapter-nn.html","id":"neural-networks-inspired-by-biological-neurons","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.1 Neural networks: inspired by biological neurons","text":"foundation neural networks deeply rooted structure function biological neurons, form basis learning decision-making animal brains. individual neurons relatively simple structure, true power lies dense intricate connectivity. networks interconnected neurons enable brain perform highly complex tasks, pattern recognition, classification, reasoning, decision-making. example, human brain contains approximately \\(10^{11}\\) neurons, neuron forming connections average 10,000 others. creates astonishing \\(10^{15}\\) synaptic connections—vast, dynamic network capable extraordinary learning adaptation.Artificial neural networks (ANNs) computational models inspired biological structures. replicate full complexity brain, abstract key principles learning interconnected units. leveraging dense networks artificial neurons, ANNs can model nonlinear dynamic processes, enabling tackle complex problems domains image recognition, speech processing, decision-making. particularly adept uncovering patterns relationships data, even cases traditional algorithms struggle.shown Figure 12.1, biological neuron designed process transmit information. Dendrites act input channels, collecting signals neurons. signals processed integrated cell body, decision made: combined input surpasses certain threshold, neuron “fires” sends output signal axon connected neurons. nonlinear behavior—firing certain input threshold exceeded—plays critical role brain’s ability process information efficiently.Similarly, artificial neuron (illustrated Figure 12.2) emulates process using mathematical model. receives inputs (\\(x_i\\)) either artificial neurons directly dataset. inputs combined using weighted summation (\\(\\sum w_i x_i\\)), weights (\\(w_i\\)) represent strength input’s influence. combined signal passed activation function (\\(f(.)\\)) introduce non-linearity, determining final output (\\(\\hat{y}\\)). output either transmitted artificial neurons used final result model. activation function crucial, enables neural networks learn model complex, nonlinear relationships data.\nFigure 12.1: Visualization biological neuron, processes input signals dendrites sends outputs axon.\n\nFigure 12.2: Illustration artificial neuron, designed emulate structure function biological neuron simplified way.\nOne key advantages artificial neural networks ability generalize data, even faced noise incomplete information. Unlike traditional algorithms rely explicit rule-based decision-making, neural networks distribute learning across many interconnected neurons weighted connections, allowing adapt identify patterns even individual data points imperfect. distributed learning process enables handle complex, high-dimensional data effectively many conventional models.However, flexibility comes cost. Neural networks often require large amounts data computational power train effectively, decision-making process less interpretable traditional models decision trees. Unlike rule-based models provide clear decision paths, neural networks embed learned knowledge within millions parameters, making challenging understand precisely prediction made.following sections, delve deeper mechanics neural networks, starting core structure algorithms enable learn data.","code":""},{"path":"chapter-nn.html","id":"how-neural-networks-work","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.2 How neural networks work","text":"Neural networks extend traditional linear models incorporating multiple layers processing capture complex relationships data. core, build upon fundamental concepts linear regression. linear regression model makes predictions using following equation:\\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_p x_p\n\\]\\(p\\) represents number predictors, \\(b_0\\) intercept, \\(b_1\\) \\(b_p\\) learned coefficients. setup, \\(\\hat{y}\\) weighted sum input features (\\(x_1\\) \\(x_p\\)), weights (\\(b_1\\) \\(b_p\\)) determine relative influence feature prediction. simple linear relationship can visualized Figure 12.3, input features prediction represented nodes, coefficients acting connecting weights.\nFigure 12.3: graphical representation regression model: input features predictions shown nodes, coefficients represented connections nodes.\nlinear models effective capturing direct relationships inputs outputs, struggle represent complex patterns, interactions variables hierarchical structures data. Neural networks address limitation introducing multiple layers artificial neurons input output, allowing model intricate, nonlinear relationships. structure illustrated Figure 12.4.architecture neural network consists following key components:input layer serves entry point data. node layer corresponds input feature, age, income, pixel intensity image.input layer serves entry point data. node layer corresponds input feature, age, income, pixel intensity image.hidden layers process data transforming inputs multiple interconnected nodes (artificial neurons). hidden layer captures increasingly abstract features, allowing network learn complex patterns. Every neuron hidden layer connected neurons preceding succeeding layers, connection assigned weight.hidden layers process data transforming inputs multiple interconnected nodes (artificial neurons). hidden layer captures increasingly abstract features, allowing network learn complex patterns. Every neuron hidden layer connected neurons preceding succeeding layers, connection assigned weight.output layer produces final prediction. classification tasks, typically represents probability given class, regression tasks, outputs continuous numerical value.output layer produces final prediction. classification tasks, typically represents probability given class, regression tasks, outputs continuous numerical value.\nFigure 12.4: Visualization multilayer neural network model two hidden layers.\nFigure 12.4, input layer passes features network, hidden layer applies transformations passing processed information forward. output layer aggregates information generate final prediction. connection network assigned weight (\\(w_i\\)), determines strength influence one neuron another. weights adjusted training optimize model’s accuracy.behavior artificial neuron can mathematically expressed :\\[\n\\hat{y} = f\\left( \\sum_{=1}^{p} w_i x_i + b \\right)\n\\]:\n- \\(x_i\\) represents input features,\n- \\(w_i\\) represents corresponding weights,\n- \\(b\\) bias term helps shift activation threshold,\n- \\(\\sum\\) represents summation weighted inputs,\n- \\(f(.)\\) activation function, \n- \\(\\hat{y}\\) output neuron.crucial component process activation function, introduces non-linearity model. Without , neural network equivalent linear model, regardless number layers. applying non-linear transformation combined input signals, activation functions allow neural networks approximate complex relationships data.","code":""},{"path":"chapter-nn.html","id":"key-characteristics-of-neural-networks","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Key characteristics of neural networks","text":"Despite diversity neural network architectures, neural networks share three fundamental characteristics define functionality:Activation functions activation function transforms neuron’s net input output signal passed next layer. Activation functions introduce non-linearity, enabling network learn complex patterns. Common choices include sigmoid function, ReLU (rectified linear unit), hyperbolic tangent (tanh).Activation functions activation function transforms neuron’s net input output signal passed next layer. Activation functions introduce non-linearity, enabling network learn complex patterns. Common choices include sigmoid function, ReLU (rectified linear unit), hyperbolic tangent (tanh).Network architecture structure neural network defines computational capacity. includes number layers, number neurons layer, neurons connected. example, deep neural networks contain many hidden layers, enabling learn hierarchical abstract representations data.Network architecture structure neural network defines computational capacity. includes number layers, number neurons layer, neurons connected. example, deep neural networks contain many hidden layers, enabling learn hierarchical abstract representations data.Training algorithm Training neural network involves adjusting weights (\\(w_i\\)) biases (\\(b\\)) model minimize prediction errors. achieved defining loss function, quantifies far network’s predictions deviate true values. Optimization algorithms gradient descent iteratively update weights reduce error, allowing network improve time.Training algorithm Training neural network involves adjusting weights (\\(w_i\\)) biases (\\(b\\)) model minimize prediction errors. achieved defining loss function, quantifies far network’s predictions deviate true values. Optimization algorithms gradient descent iteratively update weights reduce error, allowing network improve time.following sections, explore components greater detail, beginning activation functions role enabling neural networks model complex, non-linear relationships.","code":""},{"path":"chapter-nn.html","id":"activation-functions","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.3 Activation functions","text":"activation function fundamental component neural network, determining artificial neuron processes incoming signals passes information forward. Much like biological neurons, integrate input signals dendrites decide whether “fire,” artificial neurons use activation functions introduce non-linearity, allowing neural networks model complex relationships linear models capture. Without activation functions, even deep neural networks behave like simple linear models, limiting ability learn hierarchical abstract features data.mathematical terms, artificial neuron computes weighted sum inputs applies activation function \\(f(x)\\) determine output:\\[\n\\hat{y} = f\\left( \\sum_{=1}^{p} w_i x_i + b \\right)\n\\]\\(x_i\\) represents input features, \\(w_i\\) corresponding weights, \\(b\\) bias term, \\(f(x)\\) activation function. choice activation function significantly impacts network’s ability learn generalize.","code":""},{"path":"chapter-nn.html","id":"the-threshold-activation-function","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"The threshold activation function","text":"Early neural network models used simple threshold activation function, mimicking binary nature biological neurons. threshold function activates input exceeds certain value, producing output either 0 1. defined :\\[\nf(x) =\n\\begin{cases}\n1 & \\text{} x \\geq 0 \\\\\n0 & \\text{} x < 0\n\\end{cases}\n\\]step-like behavior visualized Figure 12.5. function outputs 1 input least zero 0 otherwise, making useful basic classification tasks.\nFigure 12.5: Visualization threshold activation function (unit step).\nbiologically intuitive, threshold function major drawbacks. model nuanced relationships inputs outputs, provides binary decisions. importantly, differentiable, preventing use gradient-based optimization methods backpropagation. reasons, largely replaced continuous activation functions.","code":""},{"path":"chapter-nn.html","id":"the-sigmoid-activation-function","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"The sigmoid activation function","text":"widely used alternative sigmoid activation function, also known logistic function. sigmoid function maps real-valued input smooth output 0 1, making suitable probability-based interpretations. defined :\\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\]\\(e\\) base natural logarithm (approximately 2.72). sigmoid function S-shaped curve, shown Figure 12.6, provides differentiable alternative threshold function.\nFigure 12.6: Visualization sigmoid activation function.\nAlthough sigmoid activation useful, suffers vanishing gradient problem. inputs large small, function saturates, producing values close 0 1. regions, derivative near zero, slowing learning process deep networks. result, alternative activation functions often preferred hidden layers.","code":""},{"path":"chapter-nn.html","id":"other-common-activation-functions","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Other common activation functions","text":"Several activation functions developed address limitations sigmoid function. Figure 12.7 compares commonly used activation functions.Hyperbolic Tangent (tanh): Similar sigmoid function output range \\((-1, 1)\\). symmetry around zero often leads faster learning practice.ReLU (Rectified Linear Unit): Defined \\(f(x) = \\max(0, x)\\), ReLU widely used activation function deep neural networks due computational efficiency ability mitigate vanishing gradient problem.Leaky ReLU: variation ReLU allows small negative values instead zero negative inputs, reducing problem inactive neurons.\nFigure 12.7: Comparison common activation functions: sigmoid, tanh, ReLU.\n","code":""},{"path":"chapter-nn.html","id":"choosing-the-right-activation-function","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Choosing the right activation function","text":"choice activation function depends network architecture specific learning task:Sigmoid commonly used output layer binary classification problems, output represents probability.Tanh often used hidden layers zero-centered, can lead faster convergence training.ReLU default choice hidden layers deep networks due efficiency ability mitigate vanishing gradient problem.Leaky ReLU useful cases ReLU may cause neurons become inactive, allows small negative outputs.Linear activation typically used final layer regression models, output continuous value.Activation functions sigmoid tanh can suffer saturation, input values far zero result near-constant outputs. leads vanishing gradients, slowing learning. One strategy mitigate issue preprocess input data normalization standardization, ensuring values remain within optimal range.ReLU variants become preferred choice hidden layers deep learning due computational efficiency ability propagate gradients effectively. However, selecting right activation function depends specific problem empirical evaluation.","code":""},{"path":"chapter-nn.html","id":"network-architecture","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.4 Network architecture","text":"ability neural network learn make predictions fundamentally tied architecture, topology. refers arrangement neurons connections , define data flows network. neural networks can take various forms, architecture primarily characterized three factors:number layers network,number neurons (nodes) layer, andThe connectivity neurons across layers.architecture neural network determines capacity model complex relationships. Larger networks layers neurons can capture intricate patterns decision boundaries. However, effectiveness network solely determined size also components organized interconnected.understand network architecture, consider simple example illustrated Figure 12.3. basic network consists :Input nodes, receive raw feature values dataset. input node corresponds one feature passes value network.Output nodes, provide network’s final prediction, denoted \\(p\\).single-layer network, input nodes directly connected output node set weights (\\(w_1, w_2, \\dots, w_p\\)), representing influence input feature prediction. simple architecture works well basic classification regression tasks struggles capturing complex patterns.address limitation, additional layers can introduced, shown Figure 12.4. intermediate layers, known hidden layers, allow network model nonlinear relationships discover hierarchical structures data.multilayer network typically consists three types layers:input layer, raw features enter network.One hidden layers, extract refine patterns.output layer, combines processed information generate network’s final prediction.fully connected network, neuron one layer transmits information neurons next layer, associated weight assigned connection. weights determine strongly neurons influence adjusted training optimize performance.introduction hidden layers enables neural networks process input data hierarchically. Early layers may learn basic features, edges image simple word patterns text, deeper layers capture abstract representations, object shapes semantic meaning. Networks multiple hidden layers referred deep neural networks (DNNs), training models known deep learning. approach driven advancements computer vision, speech recognition, natural language processing.number input output nodes network determined problem hand.\n- input nodes match number features dataset. example, dataset 20 features 20 input nodes.\n- output nodes depend task. regression problems, typically one output node representing continuous value. classification tasks, number output nodes corresponds number classes.number hidden nodes layer predefined must determined based complexity problem. larger number hidden nodes increases network’s ability capture intricate patterns also raises risk overfitting—model performs well training data poorly unseen data. Overly large networks can also computationally expensive slow train.Balancing network complexity efficiency essential achieving optimal performance. often guided principle Occam’s Razor, suggests simplest model adequately explains data usually preferable. practice, optimal network architecture determined experimentation, incorporating techniques cross-validation regularization methods like dropout weight decay mitigate overfitting.section focuses fully connected networks, specialized architectures convolutional neural networks (CNNs) image processing recurrent neural networks (RNNs) sequential data provide additional flexibility specific tasks. architectures build upon principles deep learning enabled major breakthroughs artificial intelligence.summary, architecture neural network defines capacity solve problems. simple single-layer networks sophisticated deep neural networks, architectures offer flexibility model wide variety tasks, ranging basic regression highly complex applications like autonomous driving medical diagnosis. next section, explore architectures trained optimize performance learn data.","code":""},{"path":"chapter-nn.html","id":"how-neural-networks-learn","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.5 How neural networks learn","text":"neural network, like untrained system, starts knowledge task hand. exposure data, gradually learns adjusting internal connections, process akin humans refine understanding experience. connections, represented weights, updated network processes data, enabling discover patterns relationships time. Just child learns recognize objects repeatedly encountering , neural network improves predictions iterative refinements based data encounters.Training neural network computationally intensive process involves adjusting weights connecting neurons. neural networks studied since mid-20th century, real-world application remained limited 1980s, major breakthrough—backpropagation algorithm—made feasible train multilayer networks efficiently. Backpropagation revolutionized neural network training enabling networks systematically learn errors, making deep learning practical real-world applications. Despite computationally demanding compared simpler machine learning algorithms, backpropagation remains foundation modern neural network training, powering advancements fields computer vision natural language processing.core, backpropagation refines network’s weights iterative learning process consisting two main phases: forward phase backward phase. iteration, known epoch, begins randomly initialized weights, network starts without prior knowledge. successive epochs, network continuously updates weights minimize prediction errors.forward phase, input data passed network, layer layer, input layer hidden layers reaching output layer. neuron processes input applying associated weights, summing weighted inputs, transforming result using activation function. output layer produces network’s final prediction, compared actual target value training data. comparison generates error signal, quantifying far network’s prediction deviates expected outcome.backward phase, error signal propagated backward network update weights. objective adjust weights way reduces prediction error subsequent forward passes. process relies gradient descent, optimization technique determines optimal direction magnitude weight changes minimize error. gradient represents rate change error respect weight, acting guide indicate adjustments made. analogous descending mountain: always moving direction steepest downward slope, network iteratively approaches point minimum error.magnitude weight adjustments controlled parameter known learning rate. high learning rate enables network make large updates, may speed training risks overshooting optimal solution. Conversely, low learning rate results gradual updates, ensuring precise refinements potentially slowing convergence. Selecting appropriate learning rate crucial efficient training, modern optimization techniques, adaptive learning rate methods, help fine-tune process dynamically.successfully apply gradient descent backpropagation, network’s activation functions must differentiable. requirement ensures gradients can computed efficiently, allowing meaningful weight updates. result, smooth, non-linear activation functions sigmoid, hyperbolic tangent (tanh), ReLU (Rectified Linear Unit) widely used neural networks. modern deep learning, variations gradient descent, stochastic gradient descent (SGD) Adam, developed improve efficiency stability, particularly large-scale datasets.repeated cycles forward backward propagation, network refines weights, reducing overall error improving ability generalize unseen data. process may seem computationally complex, modern machine learning frameworks TensorFlow PyTorch automate backpropagation, gradient descent, weight updates, allowing practitioners focus designing network architecture preparing data.development backpropagation marked turning point neural network research, enabling models solve real-world problems remarkable accuracy. Although training remains computationally demanding, advancements hardware—GPUs TPUs—significantly accelerated process, making feasible train large complex networks efficiently. clear understanding neural networks learn, now turn practical applications, examining uncover patterns data make accurate predictions real-world settings.","code":""},{"path":"chapter-nn.html","id":"case-study-bank-marketing","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.6 Case study: bank marketing","text":"can financial institution improve effectiveness future marketing campaigns? make data-driven decision, analyze bank’s previous marketing campaign identify patterns can inform future strategies. case study, apply neural network predict whether customer subscribe term deposit based demographic campaign-related features. analyzing dataset, aim uncover key factors influence customer subscription behavior, helping bank design effective marketing strategies.analysis, use bank dataset, well-known benchmark dataset sourced UC Irvine Machine Learning Repository available liver package. dataset used Sérgio Moro, Paulo Cortez, Paulo Rita12 data-driven approach predicting success bank telemarketing campaigns. goal build classification model predicts customers likely subscribe term deposit.","code":""},{"path":"chapter-nn.html","id":"problem-understanding-3","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Problem understanding","text":"Banks typically use two main strategies market financial products:Mass campaigns: campaigns reach broad audience minimal targeting, often resulting low response rates (typically 1%).Directed marketing: approach targets customers likely interested product, improving conversion rates raising potential privacy concerns.case study, aim enhance effectiveness directed marketing analyzing past campaign data identify patterns customer behavior. predicting customers likely subscribe term deposit, bank can optimize marketing efforts, reduce costs, minimize intrusive communications maintaining success rates.term deposit fixed-term savings product offers customers higher interest rates standard savings accounts. Banks use term deposits secure long-term funds strengthen financial reserves. details term deposits can found .","code":""},{"path":"chapter-nn.html","id":"overview-of-the-dataset-2","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Overview of the dataset","text":"bank dataset includes information direct phone-based marketing campaigns conducted financial institution. Customers contacted multiple times within campaign. objective dataset predict whether customer subscribe term deposit (deposit = \"yes\" \"\").load bank dataset directly R examine structure using following commands:dataset contains 4521 observations 17 variables. target variable, deposit, binary, two categories: yes . summary features:Demographic features:\n- age: Age customer (numeric).\n- job: Type job (e.g., “admin.”, “blue-collar”, “management”).\n- marital: Marital status (e.g., “married”, “single”).\n- education: Level education (e.g., “secondary”, “tertiary”).\n- default: Whether customer credit default (binary: “yes”, “”).\n- balance: Average yearly balance euros (numeric).Loan information:\n- housing: Whether customer housing loan (binary).\n- loan: Whether customer personal loan (binary).Campaign details:\n- contact: Type communication used (e.g., “telephone”, “cellular”).\n- day: Last contact day month (numeric).\n- month: Last contact month year (categorical: “jan”, “feb”, “mar”, …, “nov”, “dec”).\n- duration: Last contact duration seconds (numeric).\n- campaign: Total number contacts made customer campaign (numeric).\n- pdays: Days since customer last contacted (numeric).\n- previous: Number contacts current campaign (numeric).\n- poutcome: Outcome previous campaign (e.g., “success”, “failure”).Target variable:\n- deposit: Indicates whether customer subscribed term deposit (binary: “yes”, “”).dataset contains diverse set features related customer demographics past interactions, making well-suited building predictive models improve marketing strategies.dataset ready, now proceed data preparation model training using neural network predict customer subscription behavior.","code":"library(liver)   # Load the liver package\n\ndata(bank)   # Load the bank marketing dataset \n\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-nn.html","id":"preparing-data-for-modeling-3","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Preparing data for modeling","text":"training neural network model, need prepare dataset splitting training test sets. ensures can assess well model generalizes new, unseen data. use 80/20 split, allocating 80% data training 20% testing. maintain consistency previous chapters, apply partition() function liver package:set.seed() function ensures reproducibility fixing random seed. use train_set train classification models, test_set serves unseen data evaluation. test_labels vector contains true class labels test_set, compare model’s predictions.validate split, compare proportion deposit = \"yes\" training test sets using two-sample Z-test:test confirms proportions subsets statistically similar (p-value > 0.05), validating split. ensures training test sets representative original dataset, enabling us evaluate model’s performance effectively.objective classify customers either likely (deposit = \"yes\") unlikely (deposit = \"\") subscribe term deposit, based following predictors:age, default, balance, housing, loan, duration, campaign, pdays, previous.","code":"\nset.seed(500)\n\ndata_sets = partition(data = bank, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\ntest_labels = test_set$depositx1 = sum(train_set$deposit == \"yes\")\nx2 = sum(test_set$deposit == \"yes\")\n\nn1 = nrow(train_set)\nn2 = nrow(test_set)\n\nprop.test(x = c(x1, x2), n = c(n1, n2))\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.0014152, df = 1, p-value = 0.97\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.02516048  0.02288448\n   sample estimates:\n      prop 1    prop 2 \n   0.1150124 0.1161504"},{"path":"chapter-nn.html","id":"one-hot-encoding-3","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"One-hot encoding","text":"Since neural networks require numerical inputs, categorical variables must converted numeric representations. One-hot encoding achieves creating separate binary variables category. apply one.hot() function liver package transform categorical features numerical format suitable neural network:one.hot() function expands categorical variables binary columns. example, binary variable default transformed two new variables, default_yes default_no, indicating whether customer defaulted credit. selected predictors used following formula:","code":"categorical_vars = c(\"default\", \"housing\", \"loan\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_vars)\ntest_onehot  = one.hot(test_set,  cols = categorical_vars)\n\nstr(test_onehot)\n   'data.frame':    904 obs. of  20 variables:\n    $ age        : int  43 40 56 25 31 32 23 36 32 32 ...\n    $ job        : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 1 5 10 2 10 2 8 5 10 3 ...\n    $ marital    : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 2 3 2 2 3 3 3 3 ...\n    $ education  : Factor w/ 4 levels \"primary\",\"secondary\",..: 2 3 2 1 2 2 3 3 3 1 ...\n    $ default_no : int  1 1 1 1 1 1 1 1 1 0 ...\n    $ default_yes: int  0 0 0 0 0 0 0 0 0 1 ...\n    $ balance    : int  264 194 4073 -221 171 2089 363 553 2204 -849 ...\n    $ housing_no : int  0 1 1 0 1 0 0 1 0 0 ...\n    $ housing_yes: int  1 0 0 1 0 1 1 0 1 1 ...\n    $ loan_no    : int  1 0 1 1 1 1 1 1 1 0 ...\n    $ loan_yes   : int  0 1 0 0 0 0 0 0 0 1 ...\n    $ contact    : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 1 1 3 1 1 1 ...\n    $ day        : int  17 29 27 23 27 14 30 11 21 4 ...\n    $ month      : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 1 2 2 9 2 10 9 2 10 4 ...\n    $ duration   : int  113 189 239 250 81 132 16 106 11 204 ...\n    $ campaign   : int  2 2 5 1 3 1 18 2 4 1 ...\n    $ pdays      : int  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n    $ previous   : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ poutcome   : Factor w/ 4 levels \"failure\",\"other\",..: 4 4 4 4 4 4 4 4 4 4 ...\n    $ deposit    : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 2 1 1 1 1 ...\nformula = deposit ~ default_yes + housing_yes + loan_yes + age + balance + duration + campaign + pdays + previous"},{"path":"chapter-nn.html","id":"feature-scaling-3","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Feature scaling","text":"Neural networks perform best input features similar scale. achieve , normalize numerical variables using min-max scaling, transforming inputs standardized range 0 1. prevents features larger numerical ranges dominating learning process helps improve model convergence.prevent data leakage, scaling parameters (minimum maximum values) computed training set applied consistently test set. ensures test set remains independent evaluation dataset, preventing influencing model training.visualize effect scaling, compare distribution age applying min-max transformation:first histogram (left) displays distribution age training set scaling, second histogram (right) shows distribution applying min-max scaling. values mapped range 0 1 preserving original distribution.","code":"\nnumeric_vars = c(\"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\")\n\nmin_train = sapply(train_onehot[, numeric_vars], min)\nmax_train = sapply(train_onehot[, numeric_vars], max)\n\ntrain_scaled = minmax(train_onehot, col = numeric_vars, min = min_train, max = max_train)\ntest_scaled  = minmax(test_onehot,  col = numeric_vars, min = min_train, max = max_train)\nggplot(data = train_set) +\n  geom_histogram(mapping = aes(x = age), colour = \"darkblue\", fill = \"lightblue\") +\n  ggtitle(\"Variable 'age' before scaling\")\n\nggplot(data = train_scaled) +\n  geom_histogram(mapping = aes(x = age), colour = \"darkblue\", fill = \"lightblue\") +\n  ggtitle(\"Variable 'age' after scaling\")"},{"path":"chapter-nn.html","id":"applying-the-neural-network-algorithm","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Applying the neural network algorithm","text":"implement neural network, use neuralnet package R. package offers straightforward flexible way build neural networks provides functionality visualizing network topology. neuralnet useful learning tool, also powerful enough practical applications.neuralnet package installed, can installed using install.packages(\"neuralnet\")., load R session:Next, apply neuralnet() function training dataset build model:’s argument function call :formula: Specifies relationship target variable (deposit) predictors.data: Indicates dataset used training (train_scaled).hidden = 1: Defines number hidden layers nodes (one hidden layer single node). simplicity, start minimal architecture, complex networks additional nodes layers may improve performance.err.fct = \"sse\": Specifies sum squared errors loss function. SSE commonly used, cross-entropy loss (ce) often preferred classification tasks.linear.output = FALSE: Ensures output layer uses logistic activation function, essential classification tasks predicted values represent probabilities.training, visualize network examine topology:visualization shows network consists :9 input nodes, corresponding 9 predictors,1 hidden layer containing single node, and2 output nodes representing classification result (yes ).training process converged 1998 steps, final error rate 284.9. analysis trained network’s weights indicates duration—length last phone call—strongest influence prediction. finding aligns prior studies, call duration key indicator customer engagement marketing campaigns.simple neural network model demonstrates input features processed multiple layers extract patterns make predictions. next section, evaluate model’s performance interpret results.","code":"\nlibrary(neuralnet)\nneuralnet_bank = neuralnet(\n  formula = formula,\n  data = train_scaled,\n  hidden = 1,                # Single hidden layer with 1 node\n  err.fct = \"sse\",           # Loss function: Sum of Squared Errors\n  linear.output = FALSE      # Logistic activation function for classification\n)\nplot(neuralnet_bank, rep = \"best\")"},{"path":"chapter-nn.html","id":"prediction-and-model-evaluation-2","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Prediction and model evaluation","text":"training neural network, evaluate performance applying test set, contains customers unseen training. goal compare model’s predictions actual class labels stored test_labels.obtain predictions neural network, use predict() function:inspect model’s predictions, display first six output values:column represents one output unit. Since neuralnet() automatically apply softmax function, values interpreted raw activations rather probabilities. classify customer likely subscribe, compare output activations. activation corresponding deposit = \"yes\" greater activation deposit = \"\", customer predicted subscriber.evaluate predictions using confusion matrix cutoff value 0.5:confusion matrix summarizes model’s performance comparing predicted versus actual labels. provides insight classification accuracy, well number true positives, false positives, true negatives, false negatives.Finally, assess model’s performance plotting ROC curve calculating AUC:ROC curve evaluates well model differentiates customers subscribe (deposit = \"yes\") (deposit = \"\") varying classification thresholds. high AUC score indicates strong predictive performance, meaning model effective distinguishing two classes.","code":"\nprob_nn = predict(neuralnet_bank, test_scaled)head(prob_nn)\n             [,1]       [,2]\n   [1,] 0.9849665 0.01506696\n   [2,] 0.9782933 0.02174953\n   [3,] 0.8866629 0.11342508\n   [4,] 0.9760059 0.02403981\n   [5,] 0.9686046 0.03144922\n   [6,] 0.9840902 0.01594459# Extract the prediction for 'deposit = \"yes\"'\nprob_nn_yes = prob_nn[, 2] \n\nconf.mat(prob_nn_yes, test_labels, cutoff = 0.5, reference = \"yes\")\n          Actual\n   Predict yes  no\n       yes  22  16\n       no   83 783\nlibrary(pROC)   # For ROC curve\n\nroc_nn = roc(test_labels, prob_nn_yes)\n\nggroc(roc_nn, size = 0.8) + \n  theme_minimal() + \n  ggtitle(\"ROC for Neural Network Algorithm\") +\n  theme(legend.title = element_blank(), legend.position = c(.7, .3), text = element_text(size = 17))"},{"path":"chapter-nn.html","id":"exercises-8","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Exercises","text":"exercises reinforce concepts introduced chapter, focusing understanding neural networks, tuning hyperparameters, comparing performance tree-based models. divided conceptual questions, practical tasks using bank dataset, comparative exercises using adult dataset.","code":""},{"path":"chapter-nn.html","id":"conceptual-questions-10","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Conceptual questions","text":"Explain basic structure neural network. role input, hidden, output layers play?rule activation functions neural network? non-linear activation functions essential?Compare contrast ReLU, sigmoid, tanh activation functions. scenarios preferred?neural networks considered universal function approximators?Explain backpropagation updates weights neural network.neural networks often require large datasets perform well?Define bias-variance tradeoff neural networks. model complexity affect bias variance?purpose dropout regularization, help prevent overfitting?role loss function training neural network?weight initialization, important?Compare shallow neural networks deep neural networks. advantages deeper architectures provide?number hidden layers affect neural network’s ability model complex patterns?role hyperparameter tuning neural networks?Compare computational efficiency decision trees neural networks.","code":""},{"path":"chapter-nn.html","id":"practical-exercises-using-the-bank-dataset-1","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Practical exercises using the bank dataset","text":"","code":""},{"path":"chapter-nn.html","id":"data-preparation-and-model-training","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Data preparation and model training","text":"Load bank dataset examine structure. key predictors?Split dataset training (80%) testing (20%) sets, ensuring split statistically representative.Apply one-hot encoding categorical variables. advantages one-hot encoding label encoding?Normalize numerical features using min-max scaling. scaling important neural networks?Train basic feed-forward neural network one hidden layer containing five neurons. Evaluate accuracy.Increase number neurons hidden layer ten. accuracy improve?Train neural network two hidden layers (five neurons first layer, three second). performance compare?Change activation function ReLU sigmoid. affect model’s convergence accuracy?Train model using cross-entropy loss instead sum squared errors. performs better?","code":""},{"path":"chapter-nn.html","id":"model-evaluation-and-comparison-with-tree-based-models","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Model evaluation and comparison with tree-based models","text":"Compute confusion matrix interpret precision, recall, F1-score neural network model.Plot ROC curve calculate AUC. well model differentiate subscribed non-subscribed customers?Compare neural network’s performance decision tree classifier trained dataset.Compare neural network’s performance random forest model trained dataset.model (decision tree, random forest, neural network) performs best terms accuracy, precision, recall?","code":""},{"path":"chapter-nn.html","id":"training-neural-networks-on-the-adult-dataset","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Training neural networks on the adult dataset","text":"Load adult dataset examine structure. key differences dataset bank dataset?Preprocess categorical features using one-hot encoding.Normalize numerical features using min-max scaling.Split dataset training (80%) testing (20%) sets.Train basic neural network one hidden layer (five neurons) predict income level (<=50K >50K).Increase number neurons hidden layer ten. performance improve?Train deeper neural network two hidden layers (ten five neurons).Compare ReLU, tanh, sigmoid activation functions model performance.Train decision tree adult dataset compare accuracy neural network model.Train random forest adult dataset compare performance neural network model.Analyze feature importance random forest model compare influential features neural network model.Compare ROC curves neural network, decision tree, random forest models. model highest AUC?model performs better predicting high-income individuals, ?","code":""},{"path":"chapter-cluster.html","id":"chapter-cluster","chapter":"13 Clustering","heading":"13 Clustering","text":"Every day, interact systems organize vast amounts data without explicit instructions. Netflix recommend movies tailored taste? Amazon categorize millions products? real-world examples clustering, machine learning technique groups similar items based shared characteristics—without predefined labels.many real-world scenarios, deal large datasets structure unknown. Unlike classification, assigns predefined labels data points (e.g., distinguishing spam non-spam emails), clustering exploratory—helps uncover hidden patterns, making powerful tool knowledge discovery. identifying meaningful groups, clustering allows us make sense complex data extract valuable insights.Clustering widely used across multiple domains, including:Customer segmentation – Identifying distinct customer groups personalize marketing campaigns.Market research – Understanding consumer behavior enhance product recommendations.Fraud detection – Detecting suspicious financial transactions may indicate fraudulent activity.Document organization – Automatically grouping large collections text meaningful categories.Bioinformatics – Clustering genes similar expression patterns uncover biological insights.chapter provides comprehensive introduction clustering, covering:fundamental principles clustering differs classification.mechanics clustering algorithms define similarity.K-means clustering, one widely used clustering techniques.practical case study: segmenting cereal brands based nutritional content.end chapter, understand clustering works, apply , implement real-world scenarios. Let’s dive !","code":""},{"path":"chapter-cluster.html","id":"cluster-what","chapter":"13 Clustering","heading":"13.1 What is Cluster Analysis?","text":"Clustering unsupervised machine learning technique groups data points clusters based similarity. Unlike supervised learning, models learn labeled examples, clustering exploratory—uncovers hidden structures data without predefined labels. goal form groups data points within cluster highly similar, different clusters distinct.computer determine data points belong together? Clustering relies similarity measures quantify close distant two points . One commonly used approaches distance metrics, Euclidean distance, defined :\\[\n\\text{dist}(x, y) = \\sqrt{ \\sum_{=1}^n (x_i - y_i)^2}\n\\]\\(x = (x_1, x_2, \\ldots, x_n)\\) \\(y = (y_1, y_2, \\ldots, y_n)\\) represent two data points \\(n\\) features. closer two points, similar .However, Euclidean distance always appropriate. categorical variables, alternative strategies one-hot encoding transform categories numerical values, enabling distance-based clustering. Additionally, features often require scaling (e.g., min-max normalization) ensure single variable dominates clustering process.Clustering often compared classification, serve different purposes. Classification assigns predefined labels new data points based past examples, whereas clustering discovers groupings raw data. Classification typically used prediction, clustering primarily exploration pattern discovery. clustering generates labels rather predicting existing ones, sometimes referred unsupervised classification. cluster assignments can used inputs analysis, refining predictions neural network decision tree model.clustering algorithms aim achieve high intra-cluster similarity (data points within cluster close together) low inter-cluster similarity (clusters well separated). concept visually illustrated Figure 13.1, effective clusters minimize internal variation maximizing separation groups.\nFigure 13.1: Clustering algorithms aim minimize intra-cluster variation maximizing inter-cluster separation.\nBeyond role data exploration, clustering widely used preprocessing step machine learning. Given massive scale modern datasets, clustering helps reduce complexity identifying smaller number representative groups, leading several benefits:Reduced computation time downstream models.Improved interpretability summarizing large datasets.Enhanced predictive performance structuring inputs supervised learning.following sections, explore K-means clustering, one widely used clustering algorithms. also discuss methods selecting optimal number clusters apply clustering real-world dataset.","code":""},{"path":"chapter-cluster.html","id":"kmeans","chapter":"13 Clustering","heading":"13.2 K-means Clustering","text":"K-means clustering one simplest widely used clustering algorithms. aims partition dataset \\(k\\) clusters iteratively refining cluster centers, ensuring data points within cluster similar possible. algorithm operates iterative process assigning points clusters updating cluster centers based assignments. process stops assignments stabilize, meaning data points switch clusters.K-means algorithm requires user specify number clusters, \\(k\\), advance. follows steps:Initialize: Randomly select \\(k\\) data points initial cluster centers.Assignment: Assign data point nearest cluster center. creates \\(k\\) groups.Update: Compute centroid (mean) cluster move cluster centers new locations.Repeat: Iterate steps 2 3 convergence—cluster assignments longer change.Although K-means simple efficient, limitations. final clusters depend heavily initial choice cluster centers, meaning different runs algorithm may produce different results. Additionally, K-means sensitive outliers assumes clusters spherical similar size, may always case real-world data.illustrate K-means works, consider dataset 50 records two features, \\(x_1\\) \\(x_2\\), shown Figure 13.2. goal partition data three clusters.\nFigure 13.2: simple dataset 50 records two features, ready clustering.\nfirst step randomly select three initial cluster centers (red stars), shown left panel Figure 13.3. data point assigned nearest cluster, forming three groups labeled blue (Cluster ), green (Cluster B), orange (Cluster C). right panel Figure 13.3 displays initial assignments. dashed lines represent Voronoi diagram, divides space regions associated cluster center.\nFigure 13.3: Initial random cluster centers (left) first cluster assignments (right).\nSince K-means sensitive initialization, poor placement initial cluster centers can lead suboptimal clustering. mitigate issue, K-means++13 introduced 2007. method strategically selects initial centers improve convergence reduce randomness.initial cluster assignments made, K-means enters update phase. first step recompute centroid cluster, mean position points assigned cluster. cluster centers moved new centroid locations, shown left panel Figure 13.4. right panel illustrates Voronoi boundaries shift, causing data points reassigned different cluster.\nFigure 13.4: Updated cluster centers (left) new assignments centroid adjustment (right).\nprocess—reassigning points updating centroids—continues iteratively. another update, points switch clusters , leading refined Voronoi partition, shown Figure 13.5.\nFigure 13.5: Updated cluster centers assignments another iteration.\nalgorithm continues iterating cluster assignments stabilize—points switch clusters, shown Figure 13.6. point, algorithm converges, final clusters established.\nFigure 13.6: Final cluster assignments K-means convergence.\nclustering complete, results can presented two ways:Cluster Assignments: data point labeled belonging Cluster , B, C.Centroid Coordinates: final positions cluster centers can reported.final cluster centroids act representative points, summarizing dataset enabling analysis. K-means clustering widely used applications customer segmentation, image compression, document clustering. next section, explore methods selecting optimal number clusters ensure meaningful partitions real-world datasets.","code":""},{"path":"chapter-cluster.html","id":"kmeans-choose","chapter":"13 Clustering","heading":"13.3 Choosing the Number of Clusters","text":"One key challenges K-means clustering selecting appropriate number clusters, \\(k\\). choice \\(k\\) significantly impacts results—clusters may fail capture meaningful structures, many clusters risk overfitting creating overly fragmented groups. Unlike supervised learning, evaluation metrics like accuracy guide model selection, clustering absolute ground truth, making selection \\(k\\) subjective.cases, domain knowledge can provide useful guidance. example, clustering movies, reasonable starting point might number well-known genres. business setting, marketing teams may set \\(k = 3\\) plan design three distinct advertising campaigns. Similarly, seating arrangements conference might determine number groups based available tables. However, clear intuition exists, data-driven methods needed determine optimal \\(k\\).One widely used technique choosing \\(k\\) elbow method, evaluates within-cluster variation changes number clusters increases. clusters added, clusters become homogeneous (internal similarity increases), overall heterogeneity (difference clusters) decreases. However, improvement follows diminishing returns pattern. idea find point adding another cluster longer significantly reduces within-cluster variance.critical point, known elbow point, represents natural number clusters. concept illustrated Figure 13.7, curve shows total within-cluster sum squares (WCSS) function \\(k\\). “elbow” curve—rate improvement slows—strong candidate \\(k\\).\nFigure 13.7: elbow method helps determine optimal number clusters K-means clustering.\nelbow method provides useful heuristic, limitations. datasets, curve may exhibit clear elbow, making choice \\(k\\) ambiguous. Additionally, evaluating many different values \\(k\\) can computationally expensive, especially large datasets.techniques can supplement refine selection \\(k\\):Silhouette Score: Measures well point fits within assigned cluster compared others. higher silhouette score suggests well-defined clustering structure.Gap Statistic: Compares clustering result reference distribution assess whether structure significant.Cross-validation clustering tasks: applications clustering feeds downstream task (e.g., classification), impact different \\(k\\) values can evaluated context.Ultimately, choice \\(k\\) driven data characteristics practical considerations. Clustering often used exploratory analysis, meaning useful \\(k\\) necessarily mathematically “optimal” one rather one yields meaningful, interpretable insights.Observing cluster characteristics evolve \\(k\\) varies can informative. groups may remain stable across different \\(k\\) values, indicating strong natural boundaries, others may appear disappear, suggesting fluid structures data.Rather aiming perfect cluster count, often sufficient find reasonable interpretable clustering solution. next section, apply clustering real-world dataset, demonstrating practical knowledge can guide choice \\(k\\) actionable insights.Now explored K-means clustering methods selecting optimal number clusters, apply concepts real-world dataset.","code":""},{"path":"chapter-cluster.html","id":"kmeans-cereal","chapter":"13 Clustering","heading":"13.4 Case Study: Clustering Cereal Data","text":"case study, apply K-means clustering cereal dataset liver package. dataset contains nutritional information 77 cereal brands, including calories, protein, fat, sodium, fiber, sugar content. Understanding nutritional profiles valuable marketing strategies, consumer targeting, product positioning. goal segment cereals distinct groups based nutritional similarities.","code":""},{"path":"chapter-cluster.html","id":"dataset-overview","chapter":"13 Clustering","heading":"13.4.1 Dataset Overview","text":"cereal dataset includes 77 observations 16 variables, covering various nutritional attributes. can accessed liver package, shown :can examine structure using:dataset contains following variables:name: Name cereal (categorical).manuf: Manufacturer cereal (categorical).type: Cereal type (hot cold, categorical).calories: Calories per serving (numerical).protein: Grams protein per serving (numerical).fat: Grams fat per serving (numerical).sodium: Milligrams sodium per serving (numerical).fiber: Grams dietary fiber per serving (numerical).carbo: Grams carbohydrates per serving (numerical).sugars: Grams sugar per serving (numerical).potass: Milligrams potassium per serving (numerical).vitamins: Percentage FDA-recommended vitamins (categorical: 0, 25, 100).shelf: Display shelf position (categorical: 1, 2, 3).weight: Weight one serving ounces (numerical).cups: Number cups per serving (numerical).rating: Cereal rating score (numerical).","code":"\nlibrary(liver)  # Load the liver package\n\ndata(cereal)    # Load the cereal datasetstr(cereal)\n   'data.frame':    77 obs. of  16 variables:\n    $ name    : Factor w/ 77 levels \"100% Bran\",\"100% Natural Bran\",..: 1 2 3 4 5 6 7 8 9 10 ...\n    $ manuf   : Factor w/ 7 levels \"A\",\"G\",\"K\",\"N\",..: 4 6 3 3 7 2 3 2 7 5 ...\n    $ type    : Factor w/ 2 levels \"cold\",\"hot\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ calories: int  70 120 70 50 110 110 110 130 90 90 ...\n    $ protein : int  4 3 4 4 2 2 2 3 2 3 ...\n    $ fat     : int  1 5 1 0 2 2 0 2 1 0 ...\n    $ sodium  : int  130 15 260 140 200 180 125 210 200 210 ...\n    $ fiber   : num  10 2 9 14 1 1.5 1 2 4 5 ...\n    $ carbo   : num  5 8 7 8 14 10.5 11 18 15 13 ...\n    $ sugars  : int  6 8 5 0 8 10 14 8 6 5 ...\n    $ potass  : int  280 135 320 330 -1 70 30 100 125 190 ...\n    $ vitamins: int  25 0 25 25 25 25 25 25 25 25 ...\n    $ shelf   : int  3 3 3 3 3 1 2 3 1 3 ...\n    $ weight  : num  1 1 1 1 1 1 1 1.33 1 1 ...\n    $ cups    : num  0.33 1 0.33 0.5 0.75 0.75 1 0.75 0.67 0.67 ...\n    $ rating  : num  68.4 34 59.4 93.7 34.4 ..."},{"path":"chapter-cluster.html","id":"data-preprocessing","chapter":"13 Clustering","heading":"13.4.2 Data Preprocessing","text":"applying K-means clustering, need clean preprocess data. start summarizing dataset:Upon inspection, notice unusual values variables sugars, carbo, potass, entries set -1. Since negative values invalid nutritional attributes, replace NA:Next, handle missing values using K-nearest neighbors (KNN) imputation knnImputation() function DMwR2 package:clustering, exclude categorical identifier variables (name, manuf, rating), retaining numerical features:Since dataset includes features different scales, apply min-max scaling using minmax() function liver package ensure variables contribute equally clustering process:visualize effect normalization, plot sodium distribution scaling:scaling, values fall within 0–1 range, making distance-based clustering reliable.","code":"summary(cereal)\n                           name    manuf    type       calories    \n    100% Bran                : 1   A: 1   cold:74   Min.   : 50.0  \n    100% Natural Bran        : 1   G:22   hot : 3   1st Qu.:100.0  \n    All-Bran                 : 1   K:23             Median :110.0  \n    All-Bran with Extra Fiber: 1   N: 6             Mean   :106.9  \n    Almond Delight           : 1   P: 9             3rd Qu.:110.0  \n    Apple Cinnamon Cheerios  : 1   Q: 8             Max.   :160.0  \n    (Other)                  :71   R: 8                            \n       protein           fat            sodium          fiber       \n    Min.   :1.000   Min.   :0.000   Min.   :  0.0   Min.   : 0.000  \n    1st Qu.:2.000   1st Qu.:0.000   1st Qu.:130.0   1st Qu.: 1.000  \n    Median :3.000   Median :1.000   Median :180.0   Median : 2.000  \n    Mean   :2.545   Mean   :1.013   Mean   :159.7   Mean   : 2.152  \n    3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:210.0   3rd Qu.: 3.000  \n    Max.   :6.000   Max.   :5.000   Max.   :320.0   Max.   :14.000  \n                                                                    \n        carbo          sugars           potass          vitamins     \n    Min.   :-1.0   Min.   :-1.000   Min.   : -1.00   Min.   :  0.00  \n    1st Qu.:12.0   1st Qu.: 3.000   1st Qu.: 40.00   1st Qu.: 25.00  \n    Median :14.0   Median : 7.000   Median : 90.00   Median : 25.00  \n    Mean   :14.6   Mean   : 6.922   Mean   : 96.08   Mean   : 28.25  \n    3rd Qu.:17.0   3rd Qu.:11.000   3rd Qu.:120.00   3rd Qu.: 25.00  \n    Max.   :23.0   Max.   :15.000   Max.   :330.00   Max.   :100.00  \n                                                                     \n        shelf           weight          cups           rating     \n    Min.   :1.000   Min.   :0.50   Min.   :0.250   Min.   :18.04  \n    1st Qu.:1.000   1st Qu.:1.00   1st Qu.:0.670   1st Qu.:33.17  \n    Median :2.000   Median :1.00   Median :0.750   Median :40.40  \n    Mean   :2.208   Mean   :1.03   Mean   :0.821   Mean   :42.67  \n    3rd Qu.:3.000   3rd Qu.:1.00   3rd Qu.:1.000   3rd Qu.:50.83  \n    Max.   :3.000   Max.   :1.50   Max.   :1.500   Max.   :93.70  \n   cereal[cereal == -1] <- NA\nfind.na(cereal)  # Check missing values\n        row col\n   [1,]  58   9\n   [2,]  58  10\n   [3,]   5  11\n   [4,]  21  11library(DMwR2)\ncereal <- knnImputation(cereal, k = 3, scale = TRUE)\nfind.na(cereal)  # Verify missing values are filled\n   [1] \" No missing values (NA) in the dataset.\"\nselected_variables <- colnames(cereal)[-c(1, 2, 16)]\ncereal_subset <- cereal[, selected_variables]cereal_mm <- minmax(cereal_subset, col = \"all\")\nstr(cereal_mm)  # Check the transformed dataset\n   'data.frame':    77 obs. of  13 variables:\n    $ type    : num  0 0 0 0 0 0 0 0 0 0 ...\n    $ calories: num  0.182 0.636 0.182 0 0.545 ...\n    $ protein : num  0.6 0.4 0.6 0.6 0.2 0.2 0.2 0.4 0.2 0.4 ...\n    $ fat     : num  0.2 1 0.2 0 0.4 0.4 0 0.4 0.2 0 ...\n    $ sodium  : num  0.4062 0.0469 0.8125 0.4375 0.625 ...\n    $ fiber   : num  0.7143 0.1429 0.6429 1 0.0714 ...\n    $ carbo   : num  0 0.167 0.111 0.167 0.5 ...\n    $ sugars  : num  0.4 0.533 0.333 0 0.533 ...\n    $ potass  : num  0.841 0.381 0.968 1 0.122 ...\n    $ vitamins: num  0.25 0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ...\n    $ shelf   : num  1 1 1 1 1 0 0.5 1 0 1 ...\n    $ weight  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.83 0.5 0.5 ...\n    $ cups    : num  0.064 0.6 0.064 0.2 0.4 0.4 0.6 0.4 0.336 0.336 ...\nggplot(data = cereal) +\n    geom_histogram(aes(x = sodium), color = \"blue\", fill = \"lightblue\") +\n    theme_minimal() + ggtitle(\"Before min-max normalization\")\n\nggplot(data = cereal_mm) +\n    geom_histogram(aes(x = sodium), color = \"blue\", fill = \"lightblue\") + \n    theme_minimal() + ggtitle(\"After min-max normalization\")"},{"path":"chapter-cluster.html","id":"applying-k-means-clustering","chapter":"13 Clustering","heading":"13.4.3 Applying K-means Clustering","text":"","code":""},{"path":"chapter-cluster.html","id":"choosing-the-optimal-number-of-clusters","chapter":"13 Clustering","heading":"Choosing the Optimal Number of Clusters","text":"clustering, need determine optimal number clusters. use elbow method, plots within-cluster sum squares (WCSS) different values \\(k\\). elbow point—improvement WCSS slows—suggests ideal \\(k\\):plot, observe \\(k = 4\\) clusters reasonable choice, adding clusters beyond point yields diminishing improvements WCSS.","code":"\nlibrary(factoextra)\n\nfviz_nbclust(cereal_mm, kmeans, method = \"wss\", k.max = 15) + \n  geom_vline(xintercept = 4, linetype = 2, color = \"gray\")"},{"path":"chapter-cluster.html","id":"performing-k-means-clustering","chapter":"13 Clustering","heading":"Performing K-means Clustering","text":"now apply K-means algorithm \\(k = 4\\) clusters:check cluster sizes:","code":"\nset.seed(3)  # Ensure reproducibility\ncereal_kmeans <- kmeans(cereal_mm, centers = 4)cereal_kmeans$size\n   [1] 36 10 13 18"},{"path":"chapter-cluster.html","id":"visualizing-the-clusters","chapter":"13 Clustering","heading":"Visualizing the Clusters","text":"better understand clustering results, visualize clusters using fviz_cluster() function factoextra package:scatter plot displays four clusters, point representing cereal brand. Different colors indicate distinct clusters, ellipses represent spread cluster based standard deviation.","code":"\nfviz_cluster(cereal_kmeans, cereal_mm, geom = \"point\", ellipse.type = \"norm\", palette = \"custom_palette\")"},{"path":"chapter-cluster.html","id":"interpreting-the-results","chapter":"13 Clustering","heading":"Interpreting the Results","text":"clusters reveal natural groupings among cereals based nutritional content. example:\n- clusters may contain low-sugar, high-fiber cereals, appealing health-conscious consumers.\n- Others may group high-calorie, high-sugar cereals, often marketed children.\n- Another group may include balanced cereals, offering mix moderate calories nutrients.examine cereals belong specific cluster (e.g., Cluster 1), can use:command lists names cereals assigned Cluster 1, helping us interpret characteristics group.case study demonstrated K-means clustering can segment cereals meaningful groups based nutritional content. data preprocessing, feature scaling, cluster visualization, successfully grouped cereals similar characteristics. clustering techniques widely applicable marketing, consumer analytics, product positioning, providing actionable insights businesses researchers alike.chapter, explored fundamentals clustering, mechanics K-means algorithm, methods choosing optimal number clusters. applied concepts real-world dataset, demonstrating K-means can extract meaningful insights. Clustering remains powerful tool across various domains, marketing bioinformatics, making essential technique modern data science toolkit.","code":"\ncereal$name[cereal_kmeans$cluster == 1]"},{"path":"chapter-cluster.html","id":"exercises-9","chapter":"13 Clustering","heading":"Exercises","text":"exercises reinforce concepts introduced chapter, focusing clustering fundamentals, hyperparameter tuning, practical applications using redWines dataset. exercises divided two categories:Conceptual questions – Understanding theory behind clustering K-means.Practical exercises using redWines dataset – Applying clustering techniques real-world data.","code":""},{"path":"chapter-cluster.html","id":"conceptual-questions-11","chapter":"13 Clustering","heading":"Conceptual questions","text":"clustering, differ classification?Explain concept similarity measures clustering. commonly used distance metric numerical data?clustering considered unsupervised learning method?real-world applications clustering? Name least three.Define terms intra-cluster similarity inter-cluster separation. important clustering?K-means clustering determine data points belong cluster?Explain role centroids K-means clustering.happens number clusters \\(k\\) K-means chosen small? large?elbow method, help determine optimal number clusters?K-means sensitive initial selection cluster centers? K-means++ address issue?Describe scenario Euclidean distance might appropriate similarity measure clustering.need normalize scale variables applying K-means clustering?clustering help dimensionality reduction preprocessing supervised learning?key assumptions K-means clustering?silhouette score help evaluate quality clustering?Compare K-means hierarchical clustering. advantages disadvantages ?K-means suitable non-spherical clusters?difference hard clustering (e.g., K-means) soft clustering (e.g., Gaussian Mixture Models)?outliers, affect K-means clustering?alternative clustering methods handle outliers better K-means?","code":""},{"path":"chapter-cluster.html","id":"practical-exercises-using-the-redwines-dataset-1","chapter":"13 Clustering","heading":"Practical exercises using the redWines dataset","text":"redWines dataset contains chemical properties red wines quality scores. exercises guide clustering analysis, data preprocessing model evaluation.","code":""},{"path":"chapter-cluster.html","id":"data-preparation-and-exploratory-analysis","chapter":"13 Clustering","heading":"Data preparation and exploratory analysis","text":"Load redWines dataset liver package inspect structure.Summarize dataset using summary(). Identify missing values.Check distribution wine quality scores dataset. common wine quality score?Since clustering requires numerical features, remove non-numeric columns dataset.Apply min-max scaling normalize numerical variables clustering. step necessary?","code":"\nlibrary(liver)\ndata(redWines)\nstr(redWines)"},{"path":"chapter-cluster.html","id":"applying-k-means-clustering-1","chapter":"13 Clustering","heading":"Applying K-means clustering","text":"Use elbow method determine optimal number clusters dataset.Based elbow plot, choose appropriate value \\(k\\) perform K-means clustering.Visualize clusters using scatter plot two numerical features.Compute silhouette score evaluate cluster cohesion separation.Identify centroids final clusters interpret meaning.","code":"\nlibrary(factoextra)\nfviz_nbclust(redWines, kmeans, method = \"wss\")"},{"path":"chapter-cluster.html","id":"interpreting-the-clusters","chapter":"13 Clustering","heading":"Interpreting the clusters","text":"Assign cluster labels original dataset examine average chemical composition cluster.Compare wine quality scores across clusters. clusters contain higher-quality wines others?Identify features contribute defining clusters.certain wine types (e.g., high acidity, high alcohol content) concentrated specific clusters?Experiment different values \\(k\\) compare clustering results. increasing decreasing \\(k\\) improve clustering?Visualize wine acidity alcohol content influence cluster formation.","code":""}]
