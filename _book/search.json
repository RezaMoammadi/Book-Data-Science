[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Data science transforming way solve problems, make decisions, uncover insights data. Whether ’re beginner experienced professional, Uncovering Data Science R provides intuitive practical introduction exciting field—prior analytics programming experience required.book work progress, welcome feedback readers. comments, suggestions, corrections, please feel free contact us Contact Us.","code":""},{"path":"index.html","id":"why-this-book","chapter":"Preface","heading":"Why This Book?","text":"Data science rapidly evolving field leverages computational tools techniques transform raw data actionable insights. book, introduce fundamental skills needed work R, powerful freely available statistical programming language widely used data analysis, visualization, machine learning.Unlike many books data science, focus accessibility. aim provide intuitive practical introduction, making R data science concepts understandable little technical background. hands-approach ensures learn theoretical concepts also gain experience applying real-world datasets.Compared commercial software like SAS SPSS, R provides free, open-source, highly extensible platform statistical computing machine learning. rich ecosystem packages makes excellent alternative proprietary data mining tools.Inspired Free Open Source Software (FOSS) movement, content book open transparent, ensuring reproducibility. code, datasets, materials hosted CRAN accessible via liver package (https://CRAN.R-project.org/package=liver), allowing readers engage book interactively.","code":""},{"path":"index.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who Should Read This Book?","text":"book anyone interested learning data science, particularly new field. designed :Business professionals want leverage data decision-making,Students researchers looking apply data analysis work,Beginners prior programming experience,Anyone interested data science machine learning using R.","code":""},{"path":"index.html","id":"what-you-will-learn","chapter":"Preface","heading":"What You Will Learn","text":"primary goal book introduce data science concepts using R tool data analysis machine learning. R open-source language environment statistical computing graphics, offering vast collection packages data mining, visualization, modeling.hands-examples real-world datasets, learn:basics R set environment,core principles data science Data Science Methodology,clean, transform, explore data,fundamentals statistical analysis, machine learning, data visualization,build evaluate machine learning models, including classification, regression, clustering, neural networks,apply techniques real-world datasets.","code":""},{"path":"index.html","id":"the-data-science-process","chapter":"Preface","heading":"The Data Science Process","text":"Data science follows iterative structured methodology analyzing extracting insights data. book follows framework:Problem Understanding – Defining objective understanding data.Data Preparation – Preparing raw data analysis.Exploratory Data Analysis (EDA) – Identifying patterns relationships data.Preparing Data Modeling – Transforming data machine learning models.Modeling – Building predictive models using machine learning algorithms.Evaluation – Assessing model performance using various metrics.Deployment – Applying trained model real-world scenarios.end book, solid understanding phases able apply effectively.","code":""},{"path":"index.html","id":"how-this-book-is-structured","chapter":"Preface","heading":"How This Book Is Structured","text":"book structured hands-guide, designed take beginner practitioner R data science. chapters follow logical progression, starting foundational concepts gradually introducing advanced techniques.use real-world datasets (see Table 0.1) throughout book illustrate key concepts. datasets available liver package can accessed easily. brief overview book’s chapters:Chapter 1 – Introduction R, including installation basic operations.Chapter 2 – Introduction Data Science methodology.Chapter 3 – Data preparation techniques.Chapter 4 – Exploratory Data Analysis (EDA) using visualization summary statistics.Chapter 5 – Basics statistical analysis, including descriptive statistics hypothesis testing.Chapter 6 – Overview machine learning models.Chapter 7 – k-Nearest Neighbors (k-NN) algorithm.Chapter 8 – Model evaluation metrics techniques.Chapter 9 – Naïve Bayes classifier probabilistic modeling.Chapter 10 – Linear regression predictive modeling.Chapter 11 – Decision trees Random Forests.Chapter 12 – Neural networks deep learning basics.Chapter 13 – Clustering techniques, including k-means.end chapter, find practical exercises labs reinforce learning. exercises use real-world datasets provide step--step guidance ensure hands-experience.","code":""},{"path":"index.html","id":"how-to-use-this-book","chapter":"Preface","heading":"How to Use This Book","text":"book designed self-study classroom use. can read cover cover jump chapters interest . chapter builds previous ones, beginners encouraged follow sequence smooth learning experience.get book:Run code examples – code snippets designed executed interactively R.Complete exercises – Practical exercises reinforce key concepts improve problem-solving skills.Modify experiment – Try changing code explore different scenarios.Use reference – ’re familiar basics, use book guide working real-world data.book also used data science courses University Amsterdam. can serve textbook similar courses supplementary resource advanced analytics training.","code":""},{"path":"index.html","id":"datasets-used-in-this-book","chapter":"Preface","heading":"Datasets Used in This Book","text":"Table 0.1 lists datasets used book. real-world datasets used illustrate key concepts available liver package, can downloaded CRAN.\nTable 0.1: List datasets used case studies different chapters. Available R package liver.\n","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"prior programming experience required, basic understanding numbers logic helpful. run code book, need install R, RStudio, several R packages.","code":""},{"path":"chapter-into-R.html","id":"chapter-into-R","chapter":"1 The Basics for R","heading":"1 The Basics for R","text":"can analyze data, need way communicate computer. ’s programming languages like R Python come . Many data science teams use mix languages, R great starting point designed specifically data analysis statistical computing.","code":""},{"path":"chapter-into-R.html","id":"why-choose-r-for-data-science","chapter":"1 The Basics for R","heading":"Why Choose R for Data Science?","text":"R widely used statistics, data analysis, visualization due rich ecosystem libraries tools tailored data science. Unlike general-purpose programming languages, R built statistical analysis, allowing data scientists perform everything basic calculations advanced machine learning just lines code.Python another popular language data science, R particularly well-suited :\n- Statistical Computing – R built-statistical functions methods hypothesis testing, regression modeling, machine learning.\n- Data Visualization – Packages like ggplot2 provide powerful tools creating high-quality plots graphs minimal effort.\n- Reproducible Research – R Markdown Shiny make easy generate reports interactive dashboards directly R code.\n- Bioinformatics & Finance – Many researchers analysts fields use R due robust statistical libraries domain-specific packages.Beyond capabilities, R :Free & Open Source – Available everyone, vibrant community contributors.Cross-Platform – Runs Windows, macOS, Linux.Flexible & Powerful – Supports interactive data exploration, visualization, machine learning.R language, RStudio tool makes working R easier. RStudio integrated development environment (IDE) provides:console running R commands,script editor syntax highlighting auto-completion,Built-tools data visualization, debugging, package management.chapter, learn fundamental skills needed work R, installation running first commands. Let’s begin! 🚀","code":""},{"path":"chapter-into-R.html","id":"how-to-install-r","chapter":"1 The Basics for R","heading":"1.1 How to Install R","text":"get started R, first need install computer. Follow steps:Go CRAN website – Comprehensive R Archive Network.Select operating system – Click link Windows, macOS, Linux.Download install R – Follow -screen instructions complete installation.","code":""},{"path":"chapter-into-R.html","id":"keeping-r-up-to-date","chapter":"1 The Basics for R","heading":"Keeping R Up to Date","text":"R receives major update year, along 2-3 minor updates annually. updating R—especially major versions—requires reinstalling packages, staying date ensures :✅ Access latest features improvements,\n✅ Maintain compatibility new packages,\n✅ Benefit security patches performance enhancements.Keeping R updated might feel like hassle, postponing updates can make process cumbersome later. ’s best update regularly ensure smooth performance compatibility.","code":""},{"path":"chapter-into-R.html","id":"how-to-install-rstudio","chapter":"1 The Basics for R","heading":"1.2 How to Install RStudio","text":"RStudio open-source integrated development environment (IDE) makes working R easier, interactive, efficient. provides user-friendly interface, advanced script editor, various tools plotting, debugging, workspace management—significantly enhance R programming experience.","code":""},{"path":"chapter-into-R.html","id":"installing-rstudio","chapter":"1 The Basics for R","heading":"Installing RStudio","text":"Follow steps install RStudio:Go RStudio website.Download latest version RStudio Desktop (free, open-source edition).Run installer follow -screen instructions.Launch RStudio, ’re ready start coding R!RStudio updated several times year, notify new version available. Keeping RStudio date recommended take advantage new features performance improvements.","code":""},{"path":"chapter-into-R.html","id":"exploring-the-rstudio-interface","chapter":"1 The Basics for R","heading":"Exploring the RStudio Interface","text":"open RStudio, see window similar Figure 1.1.\nFigure 1.1: RStudio window first launch program.\nsee three panels, add fourth selecting File > New File > R Script. opens script editor can write save R code. ’s quick overview RStudio’s panels:Top-left: Script Editor – Write save R code.Bottom-left: Console – Run R commands see output.Top-right: Environment & History – View variables, datasets, past commands.Bottom-right: Plots, Help, & Files – Display graphs, access documentation, manage files.now, just know can type R code console press Enter run . progress book, ’ll become familiar RStudio’s features learn efficiently write, run, debug R code.","code":""},{"path":"chapter-into-R.html","id":"customizing-rstudio","chapter":"1 The Basics for R","heading":"Customizing RStudio","text":"RStudio highly customizable, allowing tailor workflow. adjust settings, go :Tools > Global Options – Access general settings.Appearance > Editor Theme – Change editor’s theme (e.g., “Tomorrow Night 80” dark mode).Font & Layout Settings – Modify font size, panel positions, interface options.\ncomfortable coding environment enhances productivity—feel free explore tweak settings suit preferences!","code":""},{"path":"chapter-into-R.html","id":"how-to-learn-r","chapter":"1 The Basics for R","heading":"1.3 How to Learn R","text":"Learning R exciting rewarding journey opens doors data science, statistics, machine learning. Fortunately, numerous resources—books, online courses, tutorials, forums—can help get started advance skills.","code":""},{"path":"chapter-into-R.html","id":"video-tutorials","chapter":"1 The Basics for R","heading":"1. Video Tutorials","text":"prefer learning watching, YouTube offers wealth R tutorials, ranging beginner advanced levels:R Programming – Covers R basics data science concepts.Data School – Focuses data analysis, machine learning, practical R applications.","code":""},{"path":"chapter-into-R.html","id":"books","chapter":"1 The Basics for R","heading":"2. Books","text":"Books great way build deep understanding R. top recommendations:Absolute Beginners: Hands-Programming R Garrett Grolemund1 – practical introduction new programming.Data Science R: R Data Science Hadley Wickham Garrett Grolemund2 – Covers data visualization, wrangling, modeling.Machine Learning: Machine Learning R Brett Lantz3 – comprehensive guide machine learning techniques using R.","code":""},{"path":"chapter-into-R.html","id":"online-courses","chapter":"1 The Basics for R","heading":"3. Online Courses","text":"prefer structured learning hands-exercises, online courses offer interactive experiences:DataCamp – Features beginner-friendly courses like Introduction R.Coursera – Offers courses R Programming Data Science Specialization.","code":""},{"path":"chapter-into-R.html","id":"r-communities-forums","chapter":"1 The Basics for R","heading":"4. R Communities & Forums","text":"Engaging online communities great way learn others, ask questions, get support:Stack Overflow – Find answers R-related coding questions.RStudio Community – Connect R users participate discussions.","code":""},{"path":"chapter-into-R.html","id":"practice-regularly","chapter":"1 The Basics for R","heading":"5. Practice Regularly","text":"best way learn R consistent practice. Start simple exercises, explore real-world datasets, experiment R code. combining structured learning hands-experience, ’ll quickly develop confidence proficiency R.🚀 Start today! Choose one resources begin R learning journey.","code":""},{"path":"chapter-into-R.html","id":"getting-help-and-learning-more","chapter":"1 The Basics for R","heading":"1.4 Getting Help and Learning More","text":"begin journey R, ’ll likely encounter challenges questions along way. Fortunately, many resources available help troubleshoot problems, deepen understanding, continue learning. Whether ’re stuck error message, exploring new function, looking best practices, combination built-documentation, online communities, external learning materials can guide .R comes extensive built-documentation provides details functions, packages, programming techniques. quickly look function, type ? followed function name R console. bring official documentation, including usage examples, argument details, additional references. can also use help() example() get context function works.Beyond R’s internal help system, R community invaluable resource. question, chances someone already asked (answered) . Platforms like Stack Overflow, RStudio Community, R-help mailing list contain thousands discussions common advanced topics R programming, data science, machine learning. Searching forums can often lead quick reliable solutions. don’t find existing answer, posting question clear explanation reproducible example increase chances getting helpful responses.simple Google search often fastest way troubleshoot issues. Searching error message function name usually direct blog posts, documentation, forum discussions relevant explanations. Additionally, AI tools like ChatGPT can assist R programming questions, debugging, conceptual explanations. AI-generated solutions aren’t always perfect, can provide useful insights, suggest alternative approaches, help clarify difficult concepts.Ultimately, best way master R hands-experience. Don’t afraid experiment—write code, test different functions, explore new datasets. Mistakes natural part learning, one helps reinforce understanding. practice, confident proficient ’ll become R. Keep coding, keep exploring, enjoy journey!","code":""},{"path":"chapter-into-R.html","id":"data-science-with-r","chapter":"1 The Basics for R","heading":"1.5 Data Science with R","text":"R provides strong foundation data science, real power comes extensive ecosystem packages—collections functions, datasets, documentation extend R’s capabilities. base version R includes many essential tools, come preloaded statistical machine learning algorithms may need. Instead, algorithms developed shared large community researchers practitioners free open-source R packages.package modular, reusable library enhances R’s functionality. Packages include well-documented functions, usage instructions, often sample datasets testing learning. book, frequently use liver package, developed specifically accompany book. contains datasets functions designed illustrate key data science concepts techniques. Additionally, machine learning algorithm covered book, introduce use appropriate R packages implement methods.interested exploring , Comprehensive R Archive Network (CRAN) hosts thousands packages statistical computing, data visualization, machine learning. full list available packages can browsed CRAN website, providing access tools tailored various domains data science beyond.","code":""},{"path":"chapter-into-R.html","id":"install-packages","chapter":"1 The Basics for R","heading":"1.6 How to Install R Packages","text":"two ways install R packages. first method RStudio’s graphical interface. Click “Tools” tab select “Install Packages…”. dialog box appears, enter name package(s) wish install “Packages” field click “Install” button. Make sure check “Install dependencies” option ensure necessary supporting packages installed well. See Figure 1.2 visual guide.\nFigure 1.2: visual guide installing R packages using ‘Tools’ tab RStudio.\nsecond method install packages directly using install.packages() function. example, install liver package, provides datasets functions used throughout book, enter following command R console:Press “Enter” execute command. R connect CRAN download package correct format operating system. encounter issues installation, ensure connected internet proxy firewall blocking access CRAN. first time install package, R may ask select CRAN mirror. Choose one geographically close faster downloads.install.packages() function also allows customization, installing package local file specific repository. learn , type following command R console:Packages need installed . installation, must loaded new R session using library() function. cover load packages next section.","code":"\ninstall.packages(\"liver\")\n?install.packages()"},{"path":"chapter-into-R.html","id":"how-to-load-r-packages","chapter":"1 The Basics for R","heading":"1.7 How to Load R Packages","text":"optimize memory usage, R automatically load installed packages. Instead, must explicitly load necessary packages new R session. ensures relevant functions datasets available, minimizing resource consumption.\nload package, use library() require() function. functions locate package system make functions, datasets, documentation accessible. example, load liver package, enter following command R console:Press Enter execute command. error message appears stating package found (e.g., \"package called 'liver'\"), indicates package installed. cases, refer previous section installing packages.Beyond liver, book utilizes several R packages, introduced progressively throughout chapters needed. However, R packages contain functions identical names. instance, liver* dplyr** packages include select() function. multiple packages loaded, R defaults using function recently loaded package.explicitly specify package function sourced , use :: operator. ensures clarity prevents conflicts. example, use select() function liver package, enter:approach particularly useful complex projects multiple packages required, preventing unintended overwrites functions name.","code":"\nlibrary(liver)\nliver::select()"},{"path":"chapter-into-R.html","id":"running-r-code","chapter":"1 The Basics for R","heading":"1.8 Running R Code","text":"R interactive language, allowing type commands directly console see results immediately. example, can perform basic arithmetic operations addition, subtraction, multiplication, division. add two numbers, type following R console:Press Enter execute command. R compute sum display result. can also store result variable later use:, <- assignment operator R, used assign values variables. users prefer = operator (result = 2 + 3), also works cases, <- remains recommended convention R programming.Variables R store values later use, allowing perform calculations efficiently. example, can multiply result 4:R retrieve stored value result compute multiplication.","code":"2 + 3\n   [1] 5\nresult <- 2 + 3result * 4\n   [1] 20"},{"path":"chapter-into-R.html","id":"using-comments-in-r","chapter":"1 The Basics for R","heading":"Using Comments in R","text":"Comments used explain code make easier understand. R, comment starts #, everything following line ignored interpreter.Comments affect execution code essential documentation, especially working complex projects collaborating others.","code":"\n# Store the sum of 2 and 3 in the variable `result`\nresult <- 2 + 3"},{"path":"chapter-into-R.html","id":"functions-in-r","chapter":"1 The Basics for R","heading":"1.8.1 Functions in R","text":"R provides rich set built-functions perform specific tasks. function takes input(s) (arguments), processes , returns output. example, c() function creates vectors:can apply functions vector. example, compute average numbers x, use mean() function:Functions R follow simple structure:functions require arguments, others optional. learn function, use ? followed function name:open R’s help documentation, providing details function’s purpose, usage, arguments, examples.Functions essential R programming, helping simplify complex operations making code reusable efficient. progress, also learn write functions automate tasks improve workflow.","code":"\nx <- c(1, 2, 3, 4, 5)  # Create a vectormean(x)  # Calculate the mean of x\n   [1] 3\nfunction_name(arguments)\n?mean  # or help(mean)"},{"path":"chapter-into-R.html","id":"how-to-import-data-into-r","chapter":"1 The Basics for R","heading":"1.9 How to Import Data into R","text":"performing analysis, first need load data R. R can read data multiple sources, including text files, Excel files, online datasets. Depending file format data source, can choose several methods importing data R.","code":""},{"path":"chapter-into-R.html","id":"using-rstudios-graphical-interface","chapter":"1 The Basics for R","heading":"Using RStudio’s Graphical Interface","text":"easiest way import data R RStudio’s graphical interface. Click Import Dataset button top-right panel RStudio (see Figure 1.3 visual guide). open dialog box can choose file type:\n- Text (base) – CSV tab-delimited files.\n- Excel – Microsoft Excel files.\n- formats available, depending installed packages.selecting file, RStudio display import settings window (see Figure 1.4). , can adjust column names, data types, options. first row contains column names, select Yes Heading option. Click Import, dataset appear RStudio’s Environment panel, ready analysis.\nFigure 1.3: visual guide loading dataset R using ‘Import Dataset’ tab RStudio.\n\nFigure 1.4: visual guide customizing import settings loading dataset R using ‘Import Dataset’ tab RStudio.\n","code":""},{"path":"chapter-into-R.html","id":"using-read.csv","chapter":"1 The Basics for R","heading":"Using read.csv()","text":"can also import data directly using read.csv() function, reads tabular data (CSV files) R data frame. data file stored locally, can load follows:Replace \"path///file.csv\" actual file path. file contain column names, use:","code":"\ndata <- read.csv(\"path/to/your/file.csv\")\ndata <- read.csv(\"path/to/your/file.csv\", header = FALSE)"},{"path":"chapter-into-R.html","id":"setting-the-working-directory","chapter":"1 The Basics for R","heading":"Setting the Working Directory","text":"default, R looks files current working directory. data located elsewhere, can specify full path read.csv() set working directory.check current working directory:set new working directory:Alternatively, RStudio, go Session > Set Working Directory > Choose Directory… select desired folder.","code":"\ngetwd()\nsetwd(\"~/Documents\")  # Adjust the path based on your system"},{"path":"chapter-into-R.html","id":"using-file.choose-with-read.csv","chapter":"1 The Basics for R","heading":"Using file.choose() with read.csv()","text":"interactively select file instead typing path manually, use file.choose():open file selection dialog, making convenient option working multiple datasets.","code":"\ndata <- read.csv(file.choose())"},{"path":"chapter-into-R.html","id":"loading-data-from-online-sources","chapter":"1 The Basics for R","heading":"Loading Data from Online Sources","text":"R also allows direct import datasets web sources. example, load publicly available COVID-19 dataset:approach useful accessing open datasets research institutions government agencies.","code":"\ncorona_data <- read.csv(\"https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\", na.strings = \"\", fileEncoding = \"UTF-8-BOM\")"},{"path":"chapter-into-R.html","id":"using-read_excel-for-excel-files","chapter":"1 The Basics for R","heading":"Using read_excel() for Excel Files","text":"import Excel files, use read_excel() function readxl package. First, install load package:, import Excel file:Unlike read.csv(), read_excel() supports multiple sheets within Excel file, can specified using sheet argument.\n### Loading Data R Packages {-}datasets available directly R packages require importing external file. example, liver package, developed book, contains multiple datasets. access churn dataset:Since many datasets used book included liver package (see Table 0.1), frequently use package examples demonstrations.section well-structured clearly explains fundamental data types R. concise informative, making accessible beginners maintaining professional tone suitable Springer publication. minor refinements improve clarity, consistency, readability.","code":"\ninstall.packages(\"readxl\")\n\nlibrary(readxl)\ndata <- read_excel(\"path/to/your/file.xlsx\")\nlibrary(liver)\ndata(churn)"},{"path":"chapter-into-R.html","id":"data-types-in-r","chapter":"1 The Basics for R","heading":"1.10 Data Types in R","text":"Data R can take various forms, correctly identifying types essential effective data manipulation, visualization, analysis. data type specific properties determine R processes , understanding helps avoid errors ensures accurate results.common data types R:Numeric: Represents real numbers, 3.14 -5.67. type used continuous numerical values, like heights, weights, temperatures.Integer: Represents whole numbers without decimals, 1, 42, -10. type useful count-based data, number customers items sold.Character: Represents text string data, \"Data Science\" \"R Programming\". type commonly used categorical labels, names, descriptive values.Logical: Represents Boolean values: TRUE FALSE. Logical data often used conditional statements filtering operations.Factor: Represents categorical data predefined levels. Factors commonly used storing variables \"Male\" \"Female\" dataset particularly useful statistical modeling.check data type variable, use class() function. example, determine type variable result, type:Press Enter, R display variable’s data type.Recognizing different data types essential choosing right analytical visualization techniques. explore later chapters (e.g., Chapters 4 5), numerical categorical variables require different approaches performing descriptive statistics, hypothesis testing, data visualization.","code":"class(result)\n   [1] \"numeric\""},{"path":"chapter-into-R.html","id":"data-structures-in-r","chapter":"1 The Basics for R","heading":"1.11 Data Structures in R","text":"Data structures fundamental working data R. define data stored manipulated, directly impacts efficiency accuracy data analysis. commonly used data structures R vectors, matrices, data frames, lists, illustrated Figure 1.4.\nFigure 1.5: visual guide different types data structures R.\n","code":""},{"path":"chapter-into-R.html","id":"vectors-in-r","chapter":"1 The Basics for R","heading":"Vectors in R","text":"vector simplest data structure R. one-dimensional array holds elements type (numeric, character, logical). Vectors building blocks data structures. can create vector using c() function:, x numeric vector containing five elements. .vector() function confirms x indeed vector, length(x) returns number elements vector.","code":"# Create a numeric vector\nx <- c(1, 2, 0, -3, 5)\n\n# Display the vector\nx\n   [1]  1  2  0 -3  5\n\n# Check if x is a vector\nis.vector(x)\n   [1] TRUE\n\n# Check the length of the vector\nlength(x)\n   [1] 5"},{"path":"chapter-into-R.html","id":"matrices-in-r","chapter":"1 The Basics for R","heading":"Matrices in R","text":"matrix two-dimensional array elements must type. Matrices useful mathematical operations structured numerical data. can create matrix using matrix() function:matrix m consists two rows three columns, filled row-wise. dim() function returns dimensions matrix. fill matrix column-wise, set byrow = FALSE.","code":"# Create a matrix with 2 rows and 3 columns\nm <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Display the matrix\nm\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n\n# Check if m is a matrix\nis.matrix(m)\n   [1] TRUE\n\n# Check the dimensions of the matrix\ndim(m)\n   [1] 2 3"},{"path":"chapter-into-R.html","id":"data-frames-in-r","chapter":"1 The Basics for R","heading":"Data Frames in R","text":"data frame two-dimensional table column can contain different data type (numeric, character, logical). makes data frames ideal storing tabular data, similar spreadsheets. can create data frame using data.frame() function:data frame students_df consists four columns: student_id, name, age, grade. class() function confirms object data frame, .data.frame() checks structure.inspect first rows data frame, use head() function. example, display first six rows churn dataset liver package:code loads liver package, retrieves churn dataset, provides overview structure. str() function particularly useful summarizing data frames, displays data types column values.","code":"# Create vectors for student data\nstudent_id <- c(101, 102, 103, 104)\nname       <- c(\"Emma\", \"Bob\", \"Alice\", \"Noah\")\nage        <- c(20, 21, 19, 22)\ngrade      <- c(\"A\", \"B\", \"A\", \"C\")\n\n# Create a data frame from the vectors\nstudents_df <- data.frame(student_id, name, age, grade)\n\n# Display the data frame\nstudents_df\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     A\n   4        104  Noah  22     Clibrary(liver)  # Load the liver package\ndata(churn)     # Load the churn dataset\n\n# Check the structure of the dataset\nstr(churn)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\n# Display the first six rows\nhead(churn)\n     state     area.code account.length voice.plan voice.messages intl.plan\n   1    KS area_code_415            128        yes             25        no\n   2    OH area_code_415            107        yes             26        no\n   3    NJ area_code_415            137         no              0        no\n   4    OH area_code_408             84         no              0       yes\n   5    OK area_code_415             75         no              0       yes\n   6    AL area_code_510            118         no              0       yes\n     intl.mins intl.calls intl.charge day.mins day.calls day.charge eve.mins\n   1      10.0          3        2.70    265.1       110      45.07    197.4\n   2      13.7          3        3.70    161.6       123      27.47    195.5\n   3      12.2          5        3.29    243.4       114      41.38    121.2\n   4       6.6          7        1.78    299.4        71      50.90     61.9\n   5      10.1          3        2.73    166.7       113      28.34    148.3\n   6       6.3          6        1.70    223.4        98      37.98    220.6\n     eve.calls eve.charge night.mins night.calls night.charge customer.calls churn\n   1        99      16.78      244.7          91        11.01              1    no\n   2       103      16.62      254.4         103        11.45              1    no\n   3       110      10.30      162.6         104         7.32              0    no\n   4        88       5.26      196.9          89         8.86              2    no\n   5       122      12.61      186.9         121         8.41              3    no\n   6       101      18.75      203.9         118         9.18              0    no"},{"path":"chapter-into-R.html","id":"lists-in-r","chapter":"1 The Basics for R","heading":"Lists in R","text":"list flexible data structure can contain elements different types, including vectors, matrices, data frames, even lists. Lists useful storing complex objects structured way. can create list using list() function:list my_list stores vector, matrix, data frame within single object. Lists allow efficient organization heterogeneous data. explore structure list, use str() function:Lists powerful tools R, especially handling nested hierarchical data. exploration, use ?list access documentation additional examples.","code":"# Create a list containing a vector, matrix, and data frame\nmy_list <- list(vector = x, matrix = m, data_frame = students_df)\n\n# Display the list\nmy_list\n   $vector\n   [1]  1  2  0 -3  5\n   \n   $matrix\n        [,1] [,2] [,3]\n   [1,]    1    2    3\n   [2,]    4    5    6\n   \n   $data_frame\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     A\n   4        104  Noah  22     Cstr(my_list)\n   List of 3\n    $ vector    : num [1:5] 1 2 0 -3 5\n    $ matrix    : num [1:2, 1:3] 1 4 2 5 3 6\n    $ data_frame:'data.frame':  4 obs. of  4 variables:\n     ..$ student_id: num [1:4] 101 102 103 104\n     ..$ name      : chr [1:4] \"Emma\" \"Bob\" \"Alice\" \"Noah\"\n     ..$ age       : num [1:4] 20 21 19 22\n     ..$ grade     : chr [1:4] \"A\" \"B\" \"A\" \"C\""},{"path":"chapter-into-R.html","id":"accessing-records-or-variables-in-r","chapter":"1 The Basics for R","heading":"1.12 Accessing Records or Variables in R","text":"’ve imported data R, can easily access specific records variables using $ [] operators. tools essential extracting data data frames lists.$ operator allows extract specific column data frame specific element list. example, access name column students_df data frame, use:command retrieves displays name column students_df data frame.Similarly, can use $ operator access elements within list. example, access vector element my_list list:command retrieves displays vector element my_list list. $ operator straightforward powerful way access specific variables elements within data frames lists.Another method accessing specific records variables [] operator, allows subset data frames, matrices, lists based specific conditions. example, extract first three rows students_df data frame, can use:command display first three rows students_df data frame.can also use [] operator extract specific columns. instance, select name grade columns students_df data frame:command retrieves displays name grade columns students_df data frame.[] operator versatile, enabling subset data frames, matrices, lists precision. $ [] operators fundamental tools data manipulation R, allowing efficiently access manage data need.","code":"students_df$name\n   [1] \"Emma\"  \"Bob\"   \"Alice\" \"Noah\"my_list$vector\n   [1]  1  2  0 -3  5students_df[1:3, ]\n     student_id  name age grade\n   1        101  Emma  20     A\n   2        102   Bob  21     B\n   3        103 Alice  19     Astudents_df[, c(\"name\", \"grade\")]\n      name grade\n   1  Emma     A\n   2   Bob     B\n   3 Alice     A\n   4  Noah     C"},{"path":"chapter-into-R.html","id":"visualizing-data-in-r","chapter":"1 The Basics for R","heading":"1.13 Visualizing Data in R","text":"Data visualization powerful tool exploring communicating insights data. plays crucial role exploratory data analysis (EDA), delve Chapter 4. saying goes, “picture worth thousand words,” data science, especially true. R provides broad array tools creating high-quality plots visualizations, allowing effectively present findings.R, two primary ways create plots: using base R graphics using ggplot2 package. Base R graphics offer simple direct way generate plots, ggplot2 provides greater flexibility customization. book primarily uses ggplot2, follows structured approach based grammar graphics, breaks plots three key components:Data: dataset visualized, data frame format using ggplot2.Aesthetics: visual properties data points, color, shape, size.Geometries: type plot created, scatter plots, bar plots, line plots.create plot using ggplot2, first install load package. Instructions installing packages provided Section 1.6. load ggplot2, use following command:Next, define data, aesthetics, geometries plot. example, create scatter plot miles per gallon (mpg) versus horsepower (hp) using built-mtcars dataset:code initializes plot ggplot() function, specifying dataset (mtcars). geom_point() function adds points plot, aes() function maps mpg x-axis hp y-axis.general template creating plots ggplot2 follows structure:Using template, variety visualizations can created.","code":"\nlibrary(ggplot2)\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp))ggplot(data = <DATA>) +\n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))"},{"path":"chapter-into-R.html","id":"geom-functions-in-ggplot2","chapter":"1 The Basics for R","heading":"Geom Functions in ggplot2","text":"Geom functions determine type plot created ggplot2. commonly used geom functions include:geom_point() scatter plotsgeom_bar() bar plotsgeom_line() line plotsgeom_boxplot() box plotsgeom_histogram() histogramsgeom_density() density plotsgeom_smooth() adding smoothed conditional means plotsFor example, create smoothed line plot mpg versus hp:Multiple geom functions can combined single plot. overlay scatter plot smoothed line:Alternatively, aes() function can placed inside ggplot() streamline code:Additional visualization examples can found Chapter 4. complete list geom functions, refer ggplot2 documentation.","code":"\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp))\nggplot(data = mtcars) +\n  geom_smooth(mapping = aes(x = mpg, y = hp)) + \n  geom_point(mapping = aes(x = mpg, y = hp))\nggplot(data = mtcars, mapping = aes(x = mpg, y = hp)) +\n  geom_smooth() + \n  geom_point()"},{"path":"chapter-into-R.html","id":"aesthetics-in-ggplot2","chapter":"1 The Basics for R","heading":"Aesthetics in ggplot2","text":"Aesthetics control visual properties data points, color, size, shape. properties specified within aes() function. example:, color = cyl maps color points number cylinders (cyl) mtcars dataset. ggplot2 automatically assigns unique color category adds corresponding legend.addition color, aesthetics size alpha (transparency) can used:Aesthetics can also set directly inside geom functions. example, make points blue triangles size 3:section introduced fundamentals data visualization R using ggplot2. next chapters explore visualization plays crucial role exploratory data analysis (Chapter 4) refine plots communication reporting. details visualization techniques, see ggplot2 documentation. interactive graphics, consider exploring plotly package Shiny web applications.","code":"\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, color = cyl))\n# Left plot: using the size aesthetic\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, size = cyl))\n\n# Right plot: using the alpha aesthetic\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp, alpha = cyl))\nggplot(data = mtcars) +\n  geom_point(mapping = aes(x = mpg, y = hp), \n             color = \"blue\", size = 3, shape = 2)"},{"path":"chapter-into-R.html","id":"sec-formula-in-R","chapter":"1 The Basics for R","heading":"1.14 Formula in R","text":"Formulas R provide concise intuitive way specify relationships variables statistical modeling. widely used functions regression, classification, machine learning define response variable depends one predictors.R, formulas use tilde symbol ~ express relationships variables, response variable appears left-hand side predictor variables right-hand side. example, formula y ~ x specifies y modeled function x. multiple predictors, separated +.\ninstance, using diamonds dataset, formula:models price diamond based carat, cut, color.include variables dataset predictors, can use shorthand notation:approach particularly useful large datasets listing predictors manually impractical.formula R acts quoting operator, instructing R interpret variables symbolically rather evaluating immediately. variable left-hand side ~ dependent variable (response variable), variables right-hand side independent variables (predictor variables).Example 1.1  illustrate, suppose want predict price diamond using linear regression model. can pass formula lm() function:, formula price ~ carat + cut + color defines relationship, data argument specifies dataset use.defined, formulas can used various R functions statistical modeling machine learning. progress later chapters, encounter formulas functions regression, classification, (e.g., Chapters 7, 9, 10). Mastering formula syntax enable efficiently build, customize, interpret models throughout book.","code":"\nprice ~ carat + cut + color\nprice ~ .\nmodel <- lm(price ~ carat + cut + color, data = diamonds)"},{"path":"chapter-into-R.html","id":"reporting-with-r-markdown","chapter":"1 The Basics for R","heading":"1.15 Reporting with R Markdown","text":"Thus far, book covered interact R RStudio data analysis. section focuses equally important aspect: effectively communicating analytical findings. Data scientists must present results clearly teams, stakeholders, clients. Regardless depth analysis, impact limited communicated effectively. R Markdown facilitates process enabling seamless integration code, text, output dynamic, reproducible reports.R Markdown allows users write execute R code within document, producing reports, presentations, dashboards. Unlike traditional notebooks word processors, R Markdown ensures text, code, results remain synchronized data changes. book entirely written using R Markdown generated bookdown package, ensuring fully reproducible dynamic workflow.R Markdown documents can exported multiple formats, including HTML, PDF, Word, PowerPoint, making adaptable various audiences reporting needs. Furthermore, supports creation interactive documents using Shiny, allowing users build web applications facilitate exploratory data analysis.get started, following resources provide useful references:R Markdown Cheat Sheet: R Markdown Cheat Sheet offers concise reference creating documents, including syntax, formatting, output options. available RStudio Help > Cheatsheets > R Markdown Cheat Sheet.R Markdown Reference Guide: R Markdown Reference Guide provides detailed overview R Markdown’s features, including document structure customization.","code":""},{"path":"chapter-into-R.html","id":"r-markdown-basics","chapter":"1 The Basics for R","heading":"R Markdown Basics","text":"R Markdown follows literate programming approach, combining text executable code single document. Unlike word processors formatting visible writing, R Markdown requires compilation generate final report. approach ensures automation, plots figures generated dynamically inserted document. Since code embedded, analyses fully reproducible.create R Markdown document RStudio:\nFile > New File > R Markdown\ndialog box appear, allowing selection document type. standard report, choose “Document.” options include “Presentation” slides, “Shiny” interactive applications, “Template” predefined formats. selecting document type, enter title author name. output format can set HTML, PDF, Word; HTML often recommended debugging.R Markdown files use .Rmd extension, distinguishing .R script files. newly created file contains template can modified custom text, code, formatting.","code":""},{"path":"chapter-into-R.html","id":"the-header","chapter":"1 The Basics for R","heading":"The Header","text":"header defines metadata document’s title, author, date, output format. enclosed within three dashes (---).Title: document’s title.Author: name author.Date: date creation.Output format: format final document (html_document, pdf_document, word_document).Additional metadata can included customization, table contents options formatting preferences.","code":"---\ntitle: \"An Analysis of Customer Churn\"\nauthor: \"Reza Mohammadi\"\ndate: \"Aug 12, 2024\"\noutput: html_document\n---"},{"path":"chapter-into-R.html","id":"code-chunks-and-inline-code","chapter":"1 The Basics for R","heading":"Code Chunks and Inline Code","text":"R Markdown integrates R code within documents using code chunks, enclosed triple backticks (```{r}) followed code. example:compiled, R executes code displays output within document. Code chunks used analysis, visualizations, modeling. “Run” button RStudio allows individual execution chunks. See Figure 1.6 visual guide.\nFigure 1.6: Executing code chunk R Markdown using ‘Run’ button RStudio.\nCommon chunk options include:echo = FALSE: Displays output hides code.eval = FALSE: Shows code execute .message = FALSE: Suppresses messages.warning = FALSE: Suppresses warnings.error = FALSE: Hides error messages.include = FALSE: Omits code output.inline calculations, use backticks r keyword:renders dynamically :","code":"\n```r\n2 + 3\n   [1] 5\n```The factorial of 5 is 120.The factorial of 5 is 120."},{"path":"chapter-into-R.html","id":"styling-text","chapter":"1 The Basics for R","heading":"Styling Text","text":"R Markdown supports various text formatting options:Headings: Use # section titles.Bold: Enclose text double asterisks (**bold**).Italic: Use single asterisks (*italic*).Lists: Use * bullet points.Links: [R Markdown website](https://rmarkdown.rstudio.com)Images: ![Alt text](path//image.png)mathematical notation, use LaTeX-style equations:","code":"Inline: $y = \\beta_0 + \\beta_1 x$  \nBlock: $$ y = \\beta_0 + \\beta_1 x $$"},{"path":"chapter-into-R.html","id":"mastering-r-markdown","chapter":"1 The Basics for R","heading":"Mastering R Markdown","text":"learning:Books: R Markdown: Definitive Guide.Tutorials: R Markdown website.Courses: DataCamp R Markdown course.Forums: RStudio Community.leveraging R Markdown, data scientists can produce high-quality, reproducible reports enhance collaboration communication.","code":""},{"path":"chapter-into-R.html","id":"intro-R-exercises","chapter":"1 The Basics for R","heading":"1.16 Exercises","text":"section provides hands-exercises reinforce understanding fundamental concepts covered chapter.","code":""},{"path":"chapter-into-R.html","id":"basic-exercises","chapter":"1 The Basics for R","heading":"Basic Exercises","text":"Install R RStudio computer.Use getwd() function check current working directory. , change new directory using setwd().Create numeric vector named numbers containing values 5, 10, 15, 20, 25. , calculate mean standard deviation vector.Create matrix 3 rows 4 columns, filled numbers 1 12.Create data frame containing following variables:student_id (integer)name (character)score (numeric)passed (logical, TRUE means student passed FALSE means failed)\nPrint first rows data frame using head().Install load liver ggplot2 packages R. encounter errors, check internet connection ensure CRAN accessible.Load churn dataset liver package display first rows using head() function.Report data types variables churn dataset using str() function.Report dimensions churn dataset using dim() function.Report summary statistics variables churn dataset using summary() function.Create scatter plot using ggplot2 visualizes relationship day.mins eve.mins churn dataset. Hint: See code Section 4.6.Create histogram day.calls variable churn dataset.Create boxplot day.mins variable churn dataset.Create boxplot day.mins variable churn dataset, grouped churn variable. Hint: See code Section 4.5.Use mean() function compute mean customer.calls variable churn dataset. , calculate mean customer.calls churner churn == yes.Create R Markdown document includes title, author, small analysis churn dataset. Generate HTML report.","code":""},{"path":"chapter-into-R.html","id":"more-challenges-exercise","chapter":"1 The Basics for R","heading":"More Challenges Exercise","text":"following R code generates simulated dataset 200 observations. use simulated dataset simple toy example Chapter 7 explain k-nearest neighbors algorithm works. simulated data patients three variables:Age: Age patients numeric variable range 15 75 years old.Ratio: Sodium/Potassium ratio patient’s blood numeric variable. ratio generated based Type variable.Type: factor three levels: \"\", \"B\", \"C\" representing type drug patient taking.Run code report summary statistics data.Visualize data using following ggplot2 code:Extend dataset drug_data adding new variable named Outcome, factor two levels (\"Good\" \"Bad\").Patients Type == \"\" higher probability \"Good\" outcomes.Patients Type == \"B\" Type == \"C\" lower probability \"Good\" outcomes.Use sample() appropriate probabilities generate Outcome variable.Create new scatter plot using ggplot2 visualizes relationship Age Ratio, colored Outcome variable.Create new variable Age_group drug_data dataset categorizes patients three groups:“Young” (\\(\\leq 30\\) years old)“Middle-aged” (31-50 years old)“Senior” (>50 years old).Calculate mean Ratio Age_group category drug_data dataset.Create bar chart using ggplot2 displays average Ratio Age_group.Modify drug_data dataset adding Risk_factor variable, calculated Ratio * Age / 10. Analyze Risk_factor differs Type.Create histogram Risk_factor variable, grouped Type.Generate boxplot visualize distribution Risk_factor across different Outcome categories.","code":"\n# Simulate data for kNN\nset.seed(10)\n\nn  = 200         # Number of patients\nn1 = 90          # Number of patients with drug A\nn2 = 60          # Number of patients with drug B \nn3 = n - n1 - n2 # Number of patients with drug C\n\n# Generate Age variable between 15 and 75\nAge = sample(x = 15:75, size = n, replace = TRUE)\n\n# Generate Drug Type variable with three levels\nType = sample(x = c(\"A\", \"B\", \"C\"), size = n, replace = TRUE, prob = c(n1, n2, n3))\n\n# Generate Sodium/Potassium Ratio based on Drug Type\nRatio = numeric(n)\n\nRatio[Type == \"A\"] = sample(x = 10:40, size = sum(Type == \"A\"), replace = TRUE)\nRatio[Type == \"B\"] = sample(x =  5:15, size = sum(Type == \"B\"), replace = TRUE)\nRatio[Type == \"C\"] = sample(x =  5:15, size = sum(Type == \"C\"), replace = TRUE)\n\n# Create a data frame with the generated variables\ndrug_data = data.frame(Age = Age, Ratio = Ratio, Type = Type)\nggplot(data = drug_data, aes(x = Age, y = Ratio)) +\n  geom_point(aes(color = Type, shape = Type)) + \n  labs(title = \"Age vs. Sodium/Potassium Ratio\", \n       x = \"Age\", y = \"Sodium/Potassium Ratio\")"},{"path":"chapter-intro-DS.html","id":"chapter-intro-DS","chapter":"2 Introduction to Data Science","heading":"2 Introduction to Data Science","text":"Data Science rapidly evolving field transforming industries leveraging computational, statistical, analytical techniques. 21st century, data become one valuable resources, often called “new oil” due potential drive innovation reshape future.Data science key unlocking potential. applying computational, statistical, analytical techniques, data scientists extract insights vast amounts data, enabling organizations make informed decisions, optimize processes, predict trends, develop intelligent systems. led groundbreaking advancements fields healthcare, finance, marketing, artificial intelligence (AI), beyond.Given rapid growth increasing demand, data science critical ever. chapter, ’ll explore fundamentals data science, discuss significance modern society, introduce Data Science Workflow—structured approach data scientists use transform raw data actionable insights.section well-structured provides clear introduction data science. effectively conveys interdisciplinary nature field highlights core components. However, areas clarity, consistency, flow can improved. suggestions:","code":""},{"path":"chapter-intro-DS.html","id":"what-is-data-science","chapter":"2 Introduction to Data Science","heading":"2.1 What is Data Science?","text":"Data science interdisciplinary field integrates computer science, statistics, domain expertise extract insights data. involves using analytical computational techniques process vast amounts raw data, transforming meaningful information supports decision-making strategic planning.\nFigure 2.1: Data science multidisciplinary field applies computational statistical methods extract insights data.\nAlthough term “data science” relatively new, foundations lie well-established disciplines statistics, data analysis, machine learning. exponential growth digital data, advancements computational power, increasing demand data-driven decision-making, data science emerged distinct essential field.core, data science concerned extracting knowledge data using combination statistical techniques, machine learning algorithms, domain-specific methodologies. helps organizations manage understand vast amounts information generated digital age.","code":""},{"path":"chapter-intro-DS.html","id":"key-components-of-data-science","chapter":"2 Introduction to Data Science","heading":"Key Components of Data Science","text":"field data science encompasses three main components:Data Engineering: foundation data science, responsible collecting, storing, structuring large datasets. includes development data pipelines infrastructure enable efficient analysis. crucial, data engineering beyond scope book.Data Analysis Statistics: application statistical methods explore analyze data. includes data visualization, hypothesis testing, predictive modeling. details topic covered Statistical Inference Hypothesis Testing Exploratory Data Analysis chapters.Machine Learning Artificial Intelligence: use algorithms identify patterns, make predictions, extract deeper insights. includes supervised unsupervised learning, deep learning, natural language processing. concepts discussed Modeling Process chapter.","code":""},{"path":"chapter-intro-DS.html","id":"why-data-science-matters","chapter":"2 Introduction to Data Science","heading":"2.2 Why Data Science Matters","text":"digital age, data become one valuable resources often referred “new oil” 21st century. comparison makes sense, world’s valuable companies today—including OpenAI, Google, Apple—driven artificial intelligence data science. Just wealthiest companies 20th century controlled oil energy, today’s leading enterprises leverage data key asset innovation competitive advantage.Across industries, data-driven decision-making become essential. Organizations generate vast amounts data every day, without right tools techniques, much data remain untapped. Data science helps organizations uncover patterns, detect trends, make informed decisions enhance efficiency, reduce costs, improve customer experiences.\nData science plays crucial role wide range sectors, including:Finance: Financial institutions leverage data science risk assessment, fraud detection, algorithmic trading. Machine learning models identify anomalies transaction patterns, improving fraud detection regulatory compliance.Marketing: Businesses use data science analyze customer behavior, segment audiences, create targeted marketing campaigns. Platforms Facebook Google Ads leverage sophisticated algorithms match advertisements relevant audiences, improving engagement conversion rates.Retail E-commerce: Companies like Amazon Walmart use data science optimize inventory management, predict demand, personalize recommendations. analyzing purchase history browsing behavior, retailers can offer tailored promotions enhance customer satisfaction.Healthcare: Hospitals medical researchers use data science disease diagnosis, patient risk prediction, personalized treatment plans. analyzing large datasets medical records, institutions can identify high-risk patients take preventative measures improve health outcomes.example, Netflix applies data science analyze viewing patterns recommend personalized content users, supply chain optimization Amazon ensures faster deliveries leveraging predictive analytics.","code":""},{"path":"chapter-intro-DS.html","id":"the-data-science-workflow","chapter":"2 Introduction to Data Science","heading":"2.3 The Data Science Workflow","text":"data science workflow follows iterative cyclical approach, insights gained stage inform refine subsequent steps. Unlike strictly linear process, data science involves continuous refinement enhance accuracy efficiency. structured approach ensures data-driven projects conducted systematically, balancing exploratory analysis, model building, evaluation derive meaningful conclusions.data science workflow follows phased, adaptive approach within scientific framework, transforming raw data actionable knowledge. transformation often conceptualized using DIKW Pyramid (Data → Information → Knowledge → Wisdom), illustrated Figure 2.2.\nFigure 2.2: DIKW Pyramid illustrates transformation raw data actionable insights, progressing data information, knowledge, ultimately wisdom.\nspecifics may vary across projects, data science workflows follow common structure. book, adopt Data Science Workflow guiding framework structuring data science projects. workflow inspired Cross-Industry Standard Process Data Mining (CRISP-DM) model, widely recognized methodology data-driven projects. cyclic framework guides data scientists following key stages (see Figure 2.3):Problem Understanding – Defining business research question outlining objectives.Data Preparation – Collecting, cleaning, transforming, organizing data ensure suitable analysis. step includes handling missing values, addressing inconsistencies, detecting outliers, preparing features scaling, encoding, transformation.Exploratory Data Analysis (EDA) – Identifying patterns, distributions, relationships within data.Preparing Data Modeling – Engineering relevant features, normalizing data, selecting meaningful variables.Modeling – Applying machine learning statistical techniques develop predictive descriptive models.Evaluation – Assessing model performance using appropriate metrics validation techniques.Deployment – Integrating model production environment monitoring performance time.\nFigure 2.3: Data Science Workflow iterative framework structuring data science machine learning projects. Inspired CRISP-DM model, ensures systematic problem-solving continuous refinement.\ndata science inherently iterative, steps often revisited multiple times within single project. feedback loops stages allow continuous refinement—adjusting data preprocessing, modifying features, retraining models new insights emerge. following structured workflow, data scientists can ensure rigor, accuracy, efficiency transforming data valuable insights.","code":""},{"path":"chapter-intro-DS.html","id":"problem-understanding","chapter":"2 Introduction to Data Science","heading":"2.4 Problem Understanding","text":"first step data science project clearly define problem—whether business challenge research question. phase crucial data science just building models; solving real-world problems using data-driven approaches. well-defined problem ensures efforts aligned meaningful objectives, improving likelihood delivering actionable insights.stage, data scientists work closely stakeholders understand goals, clarify expectations, define success criteria. following questions help frame problem:research business question important?desired outcome impact?can data science techniques contribute addressing question?Focusing diving essential. Simon Sinek emphasizes TED talk “Great Leaders Inspire Action”, “People don’t buy ; buy .” concept applies data science well—understanding deeper motivation behind project provides clarity direction.example, data science team business analytics department may approached client wants predictive model lacks clarity specific problem trying solve. Without clear , becomes difficult develop solution delivers real value. Similarly, students working research projects often focus want build rather needed.Suppose company aims reduce customer churn. well-defined objective might develop predictive model identifies customers risk leaving targeted retention strategies can implemented. initial understanding helps frame problem guides selection relevant data, modeling techniques, evaluation metrics.Problem understanding analytical creative process. data science provides tools methodologies, defining right problem requires domain expertise critical thinking. following steps help ensure structured approach:Clearly articulate project objectives requirements terms overall goals business research entity.Break objectives outline specific expectations desired outcomes.Translate objectives data science problem can addressed using analytical techniques.Draft preliminary strategy achieve objectives, considering potential approaches methodologies.thoroughly defining problem, data scientists set stage effective workflow, ensuring subsequent analysis modeling efforts remain aligned meaningful outcomes.","code":""},{"path":"chapter-intro-DS.html","id":"data-preparation","chapter":"2 Introduction to Data Science","heading":"2.5 Data Preparation","text":"problem well-defined, next step data preparation, ensuring data accurate, complete, well-structured. Raw data often contains missing values, inconsistencies, outliers, making phase critical reliable analysis. Poorly prepared data can lead misleading insights, even sophisticated models.Data can originate various sources, including databases, spreadsheets, APIs, web scraping. may structured (e.g., numerical data databases) unstructured (e.g., text, images). Preprocessing essential analysis.Key steps data preparation include:Data Collection Integration: Merging data multiple sources ensuring consistency.Handling Missing Values: Removing, imputing, flagging incomplete data.Outlier Detection: Identifying managing extreme values using visualization.Resolving Inconsistencies: Standardizing formats, correcting errors, aligning categorical values.Feature Engineering: Transforming data encoding, scaling, normalization model compatibility.Data Summarization: Checking variable types, computing summary statistics, detecting duplicates.Though time-consuming, data preparation essential accurate modeling meaningful analysis. Chapter 3, explore techniques real-world examples.","code":""},{"path":"chapter-intro-DS.html","id":"exploratory-data-analysis-eda","chapter":"2 Introduction to Data Science","heading":"2.6 Exploratory Data Analysis (EDA)","text":"Exploratory Data Analysis (EDA) fundamental step data science workflow, providing initial understanding dataset formal modeling. primary objective EDA uncover patterns, relationships, anomalies data, helping data scientists refine hypotheses validate assumptions. systematically examining data, EDA ensures subsequent modeling process informed solid understanding dataset’s structure characteristics.Several key techniques commonly used EDA:Summary statistics – Measures mean, median, standard deviation, interquartile range provide insights distribution central tendencies numerical variables.Data visualization – Graphical techniques, including histograms, scatter plots, box plots, reveal data distributions, trends, potential outliers.Correlation analysis – Examining relationships numerical features using correlation coefficients helps identify dependencies may influence modeling decisions.EDA serves diagnostic exploratory functions. helps detect data quality issues, missing values inconsistencies, also guiding feature selection engineering. instance, strong correlation exists certain features target variable, features may prioritized modeling phase.thorough EDA process improves quality dataset also enhances interpretability reliability analytical results. Chapter 4, explore EDA techniques greater detail, applying real-world datasets illustrate practical applications.","code":""},{"path":"chapter-intro-DS.html","id":"preparing-data-for-modeling","chapter":"2 Introduction to Data Science","heading":"2.7 Preparing Data for Modeling","text":"insights EDA, next step prepare data modeling. stage involves feature engineering, feature selection, data splitting—crucial building effective models.Feature Engineering: Creating new features transforming existing ones enhance model performance. example, deriving new variables combining existing ones applying transformations can provide additional predictive power.Feature Selection: Identifying selecting relevant features improve model efficiency prevent overfitting. Removing irrelevant redundant features simplifies model enhances interpretability.Data Splitting: Dividing dataset training, validation, testing sets. training set used develop model, validation set helps fine-tune parameters, test set assesses final model performance.end stage, data structured well-prepared format, ensuring models can learn effectively. Chapter 6, explore techniques detail apply real-world datasets.","code":""},{"path":"chapter-intro-DS.html","id":"modeling","chapter":"2 Introduction to Data Science","heading":"2.8 Modeling","text":"Modeling stage data scientists apply machine learning statistical techniques prepared data create predictive descriptive model. goal build model effectively captures relationships within data generalizes well new, unseen data.modeling process typically involves:Choosing Model: Selecting appropriate model based problem type (e.g., regression, classification, clustering) characteristics dataset.Training Model: Fitting model training data learn patterns relationships.Tuning Hyperparameters: Adjusting model parameters optimize performance validation set.Common algorithms include linear regression (Chapter 10), decision trees (Chapter 11), Naïve Bayes classifier (Chapter 9), k-Nearest Neighbors (k-NN) algorithm (Chapter 7), neural networks (Chapter 12). method strengths limitations, selecting suitable model depends nature problem, data quality, computational constraints. Often, multiple models tested compared determine best-performing approach.","code":""},{"path":"chapter-intro-DS.html","id":"evaluation","chapter":"2 Introduction to Data Science","heading":"2.9 Evaluation","text":"model built, must rigorously evaluated ensure accuracy, generalizability, robustness deployment. evaluation process relies well-defined performance metrics, vary depending type problem. classification models, commonly used metrics include accuracy, precision, recall, F1-score, area receiver operating characteristic curve (ROC-AUC). regression tasks, measures mean squared error (MSE), mean absolute error (MAE), coefficient determination (\\(R^2\\)) assess model effectiveness.ensure model overfitting training data, cross-validation techniques, k-fold cross-validation, employed. methods provide reliable estimate model’s performance partitioning data multiple subsets training validation. Beyond numerical evaluation, error analysis plays crucial role diagnosing weaknesses, particularly confusion matrix interpretation classification problems residual analysis regression. careful examination errors often reveals underlying biases, data inconsistencies, model limitations require refinement.model fails meet expectations, adjustments may necessary, feature selection, hyperparameter tuning, exploring alternative modeling approaches. Chapter 8, explore techniques detail apply real-world datasets.","code":""},{"path":"chapter-intro-DS.html","id":"deployment","chapter":"2 Introduction to Data Science","heading":"2.10 Deployment","text":"model evaluated meets project goals, final step deployment, integrated production environment generate real-time insights predictions. phase crucial ensuring model contributes tangible value, whether supporting decision-making processes automating tasks within operational systems. Models can deployed various ways, embedding web applications, integrating enterprise software, automating processes large-scale data pipelines.Beyond initial integration, continuous monitoring essential track model’s performance detect potential issues. real-world data evolves, models may experience concept drift, predictive accuracy deteriorates due changes underlying patterns. mitigate , periodic model updates retraining necessary maintain reliability. Additionally, implementing robust logging performance tracking mechanisms helps ensure discrepancies predicted actual outcomes quickly identified addressed.Deployment one-time event ongoing process. Effective deployment strategies account scalability, interpretability, maintainability, allowing models remain useful dynamic environments. field data science advances, ability manage deployed models effectively continue critical factor transforming analytical insights real-world impact.","code":""},{"path":"chapter-intro-DS.html","id":"machine-learning","chapter":"2 Introduction to Data Science","heading":"2.11 Machine Learning","text":"Data science relies machine learning techniques extract insights data, make predictions, uncover patterns. methods enable data scientists move beyond descriptive analysis explore predictive prescriptive approaches, essential real-world applications. section, provide overview machine learning, including main types—supervised learning unsupervised learning—discuss machine learning differs statistical learning.Machine learning branch artificial intelligence focuses developing algorithms learn data make predictions. Rather explicitly programmed task, machine learning models identify patterns within data use make informed decisions. approach particularly useful complex problems rule-based programming impractical.instance, rather defining fixed set rules detect spam emails, machine learning model can trained labeled dataset emails classified “spam” “spam.” model learns distinguishing patterns can classify new emails high accuracy. ability generalize data makes machine learning invaluable fields finance, healthcare, marketing.","code":""},{"path":"chapter-intro-DS.html","id":"machine-learning-tasks-supervised-vs.-unsupervised-learning","chapter":"2 Introduction to Data Science","heading":"Machine Learning Tasks: Supervised vs. Unsupervised Learning","text":"Machine learning tasks can broadly categorized supervised learning unsupervised learning, differ terms models learn data objectives analysis.Supervised learning involves training model labeled dataset, data point associated known output. goal model learn relationship input features corresponding output, enabling make accurate predictions new data. Common supervised learning tasks include classification numeric prediction. classification, model assigns data points predefined categories, detecting whether email spam identifying whether patient particular disease. book covers classification techniques decision trees (Chapter 11), Naïve Bayes classifier (Chapter 9), k-Nearest Neighbors (k-NN) algorithm (Chapter 7). Numeric prediction, also known regression, focuses estimating continuous values, forecasting house prices based location size. detailed discussion regression techniques provided Chapter 10.Unsupervised learning, hand, applied datasets lack labeled outputs. objective uncover hidden patterns, relationships, structures within data. Clustering, common unsupervised learning technique, groups data points based similarity, segmenting customers according purchasing behavior. Another important unsupervised learning method pattern discovery, also known association rule learning, identifies relationships variables. technique widely used market basket analysis detect frequently co-purchased items. concepts explored detail Chapter 13.summary, supervised learning used labeled data available specific predictive outcome required, unsupervised learning beneficial exploratory data analysis, goal identify underlying structures unlabeled data. distinction two approaches fundamental selecting appropriate machine learning techniques given data science problem.","code":""},{"path":"chapter-intro-DS.html","id":"exercises","chapter":"2 Introduction to Data Science","heading":"2.12 Exercises","text":"following exercises help reinforce key concepts covered chapter. questions range fundamental definitions applied problem-solving related data science, data science workflow, machine learning.data-driven decision-making impact businesses? Give example real-world application.Data Science Workflow inspired CRISP-DM model. CRISP-DM stand , guide data-driven projects? key stages CRISP-DM model?Data Science Workflow CRISP-DM model standard processes data science projects. methodologies used industry?think can skip Problem Understanding phase directly jump Data Preparation data science project? Justify answer.Data Preparation considered one time-consuming steps data science project? common challenges faced phase?extent can Data Science projects automated without human intervention? risks limitations relying solely automated tools?following scenarios, identify appropriate stage data science workflow:\ncompany wants predict customer churn based historical data.\nresearcher exploring relationship air pollution respiratory diseases.\ne-commerce platform analyzing user behavior personalize product recommendations.\nhospital developing predictive model patient readmission rates.\ncompany wants predict customer churn based historical data.researcher exploring relationship air pollution respiratory diseases.e-commerce platform analyzing user behavior personalize product recommendations.hospital developing predictive model patient readmission rates.task, classify supervised unsupervised learning, explain reasoning, identify suitable machine learning algorithm applied.\nIdentifying fraudulent transactions credit card dataset.\nSegmenting customers based purchasing behavior.\nPredicting stock prices based historical data.\nGrouping news articles topics using natural language processing.\nIdentifying fraudulent transactions credit card dataset.Segmenting customers based purchasing behavior.Predicting stock prices based historical data.Grouping news articles topics using natural language processing.Define training dataset test dataset. important? improper splitting datasets affect model performance? Provide example real-world issue caused poor dataset partitioning.Many AI-driven systems criticized biased predictions, hiring algorithms favor certain demographics facial recognition models misidentify certain racial groups.\ncommon sources bias data science projects?\ncan data scientists ensure fairness mitigate biases models?\nGive example real-world case bias AI led negative consequences.\ncommon sources bias data science projects?can data scientists ensure fairness mitigate biases models?Give example real-world case bias AI led negative consequences.Accuracy common metric used evaluate models, always best indicator success. Consider binary classification problem 2% cases positive (e.g., detecting rare diseases fraud).\nmight accuracy misleading case?\nalternative evaluation metrics used?\ndecide whether model truly valuable decision-making?\nmight accuracy misleading case?alternative evaluation metrics used?decide whether model truly valuable decision-making?","code":""},{"path":"chapter-data-prep.html","id":"chapter-data-prep","chapter":"3 Data Preparation","heading":"3 Data Preparation","text":"Data preparation foundational step data science project, ensuring raw data transformed clean structured format suitable analysis. process often time-consuming yet crucial stage, quality data directly influences accuracy insights effectiveness predictive models.chapter explores key data preparation techniques, including handling missing values, detecting outliers, transforming data, feature engineering. end chapter, clear understanding preprocess raw data, enabling robust statistical modeling machine learning applications.illustrate concepts, use diamonds dataset ggplot2 package. dataset contains detailed attributes diamonds, carat, cut, color, clarity, price, making excellent case study data preprocessing. chapter, focus first two steps Data Science Workflow—data cleaning transformation—laying groundwork analysis subsequent chapters.","code":""},{"path":"chapter-data-prep.html","id":"problem-understanding","chapter":"3 Data Preparation","heading":"3.1 Problem Understanding","text":"preparing data analysis, essential define problem establish clear objectives. case, aim analyze diamonds dataset gain insights diamond pricing, critical factor industries jewelry retail, gemology, e-commerce. dataset includes attributes influence diamond value, allowing us explore key factors affecting pricing.","code":""},{"path":"chapter-data-prep.html","id":"objectives-and-key-questions","chapter":"3 Data Preparation","heading":"Objectives and Key Questions","text":"primary objectives diamonds dataset :Examine relationships diamond attributes (e.g., carat, cut, color, clarity) price.Identify patterns improve price estimation.Assess data quality, ensuring consistency detecting missing values outliers may affect analysis.achieve objectives, address key questions :attributes significant influence price?pricing trends based characteristics carat weight cut quality?inconsistencies, errors, missing values need corrected?","code":""},{"path":"chapter-data-prep.html","id":"framing-the-problem-as-a-data-science-task","chapter":"3 Data Preparation","heading":"Framing the Problem as a Data Science Task","text":"business perspective, understanding diamond pricing can provide valuable insights jewelers, e-commerce platforms, gemologists. data science perspective, problem can approached two ways:Predictive modeling: Developing model estimates diamond price based attributes.Exploratory data analysis (EDA): Identifying trends relationships without building predictive model.Clearly defining objectives ensures data preparation efforts align intended analytical approach, whether exploratory insights building robust predictive models generalize well unseen data. structured problem framing guide decisions data cleaning, transformation, feature engineering, ensuring analysis remains focused actionable.","code":""},{"path":"chapter-data-prep.html","id":"Data-pre-diamonds","chapter":"3 Data Preparation","heading":"3.2 diamonds Dataset Overview","text":"diamonds dataset, included ggplot2 package, provides structured information various characteristics diamonds. row represents unique diamond, 54,940 entries total, contains 10 descriptive variables, including price, carat, cut, clarity, color. goal analysis gain deeper insights factors influence diamond pricing, understand distribution data across attributes, explore quantitative qualitative relationships variables.use diamonds dataset R, first ensure ggplot2 package installed. , install using:, load package dataset:inspect dataset structure, use:function reveals dataset 53940 observations 10 variables. summary key attributes:price: price US dollars ($326–$18,823).carat: weight diamond (0.2–5.01).cut: quality cut (Fair, Good, Good, Premium, Ideal).color: diamond color, D (best) J (worst).clarity: measurement clear diamond (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, (best)).x: length mm (0–10.74).y: width mm (0–58.9).z: depth mm (0–31.8).depth: total depth percentage = 2 * z / (x + y).table: width top diamond relative widest point.","code":"\ninstall.packages(\"ggplot2\") \nlibrary(ggplot2)  # Load ggplot2 package\ndata(diamonds)    # Load diamonds datasetstr(diamonds)   \n   tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n    $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n    $ cut    : Ord.factor w/ 5 levels \"Fair\"<\"Good\"<..: 5 4 2 4 2 3 3 3 1 3 ...\n    $ color  : Ord.factor w/ 7 levels \"D\"<\"E\"<\"F\"<\"G\"<..: 2 2 2 6 7 7 6 5 2 5 ...\n    $ clarity: Ord.factor w/ 8 levels \"I1\"<\"SI2\"<\"SI1\"<..: 2 3 5 4 2 6 7 3 4 5 ...\n    $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n    $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n    $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n    $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n    $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n    $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ..."},{"path":"chapter-data-prep.html","id":"types-of-features-in-the-diamonds-dataset","chapter":"3 Data Preparation","heading":"Types of Features in the diamonds Dataset","text":"Understanding types features dataset essential determining appropriate data preparation steps:Quantitative (Numerical) Variables: represented numbers can continuous discrete.\nContinuous Variables: variables can take value within range. dataset, carat, price, x, y, z, depth continuous.\nDiscrete Variables: variables take countable values, often integers. example, count customers number purchases discrete, though dataset doesn’t include variable.\nContinuous Variables: variables can take value within range. dataset, carat, price, x, y, z, depth continuous.Discrete Variables: variables take countable values, often integers. example, count customers number purchases discrete, though dataset doesn’t include variable.Categorical (Qualitative) Variables: describe data fits categories rather numerical value. divided three types:\nOrdinal Variables: Categorical variables meaningful order, intervals categories equal. instance, cut, color, clarity ordinal variables dataset. ordering levels variables (e.g., “Fair” “Ideal” cut) meaning.\nNominal Variables: Categorical variables without intrinsic ordering among categories. datasets, examples might include “gender” “product type,” diamonds dataset contain nominal variables.\nBinary Variables: Variables two levels, often coded 0 1. diamonds dataset doesn’t contain binary variables, example feature like “has_certificate” values “yes” “.”\nOrdinal Variables: Categorical variables meaningful order, intervals categories equal. instance, cut, color, clarity ordinal variables dataset. ordering levels variables (e.g., “Fair” “Ideal” cut) meaning.Nominal Variables: Categorical variables without intrinsic ordering among categories. datasets, examples might include “gender” “product type,” diamonds dataset contain nominal variables.Binary Variables: Variables two levels, often coded 0 1. diamonds dataset doesn’t contain binary variables, example feature like “has_certificate” values “yes” “.”Knowing type feature guides decisions data preparation. instance:\n- Numerical variables can normalized standardized using techniques like Min-Max Scaling Z-score Scaling.\n- Ordinal variables may encoded using ordinal encoding one-hot encoding, depending whether model recognize order.\n- Categorical variables without meaningful order typically one-hot encoded.understanding types variables diamonds dataset, can select appropriate transformations encoding methods prepare data effectively analysis modeling.","code":""},{"path":"chapter-data-prep.html","id":"key-considerations-for-data-preparation","chapter":"3 Data Preparation","heading":"Key Considerations for Data Preparation","text":"objectives mind, main priorities preparing dataset:Data Quality: Ensure data accurate, consistent, free major issues. involves checking missing values, outliers, inconsistencies bias analysis.Feature Engineering: Explore possibility creating new features improve predictive accuracy. instance, calculating volume (using product x, y, z dimensions) provide additional measure diamond’s size.Data Transformation: Ensure features appropriate formats. Categorical variables like cut color may need converted numeric codes dummy variables work machine learning algorithms effectively.","code":""},{"path":"chapter-data-prep.html","id":"Data-pre-outliers","chapter":"3 Data Preparation","heading":"3.3 Outliers","text":"Outliers data points significantly deviate general distribution dataset. can arise due measurement variability, data entry errors, genuinely unique observations. Identifying handling outliers crucial, can skew statistical analyses, affect model performance, lead misleading insights.Outliers play critical role multiple industries:Finance: Outliers transaction data can indicate fraud. Detecting unusually high spending patterns key fraud detection models.Healthcare: Medical records often contain anomalous lab results, may indicate rare diseases measurement errors.Manufacturing: Sensors factories may detect equipment failures unusual temperature spikes.many cases, outliers errors signals important events. Understanding role data analysis ensures don’t remove valuable insights unintentionally.","code":""},{"path":"chapter-data-prep.html","id":"identifying-outliers-using-visualization-techniques","chapter":"3 Data Preparation","heading":"Identifying Outliers Using Visualization Techniques","text":"","code":""},{"path":"chapter-data-prep.html","id":"boxplots-detecting-extreme-values","chapter":"3 Data Preparation","heading":"Boxplots: Detecting Extreme Values","text":"Boxplots visual tool detecting extreme values. boxplot y variable (diamond width) using ggplot() geom_boxplot() functions ggplot2 package:, boxplots highlight values beyond whiskers, may indicate potential outliers. Since diamonds width 0 mm, values like 32 mm 59 mm likely result data entry errors.","code":"\nggplot(data = diamonds) +\n    geom_boxplot(mapping = aes(y = y))"},{"path":"chapter-data-prep.html","id":"histograms-understanding-outlier-distribution","chapter":"3 Data Preparation","heading":"Histograms: Understanding Outlier Distribution","text":"Histograms provide another visual approach detecting outliers displaying frequency distribution values. histogram y variable using ggplot() geom_histogram() functions:enhance visibility, can zoom smaller frequencies using coord_cartesian() function ggplot2 package:useful visualization techniques include:Violin plots – Show outliers density distributions.Density plots – Provide smoother insights rare values multimodal distributions.","code":"\nggplot(data = diamonds) +\n    geom_histogram(aes(x = y), binwidth = 0.5, color = 'blue', fill = \"lightblue\")\nggplot(data = diamonds) +\n    geom_histogram(mapping = aes(x = y), binwidth = 0.5, color = 'blue', fill = \"lightblue\") + \n    coord_cartesian(ylim = c(0, 30))"},{"path":"chapter-data-prep.html","id":"handling-outliers-best-practices","chapter":"3 Data Preparation","heading":"Handling Outliers: Best Practices","text":"outliers identified, several strategies handling :Removing outliers: appropriate outlier clearly error (e.g., negative height, duplicate data entry).Transforming values: Techniques log transformation square root scaling can reduce influence extreme values preserving trends.Winsorization: Instead removing outliers, replace nearest percentile-based value (e.g., capping extreme values 95th percentile).Using robust statistical methods: algorithms, like median-based regression random forests, less sensitive outliers.Treating outliers separate category: fraud detection rare event prediction, outliers may contain valuable insights removed.Choosing right strategy depends context analysis potential impact outlier.","code":""},{"path":"chapter-data-prep.html","id":"expanded-code-example-handling-outliers-in-r","chapter":"3 Data Preparation","heading":"Expanded Code Example: Handling Outliers in R","text":"detecting outliers, can choose either replace NA values remove . , consider using mutate() function dplyr package. ’s example treating outliers missing values using mutate() ifelse():’s verify update:method ensures outliers distort dataset allowing imputation analysis.","code":"\ndiamonds_2 <- mutate(diamonds, y = ifelse(y == 0 | y > 30, NA, y))summary(diamonds_2$y)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     3.680   4.720   5.710   5.734   6.540  10.540       9"},{"path":"chapter-data-prep.html","id":"missing-values","chapter":"3 Data Preparation","heading":"3.4 Missing Values","text":"Missing values pose significant challenges data analysis, can lead biased results, reduce statistical power, impact performance machine learning models. handling missing data, typically consider two approaches:Imputation: Replacing missing values estimated values retain data integrity.Removal: Deleting records missing values, though may lead data loss potential bias.","code":""},{"path":"chapter-data-prep.html","id":"imputation-techniques","chapter":"3 Data Preparation","heading":"Imputation Techniques","text":"several strategies imputing missing values, different use cases:Mean, median, mode imputation: Replaces missing values mean, median, mode corresponding column.Random sampling: Fills missing values random observations drawn existing data distribution.Predictive imputation: Uses machine learning models regression k-nearest neighbors estimate missing values.Multiple imputation: Generates several possible values missing entries averages results reduce uncertainty.\n### Example: Random Sampling Imputation R {-}impute missing values y using random sampling, use impute() function Hmisc package:impute() function replaces missing values randomly sampled values existing distribution y, maintaining overall statistical properties dataset.","code":"\ndiamonds_2$y <- impute(diamonds_2$y, \"random\")"},{"path":"chapter-data-prep.html","id":"best-practices","chapter":"3 Data Preparation","heading":"Best Practices","text":"Use mean median imputation numerical variables missing values missing random (MAR).Use mode imputation categorical variables.Consider predictive models dataset large missing values completely random.Always assess proportion missing data—many values missing, removing variable may better approach imputation.","code":""},{"path":"chapter-data-prep.html","id":"feature-scaling","chapter":"3 Data Preparation","heading":"3.5 Feature Scaling","text":"Feature scaling, also known normalization standardization, crucial step data preprocessing. adjusts range distribution numerical features similar scale. Many machine learning algorithms, especially based distance metrics k-nearest neighbors, benefit significantly scaled input features, prevents variables larger ranges disproportionately influencing model’s outcome.instance, diamonds dataset, carat variable ranges 0.2 5, price ranges 326 18823. Without scaling, variables like price wider range can dominate model’s predictions, potentially leading suboptimal results. address , apply feature scaling techniques bring numeric variables onto comparable scale. section, explore two common scaling methods:Min-Max Scaling: Also known min-max normalization min-max transformation.Z-score Scaling: Also known standardization Z-score normalization.Feature scaling provides several benefits:Improved Model Performance: Ensures features contribute equally model, preventing features larger numerical ranges dominating learning algorithms.Better Model Convergence: Particularly useful gradient-based optimization methods logistic regression neural networks.Effective Distance-Based Learning: Algorithms k-means clustering support vector machines rely distance calculations, making feature scaling essential.Consistent Feature Interpretation: standardizing numerical values, models become easier compare interpret.However, feature scaling also drawbacks:Potential Loss Information: cases, scaling can obscure meaningful differences data points.Impact Outliers: Min-max scaling, particular, sensitive extreme values, can distort scaled representation.Additional Computation: Scaling adds preprocessing overhead, particularly working large datasets.Reduced Interpretability: original units measurement lost, making harder relate scaled values real-world meanings.Selecting right scaling method depends characteristics data requirements model. next sections, explore methods detail apply diamonds dataset.","code":""},{"path":"chapter-data-prep.html","id":"min-max-scaling","chapter":"3 Data Preparation","heading":"3.6 Min-Max Scaling","text":"Min-Max Scaling transforms values feature fixed range, typically \\([0, 1]\\). transformation ensures minimum value feature becomes 0 maximum value becomes 1. especially useful algorithms rely distance metrics, equalizes contributions features, making comparisons balanced.formula Min-Max Scaling :\\[\nx_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}},\n\\]\n\\(x\\) original feature value, \\(x_{\\text{min}}\\) \\(x_{\\text{max}}\\) minimum maximum values feature, \\(x_{\\text{scaled}}\\) scaled value, ranging 0 1.Min-Max Scaling particularly useful models require bounded input values, neural networks algorithms relying gradient-based optimization. However, method sensitive outliers, extreme values significantly affect scaled distribution.Example 3.1  demonstrate Min-Max Scaling, ’ll apply carat variable diamonds dataset, carat values range approximately 0.2 5. Using minmax() function liver package, can scale carat values fit within range [0, 1].first histogram (left) shows distribution carat without scaling, second histogram (right) shows Min-Max Scaling. scaling, carat values compressed range 0 1, allowing comparable features may different original scales. scaling method particularly beneficial distance-based algorithms, prevents features wider ranges undue influence.","code":"\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` without scaling\") + \n  xlab(\"Values for variable `carat`\")\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = minmax(carat)), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` with Min-Max Scaling\") + \n  xlab(\"Values for variable `carat`\")"},{"path":"chapter-data-prep.html","id":"z-score-scaling","chapter":"3 Data Preparation","heading":"3.7 Z-score Scaling","text":"Z-score Scaling, also known standardization, transforms feature values mean 0 standard deviation 1. method particularly useful algorithms assume normally distributed data, linear regression logistic regression, centers data around 0 normalizes spread values.formula Z-score Scaling :\\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)}\n\\]\\(x\\) original feature value, \\(\\text{mean}(x)\\) mean feature, \\(\\text{sd}(x)\\) standard deviation feature, \\(x_{\\text{scaled}}\\) standardized value, now mean 0 standard deviation 1.Z-score Scaling particularly beneficial models assume normality use gradient-based optimization, ensuring numerical features contribute equally. However, since relies mean standard deviation, sensitive outliers, can distort transformation.Example 3.2  Applying Z-score Scaling carat variable diamonds dataset, mean standard deviation carat approximately 0.8 0.47, respectively. use zscore() function liver package standardize values.first histogram (left) displays distribution carat without scaling, second histogram (right) shows distribution Z-score Scaling. transformation makes feature values comparable across different scales ensures feature contributes equally distance-based computations model training.Note: common misconception Z-score Scaling, data follows standard normal distribution. Z-score Scaling centers data mean 0 scales standard deviation 1, alter shape distribution. original distribution skewed, remain skewed scaling, seen histograms .choice Min-Max Scaling Z-score Scaling depends requirements model characteristics data. Min-Max Scaling preferable algorithms require fixed input range, Z-score Scaling better suited models assume normally distributed features. selecting appropriate scaling method, ensure balanced feature contributions improved model performance.","code":"\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` without scaling\") + \n  xlab(\"Values for variable `carat`\")\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = zscore(carat)), bins = 30,\n                 color = 'blue', fill = \"lightblue\") +\n  ggtitle(\"Histogram for `carat` with Z-score Scaling\") + \n  xlab(\"Values for variable `carat`\")"},{"path":"chapter-data-prep.html","id":"how-to-reexpress-categorical-field-values","chapter":"3 Data Preparation","heading":"3.8 How to Reexpress Categorical Field Values","text":"data science, categorical features often need transformed numeric format can used machine learning models. Algorithms like decision trees, neural networks, linear regression require numeric inputs process data effectively. Converting categorical variables numerical representations ensures features contribute appropriately model, rather ignored treated incorrectly.process reexpressing categorical values crucial part data preparation, enables us leverage full range features dataset. section, explore several methods convert categorical fields numeric representations, focus techniques like one-hot encoding ordinal encoding. demonstrate techniques using diamonds dataset, includes several categorical features cut, color, clarity.","code":""},{"path":"chapter-data-prep.html","id":"why-reexpress-categorical-fields","chapter":"3 Data Preparation","heading":"3.8.1 Why Reexpress Categorical Fields?","text":"Categorical fields, also known nominal ordinal variables, often represent qualitative aspects data, product types, user locations, levels satisfaction. diamonds dataset, example:cut indicates quality diamond’s cut (e.g., “Fair,” “Good,” “Good,” “Premium,” “Ideal”).color represents diamond’s color grade (e.g., “D,” “E,” “F,” “D” colorless thus valuable).clarity describes diamond’s clarity, reflecting absence internal external flaws.fields essential understanding predicting diamond pricing, raw form text labels, suitable machine learning algorithms. Transforming numeric form allows us include valuable insights analysis.","code":""},{"path":"chapter-data-prep.html","id":"techniques-for-reexpressing-categorical-variables","chapter":"3 Data Preparation","heading":"3.8.2 Techniques for Reexpressing Categorical Variables","text":"several approaches converting categorical variables numeric representations. method choose depends type categorical variable nature data.","code":""},{"path":"chapter-data-prep.html","id":"ordinal-encoding","chapter":"3 Data Preparation","heading":"Ordinal Encoding","text":"Ordinal encoding suitable categorical variable meaningful order. example, cut feature diamonds dataset ordinal, natural hierarchy “Fair” “Ideal.” ordinal encoding, category assigned unique integer based rank level importance.example, might assign values follows:“Fair” → 1“Good” → 2“Good” → 3“Premium” → 4“Ideal” → 5This approach preserves order categories, can useful models interpret numeric values relative way, linear regression. However, important apply ordinal encoding order meaningful. non-ordinal variables, methods like one-hot encoding appropriate.","code":""},{"path":"chapter-data-prep.html","id":"one-hot-encoding","chapter":"3 Data Preparation","heading":"One-Hot Encoding","text":"One-hot encoding preferred technique nominal variables—categorical fields without intrinsic order. approach, unique category field transformed new binary (0 1) feature. method particularly useful variables like color clarity diamonds dataset, categories follow clear sequence.example, one-hot encode color feature, create set binary columns, one color grade:color_D: 1 diamond color “D,” 0 otherwise.color_E: 1 diamond color “E,” 0 otherwise.color_F: 1 diamond color “F,” 0 otherwise.One-hot encoding avoids introducing false ordinal relationships, ensuring model treats category independent entity. However, one downside can significantly increase dimensionality dataset categorical field many unique values.Note: Many machine learning libraries automatically drop one binary columns avoid multicollinearity (perfect correlation among features). instance, seven color categories, six binary columns created, missing category implied columns zero. approach, known dummy encoding, helps avoid redundancy keeps model simpler.","code":""},{"path":"chapter-data-prep.html","id":"frequency-encoding","chapter":"3 Data Preparation","heading":"Frequency Encoding","text":"Another useful approach, especially high-cardinality categorical variables (many unique values), frequency encoding. technique replaces category frequency dataset, allowing model capture information common category . Frequency encoding can particularly helpful fields like clarity want give model indication prevalent level .example:“VS2” appears 10,000 times dataset, encoded 10,000.“” appears 500 times, encoded 500.Frequency encoding less commonly used basic machine learning workflows can valuable dealing large datasets, one-hot encoding introduce many columns. However, cautious approach, may inadvertently add implicit weight common categories.","code":""},{"path":"chapter-data-prep.html","id":"choosing-the-right-encoding-technique","chapter":"3 Data Preparation","heading":"3.8.3 Choosing the Right Encoding Technique","text":"Selecting appropriate encoding technique depends nature categorical variable requirements analysis:Ordinal variables (like cut): Use ordinal encoding preserve natural order.Nominal variables unique values (like color clarity): Use one-hot encoding represent category binary column.High-cardinality categorical variables: Consider frequency encoding one-hot encoding introduce many features.Example 3.3  Applying techniques diamonds dataset:example:Ordinal Encoding: encoded cut variable based quality hierarchy.One-Hot Encoding: applied one-hot encoding color, creating binary columns color grade.encoding categorical fields way, transform dataset format compatible machine learning algorithms preserving essential information categorical feature.dataset now cleaned, scaled, encoded, ready move next stage data analysis. upcoming chapter, explore Exploratory Data Analysis (EDA), use visualizations summary statistics gain insights structure relationships within data. combining prepared data EDA techniques, can better understand features may hold predictive value model set stage successful machine learning outcomes.","code":"\n# Example: Ordinal encoding for `cut`\ndiamonds <- diamonds %>%\n  mutate(cut_encoded = as.integer(factor(cut, levels = c(\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"))))\n\n# Example: One-hot encoding for `color`\ndiamonds <- diamonds %>%\n  mutate(\n    color_D = ifelse(color == \"D\", 1, 0),\n    color_E = ifelse(color == \"E\", 1, 0),\n    color_F = ifelse(color == \"F\", 1, 0),\n    color_G = ifelse(color == \"G\", 1, 0),\n    color_H = ifelse(color == \"H\", 1, 0),\n    color_I = ifelse(color == \"I\", 1, 0),\n    color_J = ifelse(color == \"J\", 1, 0)\n  )"},{"path":"chapter-data-prep.html","id":"Data-pre-adult","chapter":"3 Data Preparation","heading":"3.9 Case Study: Who Can Earn More Than $50K Per Year?","text":"case study, explore Adult dataset, sourced US Census Bureau. dataset contains demographic information individuals, including age, education, occupation, income. dataset available liver package. details, refer documentation.goal study predict whether individual earns $50,000 per year based attributes. Section 11.5 Chapter 11, apply decision tree random forest algorithms build predictive model. applying techniques, need preprocess dataset handling missing values, encoding categorical variables, scaling numerical features. Let’s begin loading dataset examining structure.","code":""},{"path":"chapter-data-prep.html","id":"overview-of-the-dataset","chapter":"3 Data Preparation","heading":"Overview of the Dataset","text":"use Adult dataset, first ensure liver package installed. , install using:Next, load package dataset:inspect dataset structure, use:dataset contains 48598 records 15 variables. , 14 predictors, target variable, income, categorical variable two levels: <=50K >50K. features include numerical categorical variables:age: Age years (numerical).workclass: Employment type (categorical, 6 levels).demogweight: Census weighting factor (numerical).education: Highest level education (categorical, 16 levels).education.num: Number years education (numerical).marital.status: Marital status (categorical, 5 levels).occupation: Job category (categorical, 15 levels).relationship: Family relationship status (categorical, 6 levels).race: Racial background (categorical, 5 levels).gender: Gender identity (categorical, Male/Female).capital.gain: Capital gains (numerical).capital.loss: Capital losses (numerical).hours.per.week: Hours worked per week (numerical).native.country: Country origin (categorical, 42 levels).income: Target variable indicating annual income (<=50K >50K).clarity, categorize dataset’s variables:Nominal variables: workclass, marital.status, occupation, relationship, race, native.country, gender.Ordinal variable: education.Numerical variables: age, demogweight, education.num, capital.gain, capital.loss, hours.per.week.better understand dataset, generate summary statistics:summary provides insights distribution numerical variables, missing values, categorical variable levels, guiding us preparing data analysis.","code":"\ninstall.packages(\"liver\")\nlibrary(liver)  # Load liver package\ndata(adult)     # Load Adult datasetstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education.num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital.status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital.gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital.loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours.per.week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native.country: Factor w/ 42 levels \"?\",\"Cambodia\",..: 40 40 40 40 40 40 40 40 40 40 ...\n    $ income        : Factor w/ 2 levels \"<=50K\",\">50K\": 1 1 2 2 1 1 1 2 1 1 ...summary(adult)\n         age              workclass      demogweight             education    \n    Min.   :17.0   ?           : 2794   Min.   :  12285   HS-grad     :15750  \n    1st Qu.:28.0   Gov         : 6536   1st Qu.: 117550   Some-college:10860  \n    Median :37.0   Never-worked:   10   Median : 178215   Bachelors   : 7962  \n    Mean   :38.6   Private     :33780   Mean   : 189685   Masters     : 2627  \n    3rd Qu.:48.0   Self-emp    : 5457   3rd Qu.: 237713   Assoc-voc   : 2058  \n    Max.   :90.0   Without-pay :   21   Max.   :1490400   11th        : 1812  \n                                                          (Other)     : 7529  \n    education.num         marital.status            occupation   \n    Min.   : 1.00   Divorced     : 6613   Craft-repair   : 6096  \n    1st Qu.: 9.00   Married      :22847   Prof-specialty : 6071  \n    Median :10.00   Never-married:16096   Exec-managerial: 6019  \n    Mean   :10.06   Separated    : 1526   Adm-clerical   : 5603  \n    3rd Qu.:12.00   Widowed      : 1516   Sales          : 5470  \n    Max.   :16.00                         Other-service  : 4920  \n                                          (Other)        :14419  \n            relationship                   race          gender     \n    Husband       :19537   Amer-Indian-Eskimo:  470   Female:16156  \n    Not-in-family :12546   Asian-Pac-Islander: 1504   Male  :32442  \n    Other-relative: 1506   Black             : 4675                 \n    Own-child     : 7577   Other             :  403                 \n    Unmarried     : 5118   White             :41546                 \n    Wife          : 2314                                            \n                                                                    \n     capital.gain      capital.loss     hours.per.week        native.country \n    Min.   :    0.0   Min.   :   0.00   Min.   : 1.00   United-States:43613  \n    1st Qu.:    0.0   1st Qu.:   0.00   1st Qu.:40.00   Mexico       :  949  \n    Median :    0.0   Median :   0.00   Median :40.00   ?            :  847  \n    Mean   :  582.4   Mean   :  87.94   Mean   :40.37   Philippines  :  292  \n    3rd Qu.:    0.0   3rd Qu.:   0.00   3rd Qu.:45.00   Germany      :  206  \n    Max.   :41310.0   Max.   :4356.00   Max.   :99.00   Puerto-Rico  :  184  \n                                                        (Other)      : 2507  \n      income     \n    <=50K:37155  \n    >50K :11443  \n                 \n                 \n                 \n                 \n   "},{"path":"chapter-data-prep.html","id":"missing-values-1","chapter":"3 Data Preparation","heading":"3.9.1 Missing Values","text":"summary() function reveals variables workclass native.country contain missing values, represented \"?\" category. Specifically, 2794 records workclass 847 records native.country missing values. Since \"?\" used placeholder missing data, first convert entries NA:replacing \"?\" NA, remove unused factor levels clean dataset:visualize distribution missing values, use gg_miss_var() function naniar package:plot indicates workclass, occupation, native.country contain missing values. percentage missing values variables relatively low, workclass occupation less 0.06 percent missing data, native.country 0.02 percent.","code":"\nadult[adult == \"?\"] = NA\nadult = droplevels(adult)\nlibrary(naniar)  # Load package for visualizing missing values\n\ngg_miss_var(adult, show_pct = TRUE)"},{"path":"chapter-data-prep.html","id":"imputing-missing-values","chapter":"3 Data Preparation","heading":"Imputing Missing Values","text":"Instead removing records missing values, can lead information loss, apply random imputation, missing values filled randomly selected values existing distribution variable. maintains natural proportions category.use impute() function Hmisc package purpose:confirm missing values successfully imputed, generate another missing values plot:updated plot show missing values, indicating successful imputation.","code":"\nlibrary(Hmisc)  # Load package for imputation\n\n# Impute missing values using random sampling from existing categories\nadult$workclass      = impute(adult$workclass,      'random')\nadult$native.country = impute(adult$native.country, 'random')\nadult$occupation     = impute(adult$occupation,     'random')\ngg_miss_var(adult, show_pct = TRUE)"},{"path":"chapter-data-prep.html","id":"alternative-approaches","chapter":"3 Data Preparation","heading":"Alternative Approaches","text":"impute() function allows different statistical methods mean, median, mode imputation. default behavior median imputation. advanced techniques, aregImpute() function Hmisc package offers predictive imputation using additive regression, bootstrapping, predictive mean matching.Although removing records missing values using na.omit() option, generally discouraged unless missing values excessive biased way distort analysis.properly handling missing values, ensure data completeness maintain integrity dataset subsequent preprocessing steps, recoding categorical variables grouping country-level data broader regions.","code":""},{"path":"chapter-data-prep.html","id":"encoding-categorical-variables","chapter":"3 Data Preparation","heading":"3.9.2 Encoding Categorical Variables","text":"Categorical variables often contain large number unique values, making challenging use predictive models. Adult dataset, native.country workclass multiple categories, can introduce complexity redundancy. simplify variables, group similar categories together preserving interpretability.","code":""},{"path":"chapter-data-prep.html","id":"grouping-native.country-by-continent","chapter":"3 Data Preparation","heading":"Grouping native.country by Continent","text":"native.country variable contains 41 distinct countries. make manageable, categorize countries broader geographical regions:Europe: England, France, Germany, Greece, Netherlands, Hungary, Ireland, Italy, Poland, Portugal, Scotland, YugoslaviaAsia: China, Hong Kong, India, Iran, Cambodia, Japan, Laos, Philippines, Vietnam, Taiwan, ThailandNorth America: Canada, United States, Puerto RicoSouth America: Colombia, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Mexico, Nicaragua, Outlying US territories, Peru, Jamaica, Trinidad & TobagoOther: includes ambiguous “South” category, meaning unclear dataset documentation.\nuse fct_collapse() function forcats package reassign categories:confirm changes, display frequency distribution native.country:grouping original 42 countries 5 broader regions, simplify variable maintaining relevance analysis.","code":"\nlibrary(forcats)  # Load package for categorical variable transformation\n\n# To create a new factor variable with fewer levels for `native.country`\nEurope = c(\"England\", \"France\", \"Germany\", \"Greece\", \"Holand-Netherlands\", \"Hungary\", \"Ireland\", \"Italy\", \"Poland\", \"Portugal\", \"Scotland\", \"Yugoslavia\")\n\nAsia = c(\"China\", \"Hong\", \"India\", \"Iran\", \"Cambodia\", \"Japan\", \"Laos\", \"Philippines\", \"Vietnam\", \"Taiwan\", \"Thailand\")\n\nN.America = c(\"Canada\", \"United-States\", \"Puerto-Rico\")\n\nS.America = c(\"Columbia\", \"Cuba\", \"Dominican-Republic\", \"Ecuador\", \"El-Salvador\", \"Guatemala\", \"Haiti\", \"Honduras\", \"Mexico\", \"Nicaragua\", \"Outlying-US(Guam-USVI-etc)\", \"Peru\", \"Jamaica\", \"Trinadad&Tobago\")\n\n# Reclassify native.country into broader regions\nadult$native.country = fct_collapse(adult$native.country, \n                                    \"Europe\"    = Europe,\n                                    \"Asia\"      = Asia,\n                                    \"N.America\" = N.America,\n                                    \"S.America\" = S.America,\n                                    \"Other\"     = c(\"South\") )table(adult$native.country)\n   \n        Asia N.America S.America    Europe     Other \n         993     44747      1946       797       115"},{"path":"chapter-data-prep.html","id":"simplifying-workclass","chapter":"3 Data Preparation","heading":"Simplifying workclass","text":"workclass variable originally contains several employment categories. Since “Never-worked” “Without-pay” represent similar employment statuses, merge single category labeled “Unemployed”:verify updated categories, check frequency distribution:reducing number unique categories workclass native.country, improve model interpretability reduce risk overfitting applying machine learning algorithms.","code":"\nadult$workclass = fct_collapse(adult$workclass, \"Unemployed\" = c(\"Never-worked\", \"Without-pay\"))table(adult$workclass)\n   \n          Gov Unemployed    Private   Self-emp \n         6919         32      35851       5796"},{"path":"chapter-data-prep.html","id":"outliers","chapter":"3 Data Preparation","heading":"3.9.3 Outliers","text":"Detecting handling outliers essential step data preprocessing, extreme values can significantly impact statistical analysis model performance. , examine potential outliers capital.loss variable determine whether adjustments necessary.","code":""},{"path":"chapter-data-prep.html","id":"summary-statistics","chapter":"3 Data Preparation","heading":"Summary Statistics","text":"gain initial understanding capital.loss, compute summary statistics:summary output reveals following insights:minimum value 0, maximum 4356.median 0, significantly lower mean, indicating highly skewed distribution.75% observations capital loss 0, confirming strong right-skew.mean capital loss 87.94, influenced small number extreme values.","code":"summary(adult$capital.loss)\n      Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0.00    0.00    0.00   87.94    0.00 4356.00"},{"path":"chapter-data-prep.html","id":"visualizing-outliers","chapter":"3 Data Preparation","heading":"Visualizing Outliers","text":"investigate distribution capital.loss, use boxplot histogram:plots, observe:boxplot shows strong positive skew, many extreme values upper whisker.histogram indicates observations zero capital loss, cases around 2,000 4,000.Since large proportion observations report capital loss, examine nonzero cases.","code":"\nggplot(data = adult, aes(y = capital.loss)) +\n     geom_boxplot()\nggplot(data = adult, aes(x = capital.loss)) +\n     geom_histogram(bins = 30, color = \"blue\", fill = \"lightblue\")"},{"path":"chapter-data-prep.html","id":"zooming-into-the-nonzero-distribution","chapter":"3 Data Preparation","heading":"Zooming into the Nonzero Distribution","text":"better visualize spread nonzero values, focus observations capital.loss > 0:Key takeaways refined plots:majority nonzero values 500, small number extending beyond 4,000.distribution nonzero values approximately symmetric, suggesting extreme values, follow structured pattern rather random anomalies.","code":"\nggplot(data = adult, mapping = aes(x = capital.loss)) +\n    geom_histogram(bins = 30, color = \"blue\", fill = \"lightblue\") +\n    coord_cartesian(xlim = c(500, 4000), ylim = c(0, 1000))\nggplot(data = subset(adult, capital.loss > 0)) +\n     geom_boxplot(aes(y = capital.loss)) "},{"path":"chapter-data-prep.html","id":"handling-outliers","chapter":"3 Data Preparation","heading":"Handling Outliers","text":"Although capital.loss contains many high values, appear erroneous. Instead, reflect genuine cases within dataset. Since values provide meaningful information particular individuals, retain rather applying transformations removals.However, model performance significantly affected extreme values, might consider:Winsorization: Capping values reasonable percentile (e.g., 95th percentile).Log Transformation: Applying log transformation reduce skewness.Creating Binary Indicator: Introducing new variable indicating whether capital loss occurred (capital.loss > 0).Next, perform similar outlier analysis capital.gain variable. See exercises guided approach.","code":""},{"path":"chapter-data-prep.html","id":"exercises-1","chapter":"3 Data Preparation","heading":"3.10 Exercises","text":"section provides hands-exercises reinforce key concepts covered chapter. questions include theoretical, exploratory, practical challenges related data types, outliers, encoding techniques, feature engineering.","code":""},{"path":"chapter-data-prep.html","id":"understanding-data-types","chapter":"3 Data Preparation","heading":"Understanding Data Types","text":"difference continuous discrete numerical variables? Provide example real-world data.ordinal categorical variables differ nominal categorical variables? Give example .","code":""},{"path":"chapter-data-prep.html","id":"exploring-the-diamonds-dataset","chapter":"3 Data Preparation","heading":"Exploring the diamonds Dataset","text":"Report summary statistics diamonds dataset using summary() function. insights can derive output?diamonds dataset, variables nominal, ordinal, numerical? List accordingly.","code":""},{"path":"chapter-data-prep.html","id":"detecting-and-handling-outliers","chapter":"3 Data Preparation","heading":"Detecting and Handling Outliers","text":"Identify outliers variable x. exist, handle appropriately. Follow approach Section 3.3 y variable diamonds dataset.Repeat outlier detection process variable z. necessary, apply transformations filtering techniques.Check outliers depth variable. method use detect handle ?","code":""},{"path":"chapter-data-prep.html","id":"encoding-categorical-variables-1","chapter":"3 Data Preparation","heading":"Encoding Categorical Variables","text":"cut variable diamonds dataset ordinal. can encode properly using ordinal encoding?color variable diamonds dataset nominal. can encode using one-hot encoding?","code":""},{"path":"chapter-data-prep.html","id":"analyzing-the-adult-dataset","chapter":"3 Data Preparation","heading":"Analyzing the Adult Dataset","text":"Load Adult dataset liver package examine structure. Identify categorical variables classify nominal ordinal.Compute proportion individuals earn 50K (>50K). distribution tell income levels dataset?Adult dataset, generate summary statistics, boxplot, histogram variable capital.gain. observe?Based visualizations previous question, outliers capital.gain variable? , suggest strategy handle .","code":""},{"path":"chapter-data-prep.html","id":"feature-engineering-challenge","chapter":"3 Data Preparation","heading":"Feature Engineering Challenge","text":"Create new categorical variable Age_Group Adult dataset, grouping ages :Young (≤30 years old)Middle-aged (31-50 years old)Senior (>50 years old)\nUse cut() function implement transformation.Compute mean capital.gain Age_Group. insights gain income levels across different age groups?","code":""},{"path":"chapter-data-prep.html","id":"advanced-data-preparation-challenges","chapter":"3 Data Preparation","heading":"Advanced Data Preparation Challenges","text":"Adult dataset, education variable contains 16 distinct levels. Reduce categories broader groups “Diploma,” “High School Graduate,” “College,” “Postgraduate.” Implement transformation using fct_collapse() function.Adult dataset, education variable contains 16 distinct levels. Reduce categories broader groups “Diploma,” “High School Graduate,” “College,” “Postgraduate.” Implement transformation using fct_collapse() function.capital.gain capital.loss variables represent financial assets. Create new variable net.capital computes difference capital.gain capital.loss. Analyze distribution.capital.gain capital.loss variables represent financial assets. Create new variable net.capital computes difference capital.gain capital.loss. Analyze distribution.Perform Min-Max scaling numerical variables Adult dataset (age, capital.gain, capital.loss, hours.per.week). Use mutate() function apply transformation.Perform Min-Max scaling numerical variables Adult dataset (age, capital.gain, capital.loss, hours.per.week). Use mutate() function apply transformation.Perform Z-score normalization set numerical variables. Compare results Min-Max scaling. scenarios one approach preferable ?Perform Z-score normalization set numerical variables. Compare results Min-Max scaling. scenarios one approach preferable ?Construct logistic regression model predict whether individual earns 50K (>50K) based selected numerical features (age, education.num, hours.per.week). Preprocess data accordingly interpret coefficients model.Construct logistic regression model predict whether individual earns 50K (>50K) based selected numerical features (age, education.num, hours.per.week). Preprocess data accordingly interpret coefficients model.","code":""},{"path":"chapter-EDA.html","id":"chapter-EDA","chapter":"4 Exploratory Data Analysis","heading":"4 Exploratory Data Analysis","text":"Exploratory Data Analysis (EDA) fundamental step data science helps uncover insights, detect patterns, understand relationships within dataset. applying statistical models machine learning algorithms, essential explore visualize data identify inconsistencies, anomalies, potential predictors. EDA provides structured yet flexible approach data exploration, allowing data scientists make informed decisions proceeding modeling.EDA iterative process rather rigid sequence steps. encourages curiosity adaptability, different datasets may present unique challenges insights. exploratory paths may lead inconclusive findings, others uncover valuable patterns shape direction analysis. familiarity data increases, analysts can refine focus, identifying relevant features variables modeling.primary goal EDA explore summarize data rather perform hypothesis testing confirm specific relationships. using summary statistics, visualizations, preliminary correlation analysis, EDA provides foundational understanding dataset. However, insights remain preliminary require validation statistical testing predictive modeling. Recognizing distinction ensures exploratory findings interpreted cautiously lead premature conclusions.Another key consideration EDA balancing statistical significance practical relevance. Large datasets often reveal statistically significant relationships may lack meaningful real-world implications. example, weak correlation customer engagement churn might statistically significant yet offer little actionable insight business decision-making. EDA encourages analysts integrate domain expertise practical considerations interpreting patterns data.EDA also plays crucial role data cleaning preparation. Missing values, inconsistencies, outliers often become apparent exploration, requiring careful handling ensure data quality. data cleaning distinct process, closely linked EDA, identifying resolving data issues early helps establish solid foundation analysis modeling.Selecting appropriate tools techniques EDA depends nature dataset specific analytical questions. Histograms box plots help visualize distributions, scatter plots correlation matrices assess relationships variables. following sections demonstrate techniques practical applications, including -depth analysis churn dataset illustrate EDA can uncover patterns relevant customer retention.key objectives EDA :Understanding structure data – Determine data types, variable ranges, number observations, presence missing values anomalies.Analyzing individual variable distributions – Examine numerical categorical variables assess distribution, central tendency, spread.Exploring relationships variables – Identify correlations, dependencies, interactions may influence predictive modeling.Detecting patterns outliers – Identify unusual data points assess whether result data errors represent meaningful trends.objectives ensure comprehensive understanding dataset, forming strong foundation subsequent modeling analysis.","code":""},{"path":"chapter-EDA.html","id":"guiding-questions-for-eda","chapter":"4 Exploratory Data Analysis","heading":"4.1 Guiding Questions for EDA","text":"Exploratory data analysis effective structured around key questions help uncover meaningful patterns relationships. questions generally fall two broad categories: univariate multivariate analysis.Univariate analysis examines individual variables assess distributions, central tendencies, variability, potential data quality issues missing values outliers. Typical univariate questions include:distribution target variable?numerical variables, income age, distributed?missing values, distributed across dataset?analysis helps detect skewness, irregularities, unexpected patterns may impact later modeling. Common tools univariate analysis include histograms, box plots, summary statistics mean, median, quartiles, standard deviation.Multivariate analysis explores relationships two variables, identifying dependencies, interactions, correlations influence predictive modeling. Key multivariate questions include:target variable relate predictor variables?predictors highly correlated, suggesting potential multicollinearity?categorical numerical variables interact?analyze relationships, data scientists commonly use scatter plots, correlation matrices, pairwise comparisons, help visualize dependencies variables guide feature selection.common challenge EDA choosing appropriate visualization statistical summary given analysis. selection depends type data insight sought. table provides structured guide selecting effective tools various exploratory tasks:Table 4.1: EDA Tool Selection Guide.systematically addressing univariate multivariate questions appropriate EDA techniques, gain deeper understanding dataset’s structure key patterns. process improves data quality also provides valuable insights inform subsequent modeling decision-making.","code":""},{"path":"chapter-EDA.html","id":"eda-as-data-storytelling","chapter":"4 Exploratory Data Analysis","heading":"4.2 EDA as Data Storytelling","text":"Exploratory Data Analysis just technical step; way uncover communicate meaningful insights data. Beyond summarizing numbers visualizing patterns, EDA helps shape narrative hidden within data. Effective data storytelling integrates data, visuals, context, making findings accessible actionable. Whether communicating data scientists, business professionals, decision-makers, presenting insights clearly essential.summary statistics provide overview, visualizations reveal patterns, relationships, anomalies might otherwise go unnoticed. Different types visualizations serve distinct purposes. Scatter plots correlation matrices highlight relationships numerical variables, histograms box plots illustrate distributions potential skewness. Categorical data best explored bar charts stacked visualizations, allowing comparisons across different groups. Choosing right visualization ensures insights accurate intuitive.strong narrative connects data insights real-world significance. Rather simply presenting correlation coefficient distribution plot, well-structured EDA report explains pattern matters informs decision-making. Instead stating customers high daytime phone usage higher churn rate, impactful provide context:“Customers extensive daytime usage significantly likely churn, possibly due pricing concerns dissatisfaction service quality. Targeted retention strategies, customized discounts flexible pricing plans, may help mitigate risk.”approach goes beyond numerical reporting, framing insights actionable strategies.Data storytelling widely used business, scientific research, journalism. Consider following examples illustrate visual storytelling enhances understanding.One example comes climate science. Figure 4.1 presents global mean surface temperature changes Common Era, highlighting long-term warming trends. Adapted Raphael Neukom et al.4, visualization provides historical perspective climate change, illustrating temperature anomalies time.\nFigure 4.1: Global mean surface temperature history Common Era. Temperature anomalies respect 1961–1990 CE. colored lines represent 30-year low-pass-filtered ensemble medians different reconstruction methods.\nAnother example focuses global health demographics. Figure 4.2 illustrates relationship fertility rate life expectancy across world regions 1960 2015. Adapted Hans Rosling’s TED Talk “New insights poverty”, visualization effectively conveys trends population health economic development time.\nFigure 4.2: Animated scatter plot fertility rate life expectancy birth different world regions 1960 2015.\nexamples highlight well-designed visualizations can make complex data accessible engaging.conducting EDA, essential consider broader narrative. Instead just reporting statistical results, think trend reveals, relevant, can inform decision-making. Integrating storytelling techniques ensures EDA serves data exploration step also communication tool connects technical analysis practical application.","code":""},{"path":"chapter-EDA.html","id":"EDA-sec-churn","chapter":"4 Exploratory Data Analysis","heading":"4.3 EDA in Practice: The Churn Dataset","text":"illustrate exploratory data analysis process, use churn dataset, contains information customer behavior, including whether customer churned (.e., left service) various demographic behavioral attributes.EDA help us understand dataset identifying patterns related customer churn, determining features influence retention, establishing structured approach predictive modeling. examining summary statistics visualizations, can extract meaningful insights moving machine learning techniques later chapters.","code":""},{"path":"chapter-EDA.html","id":"problem-understanding-1","chapter":"4 Exploratory Data Analysis","heading":"Problem Understanding","text":"Companies seek minimize customer churn identifying factors influence customer decisions. Key business questions include:customers leaving?primary factors contributing churn?can take action improve retention?EDA helps answer questions uncovering trends patterns customer behavior. Chapter 7, build predictive model identify customers likely churn. , essential explore data understand structure.","code":""},{"path":"chapter-EDA.html","id":"data-understanding","chapter":"4 Exploratory Data Analysis","heading":"Data Understanding","text":"churn dataset comes IBM Sample Data Sets contains 5000 customer records across 20 variables. dataset available liver package. target variable, churn, indicates whether customer left company. dataset includes mix categorical numerical variables:state : Categorical, state U.S. (51 states + D.C.).area.code : Categorical, area code assigned customer.account.length : Numerical (discrete), duration account activity (days).voice.plan : Categorical (binary), subscription voice mail plan (yes/).voice.messages : Numerical (discrete), number voice mail messages.intl.plan : Categorical (binary), subscription international calling plan (yes/).intl.mins : Numerical (continuous), total international call minutes.intl.calls : Numerical (discrete), total international calls made.intl.charge : Numerical (continuous), total international call charges.day.mins : Numerical (continuous), total daytime call minutes.day.calls : Numerical (discrete), total daytime calls made.day.charge : Numerical (continuous), total daytime call charges.eve.mins : Numerical (continuous), total evening call minutes.eve.calls : Numerical (discrete), total evening calls made.eve.charge : Numerical (continuous), total evening call charges.night.mins : Numerical (continuous), total nighttime call minutes.night.calls : Numerical (discrete), total nighttime calls made.night.charge : Numerical (continuous), total nighttime call charges.customer.calls : Numerical (discrete), number customer service calls made.churn : Categorical (binary), indicates whether customer churned (yes/).use dataset, first load liver package import dataset R:examine structure dataset, use:command reveals dataset stored data.frame 5000 observations 20 variables. target variable, churn, categorizes whether customer left service.summarize dataset, use:function provides high-level overview variables, including distributions potential missing values. dataset clean ready exploratory data analysis. next sections, explore structure using visualizations statistical summaries. help us identify key variables influencing churn ensure dataset well-prepared predictive modeling.One notable observation 51 unique states represented dataset, yet 3 unique area codes. suggests area codes correspond directly state locations, worth investigating .","code":"\nlibrary(liver)\ndata(churn)  str(churn)  \n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...summary(churn)\n        state              area.code    account.length  voice.plan\n    WV     : 158   area_code_408:1259   Min.   :  1.0   yes:1323  \n    MN     : 125   area_code_415:2495   1st Qu.: 73.0   no :3677  \n    AL     : 124   area_code_510:1246   Median :100.0             \n    ID     : 119                        Mean   :100.3             \n    VA     : 118                        3rd Qu.:127.0             \n    OH     : 116                        Max.   :243.0             \n    (Other):4240                                                  \n    voice.messages   intl.plan    intl.mins       intl.calls      intl.charge   \n    Min.   : 0.000   yes: 473   Min.   : 0.00   Min.   : 0.000   Min.   :0.000  \n    1st Qu.: 0.000   no :4527   1st Qu.: 8.50   1st Qu.: 3.000   1st Qu.:2.300  \n    Median : 0.000              Median :10.30   Median : 4.000   Median :2.780  \n    Mean   : 7.755              Mean   :10.26   Mean   : 4.435   Mean   :2.771  \n    3rd Qu.:17.000              3rd Qu.:12.00   3rd Qu.: 6.000   3rd Qu.:3.240  \n    Max.   :52.000              Max.   :20.00   Max.   :20.000   Max.   :5.400  \n                                                                                \n       day.mins       day.calls     day.charge       eve.mins       eve.calls    \n    Min.   :  0.0   Min.   :  0   Min.   : 0.00   Min.   :  0.0   Min.   :  0.0  \n    1st Qu.:143.7   1st Qu.: 87   1st Qu.:24.43   1st Qu.:166.4   1st Qu.: 87.0  \n    Median :180.1   Median :100   Median :30.62   Median :201.0   Median :100.0  \n    Mean   :180.3   Mean   :100   Mean   :30.65   Mean   :200.6   Mean   :100.2  \n    3rd Qu.:216.2   3rd Qu.:113   3rd Qu.:36.75   3rd Qu.:234.1   3rd Qu.:114.0  \n    Max.   :351.5   Max.   :165   Max.   :59.76   Max.   :363.7   Max.   :170.0  \n                                                                                 \n      eve.charge      night.mins     night.calls      night.charge   \n    Min.   : 0.00   Min.   :  0.0   Min.   :  0.00   Min.   : 0.000  \n    1st Qu.:14.14   1st Qu.:166.9   1st Qu.: 87.00   1st Qu.: 7.510  \n    Median :17.09   Median :200.4   Median :100.00   Median : 9.020  \n    Mean   :17.05   Mean   :200.4   Mean   : 99.92   Mean   : 9.018  \n    3rd Qu.:19.90   3rd Qu.:234.7   3rd Qu.:113.00   3rd Qu.:10.560  \n    Max.   :30.91   Max.   :395.0   Max.   :175.00   Max.   :17.770  \n                                                                     \n    customer.calls churn     \n    Min.   :0.00   yes: 707  \n    1st Qu.:1.00   no :4293  \n    Median :1.00             \n    Mean   :1.57             \n    3rd Qu.:2.00             \n    Max.   :9.00             \n   "},{"path":"chapter-EDA.html","id":"chapter-EDA-categorical","chapter":"4 Exploratory Data Analysis","heading":"4.4 Investigating Categorical Variables","text":"Categorical variables represent discrete values labels, names, binary indicators. churn dataset, key categorical features include state, area.code, voice.plan, intl.plan. Understanding distributions relationships target variable helps uncover trends may influence customer retention.begin, examine distribution target variable churn determine whether dataset balanced:bar plot reveals dataset imbalanced, customers staying (churn = \"\") leaving (churn = \"yes\"). proportion churners approximately 1.4 percent, proportion non-churners 8.6 percent. Since imbalanced data can impact predictive modeling, understanding churn patterns essential improving retention strategies.","code":"\nggplot(data = churn, aes(x = churn, label = scales::percent(prop.table(stat(count))))) +\n  geom_bar(fill = c(\"palevioletred1\", \"darkseagreen1\")) + \n  geom_text(stat = 'count', vjust = 0.2, size = 6)"},{"path":"chapter-EDA.html","id":"relationship-between-churn-and-subscription-plans","chapter":"4 Exploratory Data Analysis","heading":"Relationship Between Churn and Subscription Plans","text":"first analyze intl.plan, indicates whether customer international calling plan. binary variable, allows straightforward comparison churn rates subscribed non-subscribed customers.first plot (left) compares raw counts churners non-churners among customers without international plan. second plot (right) normalizes proportions, revealing customers international plan significantly higher churn rate.quantify relationship, generate contingency table:results confirm churn prevalent among customers subscribed international plan. suggests international service offerings may meeting customer expectations, leading higher attrition. Companies may need investigate whether pricing, service quality, competition influencing trend.","code":"\nggplot(data = churn) + \n  geom_bar(aes(x = intl.plan, fill = churn)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = churn) + \n  geom_bar(aes(x = intl.plan, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) addmargins(table(churn$churn, churn$intl.plan, \n                 dnn = c(\"Churn\", \"International Plan\")))\n        International Plan\n   Churn  yes   no  Sum\n     yes  199  508  707\n     no   274 4019 4293\n     Sum  473 4527 5000"},{"path":"chapter-EDA.html","id":"relationship-between-churn-and-voice-mail-plan","chapter":"4 Exploratory Data Analysis","heading":"Relationship Between Churn and Voice Mail Plan","text":"Next, examine voice.plan, indicates whether customer subscribed voice mail plan.Customers without voice mail plan appear churn slightly higher rate. confirmed using contingency table:difference less pronounced intl.plan, suggests customers actively use voice mail services may engaged therefore less likely leave.","code":"\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n\nggplot(data = churn) + \n  geom_bar(aes(x = voice.plan, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) addmargins(table(churn$churn, churn$voice.plan, dnn = c(\"Churn\", \"Voice Mail Plan\")))\n        Voice Mail Plan\n   Churn  yes   no  Sum\n     yes  102  605  707\n     no  1221 3072 4293\n     Sum 1323 3677 5000"},{"path":"chapter-EDA.html","id":"key-insights","chapter":"4 Exploratory Data Analysis","heading":"Key Insights","text":"Customers subscribed international plan significantly higher churn rate, indicating potential issue service expectations, pricing, customer satisfaction. variable likely important predictor churn models.Customers voice mail plan slightly lower churn rate, suggesting engagement additional services may contribute customer retention.insights highlight importance investigating product-specific factors analyzing churn, different subscription plans may varying impacts customer behavior.exploring categorical variables way, uncover actionable insights can inform predictive modeling business decisions aimed reducing customer churn. next section, examine numerical variables refine understanding customer behavior.","code":""},{"path":"chapter-EDA.html","id":"EDA-sec-numeric","chapter":"4 Exploratory Data Analysis","heading":"4.5 Investigating Numerical Variables","text":"now turn numerical variables churn dataset, examining distributions relationships target variable. Summary statistics provide initial understanding, visualizations histograms, box plots, density plots help reveal patterns potential predictors churn.","code":""},{"path":"chapter-EDA.html","id":"customer-service-calls-and-churn","chapter":"4 Exploratory Data Analysis","heading":"Customer Service Calls and Churn","text":"variable customer.calls represents number calls customer makes customer service. Since discrete numerical variable, use histogram examine distribution:histogram shows customers make service calls, smaller group contacts customer service frequently. right-skewed distribution suggests customers make unusually high number calls, potentially signaling dissatisfaction.investigate, overlay churn status:normalized histogram (right) reveals striking trend: customers making four service calls significantly higher churn rate. suggests frequent service interactions may indicate unresolved issues, leading customer dissatisfaction.Key Insights Business Implications:Customers making frequent service calls higher risk churning.Companies implement proactive retention strategies customer makes multiple calls, escalating issues offering incentives third call.variable likely strong predictor churn models included analysis.","code":"\nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls), \n                 bins = 10, fill = \"skyblue\", color = \"black\")\nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls, fill = churn), position = \"stack\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) \n  \nggplot(data = churn) +\n  geom_histogram(aes(x = customer.calls, fill = churn), position = \"fill\") +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) "},{"path":"chapter-EDA.html","id":"daytime-minutes-and-churn","chapter":"4 Exploratory Data Analysis","heading":"Daytime Minutes and Churn","text":"Next, examine day.mins, represents number minutes customer spends daytime calls. use box plots density plots compare distributions churners non-churners.box plot (left) shows customers churn tend higher daytime call usage. density plot (right) confirms , noticeable peak churners higher day.mins values.Key Insights Business Implications:High day.mins usage associated increased churn.Customers extensive daytime usage may dissatisfied pricing service quality.Targeted retention offers, flexible rate plans heavy users, help mitigate churn.","code":"\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = day.mins), \n                 fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = day.mins, fill = churn), alpha = 0.3)"},{"path":"chapter-EDA.html","id":"evening-and-nighttime-minutes","chapter":"4 Exploratory Data Analysis","heading":"Evening and Nighttime Minutes","text":"investigate whether evening nighttime call patterns also relate churn, plot eve.mins night.mins.slight trend suggests churners higher eve.mins, effect weaker day.mins. Similarly, analysis night.mins reveal clear distinction churners non-churners.similar distributions suggest nighttime call usage strong churn indicator.Key Insights Business Implications:Unlike daytime calls, evening nighttime minutes strongly predict churn.Focusing daytime usage service call patterns may yield better predictive power.statistical testing (e.g., t-tests logistic regression) confirm whether subtle differences exist.subsection well-structured provides clear overview analysis. However, areas improvement terms clarity, conciseness, flow. improved version maintains structure meaning enhancing readability coherence.","code":"\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = eve.mins), fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = eve.mins, fill = churn), alpha = 0.3)\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = night.mins), fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = night.mins, fill = churn), alpha = 0.3)"},{"path":"chapter-EDA.html","id":"international-calls-and-churn","chapter":"4 Exploratory Data Analysis","heading":"International Calls and Churn","text":"now examine intl.calls, represents total number international calls made customers. explore relationship churn, visualize distribution using box plots density plots.box plot (left) indicates churners (churn=yes) tend make slightly fewer international calls non-churners. density plot (right) supports , showing minor difference distribution two groups.Although slight trend suggesting churners make fewer international calls average, difference appear substantial. suggests intl.calls strong predictor churn. confirm whether relationship statistically significant, testing—two-sample t-test logistic regression—required. Section 5.6 next chapter, demonstrate formally test relationship using statistical methods.","code":"\nggplot(data = churn) +\n    geom_boxplot(aes(x = churn, y = intl.calls), \n                 fill = c(\"palevioletred1\", \"darkseagreen1\"))\n\nggplot(data = churn) +\n    geom_density(aes(x = intl.calls, fill = churn), alpha = 0.3)"},{"path":"chapter-EDA.html","id":"final-takeaways","chapter":"4 Exploratory Data Analysis","heading":"Final Takeaways","text":"customer.calls day.mins strongly associated churn key predictors churn models.Customers making four service calls high risk leaving.High daytime minute usage another important churn indicator, possibly due pricing concerns.Evening nighttime call usage shows strong relationship churn, suggesting may essential predictive feature.focusing service calls daytime minutes, companies can take targeted action reduce churn, optimizing customer support escalation processes offering personalized rate plans. findings also guide feature selection process future predictive modeling efforts.","code":""},{"path":"chapter-EDA.html","id":"EDA-sec-multivariate","chapter":"4 Exploratory Data Analysis","heading":"4.6 Investigating Multivariate Relationships","text":"univariate analysis provides insights individual variables, multivariate analysis helps uncover interactions may influence churn. Examining variable relationships can reveal behavioral patterns might evident analyzing feature isolation.useful example relationship day.mins (Day Minutes) eve.mins (Evening Minutes), visualized scatter plot :diagonal line, represented equation:\\[\n\\text{day.mins} = 400 - 0.6 \\times \\text{eve.mins}\n\\]separates dataset two regions. Customers upper-right region, day evening minutes high, exhibit noticeably higher churn rate. pattern apparent univariate analysis eve.mins, demonstrating feature interactions can provide deeper insights.quantify effect, isolate high-churn segment:Within subset, churn rate significantly higher overall dataset, reinforcing importance considering variable interactions. combination high day evening usage may indicate specific customer behavior pattern correlates dissatisfaction.Another key relationship exists customer.calls day.mins:scatter plot reveals interesting high-churn region upper left, customers make frequent customer service calls low day-minute usage. group may represent dissatisfied customers heavy users still experiencing service-related frustrations. contrast, high-minute users also make frequent service calls show lower churn rate, possibly indicating engaged customers tolerant service issues.","code":"\nggplot(data = churn) +\n    geom_point(aes(x = eve.mins, y = day.mins, color = churn), size = 0.7, alpha = 0.8) +\n    scale_color_manual(values = c(\"palevioletred1\", \"darkseagreen1\")) +\n    geom_abline(intercept = 400, slope = -0.6, color = \"blue\", size = 1)\nsub_churn = subset(churn, (day.mins > 400 - 0.6 * eve.mins))\n\nggplot(data = sub_churn, aes(x = churn, label = scales::percent(prop.table(stat(count))))) +\n    geom_bar(fill = c(\"palevioletred1\", \"darkseagreen1\")) + \n    geom_text(stat = 'count', vjust = 0.2, size = 6)\nggplot(data = churn) +\n  geom_point(aes(x = day.mins, y = customer.calls, color = churn), alpha = 0.8) +\n  scale_color_manual(values = c(\"palevioletred1\", \"darkseagreen1\"))"},{"path":"chapter-EDA.html","id":"key-takeaways","chapter":"4 Exploratory Data Analysis","heading":"Key Takeaways","text":"Multivariate analysis reveals customers high day evening call usage much higher churn rate.Customers making frequent customer service calls using daytime minutes also higher risk leaving.interaction frequent customer service calls high call usage suggests dissatisfaction alone always drive churn—usage patterns also play role.Identifying customers high-churn regions scatter plots can help targeted retention efforts.Chapter 5, move exploratory insights formal statistical analysis, applying techniques quantify relationships assess predictive value.","code":""},{"path":"chapter-EDA.html","id":"investigating-correlated-variables","chapter":"4 Exploratory Data Analysis","heading":"4.6.1 Investigating Correlated Variables","text":"Correlation measures degree two variables move together. positive correlation means one variable increases, also increases. negative correlation indicates one variable rises, decreases. two variables correlation, changes one provide information changes .common misconception correlation implies causation. example, analysis finds customers make calls customer service tend higher churn rates, necessarily mean calling customer service causes churn. dissatisfied customers likely call assistance leaving, making number service calls symptom rather cause.strength direction correlation measured correlation coefficient, denoted \\(r\\), ranges -1 1. value 1 indicates perfect positive relationship, -1 represents perfect negative correlation. value near zero suggests linear relationship. large datasets, even small correlations may statistically significant, practical significance must also considered. correlation 0.05 might significant dataset thousands observations, unlikely provide meaningful predictive power., Figure 4.3 shows examples different correlation coefficients.\nFigure 4.3: Example scatterplots showing different correlation coefficients.\nmultiple variables highly correlated, redundancy can become problem. Including variables model may add much new information can lead instability, particularly regression-based models multicollinearity makes difficult determine effect individual predictors. Instead automatically removing correlated variables, thoughtful approach involves assessing practical relevance whether provide distinct information.examine correlations churn dataset, compute correlation matrix numerical variables visualize using heatmap.correlation matrix highlights key relationships. charge variables perfectly correlated corresponding minutes variables charges calculated directly call duration. Including introduce redundancy. avoid , charge variables removed, keeping minutes variables.Another notable observation number calls within time period strongly correlated total minutes. One might expect customers make calls also spend time phone, data support assumption. suggests call frequency call duration may capture different aspects customer behavior, making valuable retain types variables modeling.addressing correlations exploratory data analysis, dataset can refined ensure informative variables used predictive modeling. Removing redundant features reduces complexity retaining meaningful signals, improving interpretability performance models.","code":"\nlibrary(ggcorrplot)  \nvariable_list = c(\"intl.mins\",  \"intl.calls\",  \"intl.charge\", \n                  \"day.mins\",   \"day.calls\",   \"day.charge\",\n                  \"eve.mins\",   \"eve.calls\",   \"eve.charge\",\n                  \"night.mins\", \"night.calls\", \"night.charge\")\n\ncor_matrix = cor(churn[, variable_list])\n\nggcorrplot(cor_matrix, type = \"lower\", lab = TRUE, lab_size = 3)"},{"path":"chapter-EDA.html","id":"key-findings-and-insights","chapter":"4 Exploratory Data Analysis","heading":"4.7 Key Findings and Insights","text":"exploratory data analysis churn dataset provided deeper understanding factors influencing customer attrition. examining individual variables interactions, identified several key trends inform predictive modeling business strategy.One striking findings role customer service interactions churn. Customers made four calls customer service significantly likely leave. suggests frequent interactions may indicate unresolved complaints, leading dissatisfaction. Additionally, customers high daytime evening call usage exhibit churn rates six times higher average, indicating high usage may correlate dissatisfaction—perhaps due service quality concerns pricing issues.international calling plan also appears strong predictor churn. Customers subscribe plan leaving much higher rate, suggesting plan may delivering sufficient value. contrast, customers voice mail plan show lower churn rate, indicating feature may contribute customer retention.Several variables, directly linked churn univariate analysis, still provide value combined features predictive modeling. example, customers relatively low daytime usage frequent customer service calls show higher likelihood leaving, pattern suggests service dissatisfaction among lower-usage customers.modeling perspective, variables introduce redundancy. charge variables (day, evening, night, international) perfectly correlated corresponding minute variables, derived directly . Retaining minute variables avoid multicollinearity preserving relevant information. Similarly, area code state fields may contribute much predictive power, show strong relationships churn dataset.","code":""},{"path":"chapter-EDA.html","id":"strategic-recommendations","chapter":"4 Exploratory Data Analysis","heading":"Strategic Recommendations","text":"findings present opportunities targeted interventions improve customer retention. Given frequent customer service calls strong indicator churn, companies implement proactive escalation strategies. Customers making third service call receive priority attention, potentially issue resolution specialists targeted retention offers.high-usage customers, personalized plans loyalty incentives help reduce churn. Offering flexible pricing additional benefits high daytime evening callers may address concerns drive switch providers. Similarly, review international plan necessary assess whether pricing, service quality, features leading dissatisfaction.variables, night minutes certain demographic features, show strong direct correlations churn, may still contribute combined predictors machine learning model. analysis determine importance predictive framework.identifying churn-related patterns early, businesses can take proactive steps improve customer satisfaction reduce attrition, strengthening overall retention efforts relying predictive modeling. insights serve foundation next stage analysis, machine learning models applied quantify relationships precisely.","code":""},{"path":"chapter-EDA.html","id":"exercises-2","chapter":"4 Exploratory Data Analysis","heading":"4.8 Exercises","text":"","code":""},{"path":"chapter-EDA.html","id":"conceptual-questions","chapter":"4 Exploratory Data Analysis","heading":"Conceptual Questions","text":"important perform exploratory data analysis proceeding modeling phase? potential risks skipping EDA directly applying data mining techniques?important perform exploratory data analysis proceeding modeling phase? potential risks skipping EDA directly applying data mining techniques?predictor exhibit clear relationship target variable exploratory data analysis, omitted modeling stage? Justify answer considering potential interactions, hidden patterns, role feature selection.predictor exhibit clear relationship target variable exploratory data analysis, omitted modeling stage? Justify answer considering potential interactions, hidden patterns, role feature selection.mean two variables correlated? Explain concept correlation, including direction strength, discuss differs causation. Provide example illustrate explanation.mean two variables correlated? Explain concept correlation, including direction strength, discuss differs causation. Provide example illustrate explanation.can identify address correlated variables exploratory data analysis? Describe steps take manage correlated predictors effectively explain benefits approach predictive modeling.can identify address correlated variables exploratory data analysis? Describe steps take manage correlated predictors effectively explain benefits approach predictive modeling.consequences including highly correlated variables predictive model? Discuss impact multicollinearity model performance, interpretability, stability, explain can detected addressed.consequences including highly correlated variables predictive model? Discuss impact multicollinearity model performance, interpretability, stability, explain can detected addressed.always advisable remove one two correlated predictors model? Discuss advantages drawbacks approach, explain circumstances keeping predictors might beneficial.always advisable remove one two correlated predictors model? Discuss advantages drawbacks approach, explain circumstances keeping predictors might beneficial.following descriptive methods, determine whether applies categorical data, continuous numerical data, . Provide brief explanation method used exploratory data analysis.\nHistograms\nBox plots\nDensity plots\nScatter plots\nSummary statistics\nCorrelation analysis\nContingency tables\nBar plots\nHeatmaps\nfollowing descriptive methods, determine whether applies categorical data, continuous numerical data, . Provide brief explanation method used exploratory data analysis.HistogramsBox plotsDensity plotsScatter plotsSummary statisticsCorrelation analysisContingency tablesBar plotsHeatmapsA telecommunications company analyzing customer data identify factors influencing churn. exploratory data analysis, discover customers high day minutes high evening minutes significantly higher churn rate. actionable insights company derive finding, might use information reduce customer attrition?telecommunications company analyzing customer data identify factors influencing churn. exploratory data analysis, discover customers high day minutes high evening minutes significantly higher churn rate. actionable insights company derive finding, might use information reduce customer attrition?Suppose conducting exploratory data analysis dataset 20 predictor variables. examining correlation matrix, find several pairs variables highly correlated (r > 0.9). address correlations ensure reliability interpretability predictive models? Describe steps take manage correlated variables effectively.Suppose conducting exploratory data analysis dataset 20 predictor variables. examining correlation matrix, find several pairs variables highly correlated (r > 0.9). address correlations ensure reliability interpretability predictive models? Describe steps take manage correlated variables effectively.Discuss importance considering statistical practical implications evaluating correlations exploratory data analysis. Provide example statistically significant correlation may real-world significance, explain essential consider aspects data analysis.Discuss importance considering statistical practical implications evaluating correlations exploratory data analysis. Provide example statistically significant correlation may real-world significance, explain essential consider aspects data analysis.important investigate multivariate relationships rather relying univariate analysis? Provide example interaction two variables reveals pattern missed separate univariate analyses.important investigate multivariate relationships rather relying univariate analysis? Provide example interaction two variables reveals pattern missed separate univariate analyses.data visualization aid exploratory data analysis process? Discuss least two specific examples visualizations provide insights summary statistics alone reveal.data visualization aid exploratory data analysis process? Discuss least two specific examples visualizations provide insights summary statistics alone reveal.Suppose discover customers high daytime call usage make frequent customer service calls likely churn. business actions taken based insight?Suppose discover customers high daytime call usage make frequent customer service calls likely churn. business actions taken based insight?Outliers can influential statistical modeling. possible causes outliers dataset? decide whether keep, modify, remove outlier?Outliers can influential statistical modeling. possible causes outliers dataset? decide whether keep, modify, remove outlier?context exploratory data analysis, explain missing values critical issue. different strategies handling missing values, circumstances appropriate?context exploratory data analysis, explain missing values critical issue. different strategies handling missing values, circumstances appropriate?","code":""},{"path":"chapter-EDA.html","id":"hands-on-practice-exploring-the-bank-dataset","chapter":"4 Exploratory Data Analysis","heading":"Hands-On Practice: Exploring the Bank Dataset","text":"hands-practice, explore bank dataset available R package liver. bank dataset related direct marketing campaigns Portuguese banking institution. campaigns conducted via phone calls, multiple contacts sometimes needed determine whether client subscribe term deposit. goal dataset predict whether client subscribe term deposit, explored using classification techniques Chapters 7 12.details dataset can found : https://rdrr.io/cran/liver/man/bank.html.can import dataset R follows:examine dataset’s structure, use:Dataset Overview: Report summary statistics dataset, including types variables. can infer structure data?Dataset Overview: Report summary statistics dataset, including types variables. can infer structure data?Target Variable Analysis: Generate bar plot target variable deposit using ggplot() function ggplot2. proportion clients subscribed term deposit?Target Variable Analysis: Generate bar plot target variable deposit using ggplot() function ggplot2. proportion clients subscribed term deposit?Binary Variable Exploration: Investigate binary categorical variables default, housing, loan. Create contingency tables bar plots visualize distributions. insights can draw variables?Binary Variable Exploration: Investigate binary categorical variables default, housing, loan. Create contingency tables bar plots visualize distributions. insights can draw variables?Exploring Numerical Variables: Analyze numerical variables dataset. Create histograms box plots visualize distributions. Identify patterns, skewness, unusual observations.Exploring Numerical Variables: Analyze numerical variables dataset. Create histograms box plots visualize distributions. Identify patterns, skewness, unusual observations.Outlier Detection: Identify whether numerical variables contain outliers. handle outliers maintain data integrity ensuring robust analysis?Outlier Detection: Identify whether numerical variables contain outliers. handle outliers maintain data integrity ensuring robust analysis?Correlation Analysis: Compute correlation matrix numerical variables. Identify pairs highly correlated variables. strategies use handle correlations avoid redundancy multicollinearity predictive modeling?Correlation Analysis: Compute correlation matrix numerical variables. Identify pairs highly correlated variables. strategies use handle correlations avoid redundancy multicollinearity predictive modeling?Key EDA Findings: Summarize key findings exploratory data analysis based exercises . preparing formal report, highlight notable patterns, relationships, anomalies?Key EDA Findings: Summarize key findings exploratory data analysis based exercises . preparing formal report, highlight notable patterns, relationships, anomalies?Business Implications: Based findings, actionable insights bank derive exploratory analysis? insights help optimizing marketing strategies improving customer targeting?Business Implications: Based findings, actionable insights bank derive exploratory analysis? insights help optimizing marketing strategies improving customer targeting?Multivariate Analysis: Investigate relationship number previous marketing campaign contacts (campaign) term deposit subscriptions (deposit). higher contact frequency correlate increased subscriptions? Use box plot bar chart support findings.Multivariate Analysis: Investigate relationship number previous marketing campaign contacts (campaign) term deposit subscriptions (deposit). higher contact frequency correlate increased subscriptions? Use box plot bar chart support findings.Feature Engineering Insight: Based EDA, propose least one new feature improve predictive power classification model term deposit subscriptions. Justify reasoning.Feature Engineering Insight: Based EDA, propose least one new feature improve predictive power classification model term deposit subscriptions. Justify reasoning.Seasonality Effects: Investigate whether time year influences term deposit subscriptions analyzing month variable. certain months higher success rate? Visualize pattern discuss potential business implications.Seasonality Effects: Investigate whether time year influences term deposit subscriptions analyzing month variable. certain months higher success rate? Visualize pattern discuss potential business implications.Effect Employment Type: Examine job variable relates term deposit subscriptions. job categories higher success rates? Present findings using suitable visualization discuss banks use insight targeted marketing.Effect Employment Type: Examine job variable relates term deposit subscriptions. job categories higher success rates? Present findings using suitable visualization discuss banks use insight targeted marketing.Interaction Effects: Analyze whether interactions different predictors, education job, influence likelihood subscribing term deposit. Use appropriate visualizations statistical summaries support findings.Interaction Effects: Analyze whether interactions different predictors, education job, influence likelihood subscribing term deposit. Use appropriate visualizations statistical summaries support findings.Effect Contact Duration: Investigate whether duration last contact (duration variable) strong relationship term deposit subscription. Visualize distribution discuss whether longer calls associated higher success rates.Effect Contact Duration: Investigate whether duration last contact (duration variable) strong relationship term deposit subscription. Visualize distribution discuss whether longer calls associated higher success rates.Comparison Campaign Outcomes: Compare subscription rates (deposit variable) across different types marketing campaigns (campaign variable). trends emerge, inform future marketing strategies?Comparison Campaign Outcomes: Compare subscription rates (deposit variable) across different types marketing campaigns (campaign variable). trends emerge, inform future marketing strategies?","code":"\nlibrary(liver)\ndata(bank)      str(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-statistics.html","id":"chapter-statistics","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5 Statistical Inference and Hypothesis Testing","text":"Statistical inference bridges gap observe sample want understand population. exploratory data analysis (EDA) helps us identify patterns relationships, statistical inference allows us determine whether patterns hold beyond sample—whether arisen chance. chapter, transition exploring data validating insights estimation, hypothesis testing, quantifying uncertainty.goals statistical inference can summarized three fundamental tasks:Estimating population characteristics, averages proportions, based sample data.Quantifying uncertainty measure confident can results.Testing hypotheses evaluate whether observed patterns statistically meaningful simply due random variation.tasks form foundation data-driven decision-making, enabling us distinguish meaningful insights statistical noise. chapter, explore three pillars—estimation, uncertainty, hypothesis testing—using intuitive explanations practical examples.statistical inference isn’t just applying formulas—’s also critical thinking. end chapter, ’ll develop two essential skills:detect statistical misuses misleading claims, helping critically evaluate data-driven arguments.avoid common pitfalls statistical analysis, ensuring conclusions sound defensible.interested art identifying statistical manipulation, Darrell Huff’s classic book, Lie Statistics, offers timeless lessons statistical skepticism. Understanding techniques valuable just avoiding errors also recognizing data used mislead.Let’s dive learn make statistical inferences confidence, curiosity, healthy dose skepticism.","code":""},{"path":"chapter-statistics.html","id":"estimation-using-data-to-make-predictions","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.1 Estimation: Using Data to Make Predictions","text":"Estimation fundamental aspect statistical inference allows us make informed guesses population based sample. Rather relying entire population, often impractical, use sample data estimate key characteristics averages proportions. instance, churn dataset, might want estimate:average number customer service calls among churners.proportion customers subscribed International Plan.two main types estimation:Point estimation provides single best guess population parameter, using sample mean estimate population mean.Interval estimation gives range plausible values (confidence interval) within true population parameter likely fall.Let’s explore examples:Example 5.1  estimate proportion churners dataset, use sample proportion point estimate population proportion. ’s calculate R:estimated proportion churners dataset 0.14, serving best guess proportion churners population.Example 5.2  Now, let’s estimate average number customer service calls customers churned. sample mean serves point estimate population mean:sample mean 4 calls, best estimate average number customer service calls among churners population.point estimates useful, provide information uncertainty. Confidence intervals help quantify precision estimate, explore next.","code":"\nprop.table(table(churn$churn))[\"yes\"]\n      yes \n   0.1414# Filter churners\nchurned_customers <- churn[churn$churn == \"yes\", ]\n\n# Calculate the mean\nmean_calls <- mean(churned_customers$customer.calls)\ncat(\"Point Estimate: Average Customer Service Calls for Churners:\", mean_calls)\n   Point Estimate: Average Customer Service Calls for Churners: 2.254597"},{"path":"chapter-statistics.html","id":"statistics-confidence-interval","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.2 Quantifying Uncertainty: Confidence Intervals","text":"Confidence intervals help quantify uncertainty estimating population parameters. Instead simply stating “average number customer service calls 4,” confidence interval provides range, “95% confident true average 3.8 4.2.” range accounts sampling variability, offering clearer picture reliable estimate .confidence interval consists point estimate, sample mean proportion, margin error, accounts uncertainty. general form confidence interval :\\[\n\\text{Point Estimate}  \\pm \\text{Margin Error}\n\\]population mean, confidence interval calculated :\\[\n\\bar{x} \\pm z_{\\frac{\\alpha}{2}} \\times \\left( \\frac{s}{\\sqrt{n}} \\right),\n\\]\\(\\bar{x}\\) sample mean, \\(z_{\\frac{\\alpha}{2}}\\) critical value standard normal distribution (1.96 95% confidence level), \\(s\\) sample standard deviation, \\(n\\) sample size. concept illustrated Figure 5.1, interval centered around point estimate width depends margin error.\nFigure 5.1: Confidence interval population mean. interval centered around point estimate, width determined margin error. confidence level specifies probability interval contains true population parameter.\nSeveral factors influence width confidence interval. Larger sample sizes generally yield narrower intervals, increasing precision, higher variability data results wider intervals. choice confidence level also affects width; example, 99% confidence level produces wider interval 90% confidence level must capture possible values.Imagine want estimate average height students university. survey 10 students, confidence interval wide little data. survey 1,000 students, estimate becomes much precise, confidence interval shrinks. illustrates larger sample sizes lead reliable estimates—data reduces uncertainty results tighter confidence intervals.illustrate, suppose want estimate average number customer service calls among churners 95% confidence:computed interval [2.12, 2.39], can say 95% confidence true average number service calls churners falls within range.smaller sample sizes, better use t-distribution instead normal distribution, accounts added uncertainty estimating population standard deviation. adjustment applied automatically R using t.test() function:Confidence intervals particularly useful comparing groups. confidence intervals two groups, churners non-churners, overlap significantly, suggests meaningful differences behavior. providing range rather single estimate, confidence intervals help balance precision uncertainty, making valuable tool statistical inference.","code":"# Calculate mean and standard error\nmean_calls <- mean(churned_customers$customer.calls)\nse_calls <- sd(churned_customers$customer.calls) / sqrt(nrow(churned_customers))\n\n# Confidence Interval\nz_score <- 1.96  # For 95% confidence\nci_lower <- mean_calls - z_score * se_calls\nci_upper <- mean_calls + z_score * se_calls\n\ncat(\"95% Confidence Interval: [\", ci_lower, \",\", ci_upper, \"]\")\n   95% Confidence Interval: [ 2.120737 , 2.388457 ]t.test(churned_customers$customer.calls, conf.level = 0.95)$conf.int\n   [1] 2.120509 2.388685\n   attr(,\"conf.level\")\n   [1] 0.95"},{"path":"chapter-statistics.html","id":"hypothesis-testing","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3 Hypothesis Testing","text":"Hypothesis testing provides structured framework evaluating claims population parameters using sample data. helps us assess whether patterns observed exploratory analysis statistically significant simply result random variation. method fundamental data-driven decision-making, enabling us distinguish meaningful insights noise.core, hypothesis testing involves two competing statements population parameter:null hypothesis (\\(H_0\\)) represents default assumption status quo, often stating difference groups, effect treatment, relationship variables.alternative hypothesis (\\(H_a\\)) challenges \\(H_0\\), suggesting difference, effect, relationship exist.Using sample evidence, decide whether :Reject \\(H_0\\) conclude data supports \\(H_a\\).Fail reject \\(H_0\\), meaning evidence insufficient dismiss \\(H_0\\), though prove true.strength evidence \\(H_0\\) quantified using p-value, represents probability obtaining observed data—something extreme—\\(H_0\\) true. smaller p-value suggests stronger evidence \\(H_0\\).\\(p < 0.05\\), reject \\(H_0\\) conclude statistical evidence \\(H_a\\).\\(p > 0.05\\), fail reject \\(H_0\\), meaning evidence strong enough support \\(H_a\\).threshold decision-making called significance level (\\(\\alpha\\)), typically set 0.05 (5%). value represents maximum probability making Type error—incorrectly rejecting \\(H_0\\). fields errors serious consequences, medicine aerospace, stricter thresholds (e.g., \\(\\alpha = 0.01\\)) often used.simple takeaway, often emphasized hypothesis testing, :Reject \\(H_0\\) \\(p\\)-value < \\(\\alpha\\).example:\n- \\(p = 0.03\\) \\(\\alpha = 0.05\\), reject \\(H_0\\) \\(p < \\alpha\\).\n- \\(p = 0.12\\), fail reject \\(H_0\\) \\(p > \\alpha\\).Although p-values provide structured way make decisions, limitations. small p-value necessarily mean result practically important—indicates statistical significance. Large datasets can generate small p-values trivial effects, small datasets may fail detect meaningful differences. Additionally, binary reject/fail--reject approach can sometimes oversimplify interpretation.","code":""},{"path":"chapter-statistics.html","id":"types-of-hypothesis-tests","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.3.1 Types of Hypothesis Tests","text":"Depending research question, hypothesis tests can take different forms:Left-tailed test: alternative hypothesis states parameter less specified value (\\(H_a: \\theta < \\theta_0\\)). Example: Testing whether average number customer service calls less 3.Right-tailed test: alternative hypothesis states parameter greater specified value (\\(H_a: \\theta > \\theta_0\\)). Example: Testing whether churn rate greater 30%.Two-tailed test: alternative hypothesis states parameter equal specified value (\\(H_a: \\theta \\neq \\theta_0\\)), evaluating deviations either direction. Example: Testing whether mean monthly charges differ $50.useful analogy hypothesis testing criminal trial. null hypothesis (\\(H_0\\)) represents presumption innocence, alternative hypothesis (\\(H_a\\)) represents guilt. jury weighs evidence either rejects \\(H_0\\) (convicts defendant) fails reject \\(H_0\\) (acquits due insufficient evidence). Just juries can make errors, hypothesis tests also two types errors summarized Table 5.1.Table 5.1:  Possible outcomes hypothesis testing two correct decisions two types errors.Type Error (\\(\\alpha\\)) occurs \\(H_0\\) rejected even though true—similar convicting innocent person. Type II Error (\\(\\beta\\)) happens \\(H_0\\) rejected even though false—similar acquitting guilty person. probability Type error controlled chosen significance level (\\(\\alpha\\)), probability Type II error depends factors like sample size test sensitivity.","code":""},{"path":"chapter-statistics.html","id":"common-hypothesis-tests","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"Common Hypothesis Tests","text":"several widely used hypothesis tests, listed Table 5.1, suited different types data.Table 5.2:  Seven commonly used hypothesis tests, null hypotheses (\\(H_0\\)), types variables apply .test serves specific purpose. t-test compares means, Z-test compares proportions, Chi-square test assesses categorical relationships, ANOVA compares means across multiple groups. tests explored following sections practical examples.","code":""},{"path":"chapter-statistics.html","id":"one-sample-t-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.4 One-sample t-test","text":"one-sample t-test evaluates whether mean numerical variable population equal specified value. commonly used compare sample mean benchmark theoretical expectation. term “one-sample” reflects single group tested fixed value, “t-test” refers fact test statistic follows t-distribution, used compute p-value.hypotheses one-sample t-test depend research question can formulated different ways. two-tailed test assesses whether mean differs specified value, regardless direction. left-tailed test evaluates whether mean lower specified value, right-tailed test examines whether mean greater. mathematical formulation :Two-Tailed Test:\n\\[\n\\begin{cases}\nH_0:  \\mu   =  \\mu_0 \\\\\nH_a:  \\mu \\neq \\mu_0\n\\end{cases}\n\\]Left-Tailed Test:\n\\[\n\\begin{cases}\nH_0:  \\mu \\geq \\mu_0 \\\\\nH_a:  \\mu  <   \\mu_0\n\\end{cases}\n\\]Right-Tailed Test:\n\\[\n\\begin{cases}\nH_0:  \\mu \\leq \\mu_0 \\\\\nH_a:  \\mu >   \\mu_0\n\\end{cases}\n\\]p-value represents probability observing sample mean, extreme value, assumption null hypothesis true. smaller p-value provides stronger evidence \\(H_0\\). p-value less significance level (\\(\\alpha = 0.05\\)), null hypothesis rejected, indicating sample mean differs significantly specified value.following example demonstrates apply one-sample t-test R using t.test() function.Example 5.3  company assumes , average, customers make two service calls churning. test whether actual average number customer service calls among churners differs assumed value, conduct one-sample t-test using churn dataset provided liver package.conduct test, set following hypotheses:Null Hypothesis (\\(H_0\\)): \\(H_0: \\mu = 2\\) (average number customer service calls 2.)Alternative Hypothesis (\\(H_a\\)): \\(H_a: \\mu \\neq 2\\) (average number customer service calls 2.)can present hypotheses mathematical form :\n\\[\n\\begin{cases}\n    H_0: \\mu = 2   \\\\\n    H_a: \\mu \\neq 2  \n\\end{cases}\n\\]begin loading churn dataset filtering customers churned:Now, conduct two-tailed one-sample t-test R using t.test() function; want know functionality t.test() function, can find typing ?t.test R console.output includes p-value, test statistic, degrees freedom, confidence interval population mean. Since p-value = 2^{-4} less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis (\\(H_0\\)). indicate sufficient evidence, 5% significance level, conclude true average number customer service calls differs 2.test also provides 95% confidence interval, [2.12, 2.39], represents range plausible values true population mean. Since 2 outside interval, evidence true average number service calls different assumed value. Additionally, sample mean, 2.25, reported best estimate population mean.Since sample standard deviation used place population standard deviation, test statistic follows t-distribution \\(n - 1\\) degrees freedom. measures far sample mean deviates hypothesized mean terms standard error. larger absolute value indicates stronger evidence \\(H_0\\).one-sample t-test provides structured approach comparing sample mean predefined benchmark. determines statistical significance also offers additional insights confidence interval, sample mean, test statistic. statistical significance important, practical relevance must also evaluated determine whether observed difference meaningful real-world implications. Even statistically significant difference detected, magnitude difference determines whether real-world implications. deviation 0.1 calls may negligible, whereas difference two calls impact customer service strategies.integrating statistical inference domain knowledge, one-sample t-test allows analysts determine whether deviations expectations statistically significant practically meaningful.","code":"\nlibrary(liver)  # Load the liver package\ndata(churn)     # Load the churn dataset\n\n# Filter churned customers\nchurned_customers <- churn[churn$churn == \"yes\", ]t_test <- t.test(churned_customers$customer.calls, mu = 2)\nt_test\n   \n    One Sample t-test\n   \n   data:  churned_customers$customer.calls\n   t = 3.7278, df = 706, p-value = 0.0002086\n   alternative hypothesis: true mean is not equal to 2\n   95 percent confidence interval:\n    2.120509 2.388685\n   sample estimates:\n   mean of x \n    2.254597"},{"path":"chapter-statistics.html","id":"hypothesis-testing-for-proportion","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.5 Hypothesis Testing for Proportion","text":"test proportion evaluates whether proportion (\\(\\pi\\)) specific category population aligns hypothesized value (\\(\\pi_0\\)). particularly useful binary categorical variables, observations fall one two groups, churned vs. churned. test helps determine whether observed sample proportion deviates significantly specified benchmark, making valuable business scientific contexts.instance, company might want assess whether proportion churners population aligns expected value based historical data industry standards. following example demonstrates apply proportion test R using prop.test() function.Example 5.4  company assumes 15% customers churn. test whether actual churn rate churn dataset differs assumption, conduct proportion test. hypotheses :\\[\n\\begin{cases}\nH_0: \\pi  =   0.15 \\\\\nH_a: \\pi \\neq 0.15\n\\end{cases}\n\\]test performed R using prop.test() function. like explore details function, can type ?prop.test R console.output provides key results, including p-value, confidence interval, sample proportion. results interpreted follows:p-value indicates probability obtaining observed sample proportion assumed population proportion true. Since p-value = 0.0923, greater \\(\\alpha = 0.05\\), reject null hypothesis. means insufficient evidence conclude churn rate population differs 15%. case, report:“statistically significant evidence suggest population proportion churners deviates 15%.”\np-value smaller 0.05, reject null hypothesis, concluding churn rate significantly different 15%.test also provides 95% confidence interval, [0.13, 0.15], represents plausible range true population proportion (\\(\\pi\\)). 0.15 lies within interval, supports failing reject \\(H_0\\). 0.15 outside interval, strengthens evidence \\(H_0\\).Additionally, test reports sample proportion, 0.14, observed proportion churners dataset. value serves estimate true population proportion.test helps assess whether observed churn rate aligns company’s expectation. p-value determines statistical significance, confidence interval sample proportion provide additional context interpretation. considering statistical results practical implications, businesses can evaluate whether assumptions churn accurate require adjustment.","code":"prop_test <- prop.test(x = sum(churn$churn == \"yes\"), \n                       n = nrow(churn), \n                       p = 0.15)\nprop_test\n   \n    1-sample proportions test with continuity correction\n   \n   data:  sum(churn$churn == \"yes\") out of nrow(churn), null probability 0.15\n   X-squared = 2.8333, df = 1, p-value = 0.09233\n   alternative hypothesis: true p is not equal to 0.15\n   95 percent confidence interval:\n    0.1319201 0.1514362\n   sample estimates:\n        p \n   0.1414"},{"path":"chapter-statistics.html","id":"two-sample-t-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.6 Two-sample T-test","text":"two-sample t-test, also known Student’s t-test, statistical method used compare means numerical variable two independent groups. assesses whether observed difference group means statistically significant simply due random variation. test named William Sealy Gosset, worked Guinness Brewery Dublin published pseudonym “Student” maintain confidentiality regarding statistical quality control methods.Section 4.5 previous chapter, explored relationship International Calls (intl.calls) churn status using visualizations like box plots density plots. visualizations help identify potential differences, statistical testing quantifies likelihood differences due chance.boxplot (left) density plot (right) illustrate distributions intl.calls churners non-churners. visualizations suggest minor differences, perform two-sample t-test assess whether differences statistically significant.conduct test, first establish hypotheses:Null Hypothesis (\\(H_0\\)): mean number international calls churners non-churners (\\(\\mu_1 = \\mu_2\\)).Alternative Hypothesis (\\(H_a\\)): mean number international calls differs churners non-churners (\\(\\mu_1 \\neq \\mu_2\\)).can also expressed mathematically :\n\\[\n\\begin{cases}\n    H_0: \\mu_1 = \\mu_2   \\\\\n    H_a: \\mu_1 \\neq \\mu_2\n\\end{cases}\n\\]perform test R using t.test() function:function evaluates difference means two groups (churn = \"yes\" vs. churn = \"\") provides p-value, confidence interval, descriptive statistics.p-value = 0.0014. Since value less significance level (\\(\\alpha = 0.05\\)), reject null hypothesis, concluding mean number international calls differs significantly churners non-churners. 95% confidence interval = [-0.53, -0.13] provides range plausible values true difference means. interval include zero, statistical evidence two groups differ significantly average number international calls. test output also provides sample means:\n- Mean churners = 4.15\n- Mean non-churners = 4.48These values allow direct comparison international call usage churners non-churners. instance, churners made average 1.5 international calls non-churners made 2.3 calls, suggests churners tend make fewer international calls.two-sample t-test assumes two groups independent variable interest normally distributed within group sample sizes small. larger samples, Central Limit Theorem ensures validity test even normality strictly met. minor deviations normality generally acceptable, large departures may require alternative tests, Mann-Whitney U test.business perspective, test results suggest international call frequency relevant factor churn. Customers churn tend make fewer international calls. Companies may explore whether international call costs contribute customer churn. higher costs deter international calls, targeted discounts low-usage customers encourage engagement improve retention. However, statistical significance always imply practical significance. Even churners make fewer international calls average, actual impact churn evaluated conjunction variables.Although example uses two-tailed test detect difference means, one-tailed test used research question specifies directional hypothesis. instance, company hypothesizes churners make fewer international calls non-churners, one-tailed test increase test’s sensitivity.two-sample t-test powerful widely used method comparing group means. provides statistical foundation validating insights suggested exploratory data analysis. integrating graphical exploration hypothesis testing, analysts can make well-informed inferences derive actionable business insights.","code":"t_test_calls <- t.test(intl.calls ~ churn, data = churn)\nt_test_calls\n   \n    Welch Two Sample t-test\n   \n   data:  intl.calls by churn\n   t = -3.2138, df = 931.13, p-value = 0.001355\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    -0.5324872 -0.1287201\n   sample estimates:\n   mean in group yes  mean in group no \n            4.151344          4.481947"},{"path":"chapter-statistics.html","id":"two-sample-z-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.7 Two-Sample Z-Test","text":"previous section, applied two-sample t-test compare mean number international calls churners non-churners. t-test useful assessing differences numerical variables, many business scientific questions involve categorical variables, proportions rather means interest. two-sample Z-test designed compare proportions two independent groups, determining whether observed difference proportions statistically significant. test particularly valuable analyzing binary categorical variables, customer churn subscription status.Section 4.4 previous chapter, examined relationship Voice Mail Plan (voice.plan) churn status using bar plots. visualizations suggest potential differences churn rates customers without Voice Mail Plan, statistical testing quantifies whether differences statistically significant.first bar plot (left) shows raw counts churners non-churners across two categories Voice Mail Plan (Yes ), second plot (right) displays proportions, allowing direct comparison churn rates. visualizations suggest customers without Voice Mail Plan may higher churn rate, hypothesis testing needed confirm whether difference statistically meaningful.formally test whether proportion churners Voice Mail Plan differs proportion non-churners plan, establish following hypotheses:Null Hypothesis (\\(H_0\\)): \\(\\pi_1 = \\pi_2\\)\n(proportions customers Voice Mail Plan churners non-churners.)Null Hypothesis (\\(H_0\\)): \\(\\pi_1 = \\pi_2\\)\n(proportions customers Voice Mail Plan churners non-churners.)Alternative Hypothesis (\\(H_a\\)): \\(\\pi_1 \\neq \\pi_2\\)\n(proportions customers Voice Mail Plan differ churners non-churners.)Alternative Hypothesis (\\(H_a\\)): \\(\\pi_1 \\neq \\pi_2\\)\n(proportions customers Voice Mail Plan differ churners non-churners.)can also expressed mathematically :\n\\[\n\\begin{cases}\n    H_0: \\pi_1 = \\pi_2   \\\\\n    H_a: \\pi_1 \\neq \\pi_2\n\\end{cases}\n\\]perform Z-test R, begin creating contingency table summarize counts customers without Voice Mail Plan churner non-churner groups. can done using table() function:table displays count churners non-churners without Voice Mail Plan. conduct Z-test, use prop.test() function:output provides p-value, confidence interval, sample proportions. Since p-value (0) smaller significance level (\\(\\alpha = 0.05\\)), reject null hypothesis. result suggests proportion customers Voice Mail Plan differs significantly churners non-churners.test also provides 95% confidence interval = [-0.1702, -0.1101] difference proportions. Since interval include zero, reinforces conclusion difference statistically significant. Additionally, sample proportions—0.1443 churners 0.2844 non-churners—provide insight magnitude difference.business perspective, finding suggests customers without Voice Mail Plan may likely churn. Companies leverage information encouraging Voice Mail Plan subscriptions among -risk customers investigating whether plan improves customer satisfaction retention. Companies leverage information encouraging Voice Mail Plan subscriptions among -risk customers investigating whether plan improves customer satisfaction retention. Although Z-test shows statistically significant difference, businesses evaluate whether promoting Voice Mail Plan meaningfully reduces churn rates justifies marketing investment.two-sample Z-test provides formal approach comparing proportions groups, complementing exploratory data analysis. integrating statistical testing business insights, companies can validate patterns take targeted actions reduce churn.","code":"table_plan = table(churn$churn, churn$voice.plan, dnn = c(\"churn\", \"voice.plan\"))\ntable_plan\n        voice.plan\n   churn  yes   no\n     yes  102  605\n     no  1221 3072z_test = prop.test(table_plan)\nz_test\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  table_plan\n   X-squared = 60.552, df = 1, p-value = 7.165e-15\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.1701734 -0.1101165\n   sample estimates:\n      prop 1    prop 2 \n   0.1442716 0.2844165"},{"path":"chapter-statistics.html","id":"chi-square-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.8 Chi-square Test","text":"two-sample Z-test effective comparing proportions two groups, many real-world analyses involve categorical variables two levels. Chi-square test allows us assess whether multiple categorical groups associated, providing broader framework understanding categorical relationships. makes particularly useful understanding customer behaviors business outcomes across multiple categories.Unlike Z-test, focuses comparing proportions two groups, Chi-square test evaluates whether distributions across multiple categories differ significantly expected independence. provides formal way test relationships categorical variables widely used marketing analysis, customer segmentation, business decision-making.illustrate, examine whether marital status associated purchasing deposit bank dataset (available liver package). dataset revisited Chapters 7 12 classification modeling. variable marital three categories: “divorced,” “married,” “single,” target variable deposit two categories: “yes” (customers purchased deposit) “” (customers ). goal determine whether marital status influences deposit purchases.begin visualizing relationship marital deposit using bar plots:first bar plot (left) displays raw counts deposit purchases across marital categories, second plot (right) presents relative proportions. Visual inspection suggests differences deposit purchase rates marital status, statistical test needed confirm whether differences significant.summarize observed counts contingency table:formally test independence, define hypotheses:\\[\n\\begin{cases}\nH_0: \\text{Deposit purchases independent marital status.} \\\\\nH_a: \\text{Deposit purchases depend marital status.}\n\\end{cases}\n\\]Chi-square test applied using chisq.test() function:output includes p-value, Chi-square test statistic, degrees freedom, expected frequencies \\(H_0\\). p-value = 7.3735354^{-5} smaller \\(\\alpha = 0.05\\), reject null hypothesis, concluding marital status deposit purchases independent. means least one marital group differs significantly others deposit purchase rates.Examining expected frequencies can reveal marital groups contribute observed association. particular group much higher lower deposit purchase rate expected, marketing efforts can tailored accordingly.business perspective, findings suggest banks may benefit personalizing marketing strategies based marital status. example, married customers significantly likely purchase deposits, targeted promotional campaigns emphasize financial planning families. Conversely, single customers exhibit lower deposit adoption rates, banks might develop incentive programs tailored financial goals.Chi-square test powerful tool identifying relationships categorical variables. integrating visual analysis, contingency tables, statistical hypothesis testing, businesses can make data-driven decisions optimize customer engagement product offerings.","code":"table_marital <- table(bank$deposit, bank$marital, dnn = c(\"deposit\", \"marital\"))\ntable_marital\n          marital\n   deposit divorced married single\n       no       451    2520   1029\n       yes       77     277    167chisq_test <- chisq.test(table_marital)\nchisq_test\n   \n    Pearson's Chi-squared test\n   \n   data:  table_marital\n   X-squared = 19.03, df = 2, p-value = 7.374e-05"},{"path":"chapter-statistics.html","id":"analysis-of-variance-anova-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.9 Analysis of Variance (ANOVA) Test","text":"previous sections, explored hypothesis tests compare two groups, two-sample t-test Z-test. However, many real-world scenarios, categorical variables two levels. cases, Analysis Variance (ANOVA) provides systematic way determine whether numerical variable differs across multiple groups. evaluates whether least one group mean differs significantly others. ANOVA especially useful analyzing relationship numerical variable categorical variable multiple levels, providing formal way determine categorical variable impacts numerical variable. test relies F-distribution assess whether observed differences means statistically significant.illustrate, let’s analyze relationship variable cut target variable price popular diamonds dataset (available ggplot2 package). See Section X.X overview dataset. variable cut five categories (“Fair,” “Good,” “Good,” “Premium,” “Ideal”), price numerical. objective test whether mean price diamonds differs across five cut categories.begin box plot visualize distribution diamond prices category cut:box plot displays spread median prices diamonds cut category. differences medians ranges suggest cut quality might influence price, statistical testing required confirm whether differences significant. apply ANOVA test formally assess relationship.test whether mean prices differ cut type, set following hypotheses:\\[\n\\begin{cases}\n    H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 \\quad \\text{(group means equal.)} \\\\\n    H_a: \\text{least one group mean differs others.}  \n\\end{cases}\n\\]conduct ANOVA test R, use aov() function:output provides test statistic (F-value), degrees freedom, p-value. Since p-value smaller significance level (\\(\\alpha = 0.05\\)), reject null hypothesis. indicates variable cut significant impact price diamonds.Rejecting \\(H_0\\) ANOVA specify groups differ. identify differences, post-hoc tests Tukey’s Honestly Significant Difference (Tukey HSD) test necessary. tests control multiple comparisons pinpointing significant pairwise differences. example, apply Tukey’s test determine cut categories (e.g., “Ideal” vs. “Good”) drive observed differences.Understanding impact diamond cut price crucial pricing strategies consumer insights. higher-quality cuts command significantly higher prices, retailers may adjust marketing efforts accordingly. Conversely, certain mid-tier cuts show meaningful price differences, companies might reconsider pricing models enhance competitiveness.ANOVA test provides structured approach evaluating whether categorical variable multiple levels influences numerical variable. case, relationship cut price suggests diamond cut type important predictor price, offering valuable insights quality impacts cost. integrating statistical testing business insights, analysts can determine whether categorical variables meaningful effects use knowledge inform data-driven decisions.","code":"\nggplot(data = diamonds) + \n  geom_boxplot(aes(x = cut, y = price, fill = cut)) +\n  scale_fill_manual(values = c(\"palevioletred1\", \"darkseagreen1\", \"skyblue1\", \"gold1\", \"lightcoral\"))anova_test <- aov(price ~ cut, data = diamonds)\nsummary(anova_test)\n                  Df    Sum Sq   Mean Sq F value Pr(>F)    \n   cut             4 1.104e+10 2.760e+09   175.7 <2e-16 ***\n   Residuals   53935 8.474e+11 1.571e+07                   \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"chapter-statistics.html","id":"correlation-test","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.10 Correlation Test","text":"previous sections, explored hypothesis tests comparing means proportions across groups. analyzing relationships two numerical variables, correlation testing provides formal method assess whether significant linear association exists. correlation test evaluates strength direction relationship testing null hypothesis population correlation coefficient (\\(\\rho\\)) equal zero. test particularly useful understanding two continuous variables co-vary, can inform business strategies, pricing models, predictive analytics.illustrate, examine whether significant relationship exists carat (diamond weight) price diamonds dataset (available ggplot2 package). Since larger diamonds generally expensive, expect positive correlation variables. scatter plot provides initial visual assessment relationship:scatter plot shows clear upward trend, suggesting carat increases, price. However, visualizations alone confirm statistical significance. formally test relationship, establish following hypotheses:\\[\n\\begin{cases}\n    H_0: \\rho   =  0 \\quad \\text{(linear correlation `carat` `price`.)} \\\\\n    H_a: \\rho \\neq 0 \\quad \\text{(significant linear correlation `carat` `price`.)}\n\\end{cases}\n\\]conduct correlation test R, use cor.test() function:output provides key results, including p-value, correlation coefficient, confidence interval:p-value: p-value = 0 smaller significance level (\\(\\alpha = 0.05\\)), reject \\(H_0\\), confirming correlation statistically significant.Correlation Coefficient: correlation coefficient (\\(r = 0.92\\)) quantifies strength direction relationship. value close 1 indicates strong positive correlation, value near 0 suggests linear association.Confidence Interval: 95% confidence interval [0.92, 0.92] provides plausible range true population correlation (\\(\\rho\\)). interval include 0, supports rejecting \\(H_0\\) confirms meaningful association.correlation coefficient 0.92 suggests strong positive relationship carat price, meaning larger diamonds tend expensive. small p-value confirms pattern unlikely due random variation, confidence interval provides estimate precisely can measure correlation.Beyond statistical significance, relationship practical implications diamond pricing strategies. correlation particularly strong, pricing models rely carat key determinant value. However, variability remains high despite significant correlation, additional factors—diamond clarity, cut, market conditions—may play influential role. analysis involve multivariate regression assess carat interacts attributes predicting price.integrating visualization, statistical inference, business insights, correlation test offers robust framework understanding numerical relationships. approach ensures observed trends statistically sound practically meaningful, laying foundation advanced modeling techniques.","code":"\nggplot(data = diamonds) +\n    geom_point(aes(x = carat, y = price), colour = \"blue\") +\n    labs(x = \"Carat\", y = \"Price\") cor_test <- cor.test(diamonds$carat, diamonds$price)\ncor_test\n   \n    Pearson's product-moment correlation\n   \n   data:  diamonds$carat and diamonds$price\n   t = 551.41, df = 53938, p-value < 2.2e-16\n   alternative hypothesis: true correlation is not equal to 0\n   95 percent confidence interval:\n    0.9203098 0.9228530\n   sample estimates:\n         cor \n   0.9215913"},{"path":"chapter-statistics.html","id":"wrapping-up","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.11 Wrapping Up","text":"chapter provided foundation statistical inference, beginning estimation, explored point estimates confidence intervals help quantify population parameters accounting uncertainty. introduced hypothesis testing, learning formulate null alternative hypotheses, compute test statistics, interpret p-values make informed decisions. practical examples, applied various statistical tests, including t-tests comparing means, proportion tests categorical data, ANOVA assessing differences across multiple groups, Chi-square test correlation analysis uncovering relationships variables. Together, tools form robust framework extracting insights answering key research questions.Statistical inference plays critical role data-driven decision-making, helping analysts distinguish meaningful patterns random variation. methods widely used business research, evaluating marketing strategies predicting customer behavior. However, reliable conclusions require statistical significance. essential check assumptions, contextualize results, integrate domain knowledge ensure findings accurate actionable.\nstatistical inference hypothesis testing essential tools data science, fall outside scope machine learning. interested exploring topics , recommend introductory statistics textbooks Intuitive Introductory Statistics Wolfe Schneider.5In next chapter, transition statistical inference predictive modeling, focusing partition datasets effectively. Just hypothesis testing helps determine whether patterns data real, proper data partitioning ensures machine learning models generalize well unseen data. move forward, ensuring data validity model robustness key building reliable predictive systems.","code":""},{"path":"chapter-statistics.html","id":"exercises-3","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"5.12 Exercises","text":"","code":""},{"path":"chapter-statistics.html","id":"conceptual-questions-1","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"Conceptual Questions","text":"hypothesis testing important data science? Explain role making data-driven decisions complements exploratory data analysis.hypothesis testing important data science? Explain role making data-driven decisions complements exploratory data analysis.difference confidence interval hypothesis test? provide different ways drawing conclusions population parameters?difference confidence interval hypothesis test? provide different ways drawing conclusions population parameters?p-value represents probability observing sample data, something extreme, assuming null hypothesis true. p-values interpreted, p-value 0.001 two-sample t-test necessarily evidence practical significance?**p-value represents probability observing sample data, something extreme, assuming null hypothesis true. p-values interpreted, p-value 0.001 two-sample t-test necessarily evidence practical significance?**Explain concepts Type Type II errors hypothesis testing. important balance risks errors designing statistical tests?Explain concepts Type Type II errors hypothesis testing. important balance risks errors designing statistical tests?hypothesis test, failing reject null hypothesis imply null hypothesis true. Explain case discuss implications result practice.hypothesis test, failing reject null hypothesis imply null hypothesis true. Explain case discuss implications result practice.working small sample sizes, t-distribution used instead normal distribution? shape t-distribution change sample size increases?working small sample sizes, t-distribution used instead normal distribution? shape t-distribution change sample size increases?One-tailed two-tailed hypothesis tests serve different purposes. one-tailed test appropriate two-tailed test? Provide example type test applicable.One-tailed two-tailed hypothesis tests serve different purposes. one-tailed test appropriate two-tailed test? Provide example type test applicable.two-sample Z-test Chi-square test analyze categorical data serve different purposes. differ, one preferred ?two-sample Z-test Chi-square test analyze categorical data serve different purposes. differ, one preferred ?Analysis Variance (ANOVA) test designed compare means across multiple groups. can’t multiple t-tests used instead? advantage using ANOVA context?Analysis Variance (ANOVA) test designed compare means across multiple groups. can’t multiple t-tests used instead? advantage using ANOVA context?","code":""},{"path":"chapter-statistics.html","id":"hands-on-practice-hypothesis-testing-in-r","chapter":"5 Statistical Inference and Hypothesis Testing","heading":"Hands-On Practice: Hypothesis Testing in R","text":"following exercises, use churn, bank, marketing, diamonds datasets available liver ggplot2 packages. previously used churn, bank, diamonds datasets earlier chapters. Chapter 10, introduce marketing dataset regression analysis.load datasets, use following commands:interested knowing 90% confidence interval population mean variable “night.calls” churn dataset. R, can obtain confidence interval population mean using t.test() function follows:Interpret confidence interval context customer service calls made night. Report 99% confidence interval population mean “night.calls” compare 90% confidence interval. interval wider, indicate precision estimates? increasing confidence level result wider interval, impact decision-making business context?Subgroup analyses help identify behavioral patterns specific customer segments. churn dataset, focus customers International Plan Voice Mail Plan make 220 daytime minutes calls. create subset, use:Next, compute 95% confidence interval proportion churners subset using prop.test():Compare confidence interval overall churn rate dataset (see Section 5.2). insights can drawn customer segment, might inform retention strategies?churn dataset, test whether mean number customer service calls (customer.calls) greater 1.5 significance level 0.01. right-tailed test formulated :\\[\n\\begin{cases}\n  H_0:  \\mu \\leq 1.5 \\\\\n  H_a:  \\mu > 1.5\n\\end{cases}\n\\]Since level significance \\(\\alpha = 0.01\\), confidence level \\(1-\\alpha = 0.99\\). perform test using:Report p-value determine whether reject null hypothesis \\(\\alpha=0.01\\). Explain decision discuss implications context customer service interactions.churn dataset, test whether proportion churners (\\(\\pi\\)) less 0.14 significance level \\(\\alpha=0.01\\). confidence level \\(99\\%\\), corresponding \\(1-\\alpha = 0.99\\). test conducted R using:State null alternative hypotheses. Report p-value determine whether reject null hypothesis \\(\\alpha=0.01\\). Explain conclusion potential impact customer retention strategies.churn dataset, examine whether number customer service calls (customer.calls) differs churners non-churners. test , perform two-sample t-test:State null alternative hypotheses. Determine whether reject null hypothesis significance level \\(\\alpha=0.05\\). Report p-value interpret results, explaining whether evidence relationship churn status customer service call frequency.marketing dataset, test whether positive relationship revenue spend significance level \\(\\alpha = 0.025\\). perform one-tailed correlation test using:State null alternative hypotheses. Report p-value determine whether reject null hypothesis. Explain decision discuss implications understanding relationship marketing spend revenue.churn dataset, variable “day.mins”, test whether mean number “Day Minutes” greater 180. Set level significance 0.05.churn dataset, variable “day.mins”, test whether mean number “Day Minutes” greater 180. Set level significance 0.05.churn dataset, variable “intl.plan” test \\(\\alpha=0.05\\) weather proportion customers international plan less 0.15.churn dataset, variable “intl.plan” test \\(\\alpha=0.05\\) weather proportion customers international plan less 0.15.churn dataset, test whether relationship target variable “churn” variable “intl.charge” \\(\\alpha=0.05\\).churn dataset, test whether relationship target variable “churn” variable “intl.charge” \\(\\alpha=0.05\\).bank dataset, test whether relationship target variable “deposit” variable “education” \\(\\alpha=0.05\\).bank dataset, test whether relationship target variable “deposit” variable “education” \\(\\alpha=0.05\\).Compute proportion customers churn dataset International Plan (intl.plan). Construct 95% confidence interval proportion using R, interpret confidence interval context customer subscriptions.Compute proportion customers churn dataset International Plan (intl.plan). Construct 95% confidence interval proportion using R, interpret confidence interval context customer subscriptions.Using churn dataset, test whether average number daytime minutes (day.mins) churners differs significantly 200 minutes. Conduct one-sample t-test R interpret results relation customer behavior.Using churn dataset, test whether average number daytime minutes (day.mins) churners differs significantly 200 minutes. Conduct one-sample t-test R interpret results relation customer behavior.Compare average number international calls (intl.calls) churners non-churners. Perform two-sample t-test evaluate whether observed differences means statistically significant.Compare average number international calls (intl.calls) churners non-churners. Perform two-sample t-test evaluate whether observed differences means statistically significant.Test whether proportion customers Voice Mail Plan (voice.plan) differs churners non-churners. Use two-sample Z-test R interpret results, considering implications customer retention strategies.Test whether proportion customers Voice Mail Plan (voice.plan) differs churners non-churners. Use two-sample Z-test R interpret results, considering implications customer retention strategies.Investigate whether marital status (marital) associated deposit subscription (deposit) bank dataset. Construct contingency table perform Chi-square test assess whether marital status significant impact deposit purchasing behavior.Investigate whether marital status (marital) associated deposit subscription (deposit) bank dataset. Construct contingency table perform Chi-square test assess whether marital status significant impact deposit purchasing behavior.Using diamonds dataset, test whether mean price diamonds differs across different diamond cuts (cut). Conduct ANOVA test interpret results. test finds significant differences, discuss post-hoc tests used explore findings.Using diamonds dataset, test whether mean price diamonds differs across different diamond cuts (cut). Conduct ANOVA test interpret results. test finds significant differences, discuss post-hoc tests used explore findings.Assess correlation carat price diamonds dataset. Perform correlation test R visualize relationship using scatter plot. Interpret results context diamond pricing.Assess correlation carat price diamonds dataset. Perform correlation test R visualize relationship using scatter plot. Interpret results context diamond pricing.Construct 95% confidence interval mean number customer service calls (customer.calls) among churners. Explain confidence interval helps quantify uncertainty might inform business decisions regarding customer support.Construct 95% confidence interval mean number customer service calls (customer.calls) among churners. Explain confidence interval helps quantify uncertainty might inform business decisions regarding customer support.Take random sample 100 observations churn dataset test whether average eve.mins differs 200. Repeat test using sample 1000 observations. Compare results discuss sample size affects hypothesis testing statistical power.Take random sample 100 observations churn dataset test whether average eve.mins differs 200. Repeat test using sample 1000 observations. Compare results discuss sample size affects hypothesis testing statistical power.Suppose hypothesis test indicates customers Voice Mail Plan significantly less likely churn (p < 0.01). potential business strategies company implement based finding? Beyond statistical significance, additional factors considered making marketing decisions?Suppose hypothesis test indicates customers Voice Mail Plan significantly less likely churn (p < 0.01). potential business strategies company implement based finding? Beyond statistical significance, additional factors considered making marketing decisions?","code":"\nlibrary(liver)\nlibrary(ggplot2)   \n\n# To import the datasets\ndata(churn)  \ndata(bank)  \ndata(marketing, package = \"liver\")  \ndata(diamonds)  t.test(x = churn$night.calls, conf.level = 0.90)$\"conf.int\"\n   [1]  99.45484 100.38356\n   attr(,\"conf.level\")\n   [1] 0.9\nsub_churn = subset(churn, (intl.plan == \"yes\") & (voice.plan == \"yes\") & (day.mins > 220)) prop.test(table(sub_churn$churn), conf.level = 0.95)$\"conf.int\"\n   [1] 0.2595701 0.5911490\n   attr(,\"conf.level\")\n   [1] 0.95t.test(x = churn$customer.calls, \n        mu = 1.5, \n        alternative = \"greater\", \n        conf.level = 0.99)\n   \n    One Sample t-test\n   \n   data:  churn$customer.calls\n   t = 3.8106, df = 4999, p-value = 7.015e-05\n   alternative hypothesis: true mean is greater than 1.5\n   99 percent confidence interval:\n    1.527407      Inf\n   sample estimates:\n   mean of x \n      1.5704prop.test(table(churn$churn), \n           p = 0.14, \n           alternative = \"less\", \n           conf.level = 0.99)\n   \n    1-sample proportions test with continuity correction\n   \n   data:  table(churn$churn), null probability 0.14\n   X-squared = 0.070183, df = 1, p-value = 0.6045\n   alternative hypothesis: true p is less than 0.14\n   99 percent confidence interval:\n    0.0000000 0.1533547\n   sample estimates:\n        p \n   0.1414t.test(customer.calls ~ churn, data = churn)\n   \n    Welch Two Sample t-test\n   \n   data:  customer.calls by churn\n   t = 11.292, df = 804.21, p-value < 2.2e-16\n   alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n   95 percent confidence interval:\n    0.6583525 0.9353976\n   sample estimates:\n   mean in group yes  mean in group no \n            2.254597          1.457722cor.test(x = marketing$spend, \n         y = marketing$revenue, \n         alternative = \"greater\", \n         conf.level = 0.975)\n   \n    Pearson's product-moment correlation\n   \n   data:  marketing$spend and marketing$revenue\n   t = 7.9284, df = 38, p-value = 7.075e-10\n   alternative hypothesis: true correlation is greater than 0\n   97.5 percent confidence interval:\n    0.6338152 1.0000000\n   sample estimates:\n        cor \n   0.789455"},{"path":"chapter-modeling.html","id":"chapter-modeling","chapter":"6 Preparing Data for Modeling","heading":"6 Preparing Data for Modeling","text":"can build reliable machine learning models, must ensure data well-prepared. previous chapters established foundation addressing key steps Data Science Workflow (Figure 2.3). Now, focus transitioning data exploration model building.Chapter 2.4, discussed defining problem aligning objectives data-driven strategies. Chapter 3 addressed handling missing values, outliers, data transformations create clean dataset. Chapter 4, visualized data uncover patterns, Chapter 5 introduced statistical inference, including hypothesis testing feature selection—tools help us validate data partitioning.diving machine learning, must complete Setup Phase, ensures dataset structured robust model development. phase involves three essential steps:Partitioning Data: Splitting dataset training testing sets create clear separation model learning evaluation.Validating Partition: Ensuring split representative unbiased insights training generalize new data.Balancing Training Dataset: Addressing potential class imbalances categorical targets prevent biased models.Although often overlooked, steps critical ensuring modeling process rigorous, fair, effective. Students often ask, “necessary partition data?” “need follow specific steps?” important questions, address throughout chapter. , ’s useful briefly examine data science process aligns diverges statistical inference. Understanding similarities differences helps bridge traditional statistics practical demands modern machine learning.","code":""},{"path":"chapter-modeling.html","id":"statistical-inference-in-the-context-of-data-science","chapter":"6 Preparing Data for Modeling","heading":"6.1 Statistical Inference in the Context of Data Science","text":"Although statistical inference remains fundamental tool data science, role shifts preparing data modeling, goals applications differ. traditional inference focuses drawing conclusions populations sample data, machine learning prioritizes predictive accuracy generalization.Statistical inference data science diverge two key ways applied modeling tasks:Significance Practicality: large datasets containing thousands even millions observations, nearly detected relationship can become statistically significant. However, statistical significance always translate practical importance. machine learning model might identify minute effect size statistically valid negligible impact decision-making. modeling, focus shifts statistical significance assessing whether effect strong enough meaningfully influence predictions.Significance Practicality: large datasets containing thousands even millions observations, nearly detected relationship can become statistically significant. However, statistical significance always translate practical importance. machine learning model might identify minute effect size statistically valid negligible impact decision-making. modeling, focus shifts statistical significance assessing whether effect strong enough meaningfully influence predictions.Exploration vs. Hypothesis Testing: Traditional statistical inference begins predefined hypothesis, testing whether new treatment improves outcomes compared control. contrast, data science often adopts exploratory approach, using data uncover patterns, relationships, predictive features without rigid hypotheses. Rather testing predefined relationships, machine learning practitioners iteratively refine datasets evaluate features contribute predictive accuracy.Exploration vs. Hypothesis Testing: Traditional statistical inference begins predefined hypothesis, testing whether new treatment improves outcomes compared control. contrast, data science often adopts exploratory approach, using data uncover patterns, relationships, predictive features without rigid hypotheses. Rather testing predefined relationships, machine learning practitioners iteratively refine datasets evaluate features contribute predictive accuracy.Despite differences, statistical inference remains crucial key stages data preparation, particularly :Partition Validation: dividing data training testing sets, statistical tests help ensure subsets representative original dataset.Feature Selection: Hypothesis testing can aid selecting features strong relationships target variable, enhancing model performance.understanding differences applying statistical inference strategically, can ensure data preparation supports building robust, interpretable, generalizable models. Throughout chapter, see inference techniques continue play role refining datasets machine learning.","code":""},{"path":"chapter-modeling.html","id":"why-is-it-necessary-to-partition-the-data","chapter":"6 Preparing Data for Modeling","heading":"6.2 Why Is It Necessary to Partition the Data?","text":"Partitioning dataset crucial step preparing data modeling. common question students ask , need partition data? answer lies generalization—ability model perform well unseen data. Without proper partitioning, models may fit training data exceptionally well fail make accurate predictions real-world scenarios. Partitioning ensures performance evaluated data model seen , providing unbiased measure ability generalize effectively.Partitioning involves dividing dataset two subsets: training set, used build model, testing set, used evaluate performance. separation simulates real-world conditions, model must make predictions new data. helps detect address two common pitfalls machine learning: overfitting underfitting. trade-offs illustrated Figure 6.1, highlights balance model complexity performance training testing datasets.\nFigure 6.1: trade-model complexity accuracy training test sets. highlights optimal model complexity (sweet spot), test set accuracy reaches highest value unseen data.\nOverfitting occurs model memorizes training data, including noise random fluctuations, instead capturing general patterns. models achieve high accuracy training set perform poorly unseen data. instance, churn prediction model might memorize specific customer IDs rather recognizing broader behavioral trends, making ineffective new customers.Underfitting, contrast, occurs model simplistic capture underlying patterns. might happen model lacks complexity preprocessing removes much useful information. underfitted churn model, example, might predict constant churn rate customers without considering individual differences, leading poor performance.Partitioning mitigates risks allowing us evaluate performance unseen data. Comparing accuracy training testing sets helps determine whether model overfitting (high training accuracy low testing accuracy) underfitting (low accuracy ). evaluation enables iterative refinements strike right balance complexity generalization.Partitioning also prevents data leakage, critical issue information testing set inadvertently influences training. Data leakage inflates performance metrics, creating false sense confidence model’s ability generalize. Strictly separating testing set training process ensures realistic assessment model performance.Beyond simple train-test split, cross-validation enhances robustness. cross-validation, dataset divided multiple subsets (folds). model trained one subset tested another, repeating process across folds. results averaged provide reliable estimate model performance. Cross-validation particularly useful working small datasets tuning hyperparameters, minimizes bias introduced single train-test split.Partitioning isn’t just technical step—’s fundamental building models generalize well. addressing overfitting, underfitting, data leakage, leveraging techniques like cross-validation, ensure models accurate reliable real-world applications.summarize, general strategy supervised machine learning consists three key steps, illustrated Figure 6.2:Partitioning dataset training testing sets, followed validating partition.Building machine learning models training data.Evaluating performance models testing data select effective approach.\nFigure 6.2: general predictive machine learning process building evaluating models. 80-20 split ratio example may vary based dataset task.\nfollowing structured process, build models robust capable making accurate predictions unseen data. chapter focuses first step: partitioning data effectively, validating partition, preparing balanced training dataset—key steps developing reliable interpretable machine learning models.","code":""},{"path":"chapter-modeling.html","id":"sec-partitioning","chapter":"6 Preparing Data for Modeling","heading":"6.3 Partitioning the Data","text":"Partitioning dataset fundamental step preparing data machine learning. common approach train-test split, also known holdout method, dataset divided two subsets: training set used build model testing set reserved evaluating performance. separation ensures model assessed unseen data, providing unbiased estimate well generalizes.typical train-test split ratio 70-30, 80-20, 90-10, depending dataset size modeling needs. training set contains available features, including target variable, used teach model patterns data. testing set, however, target variable temporarily hidden simulate real-world conditions. trained model applied testing set predict hidden values, predictions compared actual target values assess performance.","code":""},{"path":"chapter-modeling.html","id":"example-train-test-split-in-r","chapter":"6 Preparing Data for Modeling","heading":"Example: Train-Test Split in R","text":"illustrate, revisit churn dataset Section 4.3, goal predict customer churn. first load dataset:split churn dataset training testing subsets using partition() function liver package. following code demonstrates create subsets R:code begins setting seed using set.seed(43), ensures reproducibility making random split consistent across different runs. particularly important sharing results collaborating project. Next, partition() function used divide dataset two subsets: 80% data assigned train_set model training, remaining 20% stored test_set evaluation. Finally, actual_test created store true target values test set, used later assess model’s predictive accuracy.Reproducibility essential machine learning. Setting seed guarantees results remain consistent, allowing others replicate findings exactly. seed value arbitrary, using one ensures partitioning process stable across different runs.","code":"\nlibrary(liver)\ndata(churn) \nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test = test_set$churn"},{"path":"chapter-modeling.html","id":"why-partitioning-matters","chapter":"6 Preparing Data for Modeling","heading":"Why Partitioning Matters","text":"primary reason partitioning prevent data leakage—situation information testing set influences training, leading overly optimistic performance estimates. strictly separating sets, ensure performance metrics reflect model’s ability generalize new data rather just memorizing training patterns.Beyond simple train-test split, cross-validation can enhance robustness training testing model multiple subsets data. method particularly useful working small datasets tuning hyperparameters.Partitioning lays groundwork reliable machine learning models. However, well-executed split alone guarantee training testing sets representative original dataset. next section, validate partition confirm subsets retain key statistical properties, ensuring fair unbiased model evaluation.","code":""},{"path":"chapter-modeling.html","id":"sec-validate-partition","chapter":"6 Preparing Data for Modeling","heading":"6.4 Validating the Partition","text":"success entire modeling process depends quality data partition. Validating partition ensures training testing sets representative original dataset, enabling model learn diverse examples generalize effectively unseen data. Without validation, modeling process risks bias—either model fails generalize training set isn’t representative, testing set doesn’t provide accurate evaluation real-world performance.Validation involves comparing training testing sets confirm distributions statistically similar, particularly key variables. Since datasets often include many variables, step typically focuses small set randomly selected features features particular importance, target variable. choice statistical test depends type variable compared, shown Table 6.1.Table 6.1:  Suggested hypothesis tests validating partitions, based type target variable.Validating partition procedural step—safeguard biased modeling. training testing sets differ significantly, model’s performance compromised. training set representative original dataset, model may fail generalize effectively. Conversely, testing set reflect population, model evaluation misleading. Ensuring split retains characteristics original dataset allows fair reliable model assessment.","code":""},{"path":"chapter-modeling.html","id":"example-validating-the-target-variable-churn","chapter":"6 Preparing Data for Modeling","heading":"Example: Validating the Target Variable churn","text":"Let’s consider churn dataset introduced previous section. target variable, churn (whether customer churned ), binary. According Table 6.1, appropriate statistical test validate partition variable Two-Sample Z-Test, compares proportion churned customers training testing sets. Thus, hypotheses test :\\[\n\\begin{cases}\nH_0:  \\pi_{\\text{churn, train}} = \\pi_{\\text{churn, test}} \\quad \\text{(Proportions equal)} \\\\\nH_a:  \\pi_{\\text{churn, train}} \\neq \\pi_{\\text{churn, test}} \\quad \\text{(Proportions equal)}\n\\end{cases}\n\\]’s can implemented R:, \\(x_1\\) \\(x_2\\) represent number churned customers training testing sets, respectively, \\(n_1\\) \\(n_2\\) denote total number observations set. prop.test() function used compare proportions churned customers two subsets.test result provides p-value = 0.69. Since p-value greater significance level (\\(\\alpha = 0.05\\)), fail reject null hypothesis (\\(H_0\\)). indicates statistically significant difference proportions churned customers training testing sets. failing reject \\(H_0\\), confirm partition valid respect target variable churn. proportions churned customers consistent across subsets, ensuring model trained tested representative data.validating target variable crucial, extending process key predictors customer.calls day.mins ensures subsets remain representative across important features. example, numerical features can validated using two-sample t-test, categorical features multiple levels can assessed using Chi-square test. broader validation ensures important variables retain statistical properties across training testing sets.","code":"x1 <- sum(train_set$churn == \"yes\")\nx2 <- sum(test_set$churn == \"yes\")\n\nn1 <- nrow(train_set)\nn2 <- nrow(test_set)\n\ntest_churn <- prop.test(x = c(x1, x2), n = c(n1, n2))\ntest_churn\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.1566, df = 1, p-value = 0.6923\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.0190317  0.0300317\n   sample estimates:\n   prop 1 prop 2 \n   0.1425 0.1370"},{"path":"chapter-modeling.html","id":"what-if-the-partition-is-invalid","chapter":"6 Preparing Data for Modeling","heading":"What If the Partition Is Invalid?","text":"statistical tests reveal significant differences training testing sets, adjustments necessary ensure partition remains representative. Several strategies can applied:Revisiting partitioning process: Changing random seed adjusting split ratio can sometimes lead balanced split.Stratified sampling: Ensuring key categorical variables, target variable, proportionally represented subsets.Cross-validation: Using k-fold cross-validation instead single train-test split provide robust evaluation model performance.Additionally, dataset small highly variable, minor differences training testing sets might inevitable. cases, alternative approaches like bootstrapping can help validate model performance effectively.Validating partition critical step data preparation process. ensures modeling process fair, reliable, capable producing generalizable results. addressing potential discrepancies early, set stage robust machine learning models perform effectively real-world, unseen data.","code":""},{"path":"chapter-modeling.html","id":"balancing-the-training-dataset","chapter":"6 Preparing Data for Modeling","heading":"6.5 Balancing the Training Dataset","text":"many real-world classification problems, one class target variable significantly underrepresented. imbalance can lead biased models perform well majority class fail predict minority class accurately. example, fraud detection, fraudulent transactions rare compared legitimate ones, churn prediction, majority customers may churn. Without addressing issue, models may appear perform well based accuracy alone fail identify rare yet important events.Imbalanced datasets pose challenge machine learning algorithms optimize overall accuracy, can favor majority class. churn prediction model trained imbalanced dataset, example, might classify nearly customers non-churners, leading high accuracy failing detect actual churners. problematic minority class (e.g., fraud cases, churners, patients rare disease) key focus analysis.","code":""},{"path":"chapter-modeling.html","id":"techniques-for-addressing-class-imbalance","chapter":"6 Preparing Data for Modeling","heading":"Techniques for Addressing Class Imbalance","text":"Balancing training dataset ensures classes adequately represented model training, improving model’s ability generalize. Several techniques can used address class imbalance:Oversampling: Increasing number minority class examples duplicating existing observations generating synthetic samples. Synthetic Minority -sampling Technique (SMOTE) popular approach generates synthetic examples instead simple duplication.Undersampling: Reducing number majority class examples randomly removing observations.Hybrid Methods: Combining oversampling undersampling achieve balanced dataset.Class Weights: Modifying algorithm penalize misclassifications minority class heavily.choice technique depends factors dataset size, severity imbalance, specific machine learning algorithm used.","code":""},{"path":"chapter-modeling.html","id":"example-balancing-the-churn-dataset","chapter":"6 Preparing Data for Modeling","heading":"Example: Balancing the Churn Dataset","text":"First, examine distribution target variable (churn) training dataset:Suppose output shows churners (churn = \"yes\") constitute 0.14, non-churners (churn = \"\") make 0.86. significant imbalance suggests balancing may necessary, particularly churn prediction business priority.address , use ROSE package R oversample minority class (churn = \"yes\") constitutes 30% training dataset:example, ovun.sample() function increases proportion churners 30% training dataset. formula notation churn ~ . specifies balancing applied based target variable (churn). oversampling, new class distribution checked ensure desired balance.","code":"# Check the class distribution\ntable(train_set$churn)\n   \n    yes   no \n    570 3430\nprop.table(table(train_set$churn))\n   \n      yes     no \n   0.1425 0.8575# Load the ROSE package\nlibrary(ROSE)\n\n# Oversample the training set to balance the classes with 30% churners\nbalanced_train_set <- ovun.sample(churn ~ ., data = train_set, method = \"over\", p = 0.3)$data\n\n# Check the new class distribution\ntable(balanced_train_set$churn)\n   \n     no  yes \n   3430 1444\nprop.table(table(balanced_train_set$churn))\n   \n          no       yes \n   0.7037341 0.2962659"},{"path":"chapter-modeling.html","id":"key-considerations-for-balancing","chapter":"6 Preparing Data for Modeling","heading":"Key Considerations for Balancing","text":"Balancing performed training dataset, test dataset. test dataset remain representative original class distribution provide unbiased evaluation model performance. Modifying test set introduce bias make model’s performance appear artificially better real-world scenarios.Furthermore, balancing must applied partitioning dataset. balancing done splitting, information test set may influence training process (data leakage), leading misleadingly high performance.said, balancing always necessary. Many modern machine learning algorithms, random forests gradient boosting, incorporate class weighting ensemble learning handle imbalanced datasets effectively. Additionally, alternative evaluation metrics precision, recall, F1-score, AUC-ROC can provide better insights model performance dealing imbalanced classes.summary, balancing training dataset can improve model performance, especially minority class primary focus. However, always required used selectively. balancing necessary, must applied partitioning maintain validity model evaluation. ensuring classes adequately represented training, help machine learning models make accurate fair predictions.","code":""},{"path":"chapter-modeling.html","id":"exercises-4","chapter":"6 Preparing Data for Modeling","heading":"6.6 Exercises","text":"","code":""},{"path":"chapter-modeling.html","id":"conceptual-questions-2","chapter":"6 Preparing Data for Modeling","heading":"Conceptual Questions","text":"partitioning dataset crucial training machine learning model? Explain role ensuring generalization.partitioning dataset crucial training machine learning model? Explain role ensuring generalization.main risk training model without separating dataset training testing subsets? Provide example lead misleading results.main risk training model without separating dataset training testing subsets? Provide example lead misleading results.Explain difference overfitting underfitting. proper partitioning help address issues?Explain difference overfitting underfitting. proper partitioning help address issues?Describe role training set testing set machine learning. test set remain unseen model training?Describe role training set testing set machine learning. test set remain unseen model training?data leakage, can occur data partitioning? Provide example scenario data leakage lead overly optimistic model performance.data leakage, can occur data partitioning? Provide example scenario data leakage lead overly optimistic model performance.Compare contrast random partitioning stratified partitioning. stratified partitioning preferred?Compare contrast random partitioning stratified partitioning. stratified partitioning preferred?necessary validate partition splitting dataset? go wrong training test sets significantly different?necessary validate partition splitting dataset? go wrong training test sets significantly different?validate numerical variables, customer.calls churn dataset, similar distributions training testing sets?validate numerical variables, customer.calls churn dataset, similar distributions training testing sets?dataset highly imbalanced, might model trained fail generalize well? Provide example real-world domain class imbalance serious issue.dataset highly imbalanced, might model trained fail generalize well? Provide example real-world domain class imbalance serious issue.Compare oversampling, undersampling, hybrid methods handling imbalanced datasets. advantages disadvantages ?Compare oversampling, undersampling, hybrid methods handling imbalanced datasets. advantages disadvantages ?balancing techniques applied training dataset test dataset?balancing techniques applied training dataset test dataset?machine learning algorithms robust class imbalance, others require explicit handling imbalance. types models typically require class balancing, can handle imbalance naturally?machine learning algorithms robust class imbalance, others require explicit handling imbalance. types models typically require class balancing, can handle imbalance naturally?dealing class imbalance, accuracy always best metric evaluate model performance? alternative metrics considered?dealing class imbalance, accuracy always best metric evaluate model performance? alternative metrics considered?Suppose dataset rare critical class (e.g., fraud detection). steps taken data partitioning balancing phase ensure effective model?Suppose dataset rare critical class (e.g., fraud detection). steps taken data partitioning balancing phase ensure effective model?","code":""},{"path":"chapter-modeling.html","id":"hands-on-practice","chapter":"6 Preparing Data for Modeling","heading":"Hands-On Practice","text":"following exercises, use churn, bank, risk datasets available liver package. previously used churn bank datasets earlier chapters. Chapter 9, introduce risk dataset. Load datasets using:","code":"\nlibrary(liver)\n\n# Load datasets\ndata(churn)\ndata(bank)\ndata(risk)"},{"path":"chapter-modeling.html","id":"partitioning-the-data","chapter":"6 Preparing Data for Modeling","heading":"Partitioning the Data","text":"Using partition() function, split churn dataset 75% training 25% testing. Ensure reproducibility setting seed value partitioning.Using partition() function, split churn dataset 75% training 25% testing. Ensure reproducibility setting seed value partitioning.Perform 90-10 train-test split bank dataset. Report number observations subset.Perform 90-10 train-test split bank dataset. Report number observations subset.Apply stratified sampling partition churn dataset, ensuring proportion churners (churn == \"yes\") remains training test sets.Apply stratified sampling partition churn dataset, ensuring proportion churners (churn == \"yes\") remains training test sets.risk dataset, partition data using 60-40 split store training test sets train_risk test_risk.risk dataset, partition data using 60-40 split store training test sets train_risk test_risk.Compare distribution income training test sets bank dataset using density plots. appear similar?Compare distribution income training test sets bank dataset using density plots. appear similar?","code":""},{"path":"chapter-modeling.html","id":"validating-the-partition","chapter":"6 Preparing Data for Modeling","heading":"Validating the Partition","text":"churn dataset, test whether proportion churners statistically different training test sets. Use two-sample Z-test.churn dataset, test whether proportion churners statistically different training test sets. Use two-sample Z-test.bank dataset, test whether average age customers differs significantly training test sets using two-sample t-test.bank dataset, test whether average age customers differs significantly training test sets using two-sample t-test.Perform Chi-square test validate whether distribution marital status (marital) bank dataset similar training test sets.Perform Chi-square test validate whether distribution marital status (marital) bank dataset similar training test sets.Suppose churn dataset partitioned incorrectly, resulting training set 30% churners test set 15% churners. statistical test confirm issue, corrected?Suppose churn dataset partitioned incorrectly, resulting training set 30% churners test set 15% churners. statistical test confirm issue, corrected?Select three numerical variables risk dataset validate whether distributions differ training test sets using appropriate statistical tests.Select three numerical variables risk dataset validate whether distributions differ training test sets using appropriate statistical tests.","code":""},{"path":"chapter-modeling.html","id":"balancing-the-training-dataset-1","chapter":"6 Preparing Data for Modeling","heading":"Balancing the Training Dataset","text":"churn dataset, check whether churners (churn = \"yes\") underrepresented training dataset. Report class proportions.churn dataset, check whether churners (churn = \"yes\") underrepresented training dataset. Report class proportions.Use random oversampling increase number churners (churn = \"yes\") training set 40% dataset using ROSE package.Use random oversampling increase number churners (churn = \"yes\") training set 40% dataset using ROSE package.Apply undersampling bank dataset proportion customers deposit = \"yes\" deposit = \"\" equal training set.Apply undersampling bank dataset proportion customers deposit = \"yes\" deposit = \"\" equal training set.Compare class distributions balancing churn dataset. Use bar plots visualize change.Compare class distributions balancing churn dataset. Use bar plots visualize change.","code":""},{"path":"chapter-knn.html","id":"chapter-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7 Classification using k-Nearest Neighbors","text":"Classification one fundamental tasks machine learning, enabling models categorize data predefined groups. detecting spam emails predicting customer churn, classification algorithms widely used across various domains. chapter, first explore concept classification, discussing applications, key principles, commonly used algorithms.solid understanding classification, introduce k-Nearest Neighbors (kNN), simple yet effective algorithm based idea similarity data points. kNN widely used classification due intuitive approach ease implementation. delve details kNN works, demonstrate implementation R, discuss strengths, limitations, real-world applications.illustrate kNN practice, apply real-world dataset: churn dataset. goal build classification model predicts whether customer churn based service usage account features. hands-example, demonstrate data preprocessing, selecting optimal \\(k\\), evaluating model performance, interpreting results.","code":""},{"path":"chapter-knn.html","id":"classification","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.1 Classification","text":"ever wondered email app effortlessly filters spam, streaming service seems know exactly want watch next, banks detect fraudulent credit card transactions real-time? seemingly magical predictions made possible classification, fundamental task machine learning.core, classification involves assigning label category observation based features. example, given customer data, classification can predict whether likely churn stay loyal. Unlike regression, predicts continuous numerical values (e.g., house prices), classification deals discrete outcomes. target variable, often called class label, can either :Binary: Two possible categories (e.g., spam vs. spam).Multi-class: two categories (e.g., car, bicycle, pedestrian image recognition).diagnosing diseases identifying fraudulent activities, classification versatile tool used across countless domains solve practical problems.","code":""},{"path":"chapter-knn.html","id":"where-is-classification-used","chapter":"7 Classification using k-Nearest Neighbors","heading":"Where Is Classification Used?","text":"Classification algorithms power many everyday applications cutting-edge technologies. examples:\n- Email filtering: Sorting spam non-spam messages.\n- Fraud detection: Identifying suspicious credit card transactions.\n- Customer retention: Predicting whether customer churn.\n- Medical diagnosis: Diagnosing diseases based patient records.\n- Object recognition: Detecting pedestrians vehicles self-driving cars.\n- Recommendation systems: Suggesting movies, songs, products based user preferences.Every time interact technology “predicts” something , chances , classification model working behind scenes.","code":""},{"path":"chapter-knn.html","id":"how-does-classification-work","chapter":"7 Classification using k-Nearest Neighbors","heading":"How Does Classification Work?","text":"Classification involves two critical phases:Training Phase: algorithm learns patterns labeled dataset, contains predictor variables (features) target class labels. instance, fraud detection system, algorithm might learn transactions involving unusually high amounts originating foreign locations likely fraudulent.Prediction Phase: model trained, applies learned patterns classify new, unseen data. example, given new transaction, model predicts whether fraudulent legitimate.good classification model just memorize training data—generalizes well, meaning performs accurately new, unseen data. instance, model trained historical medical records able diagnose patient never encountered , rather simply repeating past diagnoses.","code":""},{"path":"chapter-knn.html","id":"which-classification-algorithm-should-you-use","chapter":"7 Classification using k-Nearest Neighbors","heading":"Which Classification Algorithm Should You Use?","text":"Different classification algorithms designed different kinds problems datasets. commonly used algorithms include:\n- k-Nearest Neighbors (kNN): simple, distance-based algorithm (introduced chapter).\n- Naive Bayes: Particularly useful text classification, like spam filtering (covered Chapter 9).\n- Logistic Regression: popular method binary classification tasks, predicting customer churn (covered Chapter 10).\n- Decision Trees Random Forests: Versatile, interpretable methods handling complex problems (covered Chapter 11).\n- Neural Networks: Effective handling high-dimensional complex data, images natural language (covered Chapter 12).choice algorithm depends factors dataset size, feature relationships, trade-interpretability performance. instance, ’re working small dataset need easy--interpret solution, kNN Decision Trees might ideal. Conversely, high-dimensional data like images speech recognition, Neural Networks effective.illustrate classification action, consider bank dataset goal predict whether customer make deposit (deposit = yes) (deposit = ). features might include customer details like age, education, job, marital status. training classification model data, bank can identify target potential customers likely invest, improving marketing strategy.","code":""},{"path":"chapter-knn.html","id":"why-is-classification-important","chapter":"7 Classification using k-Nearest Neighbors","heading":"Why Is Classification Important?","text":"Classification forms backbone countless machine learning applications drive smarter decisions actionable insights industries like finance, healthcare, retail, technology. Understanding works critical step mastering machine learning applying solve real-world problems.Among many classification techniques, k-Nearest Neighbors (kNN) stands simplicity effectiveness. easy understand requires minimal assumptions data, kNN often used baseline model exploring advanced techniques. rest chapter, explore kNN works, widely used, implement R.","code":""},{"path":"chapter-knn.html","id":"how-k-nearest-neighbors-works","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.2 How k-Nearest Neighbors Works","text":"ever sought advice trusted friends making decision? k-Nearest Neighbors (kNN) algorithm follows similar principle—“consults” closest data points determine category new observation. simple yet effective idea makes kNN one intuitive classification methods machine learning.Unlike many machine learning algorithms require explicit training phase, kNN lazy learning instance-based method. Instead constructing complex model, stores entire training dataset makes predictions demand. given new observation, kNN identifies k closest data points using predefined distance metric. class label assigned based majority vote among nearest neighbors. choice \\(k\\), number neighbors considered, plays crucial role balancing sensitivity local patterns generalization broader trends.","code":""},{"path":"chapter-knn.html","id":"how-does-knn-classify-a-new-observation","chapter":"7 Classification using k-Nearest Neighbors","heading":"How Does kNN Classify a New Observation?","text":"classify new observation, kNN calculates distance every data point training set using specified metric, Euclidean distance, instance. identifying \\(k\\)-nearest neighbors, algorithm assigns frequent class among predicted category.Figure 7.1 illustrates concept two classes: Class (red circles) Class B (blue squares). new data point, represented dark star, needs classified. figure compares predictions two different values \\(k\\):\\(k = 3\\): algorithm considers 3 closest neighbors—two blue squares one red circle. Since majority class Class B (blue squares), new point classified Class B.\\(k = 6\\): algorithm expands neighborhood include 6 nearest neighbors. larger set consists four red circles two blue squares, shifting majority class Class (red circles). result, new point classified Class .\nFigure 7.1: two-dimensional toy dataset two classes (Class Class B) new data point (dark star), illustrating k-Nearest Neighbors algorithm k = 3 k = 6.\nexamples illustrate choice \\(k\\) affects classification. smaller \\(k\\) (e.g., 3) makes predictions highly sensitive local patterns, capturing finer details also increasing risk misclassification due noise. contrast, larger \\(k\\) (e.g., 6) smooths predictions incorporating neighbors, reducing sensitivity individual data points potentially overlooking localized structures data. Selecting appropriate \\(k\\) ensures kNN generalizes well without becoming overly complex overly simplistic.","code":""},{"path":"chapter-knn.html","id":"strengths-and-limitations-of-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"Strengths and Limitations of kNN","text":"kNN algorithm widely used due simplicity intuitive nature, making excellent starting point classification problems. relying distance metrics majority voting, avoids complexity training explicit models. However, simplicity comes trade-offs, particularly handling large datasets noisy features.One kNN’s key strengths ease implementation interpretability. Since require model training, can applied directly datasets minimal preprocessing. performs well small datasets patterns well-defined feature relationships strong. However, kNN highly sensitive irrelevant noisy features, distance calculations may become less meaningful unnecessary attributes included. Additionally, can computationally expensive large datasets, since must calculate distances every training point prediction. choice \\(k\\) also plays crucial role—small \\(k\\) makes algorithm overly sensitive noise, large \\(k\\) may oversimplify patterns, leading reduced accuracy.","code":""},{"path":"chapter-knn.html","id":"knn-in-action-a-toy-example-for-drug-classification","chapter":"7 Classification using k-Nearest Neighbors","heading":"kNN in Action: A Toy Example for Drug Classification","text":"illustrate kNN, consider real-world scenario involving drug prescription classification. dataset 200 patients includes age, sodium--potassium (Na/K) ratio, drug type prescribed. dataset synthetically generated reflect real-world scenario. details dataset generated, refer Section 1.16. Figure 7.2 visualizes dataset, different drug types represented :Red circles Drug ,Green triangles Drug B, andBlue squares Drug C.\nFigure 7.2: Scatter plot Age vs. Sodium/Potassium Ratio 200 patients, drug type indicated color shape.\nSuppose three new patients arrive clinic, need determine drug suitable based age sodium--potassium ratio. details follows:Patient 1: 40 years old Na/K ratio 30.5.Patient 2: 28 years old Na/K ratio 9.6.Patient 3: 61 years old Na/K ratio 10.5.patients represented orange circles Figure 7.3. Using kNN, classify drug type patient.\nFigure 7.3: Scatter plot Age vs. Sodium/Potassium Ratio 200 patients, drug type indicated color shape. three new patients represented large orange circles.\nPatient 1, located deep within cluster red-circle points (Drug ), classification straightforward: Drug . nearest neighbors belong Drug , making easy decision.Patient 2, situation nuanced. \\(k = 1\\), nearest neighbor blue square, resulting classification Drug C. \\(k = 2\\), tie Drug B Drug C, leading clear majority. \\(k = 3\\), two three nearest neighbors blue squares, classification remains Drug C.Patient 3, classification becomes even ambiguous. \\(k = 1\\), closest neighbor blue square, classifying patient Drug C. However, \\(k = 2\\) \\(k = 3\\), nearest neighbors belong multiple classes, creating uncertainty classification.\nFigure 7.4: Zoom-plots three new patients nearest neighbors. left plot Patient 1, middle plot Patient 2, right plot Patient 3.\nexamples highlight several key aspects kNN. choice \\(k\\) significantly influences classification—small values \\(k\\) make algorithm highly sensitive local patterns, larger values introduce smoothing considering broader neighborhoods. Additionally, selection distance metrics, Euclidean distance, affects neighbors determined. Finally, proper feature scaling ensures variables contribute fairly distance calculations, preventing dominance features larger numeric ranges.example demonstrates kNN assigns labels based proximity, reinforcing importance thoughtful parameter selection preprocessing techniques. applying kNN real-world datasets, essential understand similarity measured—leads next discussion distance metrics.","code":""},{"path":"chapter-knn.html","id":"distance-metrics","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.3 Distance Metrics","text":"kNN algorithm, classification new data point determined identifying similar records training dataset. define measure similarity? similarity might seem intuitive, applying machine learning requires precise distance metrics. metrics quantify “closeness” two data points multidimensional space, directly influencing neighbors selected classification.Imagine ’re shopping online looking recommendations. ’re 50-year-old married female—’s similar : 40-year-old single female 30-year-old married male? answer depends measure distance person. kNN, distance computed using numerical features age categorical features marital status. smaller distance, “similar” two individuals , influence determining predictions. Since kNN assumes closer points (lower distance) belong class, choosing right distance metric crucial accurate classification.","code":""},{"path":"chapter-knn.html","id":"euclidean-distance","chapter":"7 Classification using k-Nearest Neighbors","heading":"Euclidean Distance","text":"widely used distance metric kNN Euclidean distance, measures straight-line distance two points. Think “--crow-flies” distance, similar shortest path two locations map. metric intuitive aligns often perceive distance real world.Mathematically, Euclidean distance two points, \\(x\\) \\(y\\), \\(n\\)-dimensional space given :\\[\n\\text{dist}(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\ldots + (x_n - y_n)^2},\n\\]\\(x = (x_1, x_2, \\ldots, x_n)\\) \\(y = (y_1, y_2, \\ldots, y_n)\\) represent feature vectors two points. differences corresponding features (\\(x_i - y_i\\)) squared, summed, square-rooted calculate distance.Example 7.1  Let’s calculate Euclidean distance two patients based age sodium--potassium (Na/K) ratio:Patient 1: \\(x = (40, 30.5)\\)Patient 2: \\(y = (28, 9.6)\\)Using formula:\\[\n\\text{dist}(x, y) = \\sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \\sqrt{(12)^2 + (20.9)^2} = 24.11\n\\]result quantifies dissimilarity two patients. kNN, distance helps determine similar Patient 1 Patient 2 whether classified drug class.","code":""},{"path":"chapter-knn.html","id":"choosing-the-right-distance-metric","chapter":"7 Classification using k-Nearest Neighbors","heading":"Choosing the Right Distance Metric","text":"Euclidean distance widely used kNN, always best choice. distance metrics can suitable depending dataset’s characteristics:Manhattan Distance: Measures distance summing absolute differences coordinates. useful movement restricted grid-like paths, city blocks.Hamming Distance: Used categorical variables, distance number positions two feature vectors differ.Cosine Similarity: Measures angle two vectors rather absolute distance. useful high-dimensional spaces, text classification.choice distance metric depends data type problem domain. dataset contains categorical high-dimensional features, exploring alternative metrics—Manhattan Cosine Similarity—might necessary. details, refer dist() function R.","code":""},{"path":"chapter-knn.html","id":"how-to-choose-an-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.4 How to Choose an Optimal \\(k\\)","text":"many opinions seek making important decision? might lead biased perspective, many might dilute relevance advice. Similarly, k-Nearest Neighbors (kNN) algorithm, choice \\(k\\)—number neighbors considered classification—directly impacts model’s performance. determine right \\(k\\)?universally “correct” value \\(k\\). optimal choice depends specific dataset classification problem, requiring careful consideration trade-offs involved.","code":""},{"path":"chapter-knn.html","id":"balancing-overfitting-and-underfitting","chapter":"7 Classification using k-Nearest Neighbors","heading":"Balancing Overfitting and Underfitting","text":"\\(k\\) small, \\(k = 1\\), algorithm becomes highly sensitive individual training points. new observation classified based single closest neighbor, making model highly reactive noise outliers. can lead overfitting, model memorizes training data fails generalize unseen data. example, small cluster mislabeled data points disproportionately influence predictions, reducing model’s reliability.Conversely, \\(k\\) increases, algorithm incorporates neighbors classification decision. Larger \\(k\\) values smooth decision boundary, reducing impact noise outliers. However, \\(k\\) large, model may oversimplify, averaging meaningful patterns data. \\(k\\) comparable size training set, majority class dominates predictions, leading underfitting, model fails capture important distinctions.Choosing appropriate \\(k\\) requires balancing extremes. Smaller values \\(k\\) capture fine-grained local structures risk overfitting, larger values provide stability expense detail.","code":""},{"path":"chapter-knn.html","id":"choosing-k-through-validation","chapter":"7 Classification using k-Nearest Neighbors","heading":"Choosing \\(k\\) Through Validation","text":"Since optimal \\(k\\) depends dataset, common approach evaluate multiple values \\(k\\) using validation set cross-validation. Performance metrics accuracy, precision, recall, F1-score help identify best \\(k\\) given problem.illustrate, use churn dataset evaluate accuracy kNN algorithm across different \\(k\\) values (ranging 1 30). Figure 7.5 shows accuracy fluctuates \\(k\\) increases. plot generated using kNN.plot() function liver package R.\nFigure 7.5: Accuracy k-Nearest Neighbors algorithm different values k range 1 30.\nplot, observe kNN accuracy fluctuates \\(k\\) increases. highest accuracy achieved \\(k = 5\\), algorithm balances sensitivity local patterns robustness noise. value, kNN delivers accuracy 0.932 error rate 0.068.Choosing optimal \\(k\\) much art science. ’s universal rule selecting \\(k\\), experimentation validation key. Start range plausible \\(k\\) values, test model’s performance, select one provides best results based chosen metric.Keep mind optimal \\(k\\) may vary across datasets. Whenever applying kNN new problem, repeating process ensures model remains accurate generalizable. carefully tuning \\(k\\), strike right balance overfitting underfitting, improving model’s predictive power.","code":""},{"path":"chapter-knn.html","id":"preparing-data-for-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5 Preparing Data for kNN","text":"effectiveness kNN algorithm relies heavily dataset prepared. Since kNN uses distance metrics evaluate similarity data points, proper preprocessing crucial ensure accurate meaningful results. Two essential steps process feature scaling one-hot encoding, enable algorithm handle numerical categorical features effectively. steps part Preparing Data Modeling stage Data Science Workflow (Figure 2.3).","code":""},{"path":"chapter-knn.html","id":"feature-scaling-1","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.1 Feature Scaling","text":"datasets, numerical features often vastly different ranges. instance, age may range 20 70, income range 20,000 150,000. Without proper scaling, features larger ranges, income, dominate distance calculations, leading biased predictions. address , numerical features must transformed comparable scales. See Section 3.5 details scaling methods.widely used method min-max scaling, transforms feature specified range, typically \\([0, 1]\\), using formula:\\[\nx_{\\text{scaled}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)},\n\\]\\(x\\) represents original feature value, \\(\\min(x)\\) \\(\\max(x)\\) minimum maximum values feature, respectively. ensures features contribute equally distance calculations preserving relative differences.Another common method z-score standardization, rescales features mean 0 standard deviation 1:\\[\nx_{\\text{scaled}} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)}\n\\]method particularly useful features contain outliers follow different distributions. Unlike min-max scaling, z-score standardization constrain values within fixed range ensures follow standard normal distribution, making robust extreme values.Choosing Right Scaling Method:\nMin-max scaling preferable feature values bounded within known range, pixel values images percentages. ensures features contribute equally distance metric maintaining relative proportions. hand, z-score standardization suitable data contains extreme values follows different distributions across features. transforms values standard normal distribution, making particularly effective datasets outliers varying units measurement.Avoiding Data Leakage:\nScaling must always performed partitioning dataset training test sets. scaling parameters, minimum maximum min-max scaling mean standard deviation z-score standardization, computed training set applied consistently training test sets. Performing scaling partitioning can introduce data leakage, information test set inadvertently influences training process. can lead misleadingly high accuracy evaluation, model indirectly gains access test data making predictions.","code":""},{"path":"chapter-knn.html","id":"scaling-training-and-test-data-the-same-way","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.2 Scaling Training and Test Data the Same Way","text":"illustrate importance consistent scaling, consider patient drug classification problem, involves two features: age sodium/potassium (Na/K) ratio. Figure 7.3 shows dataset 200 patients training set, three additional patients test set. Using minmax() function liver package, demonstrate correct incorrect ways scale data:difference illustrated Figure 7.6. middle panel shows results proper scaling, test set scaled using parameters derived training set. ensures consistency distance calculations across datasets. contrast, right panel shows improper scaling, test set scaled independently. leads distorted relationships training test data, can cause unreliable predictions.\nFigure 7.6: Visualization illustrating difference proper scaling improper scaling. left panel shows original data without scaling. middle panel shows results proper scaling. right panel shows results improper scaling.\nKey Insight: Proper scaling ensures distance metrics remain valid, improper scaling creates inconsistencies undermine kNN algorithm’s performance. Scaling parameters always derived training set applied consistently test set. Neglecting principle introduces data leakage, distorts model evaluation leads overly optimistic performance estimates.","code":"\n# Load the liver package\nlibrary(liver)\n\n# A proper way to scale the data\ntrain_scaled = minmax(train_data, col = c(\"Age\", \"Ratio\"))\n\ntest_scaled = minmax(test_data, col = c(\"Age\", \"Ratio\"), min = c(min(train_data$Age), min(train_data$Ratio)), max = c(max(train_data$Age), max(train_data$Ratio)))\n\n# An incorrect way to scale the data\ntrain_scaled_wrongly = minmax(train_data, col = c(\"Age\", \"Ratio\"))\ntest_scaled_wrongly  = minmax(test_data , col = c(\"Age\", \"Ratio\"))"},{"path":"chapter-knn.html","id":"one-hot-encoding-1","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.5.3 One-Hot Encoding","text":"Categorical features, marital status subscription type, directly used distance calculations distance metrics like Euclidean distance work numerical data. overcome , use one-hot encoding, converts categorical variables binary (dummy) variables.example, categorical variable voice.plan, levels yes , can encoded :\\[\n\\text{voice.plan-yes} =\n\\begin{cases}\n1 \\quad \\text{voice plan = yes}  \\\\\n0 \\quad \\text{voice plan = }\n\\end{cases}\n\\]categorical variables two categories, one-hot encoding creates multiple binary columns—one category except one, avoid redundancy. approach ensures categorical variable fully represented without introducing unnecessary correlations.liver package R provides one.hot() function perform one-hot encoding automatically. identifies categorical variables encodes binary columns, leaving numerical features unchanged. Applying one-hot encoding marital variable bank dataset, instance, adds binary columns encoded categories:Setting dropCols = FALSE retains original categorical column dataset, may useful reference debugging. However, cases, recommended remove original column encoding avoid redundancy.Note: One-hot encoding unnecessary ordinal features, categories natural order (e.g., low, medium, high). Ordinal variables instead assigned numerical values preserve order (e.g., low = 1, medium = 2, high = 3), enabling kNN algorithm treat numerical features. instance, education.level values {low, medium, high}, one-hot encoding lose natural progression categories. Instead, assigning numerical values (low = 1, medium = 2, high = 3) allows algorithm recognize ordinal nature feature, preserving relationship distance calculations.","code":"data(bank)\n\n# To perform one-hot encoding on the \"marital\" variable\nbank_encoded <- one.hot(bank, cols = c(\"marital\"), dropCols = FALSE)\n\nstr(bank_encoded)\n   'data.frame':    4521 obs. of  20 variables:\n    $ age             : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job             : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital         : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ marital_divorced: int  0 0 0 0 0 0 0 0 0 0 ...\n    $ marital_married : int  1 1 0 1 1 0 1 1 1 1 ...\n    $ marital_single  : int  0 0 1 0 0 1 0 0 0 0 ...\n    $ education       : Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance         : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing         : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan            : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact         : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day             : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month           : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration        : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign        : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays           : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous        : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome        : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-knn.html","id":"sec-kNN-churn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6 Applying kNN Algorithm in Practice","text":"Applying kNN algorithm involves several key steps, preparing data training model, making predictions, evaluating performance. section, demonstrate entire workflow using churn dataset liver package R. target variable, churn, indicates whether customer churned (yes) (), predictors include customer characteristics account length, international plan status, call details. details exploratory data analysis, problem understanding, data preparation dataset, refer Section 4.3.dataset data.frame R 5000 observations 19 predictor variables. target variable, churn, indicates whether customer churned (yes) ().Based insights gained Section 4.3, select following features building kNN model:account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, customer.calls.next steps involve preparing data feature scaling one-hot encoding, followed selecting optimal \\(k\\), training kNN model, evaluating performance.","code":"str(churn)\n   'data.frame':    5000 obs. of  20 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n    $ account.length: int  128 107 137 84 75 118 121 147 117 141 ...\n    $ voice.plan    : Factor w/ 2 levels \"yes\",\"no\": 1 1 2 2 2 2 1 2 2 1 ...\n    $ voice.messages: int  25 26 0 0 0 0 24 0 0 37 ...\n    $ intl.plan     : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 2 1 2 1 ...\n    $ intl.mins     : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n    $ intl.calls    : int  3 3 5 7 3 6 7 6 4 5 ...\n    $ intl.charge   : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n    $ day.mins      : num  265 162 243 299 167 ...\n    $ day.calls     : int  110 123 114 71 113 98 88 79 97 84 ...\n    $ day.charge    : num  45.1 27.5 41.4 50.9 28.3 ...\n    $ eve.mins      : num  197.4 195.5 121.2 61.9 148.3 ...\n    $ eve.calls     : int  99 103 110 88 122 101 108 94 80 111 ...\n    $ eve.charge    : num  16.78 16.62 10.3 5.26 12.61 ...\n    $ night.mins    : num  245 254 163 197 187 ...\n    $ night.calls   : int  91 103 104 89 121 118 118 96 90 97 ...\n    $ night.charge  : num  11.01 11.45 7.32 8.86 8.41 ...\n    $ customer.calls: int  1 1 0 2 3 0 3 0 1 0 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-knn.html","id":"step-1-preparing-the-data","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.1 Step 1: Preparing the Data","text":"first step applying kNN partition dataset training test sets, followed preprocessing tasks like feature scaling one-hot encoding. Since dataset already cleaned free missing values, can proceed directly partitioning applying transformations.split dataset 80% training set 20% test set using partition() function liver package:partition() function randomly splits dataset maintaining class distribution target variable, ensuring representative training test set. validated partition Section 6.4, can now proceed feature scaling one-hot encoding ensure compatibility kNN algorithm.","code":"\nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test = test_set$churn"},{"path":"chapter-knn.html","id":"one-hot-encoding-2","chapter":"7 Classification using k-Nearest Neighbors","heading":"One-Hot Encoding","text":"Since kNN relies distance calculations, categorical variables like voice.plan intl.plan must converted numerical representations. One-hot encoding achieves creating binary (dummy) variables category. apply one.hot() function liver package transform categorical features numerical format suitable kNN:binary categorical variables, one-hot encoding produces two columns (e.g., voice.plan_yes voice.plan_no). Since one variable always complement , retain one (e.g., voice.plan_yes) avoid redundancy.","code":"categorical_vars = c(\"voice.plan\", \"intl.plan\")\n\ntrain_onehot = one.hot(train_set, cols = categorical_vars)\ntest_onehot  = one.hot(test_set,  cols = categorical_vars)\n\nstr(test_onehot)\n   'data.frame':    1000 obs. of  22 variables:\n    $ state         : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 2 50 14 46 10 4 25 15 11 32 ...\n    $ area.code     : Factor w/ 3 levels \"area_code_408\",..: 3 2 1 3 2 2 2 2 2 1 ...\n    $ account.length: int  118 141 85 76 147 130 20 142 72 149 ...\n    $ voice.plan_yes: int  0 1 1 1 0 0 0 0 1 0 ...\n    $ voice.plan_no : int  1 0 0 0 1 1 1 1 0 1 ...\n    $ voice.messages: int  0 37 27 33 0 0 0 0 37 0 ...\n    $ intl.plan_yes : int  1 1 0 0 0 0 0 0 0 0 ...\n    $ intl.plan_no  : int  0 0 1 1 1 1 1 1 1 1 ...\n    $ intl.mins     : num  6.3 11.2 13.8 10 10.6 9.5 6.3 14.2 14.7 11.1 ...\n    $ intl.calls    : int  6 5 4 5 4 19 6 6 6 9 ...\n    $ intl.charge   : num  1.7 3.02 3.73 2.7 2.86 2.57 1.7 3.83 3.97 3 ...\n    $ day.mins      : num  223 259 196 190 155 ...\n    $ day.calls     : int  98 84 139 66 117 112 109 95 80 94 ...\n    $ day.charge    : num  38 44 33.4 32.2 26.4 ...\n    $ eve.mins      : num  221 222 281 213 240 ...\n    $ eve.calls     : int  101 111 90 65 93 99 84 63 102 92 ...\n    $ eve.charge    : num  18.8 18.9 23.9 18.1 20.4 ...\n    $ night.mins    : num  203.9 326.4 89.3 165.7 208.8 ...\n    $ night.calls   : int  118 97 75 108 133 78 102 148 71 108 ...\n    $ night.charge  : num  9.18 14.69 4.02 7.46 9.4 ...\n    $ customer.calls: int  0 0 1 1 0 0 0 2 3 1 ...\n    $ churn         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-knn.html","id":"feature-scaling-2","chapter":"7 Classification using k-Nearest Neighbors","heading":"Feature Scaling","text":"Since kNN calculates distances data points, features larger numerical ranges can disproportionately influence results. Scaling ensures features contribute equally distance calculations, preventing dominance high-magnitude features.standardize numerical variables, apply min-max scaling using minmax() function liver package. Scaling parameters (minimum maximum values) must computed training set applied consistently training test sets. prevents data leakage, occurs test data influences training process, leading misleadingly high performance estimates.minmax() function normalizes numerical features range \\([0, 1]\\), ensuring comparable scales preserving relative differences. transformation prevents single feature dominating kNN distance calculations, leading balanced accurate predictions.","code":"\nnumeric_vars = c(\"account.length\", \"voice.messages\", \"intl.mins\", \"intl.calls\", \n                 \"day.mins\", \"day.calls\", \"eve.mins\", \"eve.calls\", \n                 \"night.mins\", \"night.calls\", \"customer.calls\")\n\nmin_train = sapply(train_set[, numeric_vars], min)\nmax_train = sapply(train_set[, numeric_vars], max)\n\ntrain_scaled = minmax(train_onehot, col = numeric_vars, min = min_train, max = max_train)\ntest_scaled  = minmax(test_onehot,  col = numeric_vars, min = min_train, max = max_train)"},{"path":"chapter-knn.html","id":"step-2-choosing-an-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.2 Step 2: Choosing an Optimal \\(k\\)","text":"choice \\(k\\) determines trade-capturing local patterns generalizing well unseen data. Selecting inappropriate \\(k\\) may result overfitting (\\(k\\) small) oversmoothing (\\(k\\) large). identify optimal \\(k\\), evaluate model’s accuracy different values \\(k\\) using kNN.plot() function:kNN.plot() function visualizes relationship \\(k\\) model accuracy, helping us determine value \\(k\\) balances model complexity generalization. examining plot, observe highest accuracy achieved \\(k = 5\\). choice maintains sufficient flexibility capture meaningful patterns avoiding excessive sensitivity outliers.","code":"formula = churn ~ account.length + voice.plan_yes + voice.messages + \n                  intl.plan_yes + intl.mins + intl.calls + \n                  day.mins + day.calls + eve.mins + eve.calls + \n                  night.mins + night.calls + customer.calls\n\nkNN.plot(formula = formula, train = train_scaled, test = test_scaled, \n         k.max = 30, set.seed = 43)\n   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-knn.html","id":"step-3-training-the-model-and-making-predictions","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.3 Step 3: Training the Model and Making Predictions","text":"Since identified \\(k = 5\\) optimal value Step 2, now proceed train kNN model make predictions test set. apply kNN algorithm R, use kNN() function liver package follows:kNN() function automates kNN classification process computing distances test observation training data points. selects 5 closest neighbors based chosen distance metric assigns frequently occurring class among predicted label. Since test data used training, predictions provide unbiased estimate well model generalizes new observations.","code":"\nkNN_predict = kNN(formula = formula, train = train_scaled, test = test_scaled, k = 5)"},{"path":"chapter-knn.html","id":"step-4-evaluating-the-model","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.6.4 Step 4: Evaluating the Model","text":"Evaluating model performance crucial ensure kNN algorithm generalizes well unseen data makes reliable predictions. confusion matrix provides summary correct incorrect predictions comparing predicted labels actual labels test set. compute using conf.mat() function liver package:confusion matrix, see model correctly classified 910 instances, 90 instances misclassified. summary helps assess model performance identify areas improvement.","code":"conf.mat(kNN_predict, actual_test)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-knn.html","id":"final-remarks","chapter":"7 Classification using k-Nearest Neighbors","heading":"Final Remarks","text":"step--step implementation kNN highlighted crucial role data preprocessing, parameter tuning, model evaluation achieving reliable predictions. Key factors choice \\(k\\), feature scaling, encoding categorical data significantly influence accuracy generalization kNN models.confusion matrix provides initial assessment model performance, additional evaluation metrics accuracy, precision, recall, F1-score offer deeper insights. aspects explored detail next chapter (Chapter 8).","code":""},{"path":"chapter-knn.html","id":"key-takeaways-from-knn","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.7 Key Takeaways from kNN","text":"chapter, explored k-Nearest Neighbors (kNN) algorithm, simple yet effective method solving classification problems. began revisiting concept classification real-world applications, highlighting difference binary multi-class problems. examined mechanics kNN, emphasizing reliance distance metrics identify similar data points. Essential preprocessing steps, feature scaling one-hot encoding, discussed ensure accurate meaningful distance calculations. also covered importance selecting optimal \\(k\\) value demonstrated implementation kNN using liver package R churn dataset. practical examples, reinforced significance proper data preparation parameter tuning building reliable classification models.simplicity interpretability kNN make excellent starting point understanding classification exploring dataset structures. However, algorithm notable limitations, including sensitivity noise, computational inefficiency large datasets, necessity proper scaling feature selection. challenges make kNN less practical large-scale applications, remains valuable tool small medium-sized datasets serves benchmark evaluating advanced algorithms.kNN intuitive easy implement, prediction speed scalability constraints often limit use modern, large-scale datasets. Nonetheless, useful baseline method stepping stone sophisticated techniques. upcoming chapters, explore advanced classification algorithms, Decision Trees, Random Forests, Logistic Regression, address limitations kNN provide enhanced performance scalability wide range applications.","code":""},{"path":"chapter-knn.html","id":"exercises-5","chapter":"7 Classification using k-Nearest Neighbors","heading":"7.8 Exercises","text":"","code":""},{"path":"chapter-knn.html","id":"conceptual-questions-3","chapter":"7 Classification using k-Nearest Neighbors","heading":"Conceptual Questions","text":"Explain fundamental difference classification regression. Provide example .key steps applying kNN algorithm?choice \\(k\\) important kNN, happens \\(k\\) small large?Describe role distance metrics kNN classification. Euclidean distance commonly used?limitations kNN compared classification algorithms?feature scaling impact performance kNN? necessary?Describe one-hot encoding used kNN. necessary categorical variables?kNN handle missing values? strategies can used deal missing data?Explain difference lazy learning (kNN) eager learning (decision trees logistic regression).kNN considered non-parametric algorithm? advantages disadvantages bring?","code":""},{"path":"chapter-knn.html","id":"hands-on-practice-applying-knn-to-the-bank-dataset","chapter":"7 Classification using k-Nearest Neighbors","heading":"Hands-On Practice: Applying kNN to the Bank Dataset","text":", want apply concepts covered chapter using bank dataset liver package. bank dataset contains customer information, including demographics financial details, target variable deposit indicates whether customer subscribed term deposit. dataset well-suited classification problems provides opportunity practice kNN real-world scenarios.begin, load necessary package dataset:","code":"library(liver)\n\n# Load the dataset\ndata(bank)\n\n# View the structure of the dataset\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-knn.html","id":"data-exploration-and-preparation","chapter":"7 Classification using k-Nearest Neighbors","heading":"Data Exploration and Preparation","text":"Load bank dataset display structure. Identify target variable predictor variables.Count number instances customer subscribed term deposit (deposit = “yes”) versus (deposit = “”). tell dataset?Identify nominal variables dataset. Convert numerical features using one-hot encoding one.hot() function.Partition dataset 80% training 20% testing sets using partition() function. Ensure target variable remains proportionally distributed sets.Validate partitioning comparing class distribution target variable training test sets.Apply min-max scaling numerical variables training test sets. Ensure scaling parameters derived training set .","code":""},{"path":"chapter-knn.html","id":"choosing-the-optimal-k","chapter":"7 Classification using k-Nearest Neighbors","heading":"Choosing the Optimal \\(k\\)","text":"Use kNN.plot() function determine optimal \\(k\\) value classifying deposit bank dataset.best \\(k\\) value based accuracy? accuracy change \\(k\\) increases?Interpret meaning accuracy curve generated kNN.plot(). patterns observe?","code":""},{"path":"chapter-knn.html","id":"building-and-evaluating-the-knn-model","chapter":"7 Classification using k-Nearest Neighbors","heading":"Building and Evaluating the kNN Model","text":"Train kNN model using optimal \\(k\\) make predictions test set.Generate confusion matrix kNN model predictions using conf.mat() function. Interpret results.Calculate accuracy kNN model. well perform predicting deposit?Besides accuracy, evaluation metrics (e.g., precision, recall, F1-score) useful assessing kNN performance bank dataset? Compute interpret metrics.Compare performance kNN different values \\(k\\) (e.g., \\(k = 1, 5, 15, 25\\)). changing \\(k\\) affect classification results?Train kNN model using subset features: age, balance, duration, campaign. Compare accuracy full-feature model. tell feature selection?Compare accuracy kNN using min-max scaling versus z-score standardization. choice scaling method impact model performance?","code":""},{"path":"chapter-knn.html","id":"critical-thinking-and-real-world-applications","chapter":"7 Classification using k-Nearest Neighbors","heading":"Critical Thinking and Real-World Applications","text":"Suppose building fraud detection system bank. kNN suitable algorithm? advantages limitations context?handle imbalanced classes bank dataset? strategies improve classification performance?high-dimensional dataset hundreds features, kNN still effective approach? ?Imagine working dataset new data points arrive real-time. challenges kNN face, addressed?financial institution wants classify customers different risk categories loan approval, preprocessing steps essential applying kNN?dataset features irrelevant redundant, improve kNN’s performance? feature selection methods use?computation time concern, strategies apply make kNN efficient large datasets?Suppose kNN performing poorly bank dataset. possible reasons explain , troubleshoot issue?","code":""},{"path":"chapter-evaluation.html","id":"chapter-evaluation","chapter":"8 Model Evaluation","heading":"8 Model Evaluation","text":"progress Data Science Process, introduced Chapter 2 illustrated Figure 2.3, ’ve already completed first five phases Data Science Workflow:Problem Understanding: Defining problem aim solve.Data Preparation: Cleaning, transforming, organizing data analysis.Exploratory Data Analysis (EDA): Gaining insights uncovering patterns data.Preparing Data Model: Setting data modeling scaling, encoding, partitioning.Modeling: Applying algorithms make predictions extract insights—kNN classification method explored previous chapter.Now, arrive Model Evaluation phase, pivotal step Data Science Process. phase answers critical question: well model perform?","code":""},{"path":"chapter-evaluation.html","id":"why-is-model-evaluation-important","chapter":"8 Model Evaluation","heading":"Why Is Model Evaluation Important?","text":"Building model just beginning. true test model lies ability generalize new, unseen data. Without proper evaluation, model may appear successful development fail real-world applications.Consider example:\n’ve built model detect fraudulent credit card transactions, achieves 95% accuracy. Impressive, right? 1% transactions actually fraudulent, model might simply classify every transaction legitimate, ignoring fraud cases. highlights crucial point: accuracy alone can misleading, especially imbalanced datasets.Model evaluation goes beyond simplistic metrics like accuracy. provides nuanced understanding model’s:Strengths: model well (e.g., detecting true positives).Weaknesses: falls short (e.g., missing fraud cases generating false alarms).Trade-offs: balance competing priorities, sensitivity vs. specificity precision vs. recall.short, model evaluation ensures model aligns real-world goals problem. helps answer questions :well model handle imbalanced datasets?good identifying true positives (e.g., detecting cancer)?minimize false positives (e.g., incorrectly flagging legitimate emails spam)?George Box famously said, “models wrong, useful.” model always simplification reality. capture every nuance complexity, properly evaluated, can provide actionable insights guide decisions effectively. Evaluation metrics help us judge whether model “useful enough” meet needs problem ’re solving.chapter, ’ll explore evaluate classification models, starting binary classification, target variable two categories (e.g., spam vs. spam). ’ll discuss metrics multi-class classification, two categories (e.g., types vehicles: car, truck, bike). Finally, ’ll touch evaluation metrics regression models, target variable continuous (e.g., predicting house prices).goal build strong foundation model evaluation, helping confidently assess model performance make data-driven decisions. Let’s begin cornerstone classification evaluation: Confusion Matrix.","code":""},{"path":"chapter-evaluation.html","id":"confusion-matrix","chapter":"8 Model Evaluation","heading":"8.1 Confusion Matrix","text":"confusion matrix cornerstone evaluating classification models. provides detailed snapshot well model’s predictions align actual outcomes categorizing predictions four distinct groups. binary classification problems, confusion matrix typically organized shown Table 8.1.classification tasks, often focus model’s ability distinguish one class interest (positive class) another (negative class). instance, fraud detection scenario, fraudulent transactions might positive class, legitimate ones negative class.Table 8.1:  Confusion matrix summarizing correct incorrect predictions binary classification problems. positive class refers class interest, negative class represents category.Let’s break terms:True Positives (TP): Cases model correctly predicts positive class (e.g., fraud detected fraud).False Positives (FP): Cases model incorrectly predicts positive class (e.g., legitimate transactions flagged fraud).True Negatives (TN): Cases model correctly predicts negative class (e.g., legitimate transactions classified legitimate).False Negatives (FN): Cases model fails predict positive class (e.g., fraud classified legitimate).structure feels familiar, ’s mirrors concept type type II errors introduced Chapter 5 hypothesis testing. diagonal elements confusion matrix (TP TN) represent correct predictions, -diagonal elements (FP FN) capture incorrect ones.","code":""},{"path":"chapter-evaluation.html","id":"calculating-key-metrics","chapter":"8 Model Evaluation","heading":"Calculating Key Metrics","text":"Using counts confusion matrix, can calculate basic performance metrics model, accuracy (also know success rate) error rate:\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}\n\\]\\[\n\\text{Error Rate} = 1 - \\text{Accuracy} = \\frac{\\text{FP} + \\text{FN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}\n\\]Accuracy proportion correct predictions (TP TN) among predictions made model. gives general sense well model performs. conversely, Error Rate proportion incorrect predictions (FP FN) among predictions. accuracy gives overall sense model performance, differentiate types errors. example, imbalanced datasets one class dominates, accuracy may appear high even model performs poorly detecting minority class. need nuanced metrics, sensitivity, specificity, precision, recall, ’ll explore later sections.Example 8.1  Let’s revisit k-Nearest Neighbors (kNN) model built Chapter 7 classify churn dataset. Using confusion matrix, can evaluate well model performs test data.’s apply kNN model generate confusion matrix predictions:details kNN model built, refer Section 7.6.Now, ’ll generate confusion matrix predictions using conf.mat() function liver package:confusion matrix summarizes model’s performance. example:True Positives (TP): 54 cases churn correctly predicted.True Negatives (TN): 856 cases non-churn correctly predicted.False Positives (FP): 83 cases model falsely predicted churn.False Negatives (FN): 7 cases churn missed.can also visualize confusion matrix using conf.mat.plot() function liver package:Using confusion matrix, can calculate following metrics kNN model:\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total Predictions}} = \\frac{54 + 856}{1000} = 0.91\n\\]\\[\n\\text{Error Rate} = \\frac{\\text{FP} + \\text{FN}}{\\text{Total Predictions}} = \\frac{83 + 7}{1000} = 0.09\n\\]values give us sense overall performance model. However, accuracy error rate don’t provide insights specific errors, well model detects true positives avoids false positives. insights, need explore additional metrics like sensitivity, specificity, precision, recall, ’ll cover next.","code":"\n# Load the churn dataset\ndata(churn)\n\n# Partition the data into training and testing sets\nset.seed(43)\n\ndata_sets = partition(data = churn, ratio = c(0.8, 0.2))\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\nactual_test = test_set$churn\n\n# Build and predict using the kNN model\nformula = churn ~ account.length + voice.plan + voice.messages + \n                  intl.plan + intl.mins + intl.calls + \n                  day.mins + day.calls + eve.mins + eve.calls + \n                  night.mins + night.calls + customer.calls\n\nkNN_predict = kNN(formula = formula, train = train_set, \n                  test = test_set, k = 5, scaler = \"minmax\")conf.mat(kNN_predict, actual_test)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856   Setting levels: reference = \"yes\", case = \"no\"conf.mat.plot(kNN_predict, actual_test)\n   Setting levels: reference = \"yes\", case = \"no\""},{"path":"chapter-evaluation.html","id":"sensitivity-and-specificity","chapter":"8 Model Evaluation","heading":"8.2 Sensitivity and Specificity","text":"classification, ’s important evaluate just many predictions correct overall, well model identifies specific classes. Sensitivity Specificity two complementary metrics focus model’s ability distinguish positive negative classes.","code":""},{"path":"chapter-evaluation.html","id":"sensitivity","chapter":"8 Model Evaluation","heading":"Sensitivity","text":"Sensitivity (also called Recall fields, like information retrieval) measures model’s ability correctly identify positive cases. answers question:“actual positives, many model correctly predict?”Mathematically, sensitivity defined :\\[\n\\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n\\]Let’s compute sensitivity k-Nearest Neighbors (kNN) model built Chapter 7, predicted whether customers churned (churn = yes). Sensitivity case reflects percentage churners correctly identified model. Using confusion matrix Example 8.1:\\[\n\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{54}{54 + 7} = 0.885\n\\]means model correctly identified 88.5% actual churners.perfect model achieve sensitivity 1.0 (100%), meaning correctly identifies positive cases. However, ’s important note even naïve model classifies customers churners also achieve 100% sensitivity. illustrates sensitivity alone isn’t enough evaluate model’s performance—must paired metrics capture full picture.","code":""},{"path":"chapter-evaluation.html","id":"specificity","chapter":"8 Model Evaluation","heading":"Specificity","text":"sensitivity focuses positive class, Specificity measures model’s ability correctly identify negative cases. answers question:“actual negatives, many model correctly predict?”Specificity particularly important situations avoiding false positives critical. example, spam detection, incorrectly marking legitimate email spam (false positive) can severe consequences missing spam messages. Mathematically, specificity defined :\\[\n\\text{Specificity} = \\frac{\\text{True Negatives (TN)}}{\\text{True Negatives (TN)} + \\text{False Positives (FP)}}\n\\]Using kNN model confusion matrix Example 8.1, let’s calculate specificity identifying non-churners (churn = ):\\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{856}{856 + 83} = 0.912\n\\]means model correctly classified 91.2% actual non-churners leaving company.good classification model ideally achieve high sensitivity high specificity, relative importance metrics depends problem domain. example, medical diagnostics, sensitivity often prioritized ensure disease cases missed, credit scoring, specificity might take precedence avoid mistakenly classifying reliable customers risks. kNN model Example 8.1, sensitivity 0.885 specificity 0.912. trade-may acceptable instance, identifying churners (sensitivity) might critical avoiding false positives (specificity). next section, ’ll explore metrics like precision recall, refine model evaluation.","code":""},{"path":"chapter-evaluation.html","id":"precision-recall-and-f1-score","chapter":"8 Model Evaluation","heading":"8.3 Precision, Recall, and F1-Score","text":"addition sensitivity specificity, Precision, Recall, F1-Score offer deeper insights classification model’s performance. metrics particularly valuable scenarios imbalanced datasets, simple accuracy can misleading.Precision (also called positive predictive value) measures many model’s predicted positives actually positive. answers question: “model predicts positive, often correct?” formula :\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\nPrecision especially important applications false positives costly. example, fraud detection, flagging legitimate transactions fraudulent can lead customer dissatisfaction unnecessary investigations.Recall (equivalent sensitivity) measures model’s ability identify positive cases. answers question: “actual positives, many model correctly predict?” formula :\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\nrecall often used interchangeably sensitivity medical diagnostics, commonly referred recall areas like information retrieval, spam detection, text classification. Recall particularly useful cases missing positive cases (false negatives) serious consequences, failing diagnose disease missing spam emails.inherent trade-precision recall: increasing one often decreases . example, model high recall might correctly flag fraudulent transactions also mislabel many legitimate transactions fraud (low precision). Conversely, model high precision might flag transactions fraud (mostly correct), miss many actual fraud cases (low recall).balance trade-, F1-Score combines precision recall single metric. harmonic mean precision recall, emphasizing balance:\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n   = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}\n\\]\nF1-Score particularly useful cases imbalanced datasets, one class dominates . Unlike accuracy, considers false positives false negatives, providing balanced evaluation model’s predictive performance.Let’s calculate precision, recall, F1-Score k-Nearest Neighbors (kNN) model Example 8.1, predicts customer churn (churn = yes). First, precision quantifies often model’s predicted churners actual churners:\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{54}{54 + 83} = 0.394\n\\]\nmeans model predicts churn, correct 39.4% time.Next, recall measures many actual churners correctly identified model:\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{54}{54 + 7} = 0.885\n\\]\nshows model successfully identifies 88.5% actual churners.Finally, F1-Score provides single measure balances precision recall:\n\\[\nF1 = \\frac{2 \\cdot 54}{2 \\cdot 54 + 83 + 7} = 0.545\n\\]\nF1-Score indicates well model balances precision recall, offering comprehensive evaluation ability correctly identify churners minimizing false predictions. balance makes F1-Score especially useful comparing multiple models, provides single number summarizing performance.F1-Score valuable metric, assumes precision recall equally important, may always align priorities particular problem. instance, medical diagnostics, recall (ensuring cases missed) might critical precision, whereas spam filtering, precision (avoiding false positives) might take precedence. , F1-Score used alongside metrics fully understand model’s strengths weaknesses. comprehensive evaluation, now turn metrics assess performance across thresholds, described coming sections.","code":""},{"path":"chapter-evaluation.html","id":"taking-uncertainty-into-account","chapter":"8 Model Evaluation","heading":"8.4 Taking Uncertainty into Account","text":"evaluating classification model, confusion matrix derived metrics like precision, recall, F1-score provide valuable insights performance. However, metrics based discrete predictions, model already classified observations either positive negative. , lose critical layer information: uncertainty confidence behind prediction. classification models, including k-Nearest Neighbors (kNN), can provide probabilities class instead binary predictions. probabilities quantify confident model predictions, offering powerful tool fine-tuning behavior better align requirements task.Making prediction can viewed thresholding model’s probability output. default, threshold 0.5 used: probability belonging positive class 50% greater, model predicts positive class. Otherwise, predicts negative class. default threshold works many cases, universal. Adjusting threshold can significantly impact model’s performance, allowing better align business goals domain-specific needs. example, applications, false negatives may far costly false positives—vice versa. experimenting different thresholds, can explore trade-offs sensitivity, specificity, precision, recall optimize model’s performance.Example 8.2  Let’s revisit k-Nearest Neighbors (kNN) model Example 8.1 predict customer churn (churn = yes). time, instead making discrete predictions, ’ll obtain probabilities positive class setting type parameter \"prob\" kNN() function:output lists first 10 probabilities class: first column corresponds positive class (churn = yes), second column corresponds negative class (churn = ). example, first row, probability 0.4 indicates model 40% confident customer churn, probability 0.6 suggests 60% confidence customer churn. modifying threshold classification, can adjust model determines whether prediction positive negative.Now, let’s calculate confusion matrix model two different thresholds: 0.5 (default) 0.7:threshold 0.5, model classifies customer churner probability churn least 50%. threshold, confusion matrix match one Example 8.1, use default cutoff. raising threshold 0.7, model requires least 70% confidence classify customer churner. change shifts balance true positives, true negatives, false positives, false negatives. example:Lowering threshold increases sensitivity, allowing model catch true positives potentially leading false positives.Raising threshold increases specificity, reducing false positives potentially missing true positives.Adjusting threshold especially important costs false positives false negatives differ. instance, spam detection, false positives (marking legitimate emails spam) can frustrate users, raising threshold prioritize specificity may preferable. Conversely, fraud detection, missing fraudulent transaction (false negatives) may far costly, lowering threshold prioritize sensitivity make sense.Fine-tuning threshold allows us align model’s behavior business objectives application-specific goals. Suppose require sensitivity 90%. iteratively adjusting threshold recalculating sensitivity, can identify value meets target. process known defining operating point model.However, adjusting threshold always involves trade-offs. threshold maximizes sensitivity may lower precision, false positives classified positive. Similarly, threshold maximizes specificity may reduce recall, true positives misclassified negative. example, setting threshold 0.9 may result extremely high specificity cost missing true positives.Ultimately, choice threshold depends context problem specific priorities application. Whether focus minimizing errors, achieving regulatory compliance, balancing precision recall, threshold tailored meet objectives. experimenting different thresholds, can optimize model’s performance best suit needs task hand. next section, ’ll explore tools like Receiver Operating Characteristic (ROC) curve Area Curve (AUC), provide systematic way evaluate model performance across range thresholds.","code":"kNN_prob = kNN(formula = formula, train = train_set, \n               test = test_set, k = 5, scaler = \"minmax\",\n               type = \"prob\")\nkNN_prob[1:10, ]\n      yes  no\n   6  0.4 0.6\n   10 0.2 0.8\n   17 0.0 1.0\n   19 0.0 1.0\n   21 0.0 1.0\n   23 0.2 0.8\n   29 0.0 1.0\n   31 0.0 1.0\n   36 0.0 1.0\n   40 0.0 1.0conf.mat(kNN_prob[, 1], actual_test, cutoff = 0.5)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  54   7\n       no   83 856\nconf.mat(kNN_prob[, 1], actual_test, cutoff = 0.7)\n   Setting levels: reference = \"yes\", case = \"no\"\n          Actual\n   Predict yes  no\n       yes  22   1\n       no  115 862"},{"path":"chapter-evaluation.html","id":"roc-curve-and-auc","chapter":"8 Model Evaluation","heading":"8.5 ROC Curve and AUC","text":"Manually experimenting thresholds insightful often impractical. Additionally, metrics like sensitivity, specificity, precision, recall attempt summarize model’s performance, provide snapshots specific thresholds. need way evaluate model performs across range thresholds, offering broader view behavior. Models often vary achieve accuracy; two models similar overall accuracy may excel entirely different aspects prediction. example, one model might identify positives misclassify many negatives, another might opposite. systematically evaluate model’s performance across thresholds, use Receiver Operating Characteristic (ROC) curve associated metric, Area Curve (AUC). tools provide visual quantitative way assess model’s ability distinguish positive negative classes.ROC curve graphical representation trade-sensitivity (true positive rate) specificity (true negative rate) across different thresholds. plots True Positive Rate (Sensitivity) False Positive Rate (1 - Specificity). concept originated World War II measure radar receiver performance, distinguishing true signals false alarms. modern machine learning, ’s invaluable tool evaluating classifier effectiveness.characteristics ROC curve illustrated Figure 8.1. vertical axis, True Positive Rate (Sensitivity) plotted, False Positive Rate (1 - Specificity) plotted horizontal axis. Several key scenarios highlighted figure:\n- Optimal Performance (Green Curve): model near-perfect performance passes top-left corner, achieving high sensitivity high specificity.\n- Good Performance (Blue Curve): model decent perfect performance curve remains closer top-left corner diagonal line.\n- Random Classifier (Diagonal Line): diagonal line (gray dashed) represents model predictive value, classifying purely random. classifier close line offers little utility.\nFigure 8.1: ROC curve illustrates trade-sensitivity specificity different thresholds. diagonal line represents classifier predictive value (gray dashed line), curves represent varying levels performance: green optimal blue good.\npoint ROC curve corresponds specific threshold. thresholds vary, True Positive Rate (Sensitivity) False Positive Rate (1 - Specificity) change, tracing curve. closer curve top-left corner, better model’s performance distinguishing classes.construct ROC curve, classifier’s predictions sorted estimated probabilities positive class. Starting origin, prediction’s impact sensitivity specificity plotted. Correct predictions (true positives) result vertical movements, incorrect predictions (false positives) lead horizontal shifts.Let’s apply concept k-Nearest Neighbors (kNN) model Example 8.2, obtained probabilities positive class (churn = yes). ’ll use probabilities generate ROC curve model. pROC package R simplifies process. Ensure package installed using install.packages(\"pROC\") proceeding.create ROC curve, two inputs needed: estimated probabilities positive class actual class labels. Using roc() function pROC package, can create ROC curve object follows:can visualize ROC curve using ggroc() function ggplot2 package plot() function basic display. ’s ROC curve kNN model:\nFigure 8.2: ROC curve KNN k = 5, based churn data.\nROC curve visually demonstrates model’s performance across different thresholds. curve closer top-left corner indicates better performance, achieves high sensitivity specificity. diagonal line represents random classifier, providing baseline comparison. case, kNN model’s ROC curve much closer top-left corner, suggesting strong performance distinguishing churners non-churners.Another critical metric derived ROC curve Area Curve (AUC). AUC quantifies overall performance model, summarizing ROC curve single number. AUC value represents probability randomly chosen positive instance higher predicted score randomly chosen negative instance.\nFigure 8.3: AUC summarizes ROC curve single number, representing model’s ability rank positive cases higher negative ones. AUC = 1: Perfect model. AUC = 0.5: better random guessing.\n’s AUC values interpreted:AUC = 1: Perfect classifier.AUC = 0.5: better random guessing.kNN model, can calculate AUC follows:AUC value model 0.849, indicating model ranks positive cases higher negative ones probability 0.849.ROC curve AUC offer comprehensive systematic way evaluate classification models, enabling comparisons models helping identify optimal threshold specific tasks. tools particularly valuable working imbalanced datasets, account trade-offs sensitivity specificity across thresholds. combining insights metrics like precision, recall, F1-score, can develop deeper understanding model performance select best approach problem hand.","code":"\nlibrary(pROC)\n\nroc_knn <- roc(response = actual_test, predictor = kNN_prob[, 1])\nggroc(roc_knn, colour = \"blue\") +\n    ggtitle(\"ROC curve for KNN with k = 5, based on churn data\")auc(roc_knn)\n   Area under the curve: 0.8494"},{"path":"chapter-evaluation.html","id":"metrics-for-multi-class-classification","chapter":"8 Model Evaluation","heading":"8.6 Metrics for Multi-Class Classification","text":"far, ’ve focused binary classification, target variable two categories. However, many real-world problems involve multi-class classification, target variable can belong three categories. Examples include classifying species ecological studies identifying different types vehicles. Evaluating models requires extending metrics handle multiple categories effectively.confusion matrix multi-class classification expands include classes, row representing actual class column representing predicted class. Correct predictions appear diagonal, -diagonal elements indicate misclassifications. structure highlights classes model struggles distinguish.Metrics like Accuracy, Precision, Recall, F1-Score can adapted multi-class problems. class, model evaluated class “positive” class others “negative.” Precision, Recall, F1-Score calculated per class. summarize overall performance, compute averages :Macro-Average: Treats classes equally taking unweighted mean metrics across classes.Micro-Average: Aggregates predictions across classes, giving weight larger classes.Weighted-Average: Weights class’s metric frequency dataset.metrics ensure balanced evaluation, especially dealing imbalanced datasets, classes may significantly fewer samples others.metrics like ROC curve AUC designed binary classification, can adapted multi-class problems using techniques like one-vs-(evaluating class others). However, applications, metrics like macro-averaged F1-Score provide clear practical summary multi-class model’s performance.using metrics, can evaluate well model performs across categories, identify weaknesses specific classes, ensure model aligns requirements task.","code":""},{"path":"chapter-evaluation.html","id":"evaluation-metrics-for-continuous-targets","chapter":"8 Model Evaluation","heading":"8.7 Evaluation Metrics for Continuous Targets","text":"’ve focused evaluating classification models far, many real-world problems involve predicting continuous target variables, house prices, stock market trends, weather forecasts. problems require regression models, assessed using metrics designed continuous data.One common evaluation metrics regression models Mean Squared Error (MSE):\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n, \\(y_i\\) represents actual value, \\(\\hat{y}_i\\) predicted value, \\(n\\) number observations. MSE measures average squared difference predicted actual values, penalizing larger errors heavily. Lower MSE values indicate better model performance, zero representing perfect fit.Although widely used, MSE limitations, particularly sensitivity outliers. robust evaluation, can use Mean Absolute Error (MAE), calculates average absolute difference predicted actual values:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{=1}^{n} |y_i - \\hat{y}_i|\n\\]\nUnlike MSE, MAE treats errors equally, making less sensitive extreme values interpretable certain contexts. ’s particularly useful target variable skewed distribution outliers present.Another widely used metric regression models \\(R^2\\) score, coefficient determination. \\(R^2\\) score measures proportion variance target variable model explains. ranges 0 1, higher values indicate better fit. \\(R^2\\) 1 implies model perfectly predicts target variable, value 0 suggests model provides better predictions mean.metrics provide starting point evaluating regression models, choice metric depends specific problem goals. instance, prioritizing interpretability, MAE might meaningful, whereas MSE useful larger errors must penalized. explore metrics greater depth Chapter 10, dive regression modeling techniques.","code":""},{"path":"chapter-evaluation.html","id":"summary","chapter":"8 Model Evaluation","heading":"8.8 Summary","text":"chapter, explored critical step Model Evaluation, determines well model performs whether meets requirements problem hand. Starting foundational concepts, examined metrics evaluating classification models, including binary, multi-class, regression models.","code":""},{"path":"chapter-evaluation.html","id":"key-takeaways-1","chapter":"8 Model Evaluation","heading":"Key Takeaways","text":"Binary Classification Metrics:\nbegan understanding confusion matrix, categorizes predictions true positives, true negatives, false positives, false negatives. , derived metrics like accuracy, sensitivity (recall), specificity, precision, F1-Score evaluate trade-offs different types errors.Binary Classification Metrics:\nbegan understanding confusion matrix, categorizes predictions true positives, true negatives, false positives, false negatives. , derived metrics like accuracy, sensitivity (recall), specificity, precision, F1-Score evaluate trade-offs different types errors.Threshold Tuning:\nRecognizing impact probability thresholds model predictions, discussed adjusting thresholds can help align model specific goals, maximizing sensitivity critical applications prioritizing specificity avoid false positives.Threshold Tuning:\nRecognizing impact probability thresholds model predictions, discussed adjusting thresholds can help align model specific goals, maximizing sensitivity critical applications prioritizing specificity avoid false positives.ROC Curve AUC:\nevaluate model performance across possible thresholds, introduced Receiver Operating Characteristic (ROC) curve Area Curve (AUC). tools provide systematic visual way assess model’s ability distinguish classes, particularly useful comparing multiple models.ROC Curve AUC:\nevaluate model performance across possible thresholds, introduced Receiver Operating Characteristic (ROC) curve Area Curve (AUC). tools provide systematic visual way assess model’s ability distinguish classes, particularly useful comparing multiple models.Multi-Class Classification:\nmulti-class problems, extended metrics like precision, recall, F1-Score calculating per-class metrics aggregating using methods macro-average, micro-average, weighted-average. ensures balanced evaluation, even imbalanced datasets.Multi-Class Classification:\nmulti-class problems, extended metrics like precision, recall, F1-Score calculating per-class metrics aggregating using methods macro-average, micro-average, weighted-average. ensures balanced evaluation, even imbalanced datasets.Regression Metrics:\nFinally, covered metrics evaluating regression models, Mean Squared Error (MSE), Mean Absolute Error (MAE), \\(R^2\\) score, measure accuracy predictions continuous target variables. metrics offer flexibility depending whether minimizing large errors achieving interpretability important.Regression Metrics:\nFinally, covered metrics evaluating regression models, Mean Squared Error (MSE), Mean Absolute Error (MAE), \\(R^2\\) score, measure accuracy predictions continuous target variables. metrics offer flexibility depending whether minimizing large errors achieving interpretability important.","code":""},{"path":"chapter-evaluation.html","id":"closing-thoughts","chapter":"8 Model Evaluation","heading":"Closing Thoughts","text":"chapter emphasized single metric can capture full picture model’s performance. Instead, evaluation consider specific goals constraints problem, whether minimizing errors, handling imbalanced data, aligning business objectives. Proper evaluation ensures model accurate also actionable reliable real-world applications.mastering evaluation techniques, now equipped critically assess model performance, optimize thresholds, select right model task hand. following chapters, build foundation explore advanced modeling techniques evaluation greater detail.","code":""},{"path":"chapter-bayes.html","id":"chapter-bayes","chapter":"9 Naive Bayes Classifier","heading":"9 Naive Bayes Classifier","text":"Naive Bayes Classifier one simplest yet surprisingly powerful algorithms machine learning. family probabilistic classifiers based Bayes’ Theorem, key assumption—often referred “naive”—features conditionally independent given target class. Despite oversimplified assumption, Naive Bayes often delivers strong performance practice, especially domains like text classification, spam detection, sentiment analysis, medical diagnosis.Naive Bayes celebrated speed, scalability, interpretability. efficient training prediction phases, making suitable large-scale datasets high-dimensional feature spaces. example, text classification tasks, thousands features (e.g., words tokens) may involved, Naive Bayes can classify data points milliseconds. simplicity ease implementation make foundational tool beginners go-algorithm many real-world tasks.roots algorithm lie Bayes’ Theorem 6, principle introduced 18th-century mathematician Thomas Bayes. theorem provides mathematical framework updating probability hypothesis new evidence observed. core, Bayes’ Theorem refines understanding event combining prior knowledge (known prior distribution) new information observed data (resulting posterior distribution). ideas form foundation Bayesian methods, wide-ranging applications machine learning, statistics, beyond.","code":""},{"path":"chapter-bayes.html","id":"strengths-and-limitations","chapter":"9 Naive Bayes Classifier","heading":"Strengths and Limitations","text":"strength Naive Bayes lies simplicity computational efficiency. particularly effective :High-dimensional datasets (e.g., text data thousands features).Tasks requiring quick predictions, real-time spam detection.Problems feature independence approximately true independence assumption major limitation.However, limitations also important acknowledge:independence assumption often hold real-world data, especially features highly correlated.Naive Bayes may struggle scenarios continuous data unless Gaussian distributions assumed.tends underperform complex datasets compared sophisticated algorithms like random forests gradient boosting.Despite limitations, Naive Bayes remains reliable, interpretable, robust algorithm. often first choice quick prototyping serves benchmark advanced models.","code":""},{"path":"chapter-bayes.html","id":"what-will-this-chapter-cover","chapter":"9 Naive Bayes Classifier","heading":"What Will This Chapter Cover?","text":"chapter, :Explore mathematical foundations Naive Bayes, including Bayes’ Theorem application classification.Examine Naive Bayes works, step--step explanations examples.Discuss different variants algorithm, including Gaussian Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes, use cases.Highlight strengths, limitations, practical applications.Provide implementation guide R, using real-world datasets (risk dataset liver package) demonstrate effectiveness.end chapter, solid understanding Naive Bayes Classifier, theoretical underpinnings, practical applications, enabling confidently apply real-world problems.","code":""},{"path":"chapter-bayes.html","id":"bayes-theorem-and-probabilistic-foundations","chapter":"9 Naive Bayes Classifier","heading":"9.1 Bayes’ Theorem and Probabilistic Foundations","text":"explain uncertainty predict outcomes using single, elegant equation? presented Equation (9.1), Bayes’ Theorem cornerstone probabilistic reasoning, offering mathematical framework updating beliefs light new evidence. author “Everything Predictable: Bayesian Statistics Explain World” argues Bayesian statistics help predict future also explain fabric rational decision-making. heart powerful framework lies work Thomas Bayes, 18th-century Presbyterian minister self-taught mathematician, whose contributions provided systematic way refine probabilities new information becomes available.","code":""},{"path":"chapter-bayes.html","id":"the-essence-of-bayes-theorem","chapter":"9 Naive Bayes Classifier","heading":"The Essence of Bayes’ Theorem","text":"Bayes’ Theorem formula calculating probability event (\\(\\)) based prior knowledge new evidence (\\(B\\)). answers question: Given already know, belief hypothesis change observe new data?Mathematically, expressed :\\[\\begin{equation}\nP(|B) = P() \\cdot \\frac{P(B|)}{P(B)}\n\\tag{9.1}\n\\end{equation}\\]:\\(P(|B)\\): posterior probability—probability event \\(\\) (hypothesis) given event \\(B\\) (evidence) occurred.\\(P()\\): prior probability—belief \\(\\) observing \\(B\\).\\(P(B|)\\): likelihood—probability observing \\(B\\) assuming \\(\\) true.\\(P(B)\\): evidence—total probability observing \\(B\\).Bayes’ Theorem elegantly combines prior knowledge new evidence refine understanding uncertainty. foundational principle probabilistic learning, quantifying data adjust expectations.see Bayes’ Theorem action, consider practical example risk dataset liver package. , calculate probability customer good risk profile (\\(\\)) given mortgage (\\(B\\)).Example 9.1  Suppose tasked estimating probability customer good risk mortgage. risk dataset contains relevant information:Adding margins contingency table clarity:Now, define events:\\(\\): Customer “good risk”.\\(B\\): Customer mortgage (mortgage = yes).prior probability customer good risk :\\[\nP() = \\frac{\\text{Total Good Risk Cases}}{\\text{Total Cases}} = \\frac{123}{246} = 0.5\n\\]Using Bayes’ Theorem, probability good risk given customer mortgage :\\[\\begin{equation}\n\\label{eq1}\n\\begin{split}\nP(\\text{Good Risk} | \\text{Mortgage = Yes}) & = \\frac{P(\\text{Good Risk} \\cap \\text{Mortgage = Yes})}{P(\\text{Mortgage = Yes})} \\\\\n& = \\frac{\\text{Good Risk Mortgage Cases}}{\\text{Total Mortgage Cases}} \\\\\n& = \\frac{81}{175} \\\\\n& = 0.463\n\\end{split}\n\\end{equation}\\]demonstrates customers mortgages lower probability classified good risk compared overall population.","code":"data(risk)\n\nxtabs(~ risk + mortgage, data = risk)\n              mortgage\n   risk        yes no\n     good risk  81 42\n     bad risk   94 29addmargins(xtabs(~ risk + mortgage, data = risk))\n              mortgage\n   risk        yes  no Sum\n     good risk  81  42 123\n     bad risk   94  29 123\n     Sum       175  71 246"},{"path":"chapter-bayes.html","id":"how-does-bayes-theorem-work","chapter":"9 Naive Bayes Classifier","heading":"How Does Bayes’ Theorem Work?","text":"Bayes’ Theorem leverages conditional probability describe likelihood event changes based specific conditions. example:\n- medical diagnostics, estimates probability disease (\\(\\)) given positive test result (\\(B\\)), accounting test’s reliability disease prevalence.\n- spam detection, computes probability email spam (\\(\\)) based occurrence certain keywords (\\(B\\)).Probability theory provides rigorous mathematical structure reasoning uncertainty, Bayes’ Theorem transforms framework learning data making rational decisions.","code":""},{"path":"chapter-bayes.html","id":"a-gateway-to-naive-bayes","chapter":"9 Naive Bayes Classifier","heading":"A Gateway to Naive Bayes","text":"Naive Bayes Classifier builds directly Bayes’ Theorem. assuming features conditionally independent given target class, simplifies computation probabilities large, high-dimensional datasets. assumption often violated practice, frequently works well enough yield highly effective results, especially applications like text classification spam filtering.proceed, ’ll see Bayes’ Theorem forms foundation Naive Bayes algorithm, enabling handle complex datasets efficiently maintaining simplicity interpretability.","code":""},{"path":"chapter-bayes.html","id":"why-is-it-called-naive","chapter":"9 Naive Bayes Classifier","heading":"9.2 Why is it Called “Naive”?","text":"“naive” Naive Bayes reflects algorithm’s simplifying assumption features conditionally independent , given target class. reality, features often correlated (e.g., income age), assumption dramatically simplifies computations, making algorithm efficient scalable.illustrate, consider risk dataset liver package:can see dataset includes features age, income, marital status, mortgage, number loans. Naive Bayes assumes features independent conditioned target class (risk), can either good risk bad risk. Let’s express mathematically. target variable \\(Y\\) represents risk, possible values \\(y_1 = \\text{good risk}\\) \\(y_2 = \\text{bad risk}\\), predictors \\(X_1, X_2, \\dots, X_5\\). Using Bayes’ Theorem (Equation (9.1)), probability \\(Y = y_1\\) given features :\\[\nP(Y = y_1 | X_1 \\cap \\dots \\cap X_5) = \\frac{P(Y = y_1) \\cdot P(X_1 \\cap \\dots \\cap X_5 | Y = y_1)}{P(X_1 \\cap \\dots \\cap X_5)}\n\\]However, calculating \\(P(X_1 \\cap X_2 \\cap \\dots \\cap X_5 | Y = y_1)\\) computationally challenging, especially number predictors grows. example, datasets hundreds thousands features (common domains like text classification) require enormous amounts memory store probabilities possible combinations features.naive assumption conditional independence simplifies treating feature independent others, given target class. allows joint probability term \\(P(X_1 \\cap \\dots \\cap X_5 | Y = y_1)\\) expressed product individual probabilities:\\[\nP(X_1 \\cap \\dots \\cap X_5 | Y = y_1) = P(X_1 | Y = y_1) \\cdot \\dots \\cdot P(X_5 | Y = y_1)\n\\]transformation eliminates need compute complex joint probabilities allows algorithm operate efficiently, even high-dimensional datasets. Instead handling exponential number combinations, Naive Bayes calculates conditional probabilities feature independently, given class.practice, independence assumption rarely true—features often exhibit degree correlation. However, Naive Bayes frequently performs surprisingly well despite limitation. excels domains like text classification, independence assumption approximately holds, slight violations assumption significantly affect predictive accuracy. example, spam detection systems sentiment analysis often rely Naive Bayes due simplicity, speed, effectiveness.combining strengths ability handle high-dimensional data, Naive Bayes strikes balance computational efficiency predictive power, making foundational algorithm machine learning.","code":"str(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr.loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-bayes.html","id":"the-laplace-smoothing-technique","chapter":"9 Naive Bayes Classifier","heading":"9.3 The Laplace Smoothing Technique","text":"One primary challenges Naive Bayes algorithm vulnerability zero probabilities. issue arises feature category present test data missing training data. happens, algorithm assigns probability zero unseen category, since Naive Bayes multiplies probabilities prediction, even single zero probability results overall prediction probability zero affected class. effectively eliminates class possible prediction can significantly degrade classifier’s performance.address issue, Laplace Smoothing (also known add-one smoothing) employed. Named French mathematician Pierre-Simon Laplace, technique ensures every class-feature combination non-zero probability, even missing training data. Laplace smoothing works adding small constant (commonly \\(k = 1\\)) count frequency table, ensuring category left zero probability.illustrate necessity Laplace smoothing, consider marital variable risk dataset. Suppose category married entirely absent class bad risk training data due imbalance sampling limitations. Let’s visualize situation:case, probability \\(P(\\text{bad risk} | \\text{married})\\) zero. creates significant problem: Naive Bayes classifier completely ignore instance marital = married predicting bad risk class. However, intuitively, even examples absent training data, probability still small non-zero value reflect possibility combination occur test data.Laplace smoothing resolves modifying calculation. adds small constant \\(k\\) (usually \\(k = 1\\)) count frequency table. smoothed probability calculated :\\[\nP(\\text{bad risk} | \\text{married}) = \\frac{\\text{count}(\\text{bad risk} \\cap \\text{married}) + k}{\\text{count}(\\text{bad risk}) + k \\cdot \\text{total unique categories } \\text{marital}}\n\\]adjustment ensures :Counts adjusted: category receives additional \\(k\\) count numerator.Denominator expanded: total count increased \\(k \\times \\text{number categories}\\), ensuring probability distribution remains valid.guarantees every feature-class combination small, non-zero probability, thus preventing zero probabilities dominating predictions.R, naivebayes package provides laplace argument apply Laplace smoothing. default, laplace = 0, meaning smoothing applied. apply smoothing, simply set laplace = 1. instance:ensures class-feature combination zero probability, improving robustness Naive Bayes classifier, especially dealing small imbalanced training datasets. \\(k = 1\\) commonly used, value \\(k\\) can adjusted based specific domain knowledge requirements. However, practice, setting \\(k = 1\\) suffices use cases.Laplace smoothing simple yet effective technique highlights minor adjustments can address critical limitations machine learning algorithms. ensuring probabilities remain non-zero, enhances reliability robustness Naive Bayes real-world scenarios.","code":"            risk\n   marital   good risk bad risk\n     single         21       11\n     married        51        0\n     other           8       10\nlibrary(naivebayes)\n\n# Fit Naive Bayes with Laplace smoothing\nmodel <- naive_bayes(risk ~ age + income + marital + mortgage + nr.loans, \n                     data = risk, \n                     laplace = 1)"},{"path":"chapter-bayes.html","id":"types-of-naive-bayes-classifiers","chapter":"9 Naive Bayes Classifier","heading":"9.4 Types of Naive Bayes Classifiers","text":"Naive Bayes flexible algorithm variants tailored different types data problem domains. choice Naive Bayes classifier depends nature features assumptions underlying distribution. three common types :Multinomial Naive Bayes: Best suited categorical discrete count features, word frequencies text data. example, marital variable risk dataset categorical, making good fit variant.Multinomial Naive Bayes: Best suited categorical discrete count features, word frequencies text data. example, marital variable risk dataset categorical, making good fit variant.Bernoulli Naive Bayes: Designed binary features (e.g., 0s 1s). variant ideal datasets yes/presence/absence features. instance, mortgage variable risk dataset, two categories (yes ), fits variant.Bernoulli Naive Bayes: Designed binary features (e.g., 0s 1s). variant ideal datasets yes/presence/absence features. instance, mortgage variable risk dataset, two categories (yes ), fits variant.Gaussian Naive Bayes: Used continuous features assumed follow normal (Gaussian) distribution. example, age income variables risk dataset continuous thus suitable variant.Gaussian Naive Bayes: Used continuous features assumed follow normal (Gaussian) distribution. example, age income variables risk dataset continuous thus suitable variant.variant optimized different data types, making essential choose one aligns dataset’s characteristics. understanding distinctions, can select appropriate Naive Bayes classifier achieve optimal performance. following sections, delve variant, exploring unique characteristics use cases.","code":""},{"path":"chapter-bayes.html","id":"case-study-predicting-risk-profiles","chapter":"9 Naive Bayes Classifier","heading":"9.5 Case Study: Predicting Risk Profiles","text":"case study, apply Naive Bayes classifier predict financial risk using real-world risk dataset liver package R. goal classify customers either “good risk” “bad risk” based several predictors.","code":""},{"path":"chapter-bayes.html","id":"overview-of-the-dataset-1","chapter":"9 Naive Bayes Classifier","heading":"Overview of the Dataset","text":"start loading dataset examining structure:risk dataset data.frame 6 variables 246 observations. contains 5 predictors target variable risk, binary factor two levels: “good risk” “bad risk.” predictors dataset :age: Age years.marital: Marital status (levels: “single,” “married,” “”).income: Yearly income.mortgage: Whether customer mortgage (levels: “yes,” “”).nr_loans: Number loans customer .risk: target variable (levels: “good risk,” “bad risk”).additional details dataset, refer documentation .","code":"data(risk)\n\nstr(risk)\n   'data.frame':    246 obs. of  6 variables:\n    $ age     : int  34 37 29 33 39 28 28 25 41 26 ...\n    $ marital : Factor w/ 3 levels \"single\",\"married\",..: 3 3 3 3 3 3 3 3 3 3 ...\n    $ income  : num  28061 28009 27615 27287 26954 ...\n    $ mortgage: Factor w/ 2 levels \"yes\",\"no\": 1 2 2 1 1 2 2 2 2 2 ...\n    $ nr.loans: int  3 2 2 2 2 2 3 2 2 2 ...\n    $ risk    : Factor w/ 2 levels \"good risk\",\"bad risk\": 2 2 2 2 2 2 2 2 2 2 ..."},{"path":"chapter-bayes.html","id":"data-preparation-1","chapter":"9 Naive Bayes Classifier","heading":"Data Preparation","text":"evaluate model’s performance, partition dataset training testing sets, using 80/20 split. ensures classifier trained one subset tested unseen data:set.seed() function ensures reproducibility data split.validate partitioning, test whether proportions marital variable similar training testing sets. use chi-squared test compare proportions across three categories marital:hypotheses test :\\[\n\\begin{aligned}\nH_0 &: \\text{proportions `marital` categories training test sets.} \\\\\nH_a &: \\text{least one category different proportion.}\n\\end{aligned}\n\\]Since p-value exceeds \\(\\alpha = 0.05\\), fail reject \\(H_0\\). indicates proportions marital categories statistically similar training test sets, confirming partitioning valid.","code":"\nset.seed(5)\n\ndata_sets = partition(data = risk, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test = test_set$riskchisq.test(x = table(train_set$marital), y = table(test_set$marital))\n   \n    Pearson's Chi-squared test\n   \n   data:  table(train_set$marital) and table(test_set$marital)\n   X-squared = 6, df = 4, p-value = 0.1991"},{"path":"chapter-bayes.html","id":"applying-the-naive-bayes-classifier","chapter":"9 Naive Bayes Classifier","heading":"Applying the Naive Bayes Classifier","text":"specify model formula, risk target variable predictors features:Using naivebayes package, train Naive Bayes classifier training data:naive_bayes() function computes conditional probabilities feature given target class. categorical predictors (e.g., marital, mortgage), calculates class-conditional probabilities. continuous predictors (e.g., age, income, nr.loans), assumes Gaussian distribution calculates mean standard deviation class.summary output provides following details:Categorical predictors (e.g., marital, mortgage): Class-conditional probabilities.Continuous predictors (e.g., age, income, nr.loans): Means standard deviations Gaussian distributions.information forms basis making predictions trained model.","code":"\nformula = risk ~ age + income + mortgage + nr.loans + maritallibrary(naivebayes)\n\nnaive_bayes = naive_bayes(formula, data = train_set)\n\nnaive_bayes\n   \n   ================================= Naive Bayes ==================================\n   \n   Call:\n   naive_bayes.formula(formula = formula, data = train_set)\n   \n   -------------------------------------------------------------------------------- \n    \n   Laplace smoothing: 0\n   \n   -------------------------------------------------------------------------------- \n    \n   A priori probabilities: \n   \n   good risk  bad risk \n   0.4923858 0.5076142 \n   \n   -------------------------------------------------------------------------------- \n    \n   Tables: \n   \n   -------------------------------------------------------------------------------- \n   :: age (Gaussian) \n   -------------------------------------------------------------------------------- \n         \n   age    good risk  bad risk\n     mean 46.453608 35.470000\n     sd    8.563513  9.542520\n   \n   -------------------------------------------------------------------------------- \n   :: income (Gaussian) \n   -------------------------------------------------------------------------------- \n         \n   income good risk  bad risk\n     mean 48888.987 27309.560\n     sd    9986.962  7534.639\n   \n   -------------------------------------------------------------------------------- \n   :: mortgage (Bernoulli) \n   -------------------------------------------------------------------------------- \n           \n   mortgage good risk  bad risk\n        yes 0.6804124 0.7400000\n        no  0.3195876 0.2600000\n   \n   -------------------------------------------------------------------------------- \n   :: nr.loans (Gaussian) \n   -------------------------------------------------------------------------------- \n           \n   nr.loans good risk  bad risk\n       mean 1.0309278 1.6600000\n       sd   0.7282057 0.7550503\n   \n   -------------------------------------------------------------------------------- \n   :: marital (Categorical) \n   -------------------------------------------------------------------------------- \n            \n   marital    good risk   bad risk\n     single  0.38144330 0.49000000\n     married 0.52577320 0.11000000\n     other   0.09278351 0.40000000\n   \n   --------------------------------------------------------------------------------summary(naive_bayes)\n   \n   ================================= Naive Bayes ================================== \n    \n   - Call: naive_bayes.formula(formula = formula, data = train_set) \n   - Laplace: 0 \n   - Classes: 2 \n   - Samples: 197 \n   - Features: 5 \n   - Conditional distributions: \n       - Bernoulli: 1\n       - Categorical: 1\n       - Gaussian: 3\n   - Prior probabilities: \n       - good risk: 0.4924\n       - bad risk: 0.5076\n   \n   --------------------------------------------------------------------------------"},{"path":"chapter-bayes.html","id":"prediction-and-model-evaluation","chapter":"9 Naive Bayes Classifier","heading":"Prediction and Model Evaluation","text":"now use trained model predict posterior probabilities test set:output contains predicted probabilities class:first column shows probability “good risk.”second column shows probability “bad risk.”example, first row probability 0.995 “bad risk,” indicates high likelihood first customer test set belongs “bad risk” class.","code":"prob_naive_bayes = predict(naive_bayes, test_set, type = \"prob\")\n\nround(head(prob_naive_bayes, n = 10), 3)\n         good risk bad risk\n    [1,]     0.001    0.999\n    [2,]     0.013    0.987\n    [3,]     0.000    1.000\n    [4,]     0.184    0.816\n    [5,]     0.614    0.386\n    [6,]     0.193    0.807\n    [7,]     0.002    0.998\n    [8,]     0.002    0.998\n    [9,]     0.378    0.622\n   [10,]     0.283    0.717"},{"path":"chapter-bayes.html","id":"confusion-matrix-1","chapter":"9 Naive Bayes Classifier","heading":"Confusion Matrix","text":"evaluate classification performance, compute confusion matrix using cutoff 0.5:confusion matrix summarizes model’s predictions:True Positives (TP): Correctly predicted “good risk.”True Negatives (TN): Correctly predicted “bad risk.”False Positives (FP): Predicted “good risk” “bad risk.”False Negatives (FN): Predicted “bad risk” “good risk.”values confusion matrix reveal model’s accuracy types errors cutoff 0.5. example, classifier makes “24 + 20” correct predictions “3 + 2” incorrect predictions.","code":"prob_naive_bayes = prob_naive_bayes[, 1] # Probability of \"good risk\"\n\nconf.mat(prob_naive_bayes, actual_test, cutoff = 0.5, \n         reference = \"good risk\")\n              Actual\n   Predict     good risk bad risk\n     good risk        24        3\n     bad risk          2       20\n\nconf.mat.plot(prob_naive_bayes, actual_test, cutoff = 0.5, \n              reference = \"good risk\")"},{"path":"chapter-bayes.html","id":"roc-curve-and-auc-1","chapter":"9 Naive Bayes Classifier","heading":"ROC Curve and AUC","text":"evaluate model, compute ROC curve AUC value:ROC curve illustrates trade-sensitivity specificity different thresholds. closer curve top-left corner, better model’s performance.Finally, calculate AUC value:AUC value, 0.957, quantifies model’s ability rank positive instances higher negative ones. value closer 1 indicates excellent performance, value 0.5 represents random guessing.applying Naive Bayes classifier risk dataset, demonstrated ability efficiently classify customers “good risk” “bad risk.” metrics confusion matrix, ROC curve, AUC, evaluated model’s predictive power identified strengths limitations. process highlights simplicity interpretability Naive Bayes, making practical choice many real-world classification problems.","code":"\nroc_naive_bayes = roc(actual_test, prob_naive_bayes)\n\nggroc(roc_naive_bayes)round(auc(roc_naive_bayes), 3)\n   [1] 0.957"},{"path":"chapter-bayes.html","id":"exercises-6","chapter":"9 Naive Bayes Classifier","heading":"9.6 Exercises","text":"..","code":""},{"path":"chapter-regression.html","id":"chapter-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10 Regression Modeling: From Basics to Advanced Techniques","text":"Regression modelling cornerstone statistical analysis centuries, evolving one powerful versatile tools data science. origins trace back Isaac Newton’s work 1700s, term “regression” later introduced Francis Galton 19th century describe biological phenomena. Early pioneers like Legendre Gauss laid mathematical groundwork development least squares method, today, thanks advancements computing programming languages like R, regression analysis accessible, scalable, integral solving real-world problems.Charles Wheelan eloquently puts book “Naked Statistics”:Regression modelling hydrogen bomb statistics arsenal.makes regression powerful ability quantify relationships variables, uncover patterns, make predictions. Whether ’re estimating impact advertising spend sales, forecasting housing prices, identifying risk factors disease, regression modelling provides foundation evidence-based decision-making.chapter, explore essentials regression, simplest form—simple linear regression—advanced techniques like generalized linear models (GLMs) non-linear regression. end, understand mathematical principles behind regression models also gain practical experience applying R analyze interpret real-world data.","code":""},{"path":"chapter-regression.html","id":"sec-simple-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.1 Simple Linear Regression","text":"explore regression methods estimation prediction, use marketing dataset liver package. dataset provides straightforward practical example real-world scenario, company seeks optimize advertising spending maximize revenue. small size clean structure make ideal learning tool understanding regression concepts. dataset contains information advertising campaigns, including spending, clicks, impressions, transactions, daily revenue. target variable, revenue, represents daily revenue generated, seven variables serve predictors.marketing dataset, formatted data.frame, consists 40 observations (rows) 8 variables (columns):spend: Daily spending pay-per-click (PPC) advertising.clicks: Number clicks ads.impressions: Number ad impressions per day.display: Whether display campaign running (yes ).transactions: Number transactions per day.click.rate: Click-rate (CTR).conversion.rate: Conversion rate.revenue: Daily revenue (target variable).Let’s load examine structure dataset:dataset includes 8 variables 40 observations, 7 predictors one numerical-continuous target variable (revenue). clean dataset serves perfect starting point regression analysis.understand relationships variables, use pairs.panels() function psych package create visualization:plot includes: Bivariate scatter plots (bottom-left) showing relationships pairs variables; Histograms (diagonal) showing distribution variable; Correlation coefficients (top-right) quantifying strength linear relationships. example, variables spend revenue exhibit strong positive linear relationship, correlation coefficient 0.79. indicates higher spending generally associated higher revenue, supporting hypothesis linear relationship variables.","code":"data(marketing, package = \"liver\")\n\nstr(marketing)\n   'data.frame':    40 obs. of  8 variables:\n    $ spend          : num  22.6 37.3 55.6 45.4 50.2 ...\n    $ clicks         : int  165 228 291 247 290 172 68 112 306 300 ...\n    $ impressions    : int  8672 11875 14631 11709 14768 8698 2924 5919 14789 14818 ...\n    $ display        : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ transactions   : int  2 2 3 2 3 2 1 1 3 3 ...\n    $ click.rate     : num  1.9 1.92 1.99 2.11 1.96 1.98 2.33 1.89 2.07 2.02 ...\n    $ conversion.rate: num  1.21 0.88 1.03 0.81 1.03 1.16 1.47 0.89 0.98 1 ...\n    $ revenue        : num  58.9 44.9 141.6 209.8 197.7 ...\npairs.panels(marketing)"},{"path":"chapter-regression.html","id":"fitting-a-simple-linear-regression-model","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Fitting a Simple Linear Regression Model","text":"begin analysis, focus simple linear regression model examines relationship single predictor (spend) target variable (revenue). provides foundational understanding regression expanding complex models involving multiple predictors. First, let’s visualize relationship scatter plot overlay regression line:\nFigure 10.1: Scatter plot daily revenue (€) versus daily spend (€) 40 observations, fitted least-squares regression line (blue) showing linear relationship.\nFigure 10.1 displays scatter plot spend versus revenue marketing dataset, fitted least-squares regression line.regression equation :\\[\n\\hat{y} = b_0 + b_1x\n\\]\n:\\(b_0\\): Intercept y-axis (estimated revenue spending zero).\\(b_1\\): Slope line (change revenue one-unit increase spending).\\(\\hat{y}\\): Predicted value dependent variable (revenue) given independent variable (spend).\\(x\\): Independent variable (spend).","code":""},{"path":"chapter-regression.html","id":"estimating-the-model-in-r","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Estimating the Model in R","text":"use lm() function estimate regression coefficients:regression results summarized using summary() function:output includes estimated coefficients, standard errors, t-statistics, p-values, goodness--fit metrics. estimated regression equation :\\[\n\\text{revenue} = 15.71 + 5.25 \\cdot \\text{spend}\n\\]means :intercept (\\(b_0\\)) 15.71, representing estimated daily revenue money spent PPC advertising.slope (\\(b_1\\)) 5.25, meaning every additional €1 spent, daily revenue increases approximately 5.25.","code":"\nsimple_reg = lm(revenue ~ spend, data = marketing)summary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09"},{"path":"chapter-regression.html","id":"interpreting-the-regression-line","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Interpreting the Regression Line","text":"regression line serves linear approximation relationship spending revenue. can use make predictions. instance, suppose want predict revenue day €25 spent advertising. Using regression equation:\\[\n\\hat{y} = 15.71 + 5.25 \\cdot 25 = 147\n\\]Thus, estimated daily revenue €147. Suppose marketing team wants plan budget upcoming campaign. spend €25 PPC advertising, model predicts estimated revenue €147. insight can help guide spending decisions maximize returns.","code":""},{"path":"chapter-regression.html","id":"residuals-and-model-fit","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Residuals and Model Fit","text":"Residuals (\\(y - \\hat{y}\\)) represent vertical distances observed data points regression line. example, one day dataset spend €25 actual revenue 185.36. prediction error (residual) :\\[\n\\text{Residual} = y - \\hat{y} = 185.36 - 147 = 38.36\n\\]Residuals help us identify potential patterns deviations linear model may capture, ensuring model’s assumptions hold. example, residuals exhibit systematic patterns, curvature, may indicate relationship variables truly linear, signaling need model adjustments.least-squares method minimizes sum squared residuals (SSE):\\[\\begin{equation}\n\\text{SSE} = \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2,\n\\tag{10.1}\n\\end{equation}\\]\\(y_i\\) observed value, \\(\\hat{y}_i\\) predicted value, \\(n\\) number observations. Minimizing SSE ensures regression line best linear fit data, providing accurate predictions. approach remains commonly used linear regression due simplicity efficiency.","code":""},{"path":"chapter-regression.html","id":"key-insights-1","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Key Insights","text":"important takeaways simple linear regression analysis:Intercept (\\(b_0\\)): intercept represents estimated revenue spending zero. dataset, \\(b_0 = 15.71\\), makes sense days spending.Slope (\\(b_1\\)): slope indicates every €1 increase spending, revenue expected increase approximately 5.25.Prediction Accuracy: regression line provides excellent linear approximation, individual residuals reveal prediction errors specific data points.summary, simple linear regression provides effective way model interpret relationship two variables. analyzing marketing dataset, demonstrated estimate, interpret, apply regression model make predictions. next sections expand concepts exploring techniques assess model quality extend regression multiple predictors.foundational understanding simple linear regression, now ready explore techniques assess quality regression models extend methods multiple predictors next sections.","code":""},{"path":"chapter-regression.html","id":"hypothesis-testing-in-simple-linear-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.2 Hypothesis Testing in Simple Linear Regression","text":"regression analysis, use estimated slope \\(b_1\\) sample regression equation draw inferences unknown slope \\(\\beta_1\\) population regression equation. population regression equation provides linear approximation relationship predictor (\\(x\\)) response (\\(y\\)) entire population, just sample data. expressed :\\[\ny = \\beta_0 + \\beta_1x + \\epsilon\n\\]equation:\\(\\beta_0\\): population intercept, representing expected value \\(y\\) \\(x = 0\\).\\(\\beta_1\\): population slope, representing change \\(y\\) one-unit increase \\(x\\).\\(\\epsilon\\): random error term captures variability \\(y\\) explained linear relationship.goal hypothesis testing regression determine whether \\(\\beta_1\\), slope population regression line, significantly different zero. \\(\\beta_1 = 0\\), regression equation simplifies :\\[\ny = \\beta_0 + \\epsilon\n\\]indicates linear relationship predictor \\(x\\) response \\(y\\). Conversely, \\(\\beta_1 \\neq 0\\), linear relationship exists \\(x\\) \\(y\\). formally test , conduct following hypothesis test:\\[\n\\bigg\\{\n\\begin{matrix}\n  H_0: \\beta_1 =  0 \\quad \\text{(linear relationship \\(x\\) \\(y\\))} \\qquad  \\\\\n  H_a: \\beta_1 \\neq 0 \\quad \\text{(linear relationship exists \\(x\\) \\(y\\))}\n\\end{matrix}\n\\]simple linear regression, summary output model provides necessary information test hypotheses. Specifically, slope regression line estimated sample data \\(b_1\\), along following key components:Standard error: Measures variability slope estimate \\(b_1\\).t-statistic: Quantifies many standard errors \\(b_1\\) away 0.p-value: Indicates probability observing t-statistic extreme one calculated, assuming \\(H_0\\) (.e., \\(\\beta_1 = 0\\)) true.Let’s revisit simple linear regression results marketing dataset, modeled revenue (daily revenue) function spend (daily advertising spend). estimated slope \\(b_1\\) spend 5.25, following corresponding hypothesis test results:output:t-statistic slope 7.93.p-value 1.4150362^{-9} (close zero).p-value, 1.4150362^{-9}, extremely small (near zero). represents probability observing t-statistic extreme one calculated, assuming relationship spend revenue (\\(\\beta_1 = 0\\)).Since p-value much smaller commonly used significance level (\\(\\alpha = 0.05\\)), reject null hypothesis \\(H_0\\). leads conclusion slope \\(\\beta_1\\) significantly different zero, providing strong evidence linear relationship spend revenue.hypothesis test confirms predictor spend statistically significant impact response revenue. Specifically:slope estimate \\(b_1 = 5.25\\) suggests additional €1 spent advertising, daily revenue increases approximately 5.25 units.result validates use spend predictor revenue, providing quantitative measure relationship.Hypothesis testing simple linear regression provides formal way evaluate significance relationship predictor response variable. statistically significant slope (\\(\\beta_1\\)) indicates changes predictor \\(x\\) associated changes response \\(y\\).next sections, explore additional techniques evaluate regression model quality extend concepts multiple predictors, enabling comprehensive analyses better predictions.","code":"summary(simple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -175.640  -56.226    1.448   65.235  210.987 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)  15.7058    35.1727   0.447    0.658    \n   spend         5.2517     0.6624   7.928 1.42e-09 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 93.82 on 38 degrees of freedom\n   Multiple R-squared:  0.6232, Adjusted R-squared:  0.6133 \n   F-statistic: 62.86 on 1 and 38 DF,  p-value: 1.415e-09"},{"path":"chapter-regression.html","id":"measuring-the-quality-of-a-regression-model","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.3 Measuring the Quality of a Regression Model","text":"can evaluate effectiveness regression model? reject null hypothesis (\\(H_0: \\beta_1 = 0\\)) hypothesis test, model provides evidence linear relationship predictor response variable, making ineffective. However, establish \\(\\beta_1 \\neq 0\\), rely additional metrics assess quality regression model. Two key statistics purpose Residual Standard Error (SSE) \\(R^2\\) (R-squared) statistic.Residual Standard Error (RSE) indicates measure size “typical” prediction error. defined :\\[\nRSE = \\sqrt{\\frac{1}{n-p-1} SSE}\n\\]\nSSE sum squared errors defined Equation (10.1), \\(n\\) number observations, \\(p\\) number predictors model. RSE provides estimate typical prediction error, smaller values indicating better fit.smaller SSE indicates regression model accurately captures variability response variable, producing better predictions. example, simple linear regression model marketing dataset, SSE 3.3447117^{5}. value represents total squared error predicting revenue spend. closer SSE zero, smaller typical prediction error, indicates higher-quality model.\\(R^2\\) (R-squared) statistic evaluates well regression model explains variability response variable. defined :\\[\nR^2 = 1 - \\frac{SSE}{SST}\n\\]\\(SST\\) total variability response variable (\\(y\\)) fitting model, \\(SSE\\) variability remains unexplained fitting model. \\(R^2\\) represents proportion variability \\(y\\) accounted linear relationship \\(x\\). instance, \\(R^2 = 0.80\\), means 80% variability \\(y\\) explained model, remaining 20% due factors captured model.simple linear regression model marketing dataset, \\(R^2 = 62\\)% (0.62 decimal). means 62% variability daily revenue explained linear relationship daily spend. higher \\(R^2\\) value suggests better fit, values close 100% indicating excellent fit. However, \\(R^2\\) alone assess whether model generalize well new data, important combine diagnostics validation techniques.also important understand relationship \\(R^2\\) correlation coefficient (\\(r\\)). simple linear regression, square correlation coefficient (\\(r^2\\)) equal \\(R^2\\). example, correlation coefficient spend revenue marketing dataset 0.79, squaring value gives 0.62—value equals \\(R^2\\). highlights \\(R^2\\) reflects strength linear relationship predictor response variables.might also wonder difference Multiple R-squared Adjusted R-squared. \\(R^2\\) measures proportion variance \\(y\\) explained model, Adjusted \\(R^2\\) accounts number predictors model. adjustment prevents \\(R^2\\) artificially inflating irrelevant predictors added model. Adjusted \\(R^2\\) calculated :\\[\n\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\cdot \\frac{n-1}{n-p-1}\n\\]\\(n\\) number observations, \\(p\\) number predictors. Adjusted \\(R^2\\) penalizes models unnecessary predictors, providing reliable measure model performance.simple linear regression (one predictor), \\(R^2\\) Adjusted \\(R^2\\) one predictor. However, multiple linear regression, Adjusted \\(R^2\\) typically lower better indicator well model generalizes new data. particularly helpful comparing models different numbers predictors, adjusts model complexity.conclusion, Residual Standard Error (SSE), \\(R^2\\), Adjusted \\(R^2\\) essential metrics evaluating quality regression model. SSE provides measure model’s prediction accuracy, \\(R^2\\) Adjusted \\(R^2\\) assess well model explains variability response variable. metrics help guide model interpretation selection, especially extending complex models.","code":""},{"path":"chapter-regression.html","id":"sec-multiple-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.4 Multiple Linear Regression","text":"many real-world scenarios, datasets often include numerous variables, many may linear relationship target (response) variable. Multiple regression modeling provides method analyze quantify relationships incorporating multiple predictors single model. , multiple regression offers greater precision estimation prediction compared simple regression, much like regression improves upon univariate estimates.illustrate multiple regression using marketing dataset, add predictor display simple regression model (previously used spend predictor) evaluate whether improves model’s quality. general equation multiple regression model :\\[\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\\(p\\) number predictors, \\(\\beta_0\\) intercept, \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) coefficients (slopes) representing relationship predictor response variable.case, equation two predictors (spend display) becomes:\\[\n\\hat{y} = b_0 + b_1 \\cdot \\text{spend} + b_2 \\cdot \\text{display}\n\\]R, fit model using lm() function:estimated regression equation output :\\[\n\\text{revenue} = -41.44 + 5.36 \\cdot \\text{spend} + 104.29 \\cdot \\text{display}\n\\]model:intercept (\\(b_0\\)) -41.44, representing estimated revenue predictors (spend display) zero.coefficient spend (\\(b_1\\)) 5.36, indicating every additional €1 spent, revenue increases approximately 5.36 euros, holding display constant.coefficient display (\\(b_2\\)) 104.29, meaning running display campaign (display = 1) increases revenue approximately 104.29 euros, holding spend constant.illustrate, consider day company spends €25 advertising runs display campaign (display = 1). Using regression equation, predicted revenue :\\[\n\\hat{y} = -41.44 + 5.36 \\cdot 25 + 104.29 \\cdot 1 = 196.74\n\\]Thus, predicted revenue day approximately €196.74.prediction error (residual) specific day calculated difference actual revenue \\(y\\) predicted revenue \\(\\hat{y}\\):\\[\n\\text{Residual} = y - \\hat{y} = 185.36 - 196.74 = -11.49\n\\]Interestingly, prediction error smaller one produced simpler regression model, used spend predictor. simple regression model, residual 38.26. improvement prediction accuracy due inclusion display additional predictor, provides information variability revenue.Adding predictor display reduces prediction error also improves overall quality regression model. example:Residual Standard Error (RSE): RSE measures typical size prediction errors. simple regression model, RSE approximately 93.82. multiple regression model, RSE decreased approximately 78.14, indicating smaller, accurate prediction errors.Residual Standard Error (RSE): RSE measures typical size prediction errors. simple regression model, RSE approximately 93.82. multiple regression model, RSE decreased approximately 78.14, indicating smaller, accurate prediction errors.\\(R^2\\) (R-squared): \\(R^2\\) statistic measures proportion variability response variable (revenue) explained predictors. simple regression model, \\(R^2 = 62\\%\\). adding display, \\(R^2 = 75\\%\\). indicates larger proportion variability revenue now explained model.\\(R^2\\) (R-squared): \\(R^2\\) statistic measures proportion variability response variable (revenue) explained predictors. simple regression model, \\(R^2 = 62\\%\\). adding display, \\(R^2 = 75\\%\\). indicates larger proportion variability revenue now explained model.Adjusted \\(R^2\\): Unlike \\(R^2\\), Adjusted \\(R^2\\) accounts number predictors, making reliable metric comparing models different numbers predictors. Adjusted \\(R^2\\) increased 61% simple regression model 73% multiple regression model. confirms additional predictor contributes meaningfully model.Adjusted \\(R^2\\): Unlike \\(R^2\\), Adjusted \\(R^2\\) accounts number predictors, making reliable metric comparing models different numbers predictors. Adjusted \\(R^2\\) increased 61% simple regression model 73% multiple regression model. confirms additional predictor contributes meaningfully model.summary, multiple regression model demonstrates clear improvements simple regression model. including display additional predictor, achieve:Better Fit: model explains larger proportion variability revenue, reflected higher \\(R^2\\) Adjusted \\(R^2\\).Reduced Prediction Error: smaller RSE indicates model provides accurate predictions.Enhanced Interpretability: coefficients allow us quantify effect running display campaign increasing spending revenue.Multiple regression modeling offers robust framework analyzing relationships response variable multiple predictors. added flexibility makes indispensable tool understanding complex datasets improving predictive performance. next sections, explore evaluate model assumptions, perform diagnostics, refine regression models ensure validity reliability.","code":"multiple_reg = lm(revenue ~ spend + display, data = marketing)\n\nsummary(multiple_reg)\n   \n   Call:\n   lm(formula = revenue ~ spend + display, data = marketing)\n   \n   Residuals:\n        Min       1Q   Median       3Q      Max \n   -189.420  -45.527    5.566   54.943  154.340 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) -41.4377    32.2789  -1.284 0.207214    \n   spend         5.3556     0.5523   9.698 1.05e-11 ***\n   display     104.2878    24.7353   4.216 0.000154 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 78.14 on 37 degrees of freedom\n   Multiple R-squared:  0.7455, Adjusted R-squared:  0.7317 \n   F-statistic: 54.19 on 2 and 37 DF,  p-value: 1.012e-11"},{"path":"chapter-regression.html","id":"generalized-linear-models-glms","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5 Generalized Linear Models (GLMs)","text":"linear regression powerful tool modeling continuous outcomes, limited dealing non-continuous response variables, binary count data. Generalized Linear Models (GLMs) extend linear regression framework introducing link function variance function, enabling model wide range response variable distributions. makes GLMs versatile widely applicable data science, can handle binary, count, non-continuous outcomes.section, focus two widely used GLMs:Logistic regression, used binary classification tasks.Poisson regression, used modeling count data.","code":""},{"path":"chapter-regression.html","id":"logistic-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.1 Logistic Regression","text":"Logistic regression GLM designed model binary outcomes, response variable takes two values, 0/1 yes/. Instead predicting response variable directly, logistic regression estimates probability outcome one class versus . ensure predicted probabilities lie 0 1, model uses logit function, defined :\\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\], \\(p\\) represents probability outcome 1, logit function transforms linear combination predictors probability.","code":""},{"path":"chapter-regression.html","id":"logistic-regression-in-r","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.2 Logistic Regression in R","text":"illustrate logistic regression, consider task want predict whether customer churn (leave service) based set predictors. churn dataset, use example, contains several variables related customer behavior. However, based prior knowledge, select following predictors model:account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins, customer.calls.use glm() function fit logistic regression model R, specifying response variable churn (binary variable) predictors specified ::churn binary response variable.predictors specified formula.family = binomial indicates logistic regression model.view model’s summary interpret results, use summary() function:summary output provides estimated coefficients, standard errors, z-statistics, p-values predictor. Predictors high p-values (typically > 0.05) statistically significant reconsidered removal. instance, account.length high p-value, exclude model re-run regression. iterative process ensures meaningful predictors included final model.","code":"\ndata(churn)\n\nformula = churn ~ account.length + voice.messages + day.mins + eve.mins + \n                  night.mins + intl.mins + customer.calls + intl.plan + voice.plan\n\nlogreg_1 = glm(formula, data = churn, family = binomial)summary(logreg_1)\n   \n   Call:\n   glm(formula = formula, family = binomial, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n   (Intercept)     8.8917584  0.6582188  13.509  < 2e-16 ***\n   account.length -0.0013811  0.0011453  -1.206   0.2279    \n   voice.messages -0.0355317  0.0150397  -2.363   0.0182 *  \n   day.mins       -0.0136547  0.0009103 -15.000  < 2e-16 ***\n   eve.mins       -0.0071210  0.0009419  -7.561 4.02e-14 ***\n   night.mins     -0.0040518  0.0009048  -4.478 7.53e-06 ***\n   intl.mins      -0.0882514  0.0170578  -5.174 2.30e-07 ***\n   customer.calls -0.5183958  0.0328652 -15.773  < 2e-16 ***\n   intl.planno     2.0958198  0.1214476  17.257  < 2e-16 ***\n   voice.planno   -2.1637477  0.4836735  -4.474 7.69e-06 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for binomial family taken to be 1)\n   \n       Null deviance: 4075.0  on 4999  degrees of freedom\n   Residual deviance: 3174.3  on 4990  degrees of freedom\n   AIC: 3194.3\n   \n   Number of Fisher Scoring iterations: 6"},{"path":"chapter-regression.html","id":"poisson-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.3 Poisson Regression","text":"Poisson regression GLM used modeling count data, response variable represents number occurrences event within fixed interval. Examples include number customer service calls, website visits, product purchases. Poisson regression model assumes :response variable follows Poisson distribution.mean distribution equals variance.log mean (\\(\\ln(\\lambda)\\)) can expressed linear combination predictors.Poisson regression model expressed :\\[\n\\ln(\\lambda) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\], \\(\\lambda\\) expected count (mean) response variable, predictors (\\(x_1, x_2, \\dots, x_p\\)) influence log \\(\\lambda\\).","code":""},{"path":"chapter-regression.html","id":"poisson-regression-in-r","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.5.4 Poisson Regression in R","text":"demonstrate Poisson regression, consider task want predict number customer service calls (customer.calls) based following predictors churn dataset:churn, account.length, voice.plan, voice.messages, intl.plan, intl.mins, day.mins, eve.mins, night.mins.Since customer.calls integer-valued variable, Poisson regression appropriate linear regression task. fit Poisson regression model using glm() function::\n- customer.calls response variable.\n- predictors specified formula.\n- family = poisson indicates Poisson regression model.evaluate model, view summary regression output:summary output provides estimated coefficients, standard errors, z-statistics, p-values predictor. Predictors high p-values (typically > 0.05) statistically significant reconsidered. instance, predictors voice.messages, night.mins, voice.plan high p-values, can excluded model.example, coefficient significant predictor (e.g., intl.plan) can interpreted expected percentage change mean customer.calls one-unit change predictor, holding variables constant.Generalized Linear Models (GLMs) provide flexible framework modeling response variables non-normal distributions. Logistic regression Poisson regression two common GLMs allow us model binary count data, respectively. models extend applicability regression techniques wide range practical problems data science, customer churn prediction event counts. iteratively refining model excluding non-significant predictors, ensure final model interpretable predictive.next sections, explore techniques validating improving regression models ensure robustness practical utility.","code":"\nformula = customer.calls ~ churn + voice.messages + day.mins + eve.mins + \n                           night.mins + intl.mins + intl.plan + voice.plan\n\nreg_pois = glm(formula, data = churn, family = poisson)summary(reg_pois)\n   \n   Call:\n   glm(formula = formula, family = poisson, data = churn)\n   \n   Coefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n   (Intercept)     0.9957186  0.1323004   7.526 5.22e-14 ***\n   churnno        -0.5160641  0.0304013 -16.975  < 2e-16 ***\n   voice.messages  0.0034062  0.0028294   1.204 0.228646    \n   day.mins       -0.0006875  0.0002078  -3.309 0.000938 ***\n   eve.mins       -0.0005649  0.0002237  -2.525 0.011554 *  \n   night.mins     -0.0003602  0.0002245  -1.604 0.108704    \n   intl.mins      -0.0075034  0.0040886  -1.835 0.066475 .  \n   intl.planno     0.2085330  0.0407760   5.114 3.15e-07 ***\n   voice.planno    0.0735515  0.0878175   0.838 0.402284    \n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   (Dispersion parameter for poisson family taken to be 1)\n   \n       Null deviance: 5991.1  on 4999  degrees of freedom\n   Residual deviance: 5719.5  on 4991  degrees of freedom\n   AIC: 15592\n   \n   Number of Fisher Scoring iterations: 5"},{"path":"chapter-regression.html","id":"sec-stepwise-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.6 Model Selection Using Stepwise Regression","text":"Selecting appropriate predictors crucial step building regression model accurate interpretable. process, known model specification, ensures model captures underlying relationships data without overfitting including irrelevant predictors. Proper model specification enhances model’s ability make reliable predictions provides meaningful insights decision-making.many real-world applications, particularly business data science, datasets often contain dozens even hundreds potential predictors. Managing complexity requires systematic methods identify relevant predictors. One method stepwise regression, popular algorithm iterative model selection. Stepwise regression begins evaluating predictors one time, adding improve model removing contribute meaningfully. iterative process helps handle issues multicollinearity ensures helpful predictors remain final model. Additionally, stepwise regression computationally efficient small medium-sized datasets, making practical choice many scenarios.assess model quality selection process, use metrics like Akaike Information Criterion (AIC). AIC provides balance model complexity goodness fit, lower AIC value indicating better model. AIC defined :\\[\nAIC = 2p + n \\log\\left(\\frac{SSE}{n}\\right)\n\\]\\(p\\) number estimated predictors model, \\(n\\) number observations, \\(SSE\\) sum squared errors, quantifies unexplained variability response variable. Using AIC ensures select model explains data well penalizing unnecessary complexity. Unlike \\(R^2\\), always increases predictors added, AIC introduces penalty model complexity, favoring simpler models generalize better new data. makes AIC robust criterion selecting parsimonious interpretable models.demonstrate process model specification, apply stepwise regression marketing dataset, contains seven predictors. goal identify best regression model predicting revenue (daily revenue) based predictors.Example 10.1  start building linear regression model includes available predictors. allows us assess initial model use results baseline stepwise regression process. formula revenue ~ . lm() function specifies predictors dataset included initial model:output model reveals several predictors high p-values (much greater significance level \\(\\alpha = 0.05\\)), suggesting variables may significantly contribute model. High multicollinearity among predictors also affecting model’s performance.Next, apply stepwise regression using step() function. setting direction argument \"\", algorithm iteratively adds removes predictors, evaluating contribution model step:first iteration, stepwise algorithm evaluates contribution predictor removes variable spend, highest p-value initial model. Subsequent iterations continue process, adding removing variables based impact model performance. stepwise process concludes changes improve model.view final selected model, use summary() function:stepwise regression process selects simpler model two predictors: clicks display. final estimated regression equation :\\[\n\\text{revenue} = -33.63 + 0.9 \\cdot \\text{clicks} + 95.51 \\cdot \\text{display}\n\\]model demonstrates better fit compared initial full model. Specifically:Residual Standard Error (RSE) decreased approximately 93.82 72.29, indicating typical prediction error now smaller.R-squared (\\(R^2\\)) value increased 62% 77%, meaning greater proportion variability revenue explained final model.stepwise regression approach identifies relevant predictors (clicks display) discarding others significantly improve model performance. ensures final model parsimonious interpretable.example, stepwise regression helped us build efficient regression model marketing dataset iteratively adding removing predictors. using AIC selection criterion, identified model best trade-complexity goodness fit.stepwise regression popular intuitive method model selection, important recognize limitations. First, algorithm evaluates predictors sequentially rather considering possible combinations, can sometimes result suboptimal models. Second, stepwise regression prone overfitting, particularly working small datasets containing many predictors. Overfitting occurs model captures noise rather meaningful patterns, reducing generalizability new data. Finally, multicollinearity can distort p-values used stepwise regression, leading misleading results. reasons, alternative methods like LASSO (Least Absolute Shrinkage Selection Operator) Ridge Regression often preferred complex scenarios, particularly high-dimensional datasets. methods beyond scope book, exploration, refer Introduction Statistical Learning Applications R.conclusion, model specification critical step regression analysis. selecting right predictors using systematic techniques like stepwise regression, can build models accurate interpretable. stepwise regression without limitations, remains practical widely used method selecting predictors regression models, particularly datasets manageable number variables. process improves model’s predictive performance maintaining simplicity interpretability, making valuable tool data-driven decision-making.","code":"ml_all = lm(revenue ~ ., data = marketing)\n\nsummary(ml_all)\n   \n   Call:\n   lm(formula = revenue ~ ., data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -138.00  -59.12   15.16   54.58  106.99 \n   \n   Coefficients:\n                     Estimate Std. Error t value Pr(>|t|)\n   (Intercept)     -25.260020 246.988978  -0.102    0.919\n   spend            -0.025807   2.605645  -0.010    0.992\n   clicks            1.211912   1.630953   0.743    0.463\n   impressions      -0.005308   0.021588  -0.246    0.807\n   display          79.835729 117.558849   0.679    0.502\n   transactions     -7.012069  66.383251  -0.106    0.917\n   click.rate      -10.951493 106.833894  -0.103    0.919\n   conversion.rate  19.926588 135.746632   0.147    0.884\n   \n   Residual standard error: 77.61 on 32 degrees of freedom\n   Multiple R-squared:  0.7829, Adjusted R-squared:  0.7354 \n   F-statistic: 16.48 on 7 and 32 DF,  p-value: 5.498e-09ml_stepwise = step(ml_all, direction = \"both\")\n   Start:  AIC=355.21\n   revenue ~ spend + clicks + impressions + display + transactions + \n       click.rate + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - spend            1       0.6 192760 353.21\n   - click.rate       1      63.3 192822 353.23\n   - transactions     1      67.2 192826 353.23\n   - conversion.rate  1     129.8 192889 353.24\n   - impressions      1     364.2 193123 353.29\n   - display          1    2778.1 195537 353.79\n   - clicks           1    3326.0 196085 353.90\n   <none>                         192759 355.21\n   \n   Step:  AIC=353.21\n   revenue ~ clicks + impressions + display + transactions + click.rate + \n       conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - click.rate       1      67.9 192828 351.23\n   - transactions     1      75.1 192835 351.23\n   - conversion.rate  1     151.5 192911 351.24\n   - impressions      1     380.8 193141 351.29\n   - display          1    2787.2 195547 351.79\n   - clicks           1    3325.6 196085 351.90\n   <none>                         192760 353.21\n   + spend            1       0.6 192759 355.21\n   \n   Step:  AIC=351.23\n   revenue ~ clicks + impressions + display + transactions + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - transactions     1      47.4 192875 349.24\n   - conversion.rate  1     129.0 192957 349.25\n   - impressions      1     312.9 193141 349.29\n   - clicks           1    3425.7 196253 349.93\n   - display          1    3747.1 196575 350.00\n   <none>                         192828 351.23\n   + click.rate       1      67.9 192760 353.21\n   + spend            1       5.2 192822 353.23\n   \n   Step:  AIC=349.24\n   revenue ~ clicks + impressions + display + conversion.rate\n   \n                     Df Sum of Sq    RSS    AIC\n   - conversion.rate  1      89.6 192965 347.26\n   - impressions      1     480.9 193356 347.34\n   - display          1    5437.2 198312 348.35\n   <none>                         192875 349.24\n   + transactions     1      47.4 192828 351.23\n   + click.rate       1      40.2 192835 351.23\n   + spend            1      13.6 192861 351.23\n   - clicks           1   30863.2 223738 353.17\n   \n   Step:  AIC=347.26\n   revenue ~ clicks + impressions + display\n   \n                     Df Sum of Sq    RSS    AIC\n   - impressions      1       399 193364 345.34\n   <none>                         192965 347.26\n   - display          1     14392 207357 348.13\n   + conversion.rate  1        90 192875 349.24\n   + click.rate       1        52 192913 349.24\n   + spend            1        33 192932 349.25\n   + transactions     1         8 192957 349.25\n   - clicks           1     35038 228002 351.93\n   \n   Step:  AIC=345.34\n   revenue ~ clicks + display\n   \n                     Df Sum of Sq    RSS    AIC\n   <none>                         193364 345.34\n   + impressions      1       399 192965 347.26\n   + transactions     1       215 193149 347.29\n   + conversion.rate  1         8 193356 347.34\n   + click.rate       1         6 193358 347.34\n   + spend            1         2 193362 347.34\n   - display          1     91225 284589 358.80\n   - clicks           1    606800 800164 400.15summary(ml_stepwise)\n   \n   Call:\n   lm(formula = revenue ~ clicks + display, data = marketing)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -141.89  -55.92   16.44   52.70  115.46 \n   \n   Coefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) -33.63248   28.68893  -1.172 0.248564    \n   clicks        0.89517    0.08308  10.775 5.76e-13 ***\n   display      95.51462   22.86126   4.178 0.000172 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 72.29 on 37 degrees of freedom\n   Multiple R-squared:  0.7822, Adjusted R-squared:  0.7704 \n   F-statistic: 66.44 on 2 and 37 DF,  p-value: 5.682e-13"},{"path":"chapter-regression.html","id":"extending-linear-models-to-capture-non-linear-relationships","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.7 Extending Linear Models to Capture Non-Linear Relationships","text":"Thus far, focused linear regression models, simple, interpretable, easy implement. models work well relationships predictors response variables approximately linear, predictive power limited cases relationships inherently non-linear. scenarios, relying linearity assumption can lead suboptimal results.earlier sections, explored techniques like stepwise regression (Section 10.6) improve linear models reducing complexity addressing multicollinearity. However, techniques operate within constraints linear framework. capture non-linear patterns retaining interpretability, turn polynomial regression, simple yet powerful extension linear regression incorporates non-linear terms.","code":""},{"path":"chapter-regression.html","id":"the-need-for-non-linear-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"The Need for Non-Linear Regression","text":"Linear regression assumes straight-line relationship predictors response variable. However, many real-world datasets, relationships complex. example, consider scatter plot Figure 10.2, depicts relationship unit.price (house price per unit area) house.age (age house) house dataset. orange line represents fit simple linear regression model. Clearly, data exhibits curved, non-linear pattern, linear fit fails capture.address limitation, can enhance model incorporating non-linear terms. instance, data suggests quadratic relationship, can model :\\[\nunit.price = b_0 + b_1 \\cdot house.age + b_2 \\cdot house.age^2\n\\]equation predicts unit.price using house.age square (house.age^2). model accommodates non-linear relationships, remains linear model coefficients (\\(b_0, b_1, b_2\\)) estimated using linear least squares. blue curve Figure 10.2 illustrates quadratic fit, captures data’s pattern far better linear fit.\nFigure 10.2: Scatter plot house price ($) versus house age (years) house dataset, fitted simple linear regression line orange quadratic regression curve blue.\n","code":""},{"path":"chapter-regression.html","id":"polynomial-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.7.1 Polynomial Regression","text":"Polynomial regression extends linear model adding higher-order terms predictors, squared (\\(x^2\\)), cubic (\\(x^3\\)), higher-degree terms. example, cubic regression model takes form:\\[\n\\hat{y} = b_0 + b_1 \\cdot x + b_2 \\cdot x^2 + b_3 \\cdot x^3\n\\]allows polynomial regression capture increasingly complex patterns. Importantly, although model non-linear terms predictors, remains linear coefficients (\\(b_0, b_1, b_2, b_3\\)), making compatible standard least squares estimation. However, care must taken using high-degree polynomials (\\(d > 3\\)) can become overly flexible, fitting noise data rather meaningful patterns (overfitting), especially near boundaries predictor range.","code":""},{"path":"chapter-regression.html","id":"example-polynomial-regression-on-the-house-dataset","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.7.2 Example: Polynomial Regression on the House Dataset","text":"illustrate polynomial regression, use house dataset, contains information house prices features house age, distance MRT stations, number convenience stores. objective model unit.price (house price per unit area) function house.age compare performance simple linear regression polynomial regression.dataset consists six features 414 observations. target variable unit.price, predictors include:house.age: Age house (years).distance..MRT: Distance nearest MRT station.stores.number: Number convenience stores.latitude: Latitude.longitude: Longitude.unit.price: House price per unit area (target variable).First, examine structure dataset:dataset includes 414 observations 6 variables, 5 predictors one numerical target variable (unit.price).","code":"data(house)\n\nstr(house)\n   'data.frame':    414 obs. of  6 variables:\n    $ house.age      : num  32 19.5 13.3 13.3 5 7.1 34.5 20.3 31.7 17.9 ...\n    $ distance.to.MRT: num  84.9 306.6 562 562 390.6 ...\n    $ stores.number  : int  10 9 5 5 5 3 7 6 1 3 ...\n    $ latitude       : num  25 25 25 25 25 ...\n    $ longitude      : num  122 122 122 122 122 ...\n    $ unit.price     : num  37.9 42.2 47.3 54.8 43.1 32.1 40.3 46.7 18.8 22.1 ..."},{"path":"chapter-regression.html","id":"fitting-simple-linear-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Fitting Simple Linear Regression","text":"begin fitting simple linear regression model predict unit.price using house.age:R-squared (\\(R^2\\)) value model 0.04, indicating 4.43% variability house prices explained house.age. low \\(R^2\\) reflects poor fit data.","code":"simple_reg_house = lm(unit.price ~ house.age, data = house)\n\nsummary(simple_reg_house)\n   \n   Call:\n   lm(formula = unit.price ~ house.age, data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -31.113 -10.738   1.626   8.199  77.781 \n   \n   Coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n   (Intercept) 42.43470    1.21098  35.042  < 2e-16 ***\n   house.age   -0.25149    0.05752  -4.372 1.56e-05 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 13.32 on 412 degrees of freedom\n   Multiple R-squared:  0.04434,    Adjusted R-squared:  0.04202 \n   F-statistic: 19.11 on 1 and 412 DF,  p-value: 1.56e-05"},{"path":"chapter-regression.html","id":"fitting-polynomial-regression","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"Fitting Polynomial Regression","text":"Next, fit quadratic polynomial regression model better capture non-linear relationship:\\[\nunit.price = b_0 + b_1 \\cdot house.age + b_2 \\cdot house.age^2\n\\]implement model R:quadratic regression model achieves significantly higher R-squared (\\(R^2\\)) value 0.2, compared simple linear model. indicates quadratic model explains variability data. Additionally, Residual Standard Error (RSE) lower, reflecting smaller prediction errors.example, polynomial regression outperforms simple linear regression, evidenced higher \\(R^2\\) lower RSE. incorporating non-linear terms, polynomial regression provides greater flexibility modeling complex relationships. However, care must taken avoid overfitting adding higher-degree terms.Polynomial regression offers straightforward way extend linear models non-linear relationships. advanced techniques, splines generalized additive models, refer Chapter 7 Introduction Statistical Learning Applications R.","code":"reg_nonlinear_house = lm(unit.price ~ poly(house.age, 2), data = house)\n\nsummary(reg_nonlinear_house)\n   \n   Call:\n   lm(formula = unit.price ~ poly(house.age, 2), data = house)\n   \n   Residuals:\n       Min      1Q  Median      3Q     Max \n   -26.542  -9.085  -0.445   8.260  79.961 \n   \n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           37.980      0.599  63.406  < 2e-16 ***\n   poly(house.age, 2)1  -58.225     12.188  -4.777 2.48e-06 ***\n   poly(house.age, 2)2  109.635     12.188   8.995  < 2e-16 ***\n   ---\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n   \n   Residual standard error: 12.19 on 411 degrees of freedom\n   Multiple R-squared:  0.2015, Adjusted R-squared:  0.1977 \n   F-statistic: 51.87 on 2 and 411 DF,  p-value: < 2.2e-16"},{"path":"chapter-regression.html","id":"diagnosing-and-validating-regression-models","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.8 Diagnosing and Validating Regression Models","text":"deploying regression model, essential validate assumptions. Ignoring assumptions akin building house unstable foundation: predictions poorly validated model can lead erroneous, overoptimistic, misleading results costly consequences. Model diagnostics ensure model robust, reliable, appropriate making predictions.Linear regression models built following key assumptions:Independence (Random Sampling): observations independent , meaning response one observation depend response another.Linearity: relationship predictor(s) response variable linear. scatter plot predictor(s) response can help verify assumption.Normality: residuals (errors) model follow normal distribution. can assessed visually using Q-Q plot.Constant Variance (Homoscedasticity): residuals constant variance every level predictor(s). residuals vs. fitted values plot used check assumption.Violations assumptions may compromise validity model, leading inaccurate estimates, unreliable predictions, invalid statistical inferences.","code":""},{"path":"chapter-regression.html","id":"example-diagnosing-the-regression-model-for-the-marketing-dataset","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.8.1 Example: Diagnosing the Regression Model for the Marketing Dataset","text":"demonstrate model diagnostics, evaluate assumptions multiple regression model created Example 10.1 using marketing dataset. fitted model predicts daily revenue (revenue) based clicks display.generate diagnostic plots model follows:\nFigure 10.3: Diagnostic plots assessing regression model assumptions.\ndiagnostic plots help us assess four key assumptions:Normality Residuals: Normal Q-Q plot (upper-right) shows closely residuals align theoretical normal distribution. points fall along straight line, residuals approximately normally distributed. example, majority points lie close line, indicating normality assumption satisfied.Normality Residuals: Normal Q-Q plot (upper-right) shows closely residuals align theoretical normal distribution. points fall along straight line, residuals approximately normally distributed. example, majority points lie close line, indicating normality assumption satisfied.Linearity Constant Variance (Homoscedasticity): Residuals vs. Fitted plot (upper-left) helps check linearity homoscedasticity:\npoints form random scatter discernible pattern, linearity assumption holds.\nvertical spread residuals roughly uniform across fitted values, constant variance assumption satisfied.\ncase, residuals appear randomly scattered without systematic curvature changes variance, supporting validity assumptions.\nLinearity Constant Variance (Homoscedasticity): Residuals vs. Fitted plot (upper-left) helps check linearity homoscedasticity:points form random scatter discernible pattern, linearity assumption holds.vertical spread residuals roughly uniform across fitted values, constant variance assumption satisfied.case, residuals appear randomly scattered without systematic curvature changes variance, supporting validity assumptions.Independence: independence assumption pertains dataset rather diagnostic plots. marketing dataset, revenue one day unlikely depend revenue another day, making independence assumption reasonable example.Independence: independence assumption pertains dataset rather diagnostic plots. marketing dataset, revenue one day unlikely depend revenue another day, making independence assumption reasonable example.Based diagnostic plots characteristics dataset:Normality Assumption: residuals approximately normally distributed, evidenced Normal Q-Q plot.Linearity Constant Variance: residuals vs. fitted values plot shows visible patterns heteroscedasticity, validating assumptions.Independence: nature data (daily revenue) suggests observations independent one another.validating assumptions, confirm regression model marketing dataset appropriate inference prediction. Failing check assumptions lead unreliable results, emphasizing importance diagnostics regression modeling.Model diagnostics validation fundamental ensuring integrity regression model. models violate one assumptions, alternative approaches robust regression, non-linear regression, transformations variables may considered. Additionally, validation techniques cross-validation --sample testing can used evaluate model’s performance generalizability unseen data.adhering diagnostic best practices, ensure models deploy built solid statistical foundations, capable providing accurate trustworthy predictions.","code":"\nml_stepwise = lm(revenue ~ clicks + display, data = marketing)\n\nplot(ml_stepwise)  "},{"path":"chapter-regression.html","id":"exercises-7","chapter":"10 Regression Modeling: From Basics to Advanced Techniques","heading":"10.9 Exercises","text":"Hands-exercises section reinforce concepts.","code":""},{"path":"chapter-tree.html","id":"chapter-tree","chapter":"11 Decision Trees and Random Forests","heading":"11 Decision Trees and Random Forests","text":"part Data Science workflow, already explored several powerful algorithms classification regression. include k-Nearest Neighbors Naive Bayes classification, well linear regression models continuous outcomes classification tasks. chapter, introduce two additional widely-used techniques: Decision Trees Random Forests. algorithms highly versatile, capable tackling classification regression problems, built around idea making series hierarchical decisions classify data predict outcomes.core, Decision Trees employ simple decision-making rules split dataset smaller, homogeneous subsets, resulting tree-like structure decisions. step--step partitioning allows trees capture complex relationships data remaining interpretable intuitive. Random Forests, extension Decision Trees, enhance predictive power combining outputs multiple trees. ensemble learning approach improves accuracy, reduces overfitting, provides robust solution compared single tree.illustrate concept, consider simple Decision Tree shown Figure 11.1, predicts whether customer’s credit risk classified “good” “bad” based features age income. example uses risk dataset introduced Chapter 9. node tree represents question, “yearly income lower 36K? (income < 36e+3)” “age >= 29?”, terminal nodes, also known leaves, represent final predictions (e.g., “Good risk” “Bad risk”). Along predictions, tree also provides uncertainty values. process begins root node, data iteratively split branches based feature values reaches terminal nodes.\nFigure 11.1: Decision tree predicting credit risk based age income.\nshown Figure 11.1, decision trees provide visually intuitive interpretable structure require advanced statistical knowledge. makes especially valuable business contexts, simplicity, interpretability, actionable insights critical. Whether ’s customer segmentation, risk assessment, process optimization, decision trees serve user-friendly tool deriving insights informing decisions.end chapter, gain:deep understanding mechanics behind Decision Trees Random Forests,Practical knowledge building, evaluating, tuning Decision Trees using algorithms like CART C5.0, andThe ability leverage Random Forests solving real-world problems, identifying risky loans, detecting fraudulent transactions, classifying images.start exploring Decision Trees constructed, learning key concepts algorithms. , delve Random Forests, powerful ensemble learning method builds upon strengths Decision Trees deliver state---art performance across wide range applications.","code":""},{"path":"chapter-tree.html","id":"how-decision-trees-work","chapter":"11 Decision Trees and Random Forests","heading":"11.1 How Decision Trees Work","text":"Decision Trees classify data predict outcomes recursively dividing dataset smaller subsets based feature values. divide conquer approach aims maximize homogeneity subsets step, creating groups similar possible. split, algorithm identifies feature threshold best separate data, using criteria Gini Index, Entropy, Variance Reduction, depending whether task classification regression. process continues one following conditions met:tree reaches predefined maximum depth,observations subset belong class (classification) share value (regression), orFurther splits fail improve model’s performance.iterative process creates binary tree, node represents decision test (e.g., “\\(x_1 < 10\\)?”), leaf represents final prediction (e.g., “Class ” “Class B”). Decision Trees highly interpretable, structure visually represents decision-making process, making particularly useful understanding predictions made.better understand Decision Trees work, consider toy dataset two features (\\(x_1\\) \\(x_2\\)) two classes (Class Class B), shown Figure 11.2. dataset contains 50 data points, goal classify points respective classes using Decision Tree.\nFigure 11.2: two-dimensional toy dataset (50 observations) two classes (Class Class B), used illustrate build Decision Trees.\nprocess starts identifying feature threshold best separate two classes. algorithm evaluates possible thresholds feature selects split maximizes homogeneity resulting subsets. dataset, best split based feature \\(x_1\\), decision boundary \\(x_1 = 10\\).shown Figure 11.3, split divides dataset two regions:Left region: Data points \\(x_1 < 10\\), 80% Class 20% Class B.Right region: Data points \\(x_1 \\geq 10\\), 28% Class 72% Class B.test \\(x_1 < 10\\) forms root node tree, two regions correspond left right branches tree.\nFigure 11.3: Left: Decision boundary tree depth 1. Right: corresponding Decision Tree.\nAlthough first split significantly separates two classes, regions still contain points classes. improve classification accuracy, algorithm recursively evaluates additional splits within region.Figure 11.4, second split made using feature \\(x_2\\). left region (\\(x_1 < 10\\)), optimal threshold \\(x_2 = 6\\), right region (\\(x_1 \\geq 10\\)), threshold \\(x_2 = 8\\). additional splits refine boundaries feature space, creating smaller homogeneous regions.\nFigure 11.4: Left: Decision boundary tree depth 2. Right: corresponding Decision Tree.\nsplitting process continues tree meets stopping condition, reaching maximum depth producing pure leaf nodes (regions containing data points one class). Figure 11.5 shows final tree, grown depth 5. decision boundaries left illustrate feature space partitioned regions, associated specific class.\nFigure 11.5: Left: Decision boundary tree depth 5. Right: corresponding Decision Tree.\ndepth, tree created highly specific decision boundaries closely match training data. However, specificity often leads overfitting, model captures noise outliers data instead general patterns. Overfitted trees may perform poorly unseen data.make predictions Decision Tree, algorithm evaluates test conditions node follows corresponding branch reaches leaf. prediction depends type task:classification, predicted class majority class points leaf.regression, predicted value mean target variable points leaf.example, Figure 11.4, new data point \\(x_1 = 8\\) \\(x_2 = 4\\) traverse left region (\\(x_1 < 10\\)), bottom-left region (\\(x_2 < 6\\)), ultimately landing leaf labeled Class 80% confidence 20% error rate.Decision Trees can also handle regression tasks following splitting process minimizing variance target variable instead maximizing class homogeneity. regression, prediction new data point average target value training points corresponding leaf. allows Decision Trees model non-linear relationships effectively.Controlling complexity Decision Tree crucial prevent overfitting. Fully growing tree leaves pure often results highly complex model perfectly fits training data performs poorly unseen data. can observed Figure 11.5, decision boundaries overfit training set, capturing outliers noise.address , two strategies commonly used:Pre-pruning: Stop tree-building process early based criteria limiting maximum depth, number leaf nodes, minimum number points required split node.Post-pruning: Build full tree simplify removing collapsing nodes provide little additional value.effectiveness strategies depends dataset application. choice split criterion—Gini Index, Entropy, Variance Reduction—also plays crucial role determining tree’s performance. criteria foundational two widely used Decision Tree algorithms, CART C5.0, explored following sections.","code":""},{"path":"chapter-tree.html","id":"classification-and-regression-trees-cart","chapter":"11 Decision Trees and Random Forests","heading":"11.2 Classification and Regression Trees (CART)","text":"Classification Regression Trees (CART) algorithm, introduced Breiman et al. 1984,7 one widely used methods constructing decision trees. CART generates binary trees, meaning decision node splits data exactly two branches. recursively partitions training dataset subsets records share similar values target variable. partitioning guided splitting criterion designed minimize impurity resulting subsets. classification tasks, CART employs measures Gini Index Entropy evaluate splits, regression tasks, minimizes Variance target variable.example, Gini Index commonly used measure impurity classification tasks. Gini Index node calculated :\\[\nGini = 1 - \\sum_{=1}^k p_i^2\n\\]\\(p_i\\) represents proportion samples node belong class \\(\\), \\(k\\) total number classes. node considered “pure” data points belong single class, resulting Gini Index 0. tree construction, CART selects feature threshold result largest reduction impurity, splitting data create two homogeneous child nodes.recursive nature CART can result highly detailed trees perfectly fit training data. ensures lowest possible error rate training set, can lead overfitting, tree becomes overly complex fails generalize unseen data. mitigate , CART employs technique called pruning simplify tree.Pruning involves cutting back branches tree contribute meaningfully predictive accuracy validation set. achieved finding adjusted error rate penalizes overly complex trees many leaf nodes. goal pruning strike balance accuracy simplicity, enhancing tree’s ability generalize new data. pruning process described detail Breiman et al..8Despite simplicity, CART powerful algorithm widely adopted practice. key strengths include:Interpretability: tree structure intuitive easy visualize, making CART models highly explainable.Versatility: CART can handle classification regression tasks effectively.Ability handle mixed data types: CART works seamlessly datasets containing numerical categorical variables.However, CART also limitations. algorithm tends produce deep trees may overfit training data, especially dataset small noisy. Additionally, CART’s reliance greedy splitting can result suboptimal splits, evaluates one split time rather considering possible combinations.address shortcomings, advanced algorithms developed, C5.0, incorporates improvements splitting pruning techniques, Random Forests, combine multiple decision trees create robust models. approaches build foundations CART, improving performance reducing susceptibility overfitting. explore methods subsequent sections.","code":""},{"path":"chapter-tree.html","id":"the-c5.0-algorithm-for-building-decision-trees","chapter":"11 Decision Trees and Random Forests","heading":"11.3 The C5.0 Algorithm for Building Decision Trees","text":"C5.0 algorithm one well-known widely used decision tree implementations. Developed J. Ross Quinlan, C5.0 advanced iteration earlier algorithms, C4.5 ID3 (Iterative Dichotomiser 3). Building upon strengths predecessors, C5.0 introduces several improvements efficiency, flexibility, accuracy, making popular choice academic commercial applications. Quinlan offers commercial version C5.0 (available RuleQuest), single-threaded implementation made publicly available incorporated open-source tools R.C5.0 differs decision tree algorithms, CART, several key ways. Unlike CART, produces binary trees, C5.0 allows flexible tree structures non-binary splits. categorical attributes, C5.0 can create separate branches unique value attribute, can lead highly “bushy” trees attribute many categories. Another major distinction lies way node homogeneity measured. CART uses metrics like Gini Index Variance Reduction, C5.0 employs Entropy Information Gain, concepts rooted information theory, evaluate optimal splits.Entropy measures level disorder randomness dataset. High entropy indicates dataset high diversity (e.g., mix classes), whereas low entropy signifies greater homogeneity (e.g., samples belong class). goal C5.0 algorithm identify splits reduce entropy, creating purer subsets data step tree-building process. Formally, entropy variable \\(x\\) \\(k\\) classes defined :\\[\nEntropy(x) = - \\sum_{=1}^k p_i \\log_2(p_i)\n\\], \\(p_i\\) proportion samples belonging class \\(\\). example, dataset contains even split two classes, entropy maximum. Conversely, samples belong single class, entropy zero. concept extends naturally calculation Information Gain, quantifies reduction entropy achieved splitting data particular feature. Given candidate split \\(S\\) divides dataset \\(T\\) subsets \\(T_1, T_2, \\dots, T_c\\), entropy split calculated weighted sum entropies subsets:\\[\nH_S(T) = \\sum_{=1}^c \\frac{|T_i|}{|T|} \\cdot Entropy(T_i)\n\\]Information Gain split \\(S\\) :\\[\ngain(S) = H(T) - H_S(T)\n\\]\\(H(T)\\) represents entropy dataset split. decision node, C5.0 algorithm evaluates possible splits selects one maximizes information gain. process ensures splits lead progressively purer subsets, improving accuracy model.illustrate C5.0 algorithm, consider application risk dataset, predicts credit risk (“good” “bad”) based features like age income. Figure 11.6 shows resulting decision tree, created using C5.0 function C50 package R. node tree represents decision based feature value, branches lead subsets data become progressively homogeneous.\nFigure 11.6: C5.0 Decision Tree predicting credit risk based age income.\ntree Figure 11.6 demonstrates algorithm uses entropy information gain construct splits best separate classes. Unlike strictly binary splits produced CART, C5.0 allows multi-way splits working categorical attributes, can create trees variable shapes. flexibility often leads compact trees easier interpret, especially datasets categorical variables.C5.0 offers several advantages decision tree algorithms. computationally efficient, making suitable large datasets, flexibility handling non-binary splits allows nuanced tree structures. Additionally, C5.0 incorporates advanced features feature weighting, allows algorithm prioritize relevant features tree-building process. can improve model performance focusing important predictors.Despite strengths, C5.0 algorithm without limitations. Trees generated C5.0 can become overly complex “bushy,” particularly working categorical attributes many unique values. complexity can make trees harder interpret may lead overfitting. address issues, pruning techniques can applied simplify tree improve generalizability. Additionally, computational cost evaluating multiple splits categorical features may increase large datasets high cardinality, although mitigated C5.0’s overall efficiency.summary, C5.0 algorithm powerful versatile tool building decision trees. leveraging concepts like entropy information gain, constructs models accurate interpretable. shares many similarities CART, ability handle multi-way splits use information theory make distinct valuable alternative. C5.0 algorithm widely used fields finance, healthcare, marketing, decision tree models provide actionable insights transparent decision-making processes. next section, explore Random Forests, ensemble learning technique builds upon decision trees enhance accuracy robustness.","code":""},{"path":"chapter-tree.html","id":"random-forests-an-ensemble-approach","chapter":"11 Decision Trees and Random Forests","heading":"11.4 Random Forests: An Ensemble Approach","text":"Decision Trees powerful intuitive, prone overfitting, particularly grown full depth. Random Forests9 address limitation adopting ensemble approach combines predictions multiple Decision Trees produce robust accurate model. Instead relying single tree, Random Forests aggregate predictions many trees, reducing overfitting enhancing performance complex datasets.Random Forest algorithm introduces two key elements randomness improve model diversity:Bootstrap Aggregation (Bagging): tree trained random subset training data, created sampling replacement. means observations appear multiple times tree’s training data, others may excluded. diversity ensures tree learns slightly different patterns.Random Feature Selection: split, algorithm considers random subset features instead evaluating features. decorrelates trees, tree forced rely different combinations features make decisions.forest built, predictions trees aggregated produce final output:classification, final prediction determined majority voting, tree votes class, common class selected.regression, final output average predictions trees.strength Random Forests lies principle “wisdom crowd.” Individually, tree weak learner, trained limited subset data features. However, combined, collective predictions form strong learner. leveraging diversity individual trees, Random Forests reduce likelihood errors made single tree dominate overall model.Additionally, randomness introduced feature selection ensures single feature dominates model, making Random Forests particularly effective datasets correlated redundant features. feature-level decorrelation enhances ensemble’s ability generalize unseen data.Random Forests several notable advantages:Reduced Overfitting: averaging predictions multiple trees, Random Forests smooth noise variance present individual trees, leading better generalization.High Accuracy: Random Forests perform well classification regression tasks, particularly datasets non-linear relationships high-dimensional feature spaces.Feature Importance: algorithm provides feature importance scores, enabling us identify influential predictors. especially useful feature selection gaining insights underlying data.Robustness: Random Forests resilient noise outliers, ensemble effect reduces impact anomalies final prediction.Flexibility: Random Forests can handle numerical categorical data adapt well diverse types problems.Despite strengths, Random Forests limitations:Computational Complexity: Training hundreds thousands trees can computationally intensive, especially large datasets. However, can mitigated parallel processing, tree built independently.Reduced Interpretability: individual Decision Trees highly interpretable, ensemble nature Random Forests makes difficult understand collective decision-making process model.Bias-Variance Tradeoff: Although Random Forests reduce variance bagging, may sometimes smooth complex relationships data single, well-tuned Decision Tree capture.Random Forests strike balance accuracy robustness, addressing many weaknesses individual Decision Trees retaining strengths. well-suited classification regression tasks particularly effective scenarios noisy high-dimensional data. Moreover, ability compute feature importance scores provides valuable insights drivers model’s predictions, making predictive tool also exploratory one.Random Forests become one widely used machine learning algorithms due versatility, reliability, strong performance across variety applications. next section, apply Random Forests, along Decision Trees, adult dataset explore question: can earn $50K per year? case study provide practical demonstration models work can evaluated compared real-world scenario.","code":""},{"path":"chapter-tree.html","id":"tree-case-study","chapter":"11 Decision Trees and Random Forests","heading":"11.5 Case Study: Who Can Earn More Than $50K Per Year?","text":"demonstrate practical application Decision Trees Random Forests, use adult dataset, provides demographic income information individuals. dataset, sourced US Census Bureau, widely used predict whether individual earns $50,000 per year based features education, hours worked per week, marital status, . objective binary classification problem categorize individuals one two income groups: <=50K >50K, features serving predictors target variable income.","code":""},{"path":"chapter-tree.html","id":"overview-of-the-dataset-2","chapter":"11 Decision Trees and Random Forests","heading":"Overview of the Dataset","text":"begin loading dataset examining structure:dataset contains 48598 records 15 variables. , 14 predictors, target variable, income, binary categorical variable two levels: <=50K >50K. features include numerical categorical variables:age: Age years (numerical).workclass: Type employment (categorical, 6 levels).demogweight: Demographic weight (categorical).education: Highest education level (categorical, 16 levels).education.num: Years education (numerical).marital.status: Marital status (categorical, 5 levels).occupation: Type occupation (categorical, 15 levels).relationship: Type relationship (categorical, 6 levels).race: Race (categorical, 5 levels).gender: Gender (categorical, Male/Female).capital.gain: Capital gains (numerical).capital.loss: Capital losses (numerical).hours.per.week: Weekly hours worked (numerical).native.country: Country origin (categorical, 42 levels).income: Target variable, representing annual income (<=50K >50K).additional details dataset, visit documentation.","code":"data(adult)\n\nstr(adult)\n   'data.frame':    48598 obs. of  15 variables:\n    $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n    $ workclass     : Factor w/ 6 levels \"?\",\"Gov\",\"Never-worked\",..: 4 4 2 4 1 4 1 5 4 4 ...\n    $ demogweight   : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n    $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n    $ education.num : int  7 9 12 10 10 6 9 15 10 4 ...\n    $ marital.status: Factor w/ 5 levels \"Divorced\",\"Married\",..: 3 2 2 2 3 3 3 2 3 2 ...\n    $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n    $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n    $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n    $ gender        : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n    $ capital.gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n    $ capital.loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n    $ hours.per.week: int  40 50 40 40 30 30 40 32 40 10 ...\n    $ native.country: Factor w/ 42 levels \"?\",\"Cambodia\",..: 40 40 40 40 40 40 40 40 40 40 ...\n    $ income        : Factor w/ 2 levels \"<=50K\",\">50K\": 1 1 2 2 1 1 1 2 1 1 ..."},{"path":"chapter-tree.html","id":"data-cleaning-and-preparation","chapter":"11 Decision Trees and Random Forests","heading":"Data Cleaning and Preparation","text":"dataset includes missing values represented character \"?\". simplicity, rely prior data cleaning steps (see Chapter 3) handle issues. steps include recoding categorical variables, grouping country-level data broader regions, imputing missing values, demonstrated :partition cleaned dataset training (80%) testing (20%) subsets ensure models evaluated unseen data:use set.seed() ensures reproducibility.","code":"\nset.seed(6)\n\ndata_sets = partition(data = adult, ratio = c(0.8, 0.2))\n\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\n\nactual_test = test_set$income"},{"path":"chapter-tree.html","id":"decision-tree-with-cart","chapter":"11 Decision Trees and Random Forests","heading":"Decision Tree with CART","text":"predict whether individual’s income exceeds $50K, fit Decision Tree using CART algorithm. following predictors used:age, education.num, capital.gain, capital.loss, hours.per.week, marital.status, workclass, race, gender.tree built using rpart() function rpart package:resulting tree visualized using rpart.plot() function:tree identifies marital.status important predictor, followed capital.gain, education.num, capital.loss. tree contains 6 decision nodes 7 leaves, providing interpretable insights predictors.","code":"formula = income ~ age + education.num + capital.gain + capital.loss + \n                   hours.per.week + marital.status + workclass + race + gender\n\ntree_cart = rpart(formula = formula, data = train_set, method = \"class\")\n\nprint(tree_cart)\n   n= 38878 \n   \n   node), split, n, loss, yval, (yprob)\n         * denotes terminal node\n   \n    1) root 38878 9217 <=50K (0.76292505 0.23707495)  \n      2) marital.status=Divorced,Never-married,Separated,Widowed 20580 1282 <=50K (0.93770651 0.06229349)  \n        4) capital.gain< 7055.5 20261  978 <=50K (0.95172992 0.04827008) *\n        5) capital.gain>=7055.5 319   15 >50K (0.04702194 0.95297806) *\n      3) marital.status=Married 18298 7935 <=50K (0.56634605 0.43365395)  \n        6) education.num< 12.5 12944 4163 <=50K (0.67838381 0.32161619)  \n         12) capital.gain< 5095.5 12350 3582 <=50K (0.70995951 0.29004049)  \n           24) education.num< 8.5 2159  231 <=50K (0.89300602 0.10699398) *\n           25) education.num>=8.5 10191 3351 <=50K (0.67118045 0.32881955)  \n             50) capital.loss< 1846 9813 3059 <=50K (0.68827066 0.31172934) *\n             51) capital.loss>=1846 378   86 >50K (0.22751323 0.77248677) *\n         13) capital.gain>=5095.5 594   13 >50K (0.02188552 0.97811448) *\n        7) education.num>=12.5 5354 1582 >50K (0.29548001 0.70451999) *\nrpart.plot(tree_cart, type = 4, extra = 104)"},{"path":"chapter-tree.html","id":"decision-tree-with-c5.0","chapter":"11 Decision Trees and Random Forests","heading":"Decision Tree with C5.0","text":"next use C5.0 algorithm build Decision Tree, starting predictors. tree constructed using C5.0() function C50 package:output provides summary tree. full tree visualization omitted , highlights importance marital.status root node, consistent CART results.","code":"tree_C50 = C5.0(formula, data = train_set) \n\nprint(tree_C50)\n   \n   Call:\n   C5.0.formula(formula = formula, data = train_set)\n   \n   Classification Tree\n   Number of samples: 38878 \n   Number of predictors: 9 \n   \n   Tree size: 93 \n   \n   Non-standard options: attempt to group attributes"},{"path":"chapter-tree.html","id":"random-forest","chapter":"11 Decision Trees and Random Forests","heading":"Random Forest","text":"Random Forest algorithm used build ensemble Decision Trees, aggregating predictions. Using predictors, construct Random Forest model 100 trees using randomForest() function:can visualize variable importance error rate Random Forest model:","code":"\nrandom_forest = randomForest(formula = formula, data = train_set, ntree = 100)\nvarImpPlot(random_forest)\n\nplot(random_forest)"},{"path":"chapter-tree.html","id":"model-evaluation","chapter":"11 Decision Trees and Random Forests","heading":"Model Evaluation","text":"evaluate model performance, calculate confusion matrix, ROC curve, AUC three models (CART, C5.0, Random Forest):","code":""},{"path":"chapter-tree.html","id":"cart","chapter":"11 Decision Trees and Random Forests","heading":"CART:","text":"","code":"predict_cart = predict(tree_cart, test_set, type = \"class\")\n\nconf.mat(predict_cart, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\n          Actual\n   Predict <=50K >50K\n     <=50K  7061 1035\n     >50K    433 1191\nconf.mat.plot(predict_cart, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\""},{"path":"chapter-tree.html","id":"c5.0","chapter":"11 Decision Trees and Random Forests","heading":"C5.0:","text":"","code":"predict_C50 = predict(tree_C50, test_set, type = \"class\")\n\nconf.mat(predict_C50, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\n          Actual\n   Predict <=50K >50K\n     <=50K  7089  887\n     >50K    405 1339\nconf.mat.plot(predict_C50, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\""},{"path":"chapter-tree.html","id":"random-forest-1","chapter":"11 Decision Trees and Random Forests","heading":"Random Forest:","text":"Finally, ROC curves AUC models compared:black curve represents CART, red curve represents C5.0, green curve represents Random Forest. Based AUC values, C5.0 performs slightly better, three models show comparable accuracy, making reliable classification task.","code":"predict_random_forest = predict(random_forest, test_set)\n\nconf.mat(predict_random_forest, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\n          Actual\n   Predict <=50K >50K\n     <=50K  7069  913\n     >50K    425 1313\nconf.mat.plot(predict_random_forest, actual_test)\n   Setting levels: reference = \"<=50K\", case = \">50K\"\nprob_cart = predict(tree_cart, test_set, type = \"prob\")[, 1]\nprob_C50 = predict(tree_C50, test_set, type = \"prob\")[, 1]\nprob_random_forest = predict(random_forest, test_set, type = \"prob\")[, 1]\n\nroc_cart = roc(actual_test, prob_cart)\nroc_C50 = roc(actual_test, prob_C50)\nroc_random_forest = roc(actual_test, prob_random_forest)\n\nggroc(list(roc_cart, roc_C50, roc_random_forest), size = 0.8) + \n    theme_minimal() + ggtitle(\"ROC Curves with AUC for Three Models\") +\n  scale_color_manual(values = 1:3, \n    labels = c(paste(\"CART; AUC=\", round(auc(roc_cart), 3)), \n                paste(\"C5.0; AUC=\", round(auc(roc_C50), 3)), \n                paste(\"Random Forest; AUC=\", round(auc(roc_random_forest), 3)))) +\n  theme(legend.title = element_blank()) +\n  theme(legend.position = c(.7, .3), text = element_text(size = 17))"},{"path":"chapter-tree.html","id":"exercises-8","chapter":"11 Decision Trees and Random Forests","heading":"11.6 Exercises","text":"…","code":""},{"path":"chapter-nn.html","id":"chapter-nn","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12 Neural Networks: The Building Blocks of Artificial Intelligence","text":"centuries, humans dreamed creating machines capable mimicking human intelligence. Philosophers, scientists, storytellers long grappled possibilities consequences creations, weaving myths, fiction, philosophical discourse. roots fascination can traced far back ancient Greece, inventors like Daedalus Hero Alexandria said constructed mechanical devices write, generate sounds, even play music. Today, 21st century, age-old dreams longer confined realm imagination. become reality form Artificial Intelligence (AI), transformative force now deeply integrated daily lives. ChatGPT generative AI (GenAI) self-driving cars digital assistants like Siri Alexa, AI revolutionized way work, interact, make decisions. unprecedented progress driven advancements computational power, availability vast datasets, breakthroughs algorithm design.heart many cutting-edge AI systems lies class algorithms known neural networks. past decade, neural networks undergone dramatic resurgence umbrella deep learning, ushering revolutionary advancements across diverse fields like computer vision, natural language processing, generative modeling. deep learning now represents forefront machine learning, foundation rests simpler neural network architectures. chapter, focus feed-forward neural networks, also known multilayer perceptrons (MLPs). foundational models form building blocks sophisticated deep learning systems.Neural networks computational models inspired human brain. Just brain composed billions interconnected neurons work together enable complex tasks like reasoning, learning, perception, artificial neural networks replicate structure using layers interconnected nodes (artificial neurons). architecture allows neural networks process learn data identifying patterns, making uniquely suited solving problems involve complex, high-dimensional, unstructured data—images, text, sound. Unlike traditional machine learning models like Decision Trees k-Nearest Neighbors, neural networks excel ability automatically discover features representations within data, often surpassing human-engineered solutions.","code":""},{"path":"chapter-nn.html","id":"why-neural-networks-are-powerful","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Why Neural Networks Are Powerful","text":"Neural networks particularly adept solving complex nonlinear problems, making indispensable tasks involve large, diverse, intricate datasets. unique design capabilities provide several key advantages:Pattern Recognition Complex Data: Neural networks shine comes detecting patterns unstructured data, recognizing objects images, understanding spoken language, generating coherent text. tasks traditional algorithms struggle.Robustness Noise: Thanks dense networks neurons adaptable weights, neural networks can identify meaningful patterns even noisy incomplete datasets, effectively filtering irrelevant erroneous data.Scalability: Neural networks can handle vast amounts data adapt increasing complexity adding layers nodes, enabling model highly nonlinear relationships solve challenging problems.Despite strengths, neural networks without challenges. Unlike interpretable models decision trees, neural networks often referred “black boxes” due distributed opaque decision-making processes. can difficult understand neural network makes specific prediction, reasoning embedded across countless weights activations. Additionally, training neural networks can computationally intensive, often requiring specialized hardware like GPUs TPUs handle enormous volume calculations efficiently.power neural networks lies biological inspiration. Just interconnected neurons brain collaborate perform complex tasks, artificial neurons network combine outputs solve problems simpler algorithms . ability emulate nonlinear, adaptive learning brain positioned neural networks forefront academic research industry applications.","code":""},{"path":"chapter-nn.html","id":"whats-ahead","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"What’s Ahead","text":"chapter, explore key concepts behind neural networks examine transformative applications following topics:Biological Inspiration: Understanding structure function human brain inspired artificial neural networks.Core Algorithmic Principles: Exploring foundational mechanics neural networks, including layers, nodes, weights.Activation Functions: Unpacking importance introducing non-linearity enable neural networks model complex patterns.Training Neural Networks: Learning neural networks adjust parameters iterative optimization minimize errors.Case Study: Applying neural networks solve real-world problem—predicting whether customer subscribe term deposit using bank marketing dataset.Neural networks represent paradigm shift modern computing, enabling machines tackle problems considered insurmountable. powering recommendation systems driving autonomous vehicles, models transforming industries shaping future AI. chapter, uncover fundamentals neural networks, demonstrating extraordinary capabilities laying groundwork understanding operate. Let’s begin exploring inspiration behind neural networks connection biology human brain.","code":""},{"path":"chapter-nn.html","id":"neural-networks-inspired-by-biological-neurons","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.1 Neural Networks: Inspired by Biological Neurons","text":"foundation neural networks deeply rooted structure function biological neurons, form basis learning decision-making animal brains. individual neurons relatively simple structure, true power lies dense intricate connectivity. networks interconnected neurons enable brain perform highly complex tasks, pattern recognition, classification, reasoning, decision-making. example, human brain contains approximately \\(10^{11}\\) neurons, neuron forming connections average 10,000 others. creates astonishing \\(10^{15}\\) synaptic connections—vast, dynamic network capable extraordinary learning adaptation.Artificial Neural Networks (ANNs) computational abstractions biological system. far simpler biological counterparts, ANNs replicate fundamental principle learning interconnected units. leveraging dense networks artificial neurons, ANNs can model nonlinear dynamic processes, enabling tackle complex problems domains image recognition, speech processing, decision-making. particularly adept uncovering patterns relationships data, even cases traditional algorithms struggle.shown Figure 12.1, biological neuron designed process transmit information. Dendrites act input channels, collecting signals neurons. signals processed integrated cell body, decision made: combined input surpasses certain threshold, neuron “fires” sends output signal axon connected neurons. nonlinear behavior—firing certain input threshold exceeded—plays critical role brain’s ability process information efficiently.Similarly, artificial neuron (illustrated Figure 12.2) emulates process using mathematical model. receives inputs (\\(x_i\\)) either artificial neurons directly dataset. inputs combined using weighted summation (\\(\\sum w_i x_i\\)), weights (\\(w_i\\)) represent strength input’s influence. combined signal passed activation function (\\(f(.)\\)) introduce non-linearity, determining final output (\\(\\hat{y}\\)). output either passed downstream artificial neurons used final result model. activation function crucial, enables neural networks learn model complex, nonlinear relationships data.\nFigure 12.1: Visualization biological neuron, processes input signals dendrites sends outputs axon.\n\nFigure 12.2: Illustration artificial neuron, designed emulate structure function biological neuron simplified way.\nOne key advantages artificial neural networks robustness. Unlike traditional algorithms, neural networks can handle noisy incomplete data effectively. network’s many interconnected neurons weighted connections allow adapt “learn around” noise, focusing underlying patterns. However, flexibility comes cost. Neural networks often require large amounts data computational power train effectively, decision-making process less interpretable traditional models like decision trees.following sections, delve deeper mechanics neural networks, starting core structure algorithms enable learn data.","code":""},{"path":"chapter-nn.html","id":"how-neural-networks-work","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.2 How Neural Networks Work","text":"Neural networks can understood extension linear models incorporate multiple layers processing produce predictions decisions. core, build upon fundamental concepts linear regression. Recall linear regression model makes predictions using following equation:\\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_p x_p\n\\]\n\\(p\\) represents number predictors, \\(b_0\\) intercept, \\(b_1\\) \\(b_p\\) learned coefficients. setup, \\(\\hat{y}\\) weighted sum input features (\\(x_1\\) \\(x_p\\)), weights (\\(b_1\\) \\(b_p\\)) determine relative influence feature prediction. simple linear relationship can visualized shown Figure 12.3, input features prediction represented nodes, coefficients visualized connecting weights.\nFigure 12.3: graphical representation regression model: input features predictions shown nodes, coefficients represented connections nodes.\nFigure 12.3, nodes left represent input features, lines connecting represent coefficients (\\(w_i\\)), single node right represents output (\\(\\hat{y}\\)), weighted sum inputs. neural network generalizes idea introducing additional layers nodes input output, allowing model capture complex, nonlinear patterns data. structure illustrated Figure 12.4.structure neural network includes following key components:Input Layer: input layer entry point data. node layer corresponds input feature (e.g., age, income, image pixels).Hidden Layers: intermediate layers process data extract patterns. hidden layer contains multiple nodes (artificial neurons), node connected every node preceding succeeding layers. nodes hidden layers perform mathematical transformations data, enabling network learn complex relationships.Output Layer: final layer produces model’s prediction. example, classification task, output might predicted probability specific class, regression tasks, might continuous numerical value.\nFigure 12.4: Visualization multilayer neural network model two hidden layers.\nFigure 12.4, input layer passes features network, hidden layer transforms information passes next layer. output layer aggregates information generate final prediction. Every connection network associated weight (\\(w_i\\)), determines strength relationship two nodes. weights adjusted training optimize model’s accuracy.behavior artificial neuron can mathematically expressed :\\[\n\\hat{y} = f\\left( \\sum_{=1}^{p} w_i x_i + b \\right)\n\\]:\n- \\(x_i\\) represents input features,\n- \\(w_i\\) represents corresponding weights,\n- \\(b\\) bias term helps shift activation threshold,\n- \\(\\sum\\) represents summation weighted inputs,\n- \\(f(.)\\) activation function, \n- \\(\\hat{y}\\) output neuron.activation function plays vital role introducing non-linearity model. Without , neural network simply linear model, regardless complexity number layers. applying non-linear transformation combined input signals, activation functions enable neural networks approximate highly complex patterns data.","code":""},{"path":"chapter-nn.html","id":"key-characteristics-of-neural-networks","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Key Characteristics of Neural Networks","text":"Despite diversity neural network architectures, neural networks share three key characteristics define functionality (see Figure 12.2):Activation Functions:\nactivation function transforms neuron’s net input output signal passed next layer. Activation functions introduce non-linearity, critical modeling complex relationships data. Examples include sigmoid function, ReLU (Rectified Linear Unit), hyperbolic tangent (tanh).Activation Functions:\nactivation function transforms neuron’s net input output signal passed next layer. Activation functions introduce non-linearity, critical modeling complex relationships data. Examples include sigmoid function, ReLU (Rectified Linear Unit), hyperbolic tangent (tanh).Network Architecture:\narchitecture defines overall structure neural network, including number layers, number nodes layer, way nodes connected. instance, deep neural network many hidden layers, enabling learn hierarchical abstract representations data.Network Architecture:\narchitecture defines overall structure neural network, including number layers, number nodes layer, way nodes connected. instance, deep neural network many hidden layers, enabling learn hierarchical abstract representations data.Training Algorithm:\nTraining neural network involves adjusting weights (\\(w_i\\)) biases (\\(b\\)) model minimize error predicted actual outputs. achieved iterative optimization algorithms gradient descent, uses gradient loss function update weights.Training Algorithm:\nTraining neural network involves adjusting weights (\\(w_i\\)) biases (\\(b\\)) model minimize error predicted actual outputs. achieved iterative optimization algorithms gradient descent, uses gradient loss function update weights.following sections, ’ll explore components greater detail, starting activation functions role enabling neural networks learn complex, non-linear patterns.","code":""},{"path":"chapter-nn.html","id":"activation-functions","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.3 Activation Functions","text":"activation function critical component neural network, defining artificial neuron processes incoming signals passes information network. Much like biological counterpart, artificial neuron aggregates input signals, applies transformation, determines output signal send forward. biological neurons, transformation akin summing input signals dendrites deciding whether neuron “fires” based whether cumulative signal exceeds certain threshold.artificial neurons, process implemented mathematically. activation function determines whether, extent, neuron “activates” response inputs. Early models neural networks often used threshold activation function, mirrors biological concept. threshold function activates input signal surpasses certain threshold value. Mathematically, defined :\\[\nf(x) =\n\\begin{cases}\n1 & \\text{} x \\geq 0 \\\\\n0 & \\text{} x < 0\n\\end{cases}\n\\]Figure 12.5 visualizes threshold activation function. , neuron outputs value 1 input least zero, 0 otherwise. Due step-like shape, sometimes referred unit step function.\nFigure 12.5: Visualization threshold activation function (unit step).\nbiologically intuitive, threshold activation function rarely used modern neural networks handle nuanced relationships input output. rigid, providing binary outputs (0 1), differentiable, prevents use optimization algorithms like gradient descent.","code":""},{"path":"chapter-nn.html","id":"the-sigmoid-activation-function","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"The Sigmoid Activation Function","text":"widely used alternative sigmoid activation function, also known logistic sigmoid. sigmoid function provides smoother, non-binary output maps input value 0 1. defined mathematically :\\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\], \\(e\\) base natural logarithm (approximately 2.72). sigmoid function “S-shaped” curve, shown Figure 12.6, makes particularly useful modeling probabilities continuous values. Unlike threshold function, sigmoid differentiable, makes compatible modern training algorithms.\nFigure 12.6: Visualization sigmoid activation function.\nsigmoid function effective producing smooth outputs certain limitations. example, suffers vanishing gradient problem, large small input values cause gradient approach zero, slowing learning process.","code":""},{"path":"chapter-nn.html","id":"other-common-activation-functions","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Other Common Activation Functions","text":"addition sigmoid function, several activation functions commonly used, depending specific needs task. Figure 12.7 provides overview three widely used activation functions:Hyperbolic Tangent (tanh): Similar sigmoid function output range \\((-1, 1)\\). symmetry around zero often leads faster learning practice.Gaussian: Produces bell-shaped curve centered zero. less common useful specific applications like radial basis function networks.Linear Activation: Outputs input , often used final layer regression tasks.\nFigure 12.7: Comparison common activation functions: Sigmoid, tanh, Gaussian.\n","code":""},{"path":"chapter-nn.html","id":"choosing-the-right-activation-function","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Choosing the Right Activation Function","text":"choice activation function significantly impacts performance behavior neural network:Sigmoid commonly used output layer binary classification problems, output represents probability.Tanh often used hidden layers zero-centered, leading faster convergence optimization.ReLU (Rectified Linear Unit): discussed , ReLU one popular activation functions modern neural networks. outputs \\(f(x) = \\max(0, x)\\), helps address vanishing gradient problem accelerates training.Gaussian linear functions specialized chosen specific tasks, regression radial basis function networks.Activation functions like sigmoid tanh compress input values narrow output range (e.g., 0 1 sigmoid, \\(-1\\) \\(1\\) tanh). compression leads saturation problem, input values far zero produce near-constant outputs. instance:\n- Sigmoid outputs close 0 inputs \\(-5\\) close 1 inputs \\(+5\\).\n- results gradients near zero, slowing learning (vanishing gradient problem).One solution issue preprocess data normalizing standardizing input features fall within small range centered around zero. ensures inputs sensitive (non-saturated) range activation function, leading faster convergence better learning.","code":""},{"path":"chapter-nn.html","id":"network-architecture","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.4 Network Architecture","text":"capacity neural network learn make predictions deeply tied architecture, topology. refers arrangement neurons connections , define data flows network. neural networks can take countless forms, architecture primarily characterized three elements:number layers network,number neurons (nodes) layer, andThe connections neurons across layers.architecture neural network determines ability model complexity. Larger networks layers neurons can capture intricate patterns decision boundaries. However, power network just matter size also components organized interconnected.understand network architecture, let us consider simple example illustrated Figure 12.3. basic network consists :Input nodes, receive raw feature values dataset. input node corresponds one feature passes value network.Output nodes, provide network’s final prediction (denoted \\(p\\)).single-layer network, input nodes connected directly output node set weights (\\(w_1, w_2, \\dots, w_p\\)), representing influence input feature prediction. simple architecture works well basic classification regression tasks struggles complex patterns.handle sophisticated tasks, can add hidden layers, shown Figure 12.4. intermediate layers introduce additional processing steps, enabling network model nonlinear relationships discover complex patterns data.multilayer network typically consists three types layers:input layer, raw features enter network,One hidden layers, extract refine patterns, andThe output layer, combines processed information generate network’s final prediction.fully connected network, every neuron one layer connected every neuron next layer, connection assigned weight. weights determine much influence one neuron another, adjusted training optimize network’s performance.addition hidden layers allows network process input data hierarchically. Early layers may learn basic features, edges image simple word patterns text, deeper layers capture abstract representations, shapes semantic meaning. network contains multiple hidden layers, referred deep neural network (DNN). practice training networks known deep learning, enabled breakthroughs fields computer vision, speech recognition, natural language processing.number input output nodes network determined problem:\n- input nodes match number features dataset. example, dataset 20 features 20 input nodes.\n- output nodes depend task. regression, typically one output node predicted value. classification, number output nodes corresponds number classes.number hidden nodes layer predefined must decided user. larger number hidden nodes increases network’s capacity learn complex patterns, also increases risk overfitting—situation model performs well training data poorly unseen data. Overly large networks can also computationally expensive slow train.complex network may appear powerful, crucial strike balance complexity simplicity. often guided principle Occam’s Razor, suggests simplest model adequately explains data usually best choice. optimal network architecture often requires trial error, combined domain knowledge validation techniques evaluate performance unseen data.summary, architecture neural network defines capacity solve problems. simple single-layer network sophisticated deep neural networks, architectures offer flexibility model wide variety tasks, ranging basic regression highly complex problems like image recognition text generation. move forward, explore architectures trained optimize performance learn data.","code":""},{"path":"chapter-nn.html","id":"how-neural-networks-learn","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.5 How Neural Networks Learn","text":"neural network begins untrained structure—blank slate. Similar newborn child learning experience, neural network must trained data adjust internal connections. connections, represented weights, strengthened weakened network processes data, enabling learn patterns relationships time. Just baby’s brain develops interacting environment, neural network refines iteratively improving predictions based data encounters.Training neural network involves computationally intensive process adjusting weights connect neurons. neural networks studied since mid-20th century, remained impractical real-world applications 1980s, major breakthrough—backpropagation algorithm—made feasible train multilayer networks efficiently. Backpropagation, technique “back-propagating” errors network, revolutionized neural networks enabling learn complex tasks. Despite computationally expensive relative simpler algorithms, backpropagation become cornerstone modern neural network training, powering applications computer vision natural language processing.core, backpropagation works iteratively refining network’s weights process cycles two phases: forward phase backward phase. cycle, referred epoch, begins random initialization weights, network starts prior knowledge. successive epochs, network learns adjusting weights minimize prediction errors.forward phase, input data passed network, layer layer, starting input layer propagating hidden layers reaching output layer. neuron processes input applying weights, summing weighted inputs, transforming result using activation function. output layer produces network’s prediction, compared actual target value training data. comparison generates error signal—measure far network’s prediction .backward phase, error signal propagated backward network update weights. goal adjust weights way network produces predictions closer true target values subsequent forward passes. achieved using technique called gradient descent, determines optimal direction magnitude weight changes minimize error. Gradient descent relies derivative activation function compute gradient error respect weight. gradient indicates steeply error changes small adjustment weight, providing “slope” guide weight updates. process akin finding fastest route downhill mountainous terrain: always stepping direction steepest descent, network gradually approaches point minimum error.size weight adjustment controlled parameter called learning rate. high learning rate allows network make large, rapid updates weights, potentially speeding training risking overshooting optimal solution. Conversely, low learning rate ensures precise updates may result slower convergence. Striking right balance learning rate crucial effective training.successfully apply gradient descent backpropagation, network’s activation functions must differentiable. requirement smooth, non-linear activation functions sigmoid, hyperbolic tangent, ReLU (Rectified Linear Unit) widely used. differentiable nature enables network compute gradients efficiently, allowing algorithm make meaningful updates weights.repeated cycles forward backward propagation, network refines weights, reducing overall error improving ability generalize unseen data. training process may sound complex, modern machine learning libraries like TensorFlow PyTorch automate implementation backpropagation, gradient descent, weight updates. tools simplify process, enabling practitioners focus designing network architecture preparing data.development backpropagation algorithm marked turning point neural networks, enabling tackle real-world problems impressive accuracy. Although training remains computationally demanding, advancements hardware—GPUs TPUs—made practical train even large, complex networks. progress driven breakthroughs fields speech recognition, image classification, predictive modeling.Now foundation neural networks trained, explore application real-world scenarios, demonstrating can extract meaningful patterns data make predictions.","code":""},{"path":"chapter-nn.html","id":"case-study-bank-marketing","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.6 Case Study: Bank Marketing","text":"bank marketing dataset contains information customers contacted Portuguese banking institution subscribe term deposit. primary objective predict whether customer subscribe term deposit based available features. analysis helps identify profiles likely subscribers also provides insights improving future marketing campaigns.","code":""},{"path":"chapter-nn.html","id":"business-context","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Business Context","text":"Banks often rely two approaches promote products:Mass Campaigns: Targeting general public indiscriminately, typically resulting low response rates (e.g., <1%).Directed Marketing: Focusing specific, likely customers, improves effectiveness raises concerns privacy intrusion.case, goal refine effectiveness directed marketing campaigns analyzing patterns past campaign data. identifying customers likely subscribe, bank can reduce costs, minimize intrusive communications, maintain success rates.term deposit savings product fixed interest rate specified period. Customers benefit better interest rates compared regular savings accounts, banks use term deposits increase financial assets. term deposits can found .","code":""},{"path":"chapter-nn.html","id":"overview-of-the-dataset-3","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Overview of the Dataset","text":"dataset, sourced repository, includes information direct phone-based marketing campaigns. customers contacted multiple times campaign. goal classify whether customer subscribed term deposit (deposit = \"yes\" \"\").load inspect dataset:dataset contains 4521 observations 17 variables. includes 16 predictors one target variable (deposit). summary key variables:Demographic Features:\n- age: Age customer (numeric).\n- job: Type job (e.g., “admin.”, “blue-collar”, “management”).\n- marital: Marital status (e.g., “married”, “single”).\n- education: Level education (e.g., “secondary”, “tertiary”).\n- default: Whether customer credit default (binary: “yes”, “”).\n- balance: Average yearly balance euros (numeric).Loan Information:\n- housing: Whether customer housing loan (binary).\n- loan: Whether customer personal loan (binary).Campaign Details:\n- contact: Type communication (e.g., “telephone”, “cellular”).\n- duration: Last contact duration seconds (numeric).\n- campaign: Number contacts performed campaign (numeric).\n- pdays: Days since customer last contacted (numeric).\n- previous: Number contacts current campaign (numeric).\n- poutcome: Outcome previous campaign (e.g., “success”, “failure”).Target Variable:\n- deposit: Indicates whether customer subscribed term deposit (binary: “yes”, “”).","code":"data(bank)   # Load the bank marketing dataset \n\nstr(bank)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : int  30 33 35 30 59 35 36 39 41 43 ...\n    $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 11 8 5 5 2 5 7 10 3 8 ...\n    $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 3 2 2 3 2 2 2 2 ...\n    $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 1 2 3 3 2 3 3 2 3 1 ...\n    $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ balance  : int  1787 4789 1350 1476 0 747 307 147 221 -88 ...\n    $ housing  : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 2 2 1 2 2 2 2 ...\n    $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 1 1 1 1 1 2 ...\n    $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 1 1 1 3 3 1 1 1 3 1 ...\n    $ day      : int  19 11 16 3 5 23 14 6 14 17 ...\n    $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 11 9 1 7 9 4 9 9 9 1 ...\n    $ duration : int  79 220 185 199 226 141 341 151 57 313 ...\n    $ campaign : int  1 1 1 4 1 2 1 2 2 1 ...\n    $ pdays    : int  -1 339 330 -1 -1 176 330 -1 -1 147 ...\n    $ previous : int  0 4 1 0 0 3 2 0 0 2 ...\n    $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 1 1 4 4 1 2 4 4 1 ...\n    $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"chapter-nn.html","id":"data-cleaning-and-preparation-1","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Data Cleaning and Preparation","text":"prepare dataset Neural Network algorithm, variables must scaled range 0 1. apply min-max normalization using minmax() function liver package:applying min-max normalization, can compare distribution age example:Next, partition dataset training (80%) test (20%) subsets using partition() function:validate split, compare proportion deposit = \"yes\" training test sets using two-sample Z-test:test confirms proportions subsets statistically similar (p-value > 0.05), validating split.’s improved version subsection:","code":"bank_mm = minmax(bank, col = \"all\")\nstr(bank_mm)\n   'data.frame':    4521 obs. of  17 variables:\n    $ age      : num  0.162 0.206 0.235 0.162 0.588 ...\n    $ job      : num  0.9091 0.6364 0.3636 0.3636 0.0909 ...\n    $ marital  : num  0.5 0.5 1 0.5 0.5 1 0.5 0.5 0.5 0.5 ...\n    $ education: num  0 0.333 0.667 0.667 0.333 ...\n    $ default  : num  0 0 0 0 0 0 0 0 0 0 ...\n    $ balance  : num  0.0685 0.1088 0.0626 0.0643 0.0445 ...\n    $ housing  : num  0 1 1 1 1 0 1 1 1 1 ...\n    $ loan     : num  0 1 0 1 0 0 0 0 0 1 ...\n    $ contact  : num  0 0 0 1 1 0 0 0 1 0 ...\n    $ day      : num  0.6 0.3333 0.5 0.0667 0.1333 ...\n    $ month    : num  0.909 0.727 0 0.545 0.727 ...\n    $ duration : num  0.0248 0.0715 0.0599 0.0645 0.0735 ...\n    $ campaign : num  0 0 0 0.0612 0 ...\n    $ pdays    : num  0 0.39 0.38 0 0 ...\n    $ previous : num  0 0.16 0.04 0 0 0.12 0.08 0 0 0.08 ...\n    $ poutcome : num  1 0 0 1 1 ...\n    $ deposit  : num  0 0 0 0 0 0 0 0 0 0 ...\nggplot(data = bank) +\n  geom_histogram(mapping = aes(x = age), colour = \"darkblue\", fill = \"lightblue\") +\n  ggtitle(\"Variable 'age' before normalization\")\n\nggplot(data = bank_mm) +\n  geom_histogram(mapping = aes(x = age), colour = \"darkblue\", fill = \"lightblue\") +\n  ggtitle(\"Variable 'age' after normalization\")\nset.seed(500)\n\ndata_sets = partition(data = bank_mm, ratio = c(0.8, 0.2))\ntrain_set = data_sets$part1\ntest_set  = data_sets$part2\nactual_test = test_set$depositx1 = sum(train_set$deposit == 1)\nx2 = sum(test_set$deposit == 1)\n\nn1 = nrow(train_set)\nn2 = nrow(test_set)\n\nprop.test(x = c(x1, x2), n = c(n1, n2))\n   \n    2-sample test for equality of proportions with continuity correction\n   \n   data:  c(x1, x2) out of c(n1, n2)\n   X-squared = 0.0014152, df = 1, p-value = 0.97\n   alternative hypothesis: two.sided\n   95 percent confidence interval:\n    -0.02516048  0.02288448\n   sample estimates:\n      prop 1    prop 2 \n   0.1150124 0.1161504"},{"path":"chapter-nn.html","id":"applying-the-neural-network-algorithm","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Applying the Neural Network Algorithm","text":"objective classify customers either likely (deposit = \"yes\") unlikely (deposit = \"\") subscribe term deposit, based following predictors:age, default, balance, housing, loan, duration, campaign, pdays, previous.implement neural network, use neuralnet package R. package offers straightforward flexible way build neural networks provides functionality visualizing network topology. neuralnet great learning tool, also powerful enough practical applications.haven’t already installed neuralnet package, can typing:installed, load session:Next, apply neuralnet() function training dataset build model:’s argument function call :formula: Specifies target variable (deposit) predictors.data: Indicates dataset used training (train_set).hidden: Defines number hidden layers nodes (1 hidden layer 1 node case).err.fct: Sets error function minimize training; use “sse” (Sum Squared Errors).linear.output: Ensures nonlinear activation function output layer, appropriate classification tasks.training, visualize network examine topology:visualization shows network consists :\n- 9 input nodes, corresponding 9 predictors,\n- 1 hidden layer containing single node, \n- 1 output node representing classification result (yes ).training process converged 2578 steps, final error rate 142.97. Upon analyzing weights network, found duration predictor greatest influence model’s output, making significant factor determining whether customer likely subscribe term deposit.straightforward setup demonstrates neural networks process input data multiple layers extract meaningful patterns make predictions. next section, evaluate model’s performance interpret results.","code":"\ninstall.packages(\"neuralnet\")\nlibrary(neuralnet)\nformula = deposit ~ age + default + balance + housing + loan + duration + \n                    campaign + pdays + previous\n\nneuralnet_bank = neuralnet(\n  formula = formula,\n  data = train_set,\n  hidden = 1,                # Single hidden layer with 1 node\n  err.fct = \"sse\",           # Error function: Sum of Squared Errors\n  linear.output = FALSE      # Nonlinear activation function\n)\nplot(neuralnet_bank, rep = \"best\")"},{"path":"chapter-nn.html","id":"prediction-and-model-evaluation-1","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"Prediction and Model Evaluation","text":"use trained Neural Network predict outcomes test dataset:evaluate predictions using confusion matrix cutoff value 0.5:confusion matrix reveals number correct incorrect predictions (e.g., true positives, false positives). Finally, assess model’s performance plotting ROC curve calculating AUC:ROC curve illustrates model’s ability distinguish two classes. high AUC score indicates strong predictive performance.","code":"prob_nn = predict(neuralnet_bank, test_set)\nhead(prob_nn)\n            [,1]\n   12 0.01460510\n   16 0.02158387\n   17 0.11371069\n   19 0.02391761\n   32 0.03149115\n   38 0.01552204conf.mat(prob_nn, actual_test, cutoff = 0.5)\n   Setting levels: reference = \"0\", case = \"1\"\n          Actual\n   Predict   0   1\n         0  16  22\n         1 783  83\nroc_nn = roc(actual_test, prob_nn)\n\nggroc(roc_nn, size = 0.8) + \n  theme_minimal() + \n  ggtitle(\"ROC for Neural Network Algorithm\") +\n  theme(legend.title = element_blank(), legend.position = c(.7, .3), text = element_text(size = 17))"},{"path":"chapter-nn.html","id":"exercises-9","chapter":"12 Neural Networks: The Building Blocks of Artificial Intelligence","heading":"12.7 Exercises","text":"…","code":""},{"path":"chapter-cluster.html","id":"chapter-cluster","chapter":"13 Clustering","heading":"13 Clustering","text":"Every day, interact systems organize vast amounts data without explicit instructions. Netflix recommend movies tailored taste? Amazon categorize millions products? real-world examples clustering, machine learning technique groups similar items based shared characteristics—without predefined labels.many real-world scenarios, deal large datasets structure unknown. Unlike classification, assigns predefined labels data points (e.g., distinguishing spam non-spam emails), clustering exploratory—helps uncover hidden patterns, making powerful tool knowledge discovery. identifying meaningful groups, clustering allows us make sense complex data extract valuable insights.Clustering widely used across multiple domains, including:Customer segmentation – Identifying distinct customer groups personalize marketing campaigns.Market research – Understanding consumer behavior enhance product recommendations.Fraud detection – Detecting suspicious financial transactions may indicate fraudulent activity.Document organization – Automatically grouping large collections text meaningful categories.Bioinformatics – Clustering genes similar expression patterns uncover biological insights.chapter provides comprehensive introduction clustering, covering:fundamental principles clustering differs classification.mechanics clustering algorithms define similarity.K-means clustering, one widely used clustering techniques.practical case study: segmenting cereal brands based nutritional content.end chapter, understand clustering works, apply , implement real-world scenarios. Let’s dive !\n## Cluster Analysis? {#cluster-}Clustering unsupervised machine learning technique groups data points clusters based similarity. Unlike supervised learning, models learn labeled examples, clustering exploratory—uncovers hidden structures data without predefined labels. goal form groups data points within cluster highly similar, different clusters distinct.computer determine data points belong together? Clustering relies similarity measures quantify close distant two points . One commonly used approaches distance metrics, Euclidean distance, defined :\\[\n\\text{dist}(x, y) = \\sqrt{ \\sum_{=1}^n (x_i - y_i)^2}\n\\]\\(x = (x_1, x_2, \\ldots, x_n)\\) \\(y = (y_1, y_2, \\ldots, y_n)\\) represent two data points \\(n\\) features. closer two points, similar .However, Euclidean distance always appropriate. categorical variables, alternative strategies one-hot encoding transform categories numerical values, enabling distance-based clustering. Additionally, features often require scaling (e.g., min-max normalization) ensure single variable dominates clustering process.Clustering often compared classification, serve different purposes. Classification assigns predefined labels new data points based past examples, whereas clustering discovers groupings raw data. Classification typically used prediction, clustering primarily exploration pattern discovery. clustering generates labels rather predicting existing ones, sometimes referred unsupervised classification. cluster assignments can used inputs analysis, refining predictions neural network decision tree model.clustering algorithms aim achieve high intra-cluster similarity (data points within cluster close together) low inter-cluster similarity (clusters well separated). concept visually illustrated Figure 13.1, effective clusters minimize internal variation maximizing separation groups.\nFigure 13.1: Clustering algorithms aim minimize intra-cluster variation maximizing inter-cluster separation.\nBeyond role data exploration, clustering widely used preprocessing step machine learning. Given massive scale modern datasets, clustering helps reduce complexity identifying smaller number representative groups, leading several benefits:Reduced computation time downstream models.Improved interpretability summarizing large datasets.Enhanced predictive performance structuring inputs supervised learning.following sections, explore K-means clustering, one widely used clustering algorithms. also discuss methods selecting optimal number clusters apply clustering real-world dataset.","code":""},{"path":"chapter-cluster.html","id":"kmeans","chapter":"13 Clustering","heading":"13.1 K-means Clustering","text":"K-means clustering one simplest widely used clustering algorithms. aims partition dataset \\(k\\) clusters iteratively refining cluster centers, ensuring data points within cluster similar possible. algorithm operates iterative process assigning points clusters updating cluster centers based assignments. process stops assignments stabilize, meaning data points switch clusters.K-means algorithm requires user specify number clusters, \\(k\\), advance. follows steps:Initialize: Randomly select \\(k\\) data points initial cluster centers.Assignment: Assign data point nearest cluster center. creates \\(k\\) groups.Update: Compute centroid (mean) cluster move cluster centers new locations.Repeat: Iterate steps 2 3 convergence—cluster assignments longer change.Although K-means simple efficient, limitations. final clusters depend heavily initial choice cluster centers, meaning different runs algorithm may produce different results. Additionally, K-means sensitive outliers assumes clusters spherical similar size, may always case real-world data.illustrate K-means works, consider dataset 50 records two features, \\(x_1\\) \\(x_2\\), shown Figure 13.2. goal partition data three clusters.\nFigure 13.2: simple dataset 50 records two features, ready clustering.\nfirst step randomly select three initial cluster centers (red stars), shown left panel Figure 13.3. data point assigned nearest cluster, forming three groups labeled blue (Cluster ), green (Cluster B), orange (Cluster C). right panel Figure 13.3 displays initial assignments. dashed lines represent Voronoi diagram, divides space regions associated cluster center.\nFigure 13.3: Initial random cluster centers (left) first cluster assignments (right).\nSince K-means sensitive initialization, poor placement initial cluster centers can lead suboptimal clustering. mitigate issue, K-means++10 introduced 2007. method strategically selects initial centers improve convergence reduce randomness.initial cluster assignments made, K-means enters update phase. first step recompute centroid cluster, mean position points assigned cluster. cluster centers moved new centroid locations, shown left panel Figure 13.4. right panel illustrates Voronoi boundaries shift, causing data points reassigned different cluster.\nFigure 13.4: Updated cluster centers (left) new assignments centroid adjustment (right).\nprocess—reassigning points updating centroids—continues iteratively. another update, points switch clusters , leading refined Voronoi partition, shown Figure 13.5.\nFigure 13.5: Updated cluster centers assignments another iteration.\nalgorithm continues iterating cluster assignments stabilize—points switch clusters, shown Figure 13.6. point, algorithm converges, final clusters established.\nFigure 13.6: Final cluster assignments K-means convergence.\nclustering complete, results can presented two ways:Cluster Assignments: data point labeled belonging Cluster , B, C.Centroid Coordinates: final positions cluster centers can reported.final cluster centroids act representative points, summarizing dataset enabling analysis. K-means clustering widely used applications customer segmentation, image compression, document clustering. next section, explore methods selecting optimal number clusters ensure meaningful partitions real-world datasets.","code":""},{"path":"chapter-cluster.html","id":"kmeans-choose","chapter":"13 Clustering","heading":"13.2 Choosing the Number of Clusters","text":"One key challenges K-means clustering selecting appropriate number clusters, \\(k\\). choice \\(k\\) significantly impacts results—clusters may fail capture meaningful structures, many clusters risk overfitting creating overly fragmented groups. Unlike supervised learning, evaluation metrics like accuracy guide model selection, clustering absolute ground truth, making selection \\(k\\) subjective.cases, domain knowledge can provide useful guidance. example, clustering movies, reasonable starting point might number well-known genres. business setting, marketing teams may set \\(k = 3\\) plan design three distinct advertising campaigns. Similarly, seating arrangements conference might determine number groups based available tables. However, clear intuition exists, data-driven methods needed determine optimal \\(k\\).One widely used technique choosing \\(k\\) elbow method, evaluates within-cluster variation changes number clusters increases. clusters added, clusters become homogeneous (internal similarity increases), overall heterogeneity (difference clusters) decreases. However, improvement follows diminishing returns pattern. idea find point adding another cluster longer significantly reduces within-cluster variance.critical point, known elbow point, represents natural number clusters. concept illustrated Figure 13.7, curve shows total within-cluster sum squares (WCSS) function \\(k\\). “elbow” curve—rate improvement slows—strong candidate \\(k\\).\nFigure 13.7: elbow method helps determine optimal number clusters K-means clustering.\nelbow method provides useful heuristic, limitations. datasets, curve may exhibit clear elbow, making choice \\(k\\) ambiguous. Additionally, evaluating many different values \\(k\\) can computationally expensive, especially large datasets.techniques can supplement refine selection \\(k\\):Silhouette Score: Measures well point fits within assigned cluster compared others. higher silhouette score suggests well-defined clustering structure.Gap Statistic: Compares clustering result reference distribution assess whether structure significant.Cross-validation clustering tasks: applications clustering feeds downstream task (e.g., classification), impact different \\(k\\) values can evaluated context.Ultimately, choice \\(k\\) driven data characteristics practical considerations. Clustering often used exploratory analysis, meaning useful \\(k\\) necessarily mathematically “optimal” one rather one yields meaningful, interpretable insights.Observing cluster characteristics evolve \\(k\\) varies can informative. groups may remain stable across different \\(k\\) values, indicating strong natural boundaries, others may appear disappear, suggesting fluid structures data.Rather aiming perfect cluster count, often sufficient find reasonable interpretable clustering solution. next section, apply clustering real-world dataset, demonstrating practical knowledge can guide choice \\(k\\) actionable insights.Now explored K-means clustering methods selecting optimal number clusters, apply concepts real-world dataset.","code":""},{"path":"chapter-cluster.html","id":"kmeans-cereal","chapter":"13 Clustering","heading":"13.3 Case Study: Clustering Cereal Data","text":"case study, apply K-means clustering cereal dataset liver package. dataset contains nutritional information 77 cereal brands, including calories, protein, fat, sodium, fiber, sugar content. Understanding nutritional profiles valuable marketing strategies, consumer targeting, product positioning. goal segment cereals distinct groups based nutritional similarities.","code":""},{"path":"chapter-cluster.html","id":"dataset-overview","chapter":"13 Clustering","heading":"13.3.1 Dataset Overview","text":"cereal dataset includes 77 observations 16 variables, covering various nutritional attributes. can accessed liver package, shown :can examine structure using:dataset contains following variables:name: Name cereal (categorical).manuf: Manufacturer cereal (categorical).type: Cereal type (hot cold, categorical).calories: Calories per serving (numerical).protein: Grams protein per serving (numerical).fat: Grams fat per serving (numerical).sodium: Milligrams sodium per serving (numerical).fiber: Grams dietary fiber per serving (numerical).carbo: Grams carbohydrates per serving (numerical).sugars: Grams sugar per serving (numerical).potass: Milligrams potassium per serving (numerical).vitamins: Percentage FDA-recommended vitamins (categorical: 0, 25, 100).shelf: Display shelf position (categorical: 1, 2, 3).weight: Weight one serving ounces (numerical).cups: Number cups per serving (numerical).rating: Cereal rating score (numerical).","code":"\nlibrary(liver)  # Load the liver package\n\ndata(cereal)    # Load the cereal datasetstr(cereal)\n   'data.frame':    77 obs. of  16 variables:\n    $ name    : Factor w/ 77 levels \"100% Bran\",\"100% Natural Bran\",..: 1 2 3 4 5 6 7 8 9 10 ...\n    $ manuf   : Factor w/ 7 levels \"A\",\"G\",\"K\",\"N\",..: 4 6 3 3 7 2 3 2 7 5 ...\n    $ type    : Factor w/ 2 levels \"cold\",\"hot\": 1 1 1 1 1 1 1 1 1 1 ...\n    $ calories: int  70 120 70 50 110 110 110 130 90 90 ...\n    $ protein : int  4 3 4 4 2 2 2 3 2 3 ...\n    $ fat     : int  1 5 1 0 2 2 0 2 1 0 ...\n    $ sodium  : int  130 15 260 140 200 180 125 210 200 210 ...\n    $ fiber   : num  10 2 9 14 1 1.5 1 2 4 5 ...\n    $ carbo   : num  5 8 7 8 14 10.5 11 18 15 13 ...\n    $ sugars  : int  6 8 5 0 8 10 14 8 6 5 ...\n    $ potass  : int  280 135 320 330 -1 70 30 100 125 190 ...\n    $ vitamins: int  25 0 25 25 25 25 25 25 25 25 ...\n    $ shelf   : int  3 3 3 3 3 1 2 3 1 3 ...\n    $ weight  : num  1 1 1 1 1 1 1 1.33 1 1 ...\n    $ cups    : num  0.33 1 0.33 0.5 0.75 0.75 1 0.75 0.67 0.67 ...\n    $ rating  : num  68.4 34 59.4 93.7 34.4 ..."},{"path":"chapter-cluster.html","id":"data-preprocessing","chapter":"13 Clustering","heading":"13.3.2 Data Preprocessing","text":"applying K-means clustering, need clean preprocess data. start summarizing dataset:Upon inspection, notice unusual values variables sugars, carbo, potass, entries set -1. Since negative values invalid nutritional attributes, replace NA:Next, handle missing values using K-nearest neighbors (KNN) imputation knnImputation() function DMwR2 package:clustering, exclude categorical identifier variables (name, manuf, rating), retaining numerical features:Since dataset includes features different scales, apply min-max scaling using minmax() function liver package ensure variables contribute equally clustering process:visualize effect normalization, plot sodium distribution scaling:scaling, values fall within 0–1 range, making distance-based clustering reliable.","code":"summary(cereal)\n                           name    manuf    type       calories    \n    100% Bran                : 1   A: 1   cold:74   Min.   : 50.0  \n    100% Natural Bran        : 1   G:22   hot : 3   1st Qu.:100.0  \n    All-Bran                 : 1   K:23             Median :110.0  \n    All-Bran with Extra Fiber: 1   N: 6             Mean   :106.9  \n    Almond Delight           : 1   P: 9             3rd Qu.:110.0  \n    Apple Cinnamon Cheerios  : 1   Q: 8             Max.   :160.0  \n    (Other)                  :71   R: 8                            \n       protein           fat            sodium          fiber       \n    Min.   :1.000   Min.   :0.000   Min.   :  0.0   Min.   : 0.000  \n    1st Qu.:2.000   1st Qu.:0.000   1st Qu.:130.0   1st Qu.: 1.000  \n    Median :3.000   Median :1.000   Median :180.0   Median : 2.000  \n    Mean   :2.545   Mean   :1.013   Mean   :159.7   Mean   : 2.152  \n    3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:210.0   3rd Qu.: 3.000  \n    Max.   :6.000   Max.   :5.000   Max.   :320.0   Max.   :14.000  \n                                                                    \n        carbo          sugars           potass          vitamins     \n    Min.   :-1.0   Min.   :-1.000   Min.   : -1.00   Min.   :  0.00  \n    1st Qu.:12.0   1st Qu.: 3.000   1st Qu.: 40.00   1st Qu.: 25.00  \n    Median :14.0   Median : 7.000   Median : 90.00   Median : 25.00  \n    Mean   :14.6   Mean   : 6.922   Mean   : 96.08   Mean   : 28.25  \n    3rd Qu.:17.0   3rd Qu.:11.000   3rd Qu.:120.00   3rd Qu.: 25.00  \n    Max.   :23.0   Max.   :15.000   Max.   :330.00   Max.   :100.00  \n                                                                     \n        shelf           weight          cups           rating     \n    Min.   :1.000   Min.   :0.50   Min.   :0.250   Min.   :18.04  \n    1st Qu.:1.000   1st Qu.:1.00   1st Qu.:0.670   1st Qu.:33.17  \n    Median :2.000   Median :1.00   Median :0.750   Median :40.40  \n    Mean   :2.208   Mean   :1.03   Mean   :0.821   Mean   :42.67  \n    3rd Qu.:3.000   3rd Qu.:1.00   3rd Qu.:1.000   3rd Qu.:50.83  \n    Max.   :3.000   Max.   :1.50   Max.   :1.500   Max.   :93.70  \n   cereal[cereal == -1] <- NA\nfind.na(cereal)  # Check missing values\n        row col\n   [1,]  58   9\n   [2,]  58  10\n   [3,]   5  11\n   [4,]  21  11library(DMwR2)\ncereal <- knnImputation(cereal, k = 3, scale = TRUE)\nfind.na(cereal)  # Verify missing values are filled\n   [1] \" No missing values (NA) in the dataset.\"\nselected_variables <- colnames(cereal)[-c(1, 2, 16)]\ncereal_subset <- cereal[, selected_variables]cereal_mm <- minmax(cereal_subset, col = \"all\")\nstr(cereal_mm)  # Check the transformed dataset\n   'data.frame':    77 obs. of  13 variables:\n    $ type    : num  0 0 0 0 0 0 0 0 0 0 ...\n    $ calories: num  0.182 0.636 0.182 0 0.545 ...\n    $ protein : num  0.6 0.4 0.6 0.6 0.2 0.2 0.2 0.4 0.2 0.4 ...\n    $ fat     : num  0.2 1 0.2 0 0.4 0.4 0 0.4 0.2 0 ...\n    $ sodium  : num  0.4062 0.0469 0.8125 0.4375 0.625 ...\n    $ fiber   : num  0.7143 0.1429 0.6429 1 0.0714 ...\n    $ carbo   : num  0 0.167 0.111 0.167 0.5 ...\n    $ sugars  : num  0.4 0.533 0.333 0 0.533 ...\n    $ potass  : num  0.841 0.381 0.968 1 0.122 ...\n    $ vitamins: num  0.25 0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ...\n    $ shelf   : num  1 1 1 1 1 0 0.5 1 0 1 ...\n    $ weight  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.83 0.5 0.5 ...\n    $ cups    : num  0.064 0.6 0.064 0.2 0.4 0.4 0.6 0.4 0.336 0.336 ...\nggplot(data = cereal) +\n    geom_histogram(aes(x = sodium), color = \"blue\", fill = \"lightblue\") +\n    theme_minimal() + ggtitle(\"Before min-max normalization\")\n\nggplot(data = cereal_mm) +\n    geom_histogram(aes(x = sodium), color = \"blue\", fill = \"lightblue\") + \n    theme_minimal() + ggtitle(\"After min-max normalization\")"},{"path":"chapter-cluster.html","id":"applying-k-means-clustering","chapter":"13 Clustering","heading":"13.3.3 Applying K-means Clustering","text":"","code":""},{"path":"chapter-cluster.html","id":"choosing-the-optimal-number-of-clusters","chapter":"13 Clustering","heading":"Choosing the Optimal Number of Clusters","text":"clustering, need determine optimal number clusters. use elbow method, plots within-cluster sum squares (WCSS) different values \\(k\\). elbow point—improvement WCSS slows—suggests ideal \\(k\\):plot, observe \\(k = 4\\) clusters reasonable choice, adding clusters beyond point yields diminishing improvements WCSS.","code":"\nlibrary(factoextra)\n\nfviz_nbclust(cereal_mm, kmeans, method = \"wss\", k.max = 15) + \n  geom_vline(xintercept = 4, linetype = 2, color = \"gray\")"},{"path":"chapter-cluster.html","id":"performing-k-means-clustering","chapter":"13 Clustering","heading":"Performing K-means Clustering","text":"now apply K-means algorithm \\(k = 4\\) clusters:check cluster sizes:","code":"\nset.seed(3)  # Ensure reproducibility\ncereal_kmeans <- kmeans(cereal_mm, centers = 4)cereal_kmeans$size\n   [1] 36 10 13 18"},{"path":"chapter-cluster.html","id":"visualizing-the-clusters","chapter":"13 Clustering","heading":"Visualizing the Clusters","text":"better understand clustering results, visualize clusters using fviz_cluster() function factoextra package:scatter plot displays four clusters, point representing cereal brand. Different colors indicate distinct clusters, ellipses represent spread cluster based standard deviation.","code":"\nfviz_cluster(cereal_kmeans, cereal_mm, geom = \"point\", ellipse.type = \"norm\", palette = \"custom_palette\")"},{"path":"chapter-cluster.html","id":"interpreting-the-results","chapter":"13 Clustering","heading":"Interpreting the Results","text":"clusters reveal natural groupings among cereals based nutritional content. example:\n- clusters may contain low-sugar, high-fiber cereals, appealing health-conscious consumers.\n- Others may group high-calorie, high-sugar cereals, often marketed children.\n- Another group may include balanced cereals, offering mix moderate calories nutrients.examine cereals belong specific cluster (e.g., Cluster 1), can use:command lists names cereals assigned Cluster 1, helping us interpret characteristics group.case study demonstrated K-means clustering can segment cereals meaningful groups based nutritional content. data preprocessing, feature scaling, cluster visualization, successfully grouped cereals similar characteristics. clustering techniques widely applicable marketing, consumer analytics, product positioning, providing actionable insights businesses researchers alike.chapter, explored fundamentals clustering, mechanics K-means algorithm, methods choosing optimal number clusters. applied concepts real-world dataset, demonstrating K-means can extract meaningful insights. Clustering remains powerful tool across various domains, marketing bioinformatics, making essential technique modern data science toolkit.","code":"\ncereal$name[cereal_kmeans$cluster == 1]"},{"path":"chapter-cluster.html","id":"exercises-10","chapter":"13 Clustering","heading":"13.4 Exercises","text":"…","code":""}]
