<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 11 Decision Trees and Random Forests | Uncovering Data Science with R</title>
<meta name="author" content="Reza Mohammadi">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Chapter 11 Decision Trees and Random Forests | Uncovering Data Science with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://uncovering-data-science.netlify.app/chapter-tree.html">
<meta property="og:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 11 Decision Trees and Random Forests | Uncovering Data Science with R">
<meta name="twitter:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="As part of the Data Science workflow, we have already explored several powerful algorithms for classification and regression. These include k-Nearest Neighbors and Naive Bayes for classification,...">
<meta property="og:description" content="As part of the Data Science workflow, we have already explored several powerful algorithms for classification and regression. These include k-Nearest Neighbors and Naive Bayes for classification,...">
<meta name="twitter:description" content="As part of the Data Science workflow, we have already explored several powerful algorithms for classification and regression. These include k-Nearest Neighbors and Naive Bayes for classification,...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Uncovering Data Science with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="chapter-into-R.html"><span class="header-section-number">1</span> The Basics for R</a></li>
<li><a class="" href="chapter-intro-DS.html"><span class="header-section-number">2</span> Introduction to Data Science</a></li>
<li><a class="" href="chapter-data-prep.html"><span class="header-section-number">3</span> Data Preparation</a></li>
<li><a class="" href="chapter-EDA.html"><span class="header-section-number">4</span> Exploratory Data Analysis</a></li>
<li><a class="" href="chapter-statistics.html"><span class="header-section-number">5</span> Statistical Inference and Hypothesis Testing</a></li>
<li><a class="" href="chapter-modeling.html"><span class="header-section-number">6</span> Preparing Data for Modeling</a></li>
<li><a class="" href="chapter-knn.html"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></li>
<li><a class="" href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></li>
<li><a class="" href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></li>
<li><a class="" href="chapter-regression.html"><span class="header-section-number">10</span> Regression Modeling: From Basics to Advanced Techniques</a></li>
<li><a class="active" href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></li>
<li><a class="" href="chapter-nn.html"><span class="header-section-number">12</span> Neural Networks: The Building Blocks of Artificial Intelligence</a></li>
<li><a class="" href="chapter-cluster.html"><span class="header-section-number">13</span> Clustering</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/RezaMoammadi/Book-Data-Science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-tree" class="section level1" number="11">
<h1>
<span class="header-section-number">11</span> Decision Trees and Random Forests<a class="anchor" aria-label="anchor" href="#chapter-tree"><i class="fas fa-link"></i></a>
</h1>
<p>As part of the Data Science workflow, we have already explored several powerful algorithms for classification and regression. These include <strong>k-Nearest Neighbors</strong> and <strong>Naive Bayes</strong> for classification, as well as <strong>linear regression models</strong> for both continuous outcomes and classification tasks. In this chapter, we introduce two additional and widely-used techniques: <strong>Decision Trees</strong> and <strong>Random Forests</strong>. These algorithms are highly versatile, capable of tackling both classification and regression problems, and they are built around the idea of making a series of hierarchical decisions to classify data or predict outcomes.</p>
<p>At their core, <strong>Decision Trees</strong> employ simple decision-making rules to split the dataset into smaller, more homogeneous subsets, resulting in a tree-like structure of decisions. This step-by-step partitioning allows trees to capture complex relationships in the data while remaining interpretable and intuitive. <strong>Random Forests</strong>, an extension of Decision Trees, enhance their predictive power by combining the outputs of multiple trees. This ensemble learning approach improves accuracy, reduces overfitting, and provides a more robust solution compared to a single tree.</p>
<p>To illustrate the concept, consider the simple <strong>Decision Tree</strong> shown in Figure <a href="chapter-tree.html#fig:tree-0">11.1</a>, which predicts whether a customer’s credit risk is classified as “good” or “bad” based on features such as <code>age</code> and <code>income</code>. This example uses the <code>risk</code> dataset introduced in Chapter <a href="chapter-bayes.html#chapter-bayes">9</a>. Each node in the tree represents a question, such as “Is yearly income is lower than 36K? (<code>income &lt; 36e+3</code>)” or “Is age &gt;= 29?”, while the terminal nodes, also known as <strong>leaves</strong>, represent the final predictions (e.g., “Good risk” or “Bad risk”). Along with the predictions, the tree also provides uncertainty values. The process begins at the <strong>root node</strong>, and the data is iteratively split into branches based on feature values until it reaches the terminal nodes.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-0"></span>
<img src="tree_files/figure-html/tree-0-1.png" alt="Decision tree for predicting credit risk based on age and income." width="65%"><p class="caption">
Figure 11.1: Decision tree for predicting credit risk based on age and income.
</p>
</div>
<p>As shown in Figure <a href="chapter-tree.html#fig:tree-0">11.1</a>, decision trees provide a visually intuitive and interpretable structure that does not require advanced statistical knowledge. This makes them especially valuable in business contexts, where simplicity, interpretability, and actionable insights are critical. Whether it’s customer segmentation, risk assessment, or process optimization, decision trees serve as a user-friendly tool for deriving insights and informing decisions.</p>
<p>By the end of this chapter, you will gain:</p>
<ul>
<li>A deep understanding of the mechanics behind <strong>Decision Trees</strong> and <strong>Random Forests</strong>,<br>
</li>
<li>Practical knowledge of building, evaluating, and tuning Decision Trees using algorithms like <strong>CART</strong> and <strong>C5.0</strong>, and<br>
</li>
<li>The ability to leverage <strong>Random Forests</strong> for solving real-world problems, such as identifying risky loans, detecting fraudulent transactions, or classifying images.</li>
</ul>
<p>We will start by exploring how Decision Trees are constructed, learning about their key concepts and algorithms. Then, we will delve into <strong>Random Forests</strong>, a powerful ensemble learning method that builds upon the strengths of Decision Trees to deliver state-of-the-art performance across a wide range of applications.</p>
<div id="how-decision-trees-work" class="section level2" number="11.1">
<h2>
<span class="header-section-number">11.1</span> How Decision Trees Work<a class="anchor" aria-label="anchor" href="#how-decision-trees-work"><i class="fas fa-link"></i></a>
</h2>
<p>Decision Trees classify data or predict outcomes by recursively dividing a dataset into smaller subsets based on feature values. This <strong>divide and conquer</strong> approach aims to maximize the homogeneity of the subsets at each step, creating groups that are as similar as possible. At each split, the algorithm identifies the feature and threshold that best separate the data, using criteria such as the <strong>Gini Index</strong>, <strong>Entropy</strong>, or <strong>Variance Reduction</strong>, depending on whether the task is classification or regression. The process continues until one of the following conditions is met:</p>
<ol style="list-style-type: decimal">
<li>The tree reaches a predefined maximum depth,<br>
</li>
<li>All observations in a subset belong to the same class (for classification) or share the same value (for regression), or<br>
</li>
<li>Further splits fail to improve the model’s performance.</li>
</ol>
<p>This iterative process creates a <strong>binary tree</strong>, where each <strong>node</strong> represents a decision test (e.g., “Is <span class="math inline">\(x_1 &lt; 10\)</span>?”), and each <strong>leaf</strong> represents the final prediction (e.g., “Class A” or “Class B”). Decision Trees are highly interpretable, as their structure visually represents the decision-making process, making them particularly useful for understanding how predictions are made.</p>
<p>To better understand how Decision Trees work, consider a toy dataset with two features (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>) and two classes (Class A and Class B), as shown in Figure <a href="chapter-tree.html#fig:tree-1">11.2</a>. The dataset contains 50 data points, and the goal is to classify these points into their respective classes using a Decision Tree.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-1"></span>
<img src="images/ex_tree_1.png" alt="A two-dimensional toy dataset (50 observations) with two classes (Class A and Class B), used to illustrate how to build Decision Trees." width="65%"><p class="caption">
Figure 11.2: A two-dimensional toy dataset (50 observations) with two classes (Class A and Class B), used to illustrate how to build Decision Trees.
</p>
</div>
<p>The process starts by identifying the feature and threshold that best separate the two classes. The algorithm evaluates all possible thresholds for each feature and selects the split that maximizes homogeneity in the resulting subsets. For this dataset, the best split is based on the feature <span class="math inline">\(x_1\)</span>, with a decision boundary at <span class="math inline">\(x_1 = 10\)</span>.</p>
<p>As shown in Figure <a href="chapter-tree.html#fig:tree-2">11.3</a>, this split divides the dataset into two regions:</p>
<ul>
<li>
<strong>Left region</strong>: Data points where <span class="math inline">\(x_1 &lt; 10\)</span>, which are 80% Class A and 20% Class B.<br>
</li>
<li>
<strong>Right region</strong>: Data points where <span class="math inline">\(x_1 \geq 10\)</span>, which are 28% Class A and 72% Class B.</li>
</ul>
<p>The test <span class="math inline">\(x_1 &lt; 10\)</span> forms the <strong>root node</strong> of the tree, and the two regions correspond to the left and right branches of the tree.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-2"></span>
<img src="images/ex_tree_2.png" alt="Left: Decision boundary for a tree with depth 1. Right: The corresponding Decision Tree." width="90%"><p class="caption">
Figure 11.3: Left: Decision boundary for a tree with depth 1. Right: The corresponding Decision Tree.
</p>
</div>
<p>Although the first split significantly separates the two classes, both regions still contain points from both classes. To further improve classification accuracy, the algorithm recursively evaluates additional splits within each region.</p>
<p>In Figure <a href="chapter-tree.html#fig:tree-3">11.4</a>, the second split is made using the feature <span class="math inline">\(x_2\)</span>. For the left region (<span class="math inline">\(x_1 &lt; 10\)</span>), the optimal threshold is <span class="math inline">\(x_2 = 6\)</span>, while for the right region (<span class="math inline">\(x_1 \geq 10\)</span>), the threshold is <span class="math inline">\(x_2 = 8\)</span>. These additional splits refine the boundaries of the feature space, creating smaller and more homogeneous regions.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-3"></span>
<img src="images/ex_tree_3.png" alt="Left: Decision boundary for a tree with depth 2. Right: The corresponding Decision Tree." width="90%"><p class="caption">
Figure 11.4: Left: Decision boundary for a tree with depth 2. Right: The corresponding Decision Tree.
</p>
</div>
<p>The splitting process continues until the tree meets a stopping condition, such as reaching a maximum depth or producing pure leaf nodes (regions containing data points from only one class). Figure <a href="chapter-tree.html#fig:tree-4">11.5</a> shows the final tree, grown to a depth of 5. The decision boundaries on the left illustrate how the feature space is partitioned into regions, each associated with a specific class.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-4"></span>
<img src="images/ex_tree_4.png" alt="Left: Decision boundary for a tree with depth 5. Right: The corresponding Decision Tree." width="90%"><p class="caption">
Figure 11.5: Left: Decision boundary for a tree with depth 5. Right: The corresponding Decision Tree.
</p>
</div>
<p>At this depth, the tree has created highly specific decision boundaries that closely match the training data. However, this specificity often leads to <strong>overfitting</strong>, where the model captures noise or outliers in the data instead of general patterns. Overfitted trees may perform poorly on unseen data.</p>
<p>To make predictions with a Decision Tree, the algorithm evaluates the test conditions at each node and follows the corresponding branch until it reaches a leaf. The prediction depends on the type of task:</p>
<ul>
<li>
<strong>For classification</strong>, the predicted class is the majority class of the points in the leaf.</li>
<li>
<strong>For regression</strong>, the predicted value is the mean of the target variable for all points in the leaf.</li>
</ul>
<p>For example, in Figure <a href="chapter-tree.html#fig:tree-3">11.4</a>, a new data point with <span class="math inline">\(x_1 = 8\)</span> and <span class="math inline">\(x_2 = 4\)</span> would traverse to the left region (<span class="math inline">\(x_1 &lt; 10\)</span>), then to the bottom-left region (<span class="math inline">\(x_2 &lt; 6\)</span>), ultimately landing in a leaf labeled as Class A with 80% confidence and a 20% error rate.</p>
<p>Decision Trees can also handle regression tasks by following the same splitting process but minimizing the <strong>variance</strong> of the target variable instead of maximizing class homogeneity. For regression, the prediction for a new data point is the average target value of all training points in the corresponding leaf. This allows Decision Trees to model non-linear relationships effectively.</p>
<p>Controlling the complexity of a Decision Tree is crucial to prevent overfitting. Fully growing a tree until all leaves are pure often results in a highly complex model that perfectly fits the training data but performs poorly on unseen data. This can be observed in Figure <a href="chapter-tree.html#fig:tree-4">11.5</a>, where the decision boundaries overfit the training set, capturing outliers and noise.</p>
<p>To address this, two strategies are commonly used:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Pre-pruning</strong>: Stop the tree-building process early based on criteria such as limiting the maximum depth, the number of leaf nodes, or the minimum number of points required to split a node.<br>
</li>
<li>
<strong>Post-pruning</strong>: Build the full tree and then simplify it by removing or collapsing nodes that provide little additional value.</li>
</ol>
<p>The effectiveness of these strategies depends on the dataset and the application. The choice of the split criterion—such as the <strong>Gini Index</strong>, <strong>Entropy</strong>, or <strong>Variance Reduction</strong>—also plays a crucial role in determining the tree’s performance. These criteria are foundational to the two most widely used Decision Tree algorithms, <strong>CART</strong> and <strong>C5.0</strong>, which will be explored in the following sections.</p>
</div>
<div id="classification-and-regression-trees-cart" class="section level2" number="11.2">
<h2>
<span class="header-section-number">11.2</span> Classification and Regression Trees (CART)<a class="anchor" aria-label="anchor" href="#classification-and-regression-trees-cart"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>Classification and Regression Trees (CART)</strong> algorithm, introduced by Breiman et al. in 1984,<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;L Breiman et al., &lt;span&gt;“Classification and Regression Trees,”&lt;/span&gt; 1984.&lt;/p&gt;"><sup>6</sup></a></span> is one of the most widely used methods for constructing decision trees. CART generates <strong>binary trees</strong>, meaning that each decision node splits the data into exactly two branches. It recursively partitions the training dataset into subsets of records that share similar values for the target variable. This partitioning is guided by a splitting criterion designed to minimize impurity in the resulting subsets. For classification tasks, CART employs measures such as the <strong>Gini Index</strong> or <strong>Entropy</strong> to evaluate splits, while for regression tasks, it minimizes the <strong>Variance</strong> of the target variable.</p>
<p>As an example, the <strong>Gini Index</strong> is commonly used to measure impurity in classification tasks. The Gini Index for a node is calculated as:</p>
<p><span class="math display">\[
Gini = 1 - \sum_{i=1}^k p_i^2
\]</span></p>
<p>where <span class="math inline">\(p_i\)</span> represents the proportion of samples in the node that belong to class <span class="math inline">\(i\)</span>, and <span class="math inline">\(k\)</span> is the total number of classes. A node is considered “pure” when all the data points in it belong to a single class, resulting in a Gini Index of 0. During tree construction, CART selects the feature and threshold that result in the largest reduction in impurity, splitting the data to create two more homogeneous child nodes.</p>
<p>The recursive nature of CART can result in highly detailed trees that perfectly fit the training data. While this ensures the lowest possible error rate on the training set, it can lead to <strong>overfitting</strong>, where the tree becomes overly complex and fails to generalize to unseen data. To mitigate this, CART employs a technique called <strong>pruning</strong> to simplify the tree.</p>
<p>Pruning involves cutting back branches of the tree that do not contribute meaningfully to its predictive accuracy on a validation set. This is achieved by finding an adjusted error rate that penalizes overly complex trees with too many leaf nodes. The goal of pruning is to strike a balance between accuracy and simplicity, enhancing the tree’s ability to generalize to new data. The pruning process is described in detail by Breiman et al..<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Breiman et al.&lt;/p&gt;"><sup>7</sup></a></span></p>
<p>Despite its simplicity, CART is a powerful algorithm that has been widely adopted in practice. Its key strengths include:</p>
<ul>
<li>
<strong>Interpretability:</strong> The tree structure is intuitive and easy to visualize, making CART models highly explainable.<br>
</li>
<li>
<strong>Versatility:</strong> CART can handle both classification and regression tasks effectively.<br>
</li>
<li>
<strong>Ability to handle mixed data types:</strong> CART works seamlessly with datasets containing both numerical and categorical variables.</li>
</ul>
<p>However, CART also has limitations. The algorithm tends to produce deep trees that may overfit the training data, especially when the dataset is small or noisy. Additionally, CART’s reliance on greedy splitting can result in suboptimal splits, as it evaluates one split at a time rather than considering all possible combinations.</p>
<p>To address these shortcomings, more advanced algorithms have been developed, such as <strong>C5.0</strong>, which incorporates improvements in splitting and pruning techniques, and <strong>Random Forests</strong>, which combine multiple decision trees to create more robust models. These approaches build on the foundations of CART, improving its performance and reducing its susceptibility to overfitting. We will explore these methods in subsequent sections.</p>
</div>
<div id="the-c5.0-algorithm-for-building-decision-trees" class="section level2" number="11.3">
<h2>
<span class="header-section-number">11.3</span> The C5.0 Algorithm for Building Decision Trees<a class="anchor" aria-label="anchor" href="#the-c5.0-algorithm-for-building-decision-trees"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>C5.0 algorithm</strong> is one of the most well-known and widely used decision tree implementations. Developed by J. Ross Quinlan, C5.0 is an advanced iteration of his earlier algorithms, <strong>C4.5</strong> and <strong>ID3</strong> (Iterative Dichotomiser 3). Building upon the strengths of its predecessors, C5.0 introduces several improvements in efficiency, flexibility, and accuracy, making it a popular choice for both academic and commercial applications. While Quinlan offers a commercial version of C5.0 (available at <a href="http://www.rulequest.com/">RuleQuest</a>), a single-threaded implementation has been made publicly available and has been incorporated into open-source tools such as R.</p>
<p>C5.0 differs from other decision tree algorithms, such as CART, in several key ways. Unlike CART, which only produces binary trees, C5.0 allows for more flexible tree structures with non-binary splits. For categorical attributes, C5.0 can create separate branches for each unique value of the attribute, which can lead to highly “bushy” trees if the attribute has many categories. Another major distinction lies in the way node homogeneity is measured. While CART uses metrics like the Gini Index or Variance Reduction, C5.0 employs <strong>Entropy</strong> and <strong>Information Gain</strong>, concepts rooted in information theory, to evaluate the optimal splits.</p>
<p>Entropy measures the level of disorder or randomness in a dataset. High entropy indicates a dataset with high diversity (e.g., a mix of classes), whereas low entropy signifies greater homogeneity (e.g., all samples belong to the same class). The goal of the C5.0 algorithm is to identify splits that reduce entropy, creating purer subsets of data at each step of the tree-building process. Formally, entropy for a variable <span class="math inline">\(x\)</span> with <span class="math inline">\(k\)</span> classes is defined as:</p>
<p><span class="math display">\[
Entropy(x) = - \sum_{i=1}^k p_i \log_2(p_i)
\]</span></p>
<p>Here, <span class="math inline">\(p_i\)</span> is the proportion of samples belonging to class <span class="math inline">\(i\)</span>. For example, if a dataset contains an even split between two classes, the entropy is at its maximum. Conversely, if all samples belong to a single class, entropy is zero. This concept extends naturally to the calculation of <strong>Information Gain</strong>, which quantifies the reduction in entropy achieved by splitting the data on a particular feature. Given a candidate split <span class="math inline">\(S\)</span> that divides a dataset <span class="math inline">\(T\)</span> into subsets <span class="math inline">\(T_1, T_2, \dots, T_c\)</span>, the entropy after the split is calculated as a weighted sum of the entropies of the subsets:</p>
<p><span class="math display">\[
H_S(T) = \sum_{i=1}^c \frac{|T_i|}{|T|} \cdot Entropy(T_i)
\]</span></p>
<p>The <strong>Information Gain</strong> for the split <span class="math inline">\(S\)</span> is then:</p>
<p><span class="math display">\[
gain(S) = H(T) - H_S(T)
\]</span></p>
<p>where <span class="math inline">\(H(T)\)</span> represents the entropy of the dataset before the split. At each decision node, the C5.0 algorithm evaluates all possible splits and selects the one that maximizes information gain. This process ensures that the splits lead to progressively purer subsets, improving the accuracy of the model.</p>
<p>To illustrate the C5.0 algorithm, consider its application to the <code>risk</code> dataset, which predicts credit risk (“good” or “bad”) based on features like <code>age</code> and <code>income</code>. Figure <a href="chapter-tree.html#fig:tree-C50">11.6</a> shows the resulting decision tree, created using the <code>C5.0</code> function from the <code>C50</code> package in R. Each node in the tree represents a decision based on a feature value, and the branches lead to subsets of data that become progressively more homogeneous.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-C50"></span>
<img src="tree_files/figure-html/tree-C50-1.png" alt="C5.0 Decision Tree for predicting credit risk based on age and income." width="65%"><p class="caption">
Figure 11.6: C5.0 Decision Tree for predicting credit risk based on age and income.
</p>
</div>
<p>The tree in Figure <a href="chapter-tree.html#fig:tree-C50">11.6</a> demonstrates how the algorithm uses entropy and information gain to construct splits that best separate the classes. Unlike the strictly binary splits produced by CART, C5.0 allows for multi-way splits when working with categorical attributes, which can create trees with variable shapes. This flexibility often leads to more compact trees that are easier to interpret, especially for datasets with categorical variables.</p>
<p>C5.0 offers several advantages over other decision tree algorithms. It is computationally efficient, making it suitable for large datasets, and its flexibility in handling non-binary splits allows for more nuanced tree structures. Additionally, C5.0 incorporates advanced features such as <strong>feature weighting</strong>, which allows the algorithm to prioritize more relevant features during the tree-building process. This can improve model performance by focusing on the most important predictors.</p>
<p>Despite its strengths, the C5.0 algorithm is not without limitations. Trees generated by C5.0 can become overly complex or “bushy,” particularly when working with categorical attributes that have many unique values. This complexity can make the trees harder to interpret and may lead to overfitting. To address these issues, pruning techniques can be applied to simplify the tree and improve its generalizability. Additionally, the computational cost of evaluating multiple splits for categorical features may increase for large datasets with high cardinality, although this is mitigated by C5.0’s overall efficiency.</p>
<p>In summary, the C5.0 algorithm is a powerful and versatile tool for building decision trees. By leveraging concepts like entropy and information gain, it constructs models that are both accurate and interpretable. While it shares many similarities with CART, its ability to handle multi-way splits and its use of information theory make it a distinct and valuable alternative. The C5.0 algorithm is widely used in fields such as finance, healthcare, and marketing, where decision tree models provide actionable insights and transparent decision-making processes. In the next section, we will explore <strong>Random Forests</strong>, an ensemble learning technique that builds upon decision trees to further enhance accuracy and robustness.</p>
</div>
<div id="random-forests-an-ensemble-approach" class="section level2" number="11.4">
<h2>
<span class="header-section-number">11.4</span> Random Forests: An Ensemble Approach<a class="anchor" aria-label="anchor" href="#random-forests-an-ensemble-approach"><i class="fas fa-link"></i></a>
</h2>
<p>While Decision Trees are powerful and intuitive, they are prone to overfitting, particularly when grown to their full depth. <strong>Random Forests</strong><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Leo Breiman, &lt;span&gt;“Random Forests,”&lt;/span&gt; &lt;em&gt;Machine Learning&lt;/em&gt; 45 (2001): 5–32.&lt;/p&gt;"><sup>8</sup></a></span> address this limitation by adopting an ensemble approach that combines the predictions of multiple Decision Trees to produce a more robust and accurate model. Instead of relying on a single tree, Random Forests aggregate the predictions of many trees, reducing overfitting and enhancing performance on complex datasets.</p>
<p>The Random Forest algorithm introduces two key elements of randomness to improve model diversity:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Bootstrap Aggregation (Bagging):</strong> Each tree is trained on a random subset of the training data, created by sampling with replacement. This means that some observations appear multiple times in a tree’s training data, while others may be excluded. This diversity ensures that each tree learns slightly different patterns.<br>
</li>
<li>
<strong>Random Feature Selection:</strong> At each split, the algorithm considers a random subset of features instead of evaluating all features. This further decorrelates the trees, as each tree is forced to rely on different combinations of features to make decisions.</li>
</ol>
<p>Once the forest is built, the predictions from all trees are aggregated to produce the final output:</p>
<ul>
<li>For <strong>classification</strong>, the final prediction is determined by <strong>majority voting</strong>, where each tree votes for a class, and the most common class is selected.<br>
</li>
<li>For <strong>regression</strong>, the final output is the average of the predictions from all trees.</li>
</ul>
<p>The strength of Random Forests lies in the principle of the <strong>“wisdom of the crowd.”</strong> Individually, each tree is a weak learner, as it is trained on a limited subset of data and features. However, when combined, their collective predictions form a strong learner. By leveraging the diversity of individual trees, Random Forests reduce the likelihood that errors made by any single tree will dominate the overall model.</p>
<p>Additionally, the randomness introduced through feature selection ensures that no single feature dominates the model, making Random Forests particularly effective for datasets with <strong>correlated</strong> or <strong>redundant features</strong>. This feature-level decorrelation enhances the ensemble’s ability to generalize to unseen data.</p>
<p>Random Forests have several notable advantages:</p>
<ul>
<li>
<strong>Reduced Overfitting:</strong> By averaging the predictions of multiple trees, Random Forests smooth out the noise and variance present in individual trees, leading to better generalization.<br>
</li>
<li>
<strong>High Accuracy:</strong> Random Forests perform well on both classification and regression tasks, particularly for datasets with non-linear relationships or high-dimensional feature spaces.<br>
</li>
<li>
<strong>Feature Importance:</strong> The algorithm provides <strong>feature importance scores</strong>, enabling us to identify the most influential predictors. This is especially useful for feature selection and gaining insights into the underlying data.<br>
</li>
<li>
<strong>Robustness:</strong> Random Forests are resilient to noise and outliers, as the ensemble effect reduces the impact of anomalies on the final prediction.<br>
</li>
<li>
<strong>Flexibility:</strong> Random Forests can handle both numerical and categorical data and adapt well to diverse types of problems.</li>
</ul>
<p>Despite their strengths, Random Forests have some limitations:</p>
<ul>
<li>
<strong>Computational Complexity:</strong> Training hundreds or thousands of trees can be computationally intensive, especially on large datasets. However, this can be mitigated through parallel processing, as each tree is built independently.<br>
</li>
<li>
<strong>Reduced Interpretability:</strong> While individual Decision Trees are highly interpretable, the ensemble nature of Random Forests makes it difficult to understand the collective decision-making process of the model.<br>
</li>
<li>
<strong>Bias-Variance Tradeoff:</strong> Although Random Forests reduce variance through bagging, they may sometimes smooth over complex relationships in the data that a single, well-tuned Decision Tree could capture.</li>
</ul>
<p>Random Forests strike a balance between accuracy and robustness, addressing many of the weaknesses of individual Decision Trees while retaining their strengths. They are well-suited for both classification and regression tasks and are particularly effective in scenarios with noisy or high-dimensional data. Moreover, their ability to compute feature importance scores provides valuable insights into the drivers of the model’s predictions, making them not only a predictive tool but also an exploratory one.</p>
<p>Random Forests have become one of the most widely used machine learning algorithms due to their versatility, reliability, and strong performance across a variety of applications. In the next section, we will apply Random Forests, along with Decision Trees, to the <strong>adult dataset</strong> to explore the question: <em>Who can earn more than $50K per year?</em> This case study will provide a practical demonstration of how these models work and how they can be evaluated and compared in a real-world scenario.</p>
</div>
<div id="tree-case-study" class="section level2" number="11.5">
<h2>
<span class="header-section-number">11.5</span> Case Study: Who Can Earn More Than $50K Per Year?<a class="anchor" aria-label="anchor" href="#tree-case-study"><i class="fas fa-link"></i></a>
</h2>
<p>To demonstrate the practical application of Decision Trees and Random Forests, we use the <em>adult</em> dataset, which provides demographic and income information about individuals. This dataset, sourced from the <a href="https://www.census.gov">US Census Bureau</a>, is widely used to predict whether an individual earns more than $50,000 per year based on features such as education, hours worked per week, marital status, and more. The objective of this binary classification problem is to categorize individuals into one of two income groups: <code>&lt;=50K</code> or <code>&gt;50K</code>, with the features serving as predictors and the target variable being <code>income</code>.</p>
<div id="overview-of-the-dataset-2" class="section level3 unnumbered">
<h3>Overview of the Dataset<a class="anchor" aria-label="anchor" href="#overview-of-the-dataset-2"><i class="fas fa-link"></i></a>
</h3>
<p>We begin by loading the dataset and examining its structure:</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="chapter-tree.html#cb173-1" tabindex="-1"></a><span class="fu">data</span>(adult)</span>
<span id="cb173-2"><a href="chapter-tree.html#cb173-2" tabindex="-1"></a></span>
<span id="cb173-3"><a href="chapter-tree.html#cb173-3" tabindex="-1"></a><span class="fu">str</span>(adult)</span>
<span id="cb173-4"><a href="chapter-tree.html#cb173-4" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">48598</span> obs. of  <span class="dv">15</span> variables<span class="sc">:</span></span>
<span id="cb173-5"><a href="chapter-tree.html#cb173-5" tabindex="-1"></a>    <span class="er">$</span> age           <span class="sc">:</span> int  <span class="dv">25</span> <span class="dv">38</span> <span class="dv">28</span> <span class="dv">44</span> <span class="dv">18</span> <span class="dv">34</span> <span class="dv">29</span> <span class="dv">63</span> <span class="dv">24</span> <span class="dv">55</span> ...</span>
<span id="cb173-6"><a href="chapter-tree.html#cb173-6" tabindex="-1"></a>    <span class="sc">$</span> workclass     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">6</span> levels <span class="st">"?"</span>,<span class="st">"Gov"</span>,<span class="st">"Never-worked"</span>,..<span class="sc">:</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">2</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">5</span> <span class="dv">4</span> <span class="dv">4</span> ...</span>
<span id="cb173-7"><a href="chapter-tree.html#cb173-7" tabindex="-1"></a>    <span class="sc">$</span> demogweight   <span class="sc">:</span> int  <span class="dv">226802</span> <span class="dv">89814</span> <span class="dv">336951</span> <span class="dv">160323</span> <span class="dv">103497</span> <span class="dv">198693</span> <span class="dv">227026</span> <span class="dv">104626</span> <span class="dv">369667</span> <span class="dv">104996</span> ...</span>
<span id="cb173-8"><a href="chapter-tree.html#cb173-8" tabindex="-1"></a>    <span class="sc">$</span> education     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">16</span> levels <span class="st">"10th"</span>,<span class="st">"11th"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">12</span> <span class="dv">8</span> <span class="dv">16</span> <span class="dv">16</span> <span class="dv">1</span> <span class="dv">12</span> <span class="dv">15</span> <span class="dv">16</span> <span class="dv">6</span> ...</span>
<span id="cb173-9"><a href="chapter-tree.html#cb173-9" tabindex="-1"></a>    <span class="sc">$</span> education.num <span class="sc">:</span> int  <span class="dv">7</span> <span class="dv">9</span> <span class="dv">12</span> <span class="dv">10</span> <span class="dv">10</span> <span class="dv">6</span> <span class="dv">9</span> <span class="dv">15</span> <span class="dv">10</span> <span class="dv">4</span> ...</span>
<span id="cb173-10"><a href="chapter-tree.html#cb173-10" tabindex="-1"></a>    <span class="sc">$</span> marital.status<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">5</span> levels <span class="st">"Divorced"</span>,<span class="st">"Married"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> ...</span>
<span id="cb173-11"><a href="chapter-tree.html#cb173-11" tabindex="-1"></a>    <span class="sc">$</span> occupation    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">15</span> levels <span class="st">"?"</span>,<span class="st">"Adm-clerical"</span>,..<span class="sc">:</span> <span class="dv">8</span> <span class="dv">6</span> <span class="dv">12</span> <span class="dv">8</span> <span class="dv">1</span> <span class="dv">9</span> <span class="dv">1</span> <span class="dv">11</span> <span class="dv">9</span> <span class="dv">4</span> ...</span>
<span id="cb173-12"><a href="chapter-tree.html#cb173-12" tabindex="-1"></a>    <span class="sc">$</span> relationship  <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">6</span> levels <span class="st">"Husband"</span>,<span class="st">"Not-in-family"</span>,..<span class="sc">:</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">2</span> <span class="dv">5</span> <span class="dv">1</span> <span class="dv">5</span> <span class="dv">1</span> ...</span>
<span id="cb173-13"><a href="chapter-tree.html#cb173-13" tabindex="-1"></a>    <span class="sc">$</span> race          <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">5</span> levels <span class="st">"Amer-Indian-Eskimo"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">5</span> ...</span>
<span id="cb173-14"><a href="chapter-tree.html#cb173-14" tabindex="-1"></a>    <span class="sc">$</span> gender        <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"Female"</span>,<span class="st">"Male"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> ...</span>
<span id="cb173-15"><a href="chapter-tree.html#cb173-15" tabindex="-1"></a>    <span class="sc">$</span> capital.gain  <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">7688</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">3103</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb173-16"><a href="chapter-tree.html#cb173-16" tabindex="-1"></a>    <span class="sc">$</span> capital.loss  <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb173-17"><a href="chapter-tree.html#cb173-17" tabindex="-1"></a>    <span class="sc">$</span> hours.per.week<span class="sc">:</span> int  <span class="dv">40</span> <span class="dv">50</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">30</span> <span class="dv">30</span> <span class="dv">40</span> <span class="dv">32</span> <span class="dv">40</span> <span class="dv">10</span> ...</span>
<span id="cb173-18"><a href="chapter-tree.html#cb173-18" tabindex="-1"></a>    <span class="sc">$</span> native.country<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">42</span> levels <span class="st">"?"</span>,<span class="st">"Cambodia"</span>,..<span class="sc">:</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> <span class="dv">40</span> ...</span>
<span id="cb173-19"><a href="chapter-tree.html#cb173-19" tabindex="-1"></a>    <span class="sc">$</span> income        <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"&lt;=50K"</span>,<span class="st">"&gt;50K"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> ...</span></code></pre></div>
<p>The dataset contains 48598 records and 15 variables. Of these, 14 are predictors, while the target variable, <code>income</code>, is a binary categorical variable with two levels: <code>&lt;=50K</code> and <code>&gt;50K</code>. The features include both numerical and categorical variables:</p>
<ul>
<li>
<code>age</code>: Age in years (numerical).<br>
</li>
<li>
<code>workclass</code>: Type of employment (categorical, 6 levels).<br>
</li>
<li>
<code>demogweight</code>: Demographic weight (categorical).<br>
</li>
<li>
<code>education</code>: Highest education level (categorical, 16 levels).<br>
</li>
<li>
<code>education.num</code>: Years of education (numerical).<br>
</li>
<li>
<code>marital.status</code>: Marital status (categorical, 5 levels).<br>
</li>
<li>
<code>occupation</code>: Type of occupation (categorical, 15 levels).<br>
</li>
<li>
<code>relationship</code>: Type of relationship (categorical, 6 levels).<br>
</li>
<li>
<code>race</code>: Race (categorical, 5 levels).<br>
</li>
<li>
<code>gender</code>: Gender (categorical, Male/Female).<br>
</li>
<li>
<code>capital.gain</code>: Capital gains (numerical).<br>
</li>
<li>
<code>capital.loss</code>: Capital losses (numerical).<br>
</li>
<li>
<code>hours.per.week</code>: Weekly hours worked (numerical).<br>
</li>
<li>
<code>native.country</code>: Country of origin (categorical, 42 levels).<br>
</li>
<li>
<code>income</code>: Target variable, representing annual income (<code>&lt;=50K</code> or <code>&gt;50K</code>).</li>
</ul>
<p>For additional details about the dataset, visit the <a href="https://www.rdocumentation.org/packages/liver/versions/1.3/topics/adult">documentation</a>.</p>
</div>
<div id="data-cleaning-and-preparation" class="section level3 unnumbered">
<h3>Data Cleaning and Preparation<a class="anchor" aria-label="anchor" href="#data-cleaning-and-preparation"><i class="fas fa-link"></i></a>
</h3>
<p>The dataset includes missing values represented by the character <code>"?"</code>. For simplicity, we rely on prior data cleaning steps (see Chapter <a href="chapter-data-prep.html#chapter-data-prep">3</a>) to handle these issues. These steps include recoding categorical variables, grouping country-level data into broader regions, and imputing missing values, as demonstrated below:</p>
<p>We then partition the cleaned dataset into <strong>training</strong> (80%) and <strong>testing</strong> (20%) subsets to ensure that the models are evaluated on unseen data:</p>
<div class="sourceCode" id="cb174"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">6</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/partition.html">partition</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">adult</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">actual_test</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">income</span></span></code></pre></div>
<p>The use of <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> ensures reproducibility.</p>
</div>
<div id="decision-tree-with-cart" class="section level3 unnumbered">
<h3>Decision Tree with CART<a class="anchor" aria-label="anchor" href="#decision-tree-with-cart"><i class="fas fa-link"></i></a>
</h3>
<p>To predict whether an individual’s income exceeds $50K, we fit a Decision Tree using the <strong>CART algorithm</strong>. The following predictors are used:</p>
<p><code>age</code>, <code>education.num</code>, <code>capital.gain</code>, <code>capital.loss</code>, <code>hours.per.week</code>, <code>marital.status</code>, <code>workclass</code>, <code>race</code>, and <code>gender</code>.</p>
<p>The tree is built using the <code>rpart()</code> function from the <strong>rpart</strong> package:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="chapter-tree.html#cb175-1" tabindex="-1"></a>formula <span class="ot">=</span> income <span class="sc">~</span> age <span class="sc">+</span> education.num <span class="sc">+</span> capital.gain <span class="sc">+</span> capital.loss <span class="sc">+</span> </span>
<span id="cb175-2"><a href="chapter-tree.html#cb175-2" tabindex="-1"></a>                   hours.per.week <span class="sc">+</span> marital.status <span class="sc">+</span> workclass <span class="sc">+</span> race <span class="sc">+</span> gender</span>
<span id="cb175-3"><a href="chapter-tree.html#cb175-3" tabindex="-1"></a></span>
<span id="cb175-4"><a href="chapter-tree.html#cb175-4" tabindex="-1"></a>tree_cart <span class="ot">=</span> <span class="fu">rpart</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb175-5"><a href="chapter-tree.html#cb175-5" tabindex="-1"></a></span>
<span id="cb175-6"><a href="chapter-tree.html#cb175-6" tabindex="-1"></a><span class="fu">print</span>(tree_cart)</span>
<span id="cb175-7"><a href="chapter-tree.html#cb175-7" tabindex="-1"></a>   n<span class="ot">=</span> <span class="dv">38878</span> </span>
<span id="cb175-8"><a href="chapter-tree.html#cb175-8" tabindex="-1"></a>   </span>
<span id="cb175-9"><a href="chapter-tree.html#cb175-9" tabindex="-1"></a>   node<span class="er">)</span>, split, n, loss, yval, (yprob)</span>
<span id="cb175-10"><a href="chapter-tree.html#cb175-10" tabindex="-1"></a>         <span class="sc">*</span> denotes terminal node</span>
<span id="cb175-11"><a href="chapter-tree.html#cb175-11" tabindex="-1"></a>   </span>
<span id="cb175-12"><a href="chapter-tree.html#cb175-12" tabindex="-1"></a>    <span class="dv">1</span><span class="er">)</span> root <span class="dv">38878</span> <span class="dv">9217</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.76292505</span> <span class="fl">0.23707495</span>)  </span>
<span id="cb175-13"><a href="chapter-tree.html#cb175-13" tabindex="-1"></a>      <span class="dv">2</span><span class="er">)</span> marital.status<span class="ot">=</span>Divorced,Never<span class="sc">-</span>married,Separated,Widowed <span class="dv">20580</span> <span class="dv">1282</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.93770651</span> <span class="fl">0.06229349</span>)  </span>
<span id="cb175-14"><a href="chapter-tree.html#cb175-14" tabindex="-1"></a>        <span class="dv">4</span><span class="er">)</span> capital.gain<span class="sc">&lt;</span> <span class="fl">7055.5</span> <span class="dv">20261</span>  <span class="dv">978</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.95172992</span> <span class="fl">0.04827008</span>) <span class="sc">*</span></span>
<span id="cb175-15"><a href="chapter-tree.html#cb175-15" tabindex="-1"></a>        <span class="dv">5</span><span class="er">)</span> capital.gain<span class="sc">&gt;=</span><span class="fl">7055.5</span> <span class="dv">319</span>   <span class="dv">15</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.04702194</span> <span class="fl">0.95297806</span>) <span class="sc">*</span></span>
<span id="cb175-16"><a href="chapter-tree.html#cb175-16" tabindex="-1"></a>      <span class="dv">3</span><span class="er">)</span> marital.status<span class="ot">=</span>Married <span class="dv">18298</span> <span class="dv">7935</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.56634605</span> <span class="fl">0.43365395</span>)  </span>
<span id="cb175-17"><a href="chapter-tree.html#cb175-17" tabindex="-1"></a>        <span class="dv">6</span><span class="er">)</span> education.num<span class="sc">&lt;</span> <span class="fl">12.5</span> <span class="dv">12944</span> <span class="dv">4163</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.67838381</span> <span class="fl">0.32161619</span>)  </span>
<span id="cb175-18"><a href="chapter-tree.html#cb175-18" tabindex="-1"></a>         <span class="dv">12</span><span class="er">)</span> capital.gain<span class="sc">&lt;</span> <span class="fl">5095.5</span> <span class="dv">12350</span> <span class="dv">3582</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.70995951</span> <span class="fl">0.29004049</span>)  </span>
<span id="cb175-19"><a href="chapter-tree.html#cb175-19" tabindex="-1"></a>           <span class="dv">24</span><span class="er">)</span> education.num<span class="sc">&lt;</span> <span class="fl">8.5</span> <span class="dv">2159</span>  <span class="dv">231</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.89300602</span> <span class="fl">0.10699398</span>) <span class="sc">*</span></span>
<span id="cb175-20"><a href="chapter-tree.html#cb175-20" tabindex="-1"></a>           <span class="dv">25</span><span class="er">)</span> education.num<span class="sc">&gt;=</span><span class="fl">8.5</span> <span class="dv">10191</span> <span class="dv">3351</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.67118045</span> <span class="fl">0.32881955</span>)  </span>
<span id="cb175-21"><a href="chapter-tree.html#cb175-21" tabindex="-1"></a>             <span class="dv">50</span><span class="er">)</span> capital.loss<span class="sc">&lt;</span> <span class="dv">1846</span> <span class="dv">9813</span> <span class="dv">3059</span> <span class="sc">&lt;=</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.68827066</span> <span class="fl">0.31172934</span>) <span class="sc">*</span></span>
<span id="cb175-22"><a href="chapter-tree.html#cb175-22" tabindex="-1"></a>             <span class="dv">51</span><span class="er">)</span> capital.loss<span class="sc">&gt;=</span><span class="dv">1846</span> <span class="dv">378</span>   <span class="dv">86</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.22751323</span> <span class="fl">0.77248677</span>) <span class="sc">*</span></span>
<span id="cb175-23"><a href="chapter-tree.html#cb175-23" tabindex="-1"></a>         <span class="dv">13</span><span class="er">)</span> capital.gain<span class="sc">&gt;=</span><span class="fl">5095.5</span> <span class="dv">594</span>   <span class="dv">13</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.02188552</span> <span class="fl">0.97811448</span>) <span class="sc">*</span></span>
<span id="cb175-24"><a href="chapter-tree.html#cb175-24" tabindex="-1"></a>        <span class="dv">7</span><span class="er">)</span> education.num<span class="sc">&gt;=</span><span class="fl">12.5</span> <span class="dv">5354</span> <span class="dv">1582</span> <span class="sc">&gt;</span><span class="dv">50</span><span class="fu">K</span> (<span class="fl">0.29548001</span> <span class="fl">0.70451999</span>) <span class="sc">*</span></span></code></pre></div>
<p>The resulting tree is visualized using the <code>rpart.plot()</code> function:</p>
<div class="sourceCode" id="cb176"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">rpart.plot</span><span class="op">(</span><span class="va">tree_cart</span>, type <span class="op">=</span> <span class="fl">4</span>, extra <span class="op">=</span> <span class="fl">104</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="tree_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>The tree identifies <strong><code>marital.status</code></strong> as the most important predictor, followed by <code>capital.gain</code>, <code>education.num</code>, and <code>capital.loss</code>. The tree contains 6 decision nodes and 7 leaves, providing interpretable insights into the predictors.</p>
</div>
<div id="decision-tree-with-c5.0" class="section level3 unnumbered">
<h3>Decision Tree with C5.0<a class="anchor" aria-label="anchor" href="#decision-tree-with-c5.0"><i class="fas fa-link"></i></a>
</h3>
<p>We next use the C5.0 algorithm to build a Decision Tree, starting with the same predictors. The tree is constructed using the <code>C5.0()</code> function from the <strong>C50</strong> package:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="chapter-tree.html#cb177-1" tabindex="-1"></a>tree_C50 <span class="ot">=</span> <span class="fu">C5.0</span>(formula, <span class="at">data =</span> train_set) </span>
<span id="cb177-2"><a href="chapter-tree.html#cb177-2" tabindex="-1"></a></span>
<span id="cb177-3"><a href="chapter-tree.html#cb177-3" tabindex="-1"></a><span class="fu">print</span>(tree_C50)</span>
<span id="cb177-4"><a href="chapter-tree.html#cb177-4" tabindex="-1"></a>   </span>
<span id="cb177-5"><a href="chapter-tree.html#cb177-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb177-6"><a href="chapter-tree.html#cb177-6" tabindex="-1"></a>   <span class="fu">C5.0.formula</span>(<span class="at">formula =</span> formula, <span class="at">data =</span> train_set)</span>
<span id="cb177-7"><a href="chapter-tree.html#cb177-7" tabindex="-1"></a>   </span>
<span id="cb177-8"><a href="chapter-tree.html#cb177-8" tabindex="-1"></a>   Classification Tree</span>
<span id="cb177-9"><a href="chapter-tree.html#cb177-9" tabindex="-1"></a>   Number of samples<span class="sc">:</span> <span class="dv">38878</span> </span>
<span id="cb177-10"><a href="chapter-tree.html#cb177-10" tabindex="-1"></a>   Number of predictors<span class="sc">:</span> <span class="dv">9</span> </span>
<span id="cb177-11"><a href="chapter-tree.html#cb177-11" tabindex="-1"></a>   </span>
<span id="cb177-12"><a href="chapter-tree.html#cb177-12" tabindex="-1"></a>   Tree size<span class="sc">:</span> <span class="dv">93</span> </span>
<span id="cb177-13"><a href="chapter-tree.html#cb177-13" tabindex="-1"></a>   </span>
<span id="cb177-14"><a href="chapter-tree.html#cb177-14" tabindex="-1"></a>   Non<span class="sc">-</span>standard options<span class="sc">:</span> attempt to group attributes</span></code></pre></div>
<p>The output provides a summary of the tree. While the full tree visualization is omitted here, it highlights the importance of <code>marital.status</code> as the root node, consistent with the CART results.</p>
</div>
<div id="random-forest" class="section level3 unnumbered">
<h3>Random Forest<a class="anchor" aria-label="anchor" href="#random-forest"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Random Forest</strong> algorithm is used to build an ensemble of Decision Trees, aggregating their predictions. Using the same predictors, we construct a Random Forest model with 100 trees using the <code>randomForest()</code> function:</p>
<div class="sourceCode" id="cb178"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">random_forest</span> <span class="op">=</span> <span class="fu">randomForest</span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, data <span class="op">=</span> <span class="va">train_set</span>, ntree <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<p>We can visualize the variable importance and the error rate of the Random Forest model:</p>
<div class="sourceCode" id="cb179"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">varImpPlot</span><span class="op">(</span><span class="va">random_forest</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">random_forest</span><span class="op">)</span></span></code></pre></div>
<p><img src="tree_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;"><img src="tree_files/figure-html/unnamed-chunk-10-2.png" width="672" style="display: block; margin: auto;"></p>
</div>
<div id="model-evaluation" class="section level3 unnumbered">
<h3>Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"><i class="fas fa-link"></i></a>
</h3>
<p>To evaluate model performance, we calculate the <strong>confusion matrix</strong>, <strong>ROC curve</strong>, and <strong>AUC</strong> for all three models (CART, C5.0, and Random Forest):</p>
<div id="cart" class="section level4 unnumbered">
<h4>CART:<a class="anchor" aria-label="anchor" href="#cart"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="chapter-tree.html#cb180-1" tabindex="-1"></a>predict_cart <span class="ot">=</span> <span class="fu">predict</span>(tree_cart, test_set, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb180-2"><a href="chapter-tree.html#cb180-2" tabindex="-1"></a></span>
<span id="cb180-3"><a href="chapter-tree.html#cb180-3" tabindex="-1"></a><span class="fu">conf.mat</span>(predict_cart, actual_test)</span>
<span id="cb180-4"><a href="chapter-tree.html#cb180-4" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"&lt;=50K"</span>, case <span class="ot">=</span> <span class="st">"&gt;50K"</span></span>
<span id="cb180-5"><a href="chapter-tree.html#cb180-5" tabindex="-1"></a>          Actual</span>
<span id="cb180-6"><a href="chapter-tree.html#cb180-6" tabindex="-1"></a>   Predict <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb180-7"><a href="chapter-tree.html#cb180-7" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7061</span> <span class="dv">1035</span></span>
<span id="cb180-8"><a href="chapter-tree.html#cb180-8" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">433</span> <span class="dv">1191</span></span>
<span id="cb180-9"><a href="chapter-tree.html#cb180-9" tabindex="-1"></a><span class="fu">conf.mat.plot</span>(predict_cart, actual_test)</span>
<span id="cb180-10"><a href="chapter-tree.html#cb180-10" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"&lt;=50K"</span>, case <span class="ot">=</span> <span class="st">"&gt;50K"</span></span></code></pre></div>
<div class="inline-figure"><img src="tree_files/figure-html/unnamed-chunk-11-1.png" width="30%" style="display: block; margin: auto;"></div>
</div>
<div id="c5.0" class="section level4 unnumbered">
<h4>C5.0:<a class="anchor" aria-label="anchor" href="#c5.0"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="chapter-tree.html#cb181-1" tabindex="-1"></a>predict_C50 <span class="ot">=</span> <span class="fu">predict</span>(tree_C50, test_set, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb181-2"><a href="chapter-tree.html#cb181-2" tabindex="-1"></a></span>
<span id="cb181-3"><a href="chapter-tree.html#cb181-3" tabindex="-1"></a><span class="fu">conf.mat</span>(predict_C50, actual_test)</span>
<span id="cb181-4"><a href="chapter-tree.html#cb181-4" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"&lt;=50K"</span>, case <span class="ot">=</span> <span class="st">"&gt;50K"</span></span>
<span id="cb181-5"><a href="chapter-tree.html#cb181-5" tabindex="-1"></a>          Actual</span>
<span id="cb181-6"><a href="chapter-tree.html#cb181-6" tabindex="-1"></a>   Predict <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb181-7"><a href="chapter-tree.html#cb181-7" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7089</span>  <span class="dv">887</span></span>
<span id="cb181-8"><a href="chapter-tree.html#cb181-8" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">405</span> <span class="dv">1339</span></span>
<span id="cb181-9"><a href="chapter-tree.html#cb181-9" tabindex="-1"></a><span class="fu">conf.mat.plot</span>(predict_C50, actual_test)</span>
<span id="cb181-10"><a href="chapter-tree.html#cb181-10" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"&lt;=50K"</span>, case <span class="ot">=</span> <span class="st">"&gt;50K"</span></span></code></pre></div>
<div class="inline-figure"><img src="tree_files/figure-html/unnamed-chunk-12-1.png" width="30%" style="display: block; margin: auto;"></div>
</div>
<div id="random-forest-1" class="section level4 unnumbered">
<h4>Random Forest:<a class="anchor" aria-label="anchor" href="#random-forest-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="chapter-tree.html#cb182-1" tabindex="-1"></a>predict_random_forest <span class="ot">=</span> <span class="fu">predict</span>(random_forest, test_set)</span>
<span id="cb182-2"><a href="chapter-tree.html#cb182-2" tabindex="-1"></a></span>
<span id="cb182-3"><a href="chapter-tree.html#cb182-3" tabindex="-1"></a><span class="fu">conf.mat</span>(predict_random_forest, actual_test)</span>
<span id="cb182-4"><a href="chapter-tree.html#cb182-4" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"&lt;=50K"</span>, case <span class="ot">=</span> <span class="st">"&gt;50K"</span></span>
<span id="cb182-5"><a href="chapter-tree.html#cb182-5" tabindex="-1"></a>          Actual</span>
<span id="cb182-6"><a href="chapter-tree.html#cb182-6" tabindex="-1"></a>   Predict <span class="sc">&lt;=</span><span class="dv">50</span>K <span class="sc">&gt;</span><span class="dv">50</span>K</span>
<span id="cb182-7"><a href="chapter-tree.html#cb182-7" tabindex="-1"></a>     <span class="sc">&lt;=</span><span class="dv">50</span>K  <span class="dv">7069</span>  <span class="dv">913</span></span>
<span id="cb182-8"><a href="chapter-tree.html#cb182-8" tabindex="-1"></a>     <span class="sc">&gt;</span><span class="dv">50</span>K    <span class="dv">425</span> <span class="dv">1313</span></span>
<span id="cb182-9"><a href="chapter-tree.html#cb182-9" tabindex="-1"></a><span class="fu">conf.mat.plot</span>(predict_random_forest, actual_test)</span>
<span id="cb182-10"><a href="chapter-tree.html#cb182-10" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"&lt;=50K"</span>, case <span class="ot">=</span> <span class="st">"&gt;50K"</span></span></code></pre></div>
<div class="inline-figure"><img src="tree_files/figure-html/unnamed-chunk-13-1.png" width="30%" style="display: block; margin: auto;"></div>
<p>Finally, the ROC curves and AUC for the models are compared:</p>
<div class="sourceCode" id="cb183"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prob_cart</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tree_cart</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">prob_C50</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tree_C50</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">prob_random_forest</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">random_forest</span>, <span class="va">test_set</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span></span>
<span></span>
<span><span class="va">roc_cart</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">actual_test</span>, <span class="va">prob_cart</span><span class="op">)</span></span>
<span><span class="va">roc_C50</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">actual_test</span>, <span class="va">prob_C50</span><span class="op">)</span></span>
<span><span class="va">roc_random_forest</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">actual_test</span>, <span class="va">prob_random_forest</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/ggroc.html">ggroc</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">roc_cart</span>, <span class="va">roc_C50</span>, <span class="va">roc_random_forest</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"ROC Curves with AUC for Three Models"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, </span>
<span>    labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"CART; AUC="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">roc_cart</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"C5.0; AUC="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">roc_C50</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Random Forest; AUC="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="va">roc_random_forest</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.title <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_blank</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.7</span>, <span class="fl">.3</span><span class="op">)</span>, text <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">17</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="tree_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;"></div>
<p>The <strong>black</strong> curve represents CART, the <span style="color:red"><strong>red</strong></span> curve represents C5.0, and the <span style="color:green"><strong>green</strong></span> curve represents Random Forest. Based on AUC values, C5.0 performs slightly better, but all three models show comparable accuracy, making them reliable for this classification task.</p>
</div>
</div>
</div>
<div id="exercises-6" class="section level2" number="11.6">
<h2>
<span class="header-section-number">11.6</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-6"><i class="fas fa-link"></i></a>
</h2>
<p>To do …</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="chapter-regression.html"><span class="header-section-number">10</span> Regression Modeling: From Basics to Advanced Techniques</a></div>
<div class="next"><a href="chapter-nn.html"><span class="header-section-number">12</span> Neural Networks: The Building Blocks of Artificial Intelligence</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-tree"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></li>
<li><a class="nav-link" href="#how-decision-trees-work"><span class="header-section-number">11.1</span> How Decision Trees Work</a></li>
<li><a class="nav-link" href="#classification-and-regression-trees-cart"><span class="header-section-number">11.2</span> Classification and Regression Trees (CART)</a></li>
<li><a class="nav-link" href="#the-c5.0-algorithm-for-building-decision-trees"><span class="header-section-number">11.3</span> The C5.0 Algorithm for Building Decision Trees</a></li>
<li><a class="nav-link" href="#random-forests-an-ensemble-approach"><span class="header-section-number">11.4</span> Random Forests: An Ensemble Approach</a></li>
<li>
<a class="nav-link" href="#tree-case-study"><span class="header-section-number">11.5</span> Case Study: Who Can Earn More Than $50K Per Year?</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-of-the-dataset-2">Overview of the Dataset</a></li>
<li><a class="nav-link" href="#data-cleaning-and-preparation">Data Cleaning and Preparation</a></li>
<li><a class="nav-link" href="#decision-tree-with-cart">Decision Tree with CART</a></li>
<li><a class="nav-link" href="#decision-tree-with-c5.0">Decision Tree with C5.0</a></li>
<li><a class="nav-link" href="#random-forest">Random Forest</a></li>
<li><a class="nav-link" href="#model-evaluation">Model Evaluation</a></li>
</ul>
</li>
<li><a class="nav-link" href="#exercises-6"><span class="header-section-number">11.6</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/RezaMoammadi/Book-Data-Science/blob/master/tree.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/RezaMoammadi/Book-Data-Science/edit/master/tree.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Uncovering Data Science with R</strong>" was written by Reza Mohammadi. It was last built on 2025-02-08.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
