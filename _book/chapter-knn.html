<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Classification using k-Nearest Neighbors | Data Science Foundations and Machine Learning Using R</title>
<meta name="author" content="Reza Mohammadi">
<meta name="description" content="Classification is one of the fundamental tasks in machine learning, enabling models to categorize data into predefined groups. From detecting spam emails to predicting customer churn,...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="Chapter 7 Classification using k-Nearest Neighbors | Data Science Foundations and Machine Learning Using R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://uncovering-data-science.netlify.app/chapter-knn.html">
<meta property="og:image" content="https://uncovering-data-science.netlify.app/images/logo_black.png">
<meta property="og:description" content="Classification is one of the fundamental tasks in machine learning, enabling models to categorize data into predefined groups. From detecting spam emails to predicting customer churn,...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 7 Classification using k-Nearest Neighbors | Data Science Foundations and Machine Learning Using R">
<meta name="twitter:description" content="Classification is one of the fundamental tasks in machine learning, enabling models to categorize data into predefined groups. From detecting spam emails to predicting customer churn,...">
<meta name="twitter:image" content="https://uncovering-data-science.netlify.app/images/logo_black.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Data Science Foundations and Machine Learning Using R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="chapter-into-R.html"><span class="header-section-number">1</span> The Basics for R</a></li>
<li><a class="" href="chapter-intro-DS.html"><span class="header-section-number">2</span> Introduction to Data Science</a></li>
<li><a class="" href="chapter-data-prep.html"><span class="header-section-number">3</span> Data Preparation</a></li>
<li><a class="" href="chapter-EDA.html"><span class="header-section-number">4</span> Exploratory Data Analysis</a></li>
<li><a class="" href="chapter-statistics.html"><span class="header-section-number">5</span> Statistical Inference and Hypothesis Testing</a></li>
<li><a class="" href="chapter-modeling.html"><span class="header-section-number">6</span> Preparing Data for Modeling</a></li>
<li><a class="active" href="chapter-knn.html"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></li>
<li><a class="" href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></li>
<li><a class="" href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></li>
<li><a class="" href="chapter-regression.html"><span class="header-section-number">10</span> Regression Analysis: Foundations and Applications</a></li>
<li><a class="" href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></li>
<li><a class="" href="chapter-nn.html"><span class="header-section-number">12</span> Neural Networks: The Building Blocks of Artificial Intelligence</a></li>
<li><a class="" href="chapter-cluster.html"><span class="header-section-number">13</span> Clustering</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/RezaMoammadi/Book-Data-Science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-knn" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Classification using k-Nearest Neighbors<a class="anchor" aria-label="anchor" href="#chapter-knn"><i class="fas fa-link"></i></a>
</h1>
<p><em>Classification</em> is one of the fundamental tasks in machine learning, enabling models to categorize data into predefined groups. From detecting spam emails to predicting customer churn, classification algorithms are widely used across various domains. In this chapter, we will first explore the concept of classification, discussing its applications, key principles, and commonly used algorithms.</p>
<p>Once we have a solid understanding of classification, we will introduce <em>k-Nearest Neighbors (kNN)</em>, a simple yet effective algorithm based on the idea of similarity between data points. kNN is widely used for classification due to its intuitive approach and ease of implementation. We will delve into the details of how kNN works, demonstrate its implementation in R, and discuss its strengths, limitations, and real-world applications.</p>
<p>To illustrate kNN in practice, we will apply it to a real-world dataset: the <em>churn</em> dataset. Our goal will be to build a classification model that predicts whether a customer will churn based on their service usage and account features. Through this hands-on example, we will demonstrate data preprocessing, selecting the optimal <span class="math inline">\(k\)</span>, evaluating model performance, and interpreting results.</p>
<div id="classification" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Classification<a class="anchor" aria-label="anchor" href="#classification"><i class="fas fa-link"></i></a>
</h2>
<p>Have you ever wondered how your email app effortlessly filters spam, how your streaming service seems to know exactly what you want to watch next, or how banks detect fraudulent credit card transactions in real-time? These seemingly magical predictions are made possible by <em>classification</em>, a fundamental task in machine learning.</p>
<p>At its core, classification involves assigning a label or category to an observation based on its features. For example, given customer data, classification can predict whether they are likely to churn or stay loyal. Unlike regression, which predicts continuous numerical values (e.g., house prices), classification deals with discrete outcomes. The target variable, often called the <em>class</em> or <em>label</em>, can either be:</p>
<ul>
<li>
<em>Binary</em>: Two possible categories (e.g., spam vs. not spam).<br>
</li>
<li>
<em>Multi-class</em>: More than two categories (e.g., car, bicycle, or pedestrian in image recognition).</li>
</ul>
<p>From diagnosing diseases to identifying fraudulent activities, classification is a versatile tool used across countless domains to solve practical problems.</p>
<div id="where-is-classification-used" class="section level3 unnumbered">
<h3>Where Is Classification Used?<a class="anchor" aria-label="anchor" href="#where-is-classification-used"><i class="fas fa-link"></i></a>
</h3>
<p>Classification algorithms power many everyday applications and cutting-edge technologies. Here are some examples:<br>
- <em>Email filtering</em>: Sorting spam from non-spam messages.<br>
- <em>Fraud detection</em>: Identifying suspicious credit card transactions.<br>
- <em>Customer retention</em>: Predicting whether a customer will churn.<br>
- <em>Medical diagnosis</em>: Diagnosing diseases based on patient records.<br>
- <em>Object recognition</em>: Detecting pedestrians and vehicles in self-driving cars.<br>
- <em>Recommendation systems</em>: Suggesting movies, songs, or products based on user preferences.</p>
<p>Every time you interact with technology that “predicts” something for you, chances are, a classification model is working behind the scenes.</p>
</div>
<div id="how-does-classification-work" class="section level3 unnumbered">
<h3>How Does Classification Work?<a class="anchor" aria-label="anchor" href="#how-does-classification-work"><i class="fas fa-link"></i></a>
</h3>
<p>Classification involves two critical phases:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Training Phase</strong>: The algorithm learns patterns from a labeled dataset, which contains both predictor variables (features) and target class labels. For instance, in a fraud detection system, the algorithm might learn that transactions involving unusually high amounts and originating from foreign locations are more likely to be fraudulent.<br>
</li>
<li>
<strong>Prediction Phase</strong>: Once the model is trained, it applies these learned patterns to classify new, unseen data. For example, given a new transaction, the model predicts whether it is fraudulent or legitimate.</li>
</ol>
<p>A good classification model does more than just memorize the training data—it <em>generalizes</em> well, meaning it performs accurately on new, unseen data. For instance, a model trained on historical medical records should be able to diagnose a patient it has never encountered before, rather than simply repeating past diagnoses.</p>
</div>
<div id="which-classification-algorithm-should-you-use" class="section level3 unnumbered">
<h3>Which Classification Algorithm Should You Use?<a class="anchor" aria-label="anchor" href="#which-classification-algorithm-should-you-use"><i class="fas fa-link"></i></a>
</h3>
<p>Different classification algorithms are designed for different kinds of problems and datasets. Some commonly used algorithms include:<br>
- <em>k-Nearest Neighbors (kNN)</em>: A simple, distance-based algorithm (introduced in this chapter).<br>
- <em>Naive Bayes</em>: Particularly useful for text classification, like spam filtering (covered in Chapter <a href="chapter-bayes.html#chapter-bayes">9</a>).<br>
- <em>Logistic Regression</em>: A popular method for binary classification tasks, such as predicting customer churn (covered in Chapter <a href="chapter-regression.html#chapter-regression">10</a>).<br>
- <em>Decision Trees and Random Forests</em>: Versatile, interpretable methods for handling complex problems (covered in Chapter <a href="chapter-tree.html#chapter-tree">11</a>).<br>
- <em>Neural Networks</em>: Effective for handling high-dimensional and complex data, such as images or natural language (covered in Chapter <a href="chapter-nn.html#chapter-nn">12</a>).</p>
<p>The choice of algorithm depends on factors such as dataset size, feature relationships, and the trade-off between interpretability and performance. For instance, if you’re working with a small dataset and need an easy-to-interpret solution, kNN or Decision Trees might be ideal. Conversely, for high-dimensional data like images or speech recognition, Neural Networks could be more effective.</p>
<p>To illustrate classification in action, consider a <em>bank</em> dataset where the goal is to predict whether a customer will make a deposit (<code>deposit = yes</code>) or not (<code>deposit = no</code>). The features might include customer details like <code>age</code>, <code>education</code>, <code>job</code>, and <code>marital status</code>. By training a classification model on this data, the bank can identify and target potential customers who are likely to invest, improving their marketing strategy.</p>
</div>
<div id="why-is-classification-important" class="section level3 unnumbered">
<h3>Why Is Classification Important?<a class="anchor" aria-label="anchor" href="#why-is-classification-important"><i class="fas fa-link"></i></a>
</h3>
<p>Classification forms the backbone of countless machine learning applications that drive smarter decisions and actionable insights in industries like finance, healthcare, retail, and technology. Understanding how it works is a critical step in mastering machine learning and applying it to solve real-world problems.</p>
<p>Among the many classification techniques, <em>k-Nearest Neighbors (kNN)</em> stands out for its simplicity and effectiveness. Because it is easy to understand and requires minimal assumptions about the data, kNN is often used as a baseline model before exploring more advanced techniques. In the rest of this chapter, we will explore how kNN works, why it is widely used, and how to implement it in R.</p>
</div>
</div>
<div id="how-k-nearest-neighbors-works" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> How k-Nearest Neighbors Works<a class="anchor" aria-label="anchor" href="#how-k-nearest-neighbors-works"><i class="fas fa-link"></i></a>
</h2>
<p>Have you ever sought advice from a few trusted friends before making a decision? The <em>k-Nearest Neighbors (kNN)</em> algorithm follows a similar principle—it “consults” the closest data points to determine the category of a new observation. This simple yet effective idea makes kNN one of the most intuitive classification methods in machine learning.</p>
<p>Unlike many machine learning algorithms that require an explicit training phase, kNN is a <em>lazy learning</em> or <em>instance-based</em> method. Instead of constructing a complex model, it stores the entire training dataset and makes predictions on demand. When given a new observation, kNN identifies the <em>k</em> closest data points using a predefined distance metric. The class label is then assigned based on a <em>majority vote</em> among these nearest neighbors. The choice of <span class="math inline">\(k\)</span>, the number of neighbors considered, plays a crucial role in balancing sensitivity to local patterns and generalization to broader trends.</p>
<div id="how-does-knn-classify-a-new-observation" class="section level3 unnumbered">
<h3>How Does kNN Classify a New Observation?<a class="anchor" aria-label="anchor" href="#how-does-knn-classify-a-new-observation"><i class="fas fa-link"></i></a>
</h3>
<p>To classify a new observation, kNN calculates its <em>distance</em> from every data point in the training set using a specified metric, such as <em>Euclidean distance</em>, for instance. After identifying the <span class="math inline">\(k\)</span>-nearest neighbors, the algorithm assigns the most frequent class among them as the predicted category.</p>
<p>Figure <a href="chapter-knn.html#fig:knn-image">7.1</a> illustrates this concept with two classes: <span style="color: red;">Class A (red circles)</span> and <span style="color: blue;">Class B (blue squares)</span>. A new data point, represented by a <em>dark star</em>, needs to be classified. The figure compares predictions for two different values of <span class="math inline">\(k\)</span>:</p>
<ul>
<li>
<em>When <span class="math inline">\(k = 3\)</span></em>: The algorithm considers the 3 closest neighbors—two blue squares and one red circle. Since the majority class is <em>Class B (blue squares)</em>, the new point is classified as Class B.<br>
</li>
<li>
<em>When <span class="math inline">\(k = 6\)</span></em>: The algorithm expands the neighborhood to include the 6 nearest neighbors. This larger set consists of four red circles and two blue squares, shifting the majority class to <em>Class A (red circles)</em>. As a result, the new point is classified as Class A.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:knn-image"></span>
<img src="images/ch7_knn.png" alt="A two-dimensional toy dataset with two classes (Class A and Class B) and a new data point (dark star), illustrating the k-Nearest Neighbors algorithm with k = 3 and k = 6." width="75%"><p class="caption">
Figure 7.1: A two-dimensional toy dataset with two classes (Class A and Class B) and a new data point (dark star), illustrating the k-Nearest Neighbors algorithm with k = 3 and k = 6.
</p>
</div>
<p>These examples illustrate how the choice of <span class="math inline">\(k\)</span> affects classification. A smaller <span class="math inline">\(k\)</span> (e.g., 3) makes predictions highly sensitive to local patterns, capturing finer details but also increasing the risk of misclassification due to noise. In contrast, a larger <span class="math inline">\(k\)</span> (e.g., 6) smooths predictions by incorporating more neighbors, reducing sensitivity to individual data points but potentially overlooking localized structures in the data. Selecting an appropriate <span class="math inline">\(k\)</span> ensures that kNN generalizes well without becoming overly complex or overly simplistic.</p>
</div>
<div id="strengths-and-limitations-of-knn" class="section level3 unnumbered">
<h3>Strengths and Limitations of kNN<a class="anchor" aria-label="anchor" href="#strengths-and-limitations-of-knn"><i class="fas fa-link"></i></a>
</h3>
<p>The kNN algorithm is widely used due to its simplicity and intuitive nature, making it an excellent starting point for classification problems. By relying only on distance metrics and majority voting, it avoids the complexity of training explicit models. However, this simplicity comes with trade-offs, particularly in handling large datasets and noisy features.</p>
<p>One of kNN’s key strengths is its ease of implementation and interpretability. Since it does not require model training, it can be applied directly to datasets with minimal preprocessing. It performs well on small datasets where patterns are well-defined and feature relationships are strong. However, kNN is highly sensitive to irrelevant or noisy features, as distance calculations may become less meaningful when unnecessary attributes are included. Additionally, it can be computationally expensive for large datasets, since it must calculate distances for every training point during prediction. The choice of <span class="math inline">\(k\)</span> also plays a crucial role—too small a <span class="math inline">\(k\)</span> makes the algorithm overly sensitive to noise, while too large a <span class="math inline">\(k\)</span> may oversimplify patterns, leading to reduced accuracy.</p>
</div>
</div>
<div id="knn-in-action-a-toy-example-for-drug-classification" class="section level2 unnumbered">
<h2>kNN in Action: A Toy Example for Drug Classification<a class="anchor" aria-label="anchor" href="#knn-in-action-a-toy-example-for-drug-classification"><i class="fas fa-link"></i></a>
</h2>
<p>To further illustrate kNN, consider a real-world scenario involving drug prescription classification. A dataset of 200 patients includes their <em>age</em>, <em>sodium-to-potassium (Na/K) ratio</em>, and the drug type they were prescribed. This dataset is synthetically generated to reflect a real-world scenario. For details on how this dataset was generated, refer to Section <a href="chapter-into-R.html#intro-R-exercises">1.16</a>. Figure <a href="chapter-knn.html#fig:scatter-plot-ex-drug">7.2</a> visualizes this dataset, where different drug types are represented by:</p>
<ul>
<li>
<em>Red circles</em> for Drug A,<br>
</li>
<li>
<em>Green triangles</em> for Drug B, and<br>
</li>
<li>
<em>Blue squares</em> for Drug C.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scatter-plot-ex-drug"></span>
<img src="knn_files/figure-html/scatter-plot-ex-drug-1.png" alt="Scatter plot of Age vs. Sodium/Potassium Ratio for 200 patients, with drug type indicated by color and shape." width="95%"><p class="caption">
Figure 7.2: Scatter plot of Age vs. Sodium/Potassium Ratio for 200 patients, with drug type indicated by color and shape.
</p>
</div>
<p>Suppose three new patients arrive at the clinic, and we need to determine which drug is most suitable for them based on their <em>age</em> and <em>sodium-to-potassium ratio</em>. Their details are as follows:</p>
<ol style="list-style-type: decimal">
<li>
<em>Patient 1</em>: 40 years old with a Na/K ratio of 30.5.<br>
</li>
<li>
<em>Patient 2</em>: 28 years old with a Na/K ratio of 9.6.<br>
</li>
<li>
<em>Patient 3</em>: 61 years old with a Na/K ratio of 10.5.</li>
</ol>
<p>These patients are represented as <em>orange circles</em> in Figure <a href="chapter-knn.html#fig:scatter-plot-ex-drug-2">7.3</a>. Using kNN, we will classify the drug type for each patient.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scatter-plot-ex-drug-2"></span>
<img src="knn_files/figure-html/scatter-plot-ex-drug-2-1.png" alt="Scatter plot of Age vs. Sodium/Potassium Ratio for 200 patients, with drug type indicated by color and shape. The three new patients are represented by large orange circles." width="95%"><p class="caption">
Figure 7.3: Scatter plot of Age vs. Sodium/Potassium Ratio for 200 patients, with drug type indicated by color and shape. The three new patients are represented by large orange circles.
</p>
</div>
<p>For <em>Patient 1</em>, who is located deep within a cluster of red-circle points (Drug A), the classification is straightforward: <em>Drug A</em>. All the nearest neighbors belong to Drug A, making it an easy decision.</p>
<p>For <em>Patient 2</em>, the situation is more nuanced. If <span class="math inline">\(k = 1\)</span>, the nearest neighbor is a blue square, resulting in the classification <em>Drug C</em>. When <span class="math inline">\(k = 2\)</span>, there is a tie between Drug B and Drug C, leading to no clear majority. With <span class="math inline">\(k = 3\)</span>, two out of the three nearest neighbors are blue squares, so the classification remains <em>Drug C</em>.</p>
<p>For <em>Patient 3</em>, classification becomes even more ambiguous. With <span class="math inline">\(k = 1\)</span>, the closest neighbor is a blue square, classifying the patient as <em>Drug C</em>. However, for <span class="math inline">\(k = 2\)</span> or <span class="math inline">\(k = 3\)</span>, the nearest neighbors belong to multiple classes, creating uncertainty in classification.</p>
<div class="figure">
<span style="display:block;" id="fig:scatter-plot-ex-drug-3"></span>
<img src="knn_files/figure-html/scatter-plot-ex-drug-3-1.png" alt="Zoom-in plots for the three new patients and their nearest neighbors. The left plot is for Patient 1, the middle plot is for Patient 2, and the right plot is for Patient 3." width="33%"><img src="knn_files/figure-html/scatter-plot-ex-drug-3-2.png" alt="Zoom-in plots for the three new patients and their nearest neighbors. The left plot is for Patient 1, the middle plot is for Patient 2, and the right plot is for Patient 3." width="33%"><img src="knn_files/figure-html/scatter-plot-ex-drug-3-3.png" alt="Zoom-in plots for the three new patients and their nearest neighbors. The left plot is for Patient 1, the middle plot is for Patient 2, and the right plot is for Patient 3." width="33%"><p class="caption">
Figure 7.4: Zoom-in plots for the three new patients and their nearest neighbors. The left plot is for Patient 1, the middle plot is for Patient 2, and the right plot is for Patient 3.
</p>
</div>
<p>These examples highlight several key aspects of kNN. The choice of <span class="math inline">\(k\)</span> significantly influences classification—small values of <span class="math inline">\(k\)</span> make the algorithm highly sensitive to local patterns, while larger values introduce smoothing by considering broader neighborhoods. Additionally, the selection of distance metrics, such as Euclidean distance, affects how neighbors are determined. Finally, proper feature scaling ensures that all variables contribute fairly to distance calculations, preventing dominance by features with larger numeric ranges.</p>
<p>This example demonstrates how kNN assigns labels based on proximity, reinforcing the importance of thoughtful parameter selection and preprocessing techniques. Before applying kNN to real-world datasets, it is essential to understand <em>how</em> similarity is measured—this leads to the next discussion on distance metrics.</p>
</div>
<div id="distance-metrics" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Distance Metrics<a class="anchor" aria-label="anchor" href="#distance-metrics"><i class="fas fa-link"></i></a>
</h2>
<p>In the kNN algorithm, the classification of a new data point is determined by identifying the most <em>similar</em> records from the training dataset. But how do we define and measure <em>similarity</em>? While similarity might seem intuitive, applying it in machine learning requires precise <em>distance metrics</em>. These metrics quantify the “closeness” between two data points in a multidimensional space, directly influencing how neighbors are selected for classification.</p>
<p>Imagine you’re shopping online and looking for recommendations. You’re a 50-year-old married female—who’s more similar to you: a 40-year-old single female or a 30-year-old married male? The answer depends on how we measure the distance between you and each person. In kNN, this distance is computed using numerical features such as age and categorical features such as marital status. The smaller the distance, the more “similar” two individuals are, and the more influence they have in determining predictions. Since kNN assumes that closer points (lower distance) belong to the same class, choosing the right distance metric is crucial for accurate classification.</p>
<div id="euclidean-distance" class="section level3 unnumbered">
<h3>Euclidean Distance<a class="anchor" aria-label="anchor" href="#euclidean-distance"><i class="fas fa-link"></i></a>
</h3>
<p>The most widely used distance metric in kNN is <em>Euclidean distance</em>, which measures the straight-line distance between two points. Think of it as the “as-the-crow-flies” distance, similar to the shortest path between two locations on a map. This metric is intuitive and aligns with how we often perceive distance in the real world.</p>
<p>Mathematically, the Euclidean distance between two points, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, in <span class="math inline">\(n\)</span>-dimensional space is given by:</p>
<p><span class="math display">\[
\text{dist}(x, y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots + (x_n - y_n)^2},
\]</span></p>
<p>where <span class="math inline">\(x = (x_1, x_2, \ldots, x_n)\)</span> and <span class="math inline">\(y = (y_1, y_2, \ldots, y_n)\)</span> represent the feature vectors of the two points. The differences between corresponding features (<span class="math inline">\(x_i - y_i\)</span>) are squared, summed, and then square-rooted to calculate the distance.</p>
<div class="example">
<p><span id="exm:ex-knn-euclidean-distance" class="example"><strong>Example 7.1  </strong></span>Let’s calculate the Euclidean distance between two patients based on their <em>age</em> and <em>sodium-to-potassium (Na/K) ratio</em>:</p>
<ul>
<li>Patient 1: <span class="math inline">\(x = (40, 30.5)\)</span><br>
</li>
<li>Patient 2: <span class="math inline">\(y = (28, 9.6)\)</span>
</li>
</ul>
<p>Using the formula:<br><span class="math display">\[
\text{dist}(x, y) = \sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \sqrt{(12)^2 + (20.9)^2} = 24.11
\]</span></p>
<p>This result quantifies the dissimilarity between the two patients. In kNN, this distance helps determine how similar Patient 1 is to Patient 2 and whether they should be classified into the same drug class.</p>
</div>
</div>
<div id="choosing-the-right-distance-metric" class="section level3 unnumbered">
<h3>Choosing the Right Distance Metric<a class="anchor" aria-label="anchor" href="#choosing-the-right-distance-metric"><i class="fas fa-link"></i></a>
</h3>
<p>While Euclidean distance is widely used in kNN, it is not always the best choice. Other distance metrics can be more suitable depending on the dataset’s characteristics:</p>
<ul>
<li>
<em>Manhattan Distance</em>: Measures distance by summing the absolute differences between coordinates. This is useful when movement is restricted to grid-like paths, such as city blocks.</li>
<li>
<em>Hamming Distance</em>: Used for categorical variables, where the distance is the number of positions at which two feature vectors differ.</li>
<li>
<em>Cosine Similarity</em>: Measures the angle between two vectors rather than their absolute distance. This is useful in high-dimensional spaces, such as text classification.</li>
</ul>
<p>The choice of distance metric depends on the data type and problem domain. If your dataset contains categorical or high-dimensional features, exploring alternative metrics—such as Manhattan or Cosine Similarity—might be necessary. For further details, refer to the <code><a href="https://rdrr.io/r/stats/dist.html">dist()</a></code> function in R.</p>
</div>
</div>
<div id="how-to-choose-an-optimal-k" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> How to Choose an Optimal <span class="math inline">\(k\)</span><a class="anchor" aria-label="anchor" href="#how-to-choose-an-optimal-k"><i class="fas fa-link"></i></a>
</h2>
<p>How many opinions do you seek before making an important decision? Too few might lead to a biased perspective, while too many might dilute the relevance of the advice. Similarly, in the k-Nearest Neighbors (kNN) algorithm, the choice of <span class="math inline">\(k\)</span>—the number of neighbors considered for classification—directly impacts the model’s performance. But how do we determine the right <span class="math inline">\(k\)</span>?</p>
<p>There is no universally “correct” value for <span class="math inline">\(k\)</span>. The optimal choice depends on the specific dataset and classification problem, requiring careful consideration of the trade-offs involved.</p>
<div id="balancing-overfitting-and-underfitting" class="section level3 unnumbered">
<h3>Balancing Overfitting and Underfitting<a class="anchor" aria-label="anchor" href="#balancing-overfitting-and-underfitting"><i class="fas fa-link"></i></a>
</h3>
<p>When <span class="math inline">\(k\)</span> is too small, such as <span class="math inline">\(k = 1\)</span>, the algorithm becomes highly sensitive to individual training points. Each new observation is classified based on its single closest neighbor, making the model highly reactive to noise and outliers. This can lead to <em>overfitting</em>, where the model memorizes the training data but fails to generalize to unseen data. For example, a small cluster of mislabeled data points could disproportionately influence predictions, reducing the model’s reliability.</p>
<p>Conversely, as <span class="math inline">\(k\)</span> increases, the algorithm incorporates more neighbors into the classification decision. Larger <span class="math inline">\(k\)</span> values smooth the decision boundary, reducing the impact of noise and outliers. However, if <span class="math inline">\(k\)</span> is too large, the model may oversimplify, averaging out meaningful patterns in the data. When <span class="math inline">\(k\)</span> is comparable to the size of the training set, the majority class dominates predictions, leading to <em>underfitting</em>, where the model fails to capture important distinctions.</p>
<p>Choosing an appropriate <span class="math inline">\(k\)</span> requires balancing these extremes. Smaller values of <span class="math inline">\(k\)</span> capture fine-grained local structures but risk overfitting, while larger values provide more stability at the expense of detail.</p>
</div>
<div id="choosing-k-through-validation" class="section level3 unnumbered">
<h3>Choosing <span class="math inline">\(k\)</span> Through Validation<a class="anchor" aria-label="anchor" href="#choosing-k-through-validation"><i class="fas fa-link"></i></a>
</h3>
<p>Since the optimal <span class="math inline">\(k\)</span> depends on the dataset, a common approach is to evaluate multiple values of <span class="math inline">\(k\)</span> using a <em>validation set</em> or <em>cross-validation</em>. Performance metrics such as accuracy, precision, recall, and F1-score help identify the best <span class="math inline">\(k\)</span> for a given problem.</p>
<p>To illustrate, we use the <em>churn</em> dataset and evaluate the accuracy of the kNN algorithm across different <span class="math inline">\(k\)</span> values (ranging from 1 to 30). Figure <a href="chapter-knn.html#fig:kNN-plot">7.5</a> shows how accuracy fluctuates as <span class="math inline">\(k\)</span> increases. The plot is generated using the <code><a href="https://rdrr.io/pkg/liver/man/kNN.plot.html">kNN.plot()</a></code> function from the <strong>liver</strong> package in R.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kNN-plot"></span>
<img src="knn_files/figure-html/kNN-plot-1.png" alt="Accuracy of the k-Nearest Neighbors algorithm for different values of k in the range from 1 to 30." width="85%"><p class="caption">
Figure 7.5: Accuracy of the k-Nearest Neighbors algorithm for different values of k in the range from 1 to 30.
</p>
</div>
<p>From the plot, we observe that kNN accuracy fluctuates as <span class="math inline">\(k\)</span> increases. The highest accuracy is achieved when <span class="math inline">\(k = 5\)</span>, where the algorithm balances sensitivity to local patterns with robustness to noise. At this value, kNN delivers an accuracy of 0.932 and an error rate of 0.068.</p>
<p>Choosing the optimal <span class="math inline">\(k\)</span> is as much an art as it is a science. While there’s no universal rule for selecting <span class="math inline">\(k\)</span>, experimentation and validation are key. Start with a range of plausible <span class="math inline">\(k\)</span> values, test the model’s performance, and select the one that provides the best results based on your chosen metric.</p>
<p>Keep in mind that the optimal <span class="math inline">\(k\)</span> may vary across datasets. Whenever applying kNN to a new problem, repeating this process ensures the model remains both accurate and generalizable. By carefully tuning <span class="math inline">\(k\)</span>, we strike the right balance between overfitting and underfitting, improving the model’s predictive power.</p>
</div>
</div>
<div id="preparing-data-for-knn" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Preparing Data for kNN<a class="anchor" aria-label="anchor" href="#preparing-data-for-knn"><i class="fas fa-link"></i></a>
</h2>
<p>The effectiveness of the kNN algorithm relies heavily on how the dataset is prepared. Since kNN uses distance metrics to evaluate similarity between data points, proper preprocessing is crucial to ensure accurate and meaningful results. Two essential steps in this process are <em>feature scaling</em> and <em>one-hot encoding</em>, which enable the algorithm to handle numerical and categorical features effectively. These steps are part of the <em>Preparing Data for Modeling</em> stage in the Data Science Workflow (Figure <a href="chapter-intro-DS.html#fig:CRISP-DM">2.3</a>).</p>
<div id="feature-scaling-1" class="section level3" number="7.5.1">
<h3>
<span class="header-section-number">7.5.1</span> Feature Scaling<a class="anchor" aria-label="anchor" href="#feature-scaling-1"><i class="fas fa-link"></i></a>
</h3>
<p>In most datasets, numerical features often have vastly different ranges. For instance, <em>age</em> may range from 20 to 70, while <em>income</em> could range from 20,000 to 150,000. Without proper scaling, features with larger ranges, such as income, will dominate distance calculations, leading to biased predictions. To address this, all numerical features must be transformed to comparable scales. See Section <a href="chapter-data-prep.html#feature-scaling">3.5</a> for more details on scaling methods.</p>
<p>A widely used method is <em>min-max scaling</em>, which transforms each feature to a specified range, typically <span class="math inline">\([0, 1]\)</span>, using the formula:
<span class="math display">\[
x_{\text{scaled}} = \frac{x - \min(x)}{\max(x) - \min(x)},
\]</span>
where <span class="math inline">\(x\)</span> represents the original feature value, and <span class="math inline">\(\min(x)\)</span> and <span class="math inline">\(\max(x)\)</span> are the minimum and maximum values of the feature, respectively. This formula rescales each feature to a <span class="math inline">\([0,1]\)</span> range, ensuring that no single feature dominates the distance calculation.</p>
<p>Another common method is <em>z-score standardization</em>, which rescales features so that they have a mean of 0 and a standard deviation of 1:
<span class="math display">\[
x_{\text{scaled}} = \frac{x - \text{mean}(x)}{\text{sd}(x)}
\]</span>
This method is particularly useful when features contain outliers or follow different distributions. Unlike min-max scaling, z-score standardization does not constrain values within a fixed range but ensures that they follow a standard normal distribution, making it more robust to extreme values.</p>
<blockquote>
<p><strong>Choosing the Right Scaling Method:</strong><br>
Min-max scaling is preferable when feature values are bounded within a known range, such as pixel values in images or percentages. This ensures that all features contribute equally to the distance metric while maintaining their relative proportions. On the other hand, <em>z-score standardization</em> is more suitable when data contains extreme values or follows different distributions across features. It transforms values into a standard normal distribution, making it particularly effective for datasets with outliers or varying units of measurement.</p>
</blockquote>
<blockquote>
<p><strong>Avoiding Data Leakage:</strong>
Scaling must always be performed <em>after partitioning</em> the dataset into training and test sets. The scaling parameters, such as the minimum and maximum for min-max scaling or the mean and standard deviation for z-score standardization, should be computed from the <em>training set only</em> and then applied consistently to both the training and test sets. Performing scaling before partitioning can introduce <em>data leakage</em>, where information from the test set inadvertently influences the training process. This can lead to misleadingly high accuracy during evaluation, as the model indirectly gains access to test data before making predictions.</p>
</blockquote>
</div>
<div id="scaling-training-and-test-data-the-same-way" class="section level3" number="7.5.2">
<h3>
<span class="header-section-number">7.5.2</span> Scaling Training and Test Data the Same Way<a class="anchor" aria-label="anchor" href="#scaling-training-and-test-data-the-same-way"><i class="fas fa-link"></i></a>
</h3>
<p>To illustrate the importance of consistent scaling, consider the <em>patient drug classification problem</em>, which involves two features: <code>age</code> and <code>sodium/potassium (Na/K) ratio</code>. Figure <a href="chapter-knn.html#fig:scatter-plot-ex-drug-2">7.3</a> shows a dataset of 200 patients as the training set, with three additional patients in the test set. Using the <code><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax()</a></code> function from the <strong>liver</strong> package, we demonstrate both correct and incorrect ways to scale the data:</p>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load the liver package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.uva.nl/profile/a.mohammadi">liver</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># A proper way to scale the data</span></span>
<span><span class="va">train_scaled</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax</a></span><span class="op">(</span><span class="va">train_data</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Age"</span>, <span class="st">"Ratio"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">test_scaled</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax</a></span><span class="op">(</span><span class="va">test_data</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Age"</span>, <span class="st">"Ratio"</span><span class="op">)</span>, min <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">train_data</span><span class="op">$</span><span class="va">Age</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">train_data</span><span class="op">$</span><span class="va">Ratio</span><span class="op">)</span><span class="op">)</span>, max <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">train_data</span><span class="op">$</span><span class="va">Age</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">train_data</span><span class="op">$</span><span class="va">Ratio</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># An incorrect way to scale the data</span></span>
<span><span class="va">train_scaled_wrongly</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax</a></span><span class="op">(</span><span class="va">train_data</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Age"</span>, <span class="st">"Ratio"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">test_scaled_wrongly</span>  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax</a></span><span class="op">(</span><span class="va">test_data</span> , col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Age"</span>, <span class="st">"Ratio"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The difference is illustrated in Figure <a href="chapter-knn.html#fig:ex-proper-scaling">7.6</a>. The middle panel shows the results of proper scaling, where the test set is scaled using the same parameters derived from the training set. This ensures consistency in distance calculations across both datasets. In contrast, the right panel shows improper scaling, where the test set is scaled independently. This leads to distorted relationships between the training and test data, which can cause unreliable predictions.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ex-proper-scaling"></span>
<img src="knn_files/figure-html/ex-proper-scaling-1.png" alt="Visualization illustrating the difference between proper scaling and improper scaling. The left panel shows the original data without scaling. The middle panel shows the results of proper scaling. The right panel shows the results of improper scaling." width="50%"><img src="knn_files/figure-html/ex-proper-scaling-2.png" alt="Visualization illustrating the difference between proper scaling and improper scaling. The left panel shows the original data without scaling. The middle panel shows the results of proper scaling. The right panel shows the results of improper scaling." width="50%"><img src="knn_files/figure-html/ex-proper-scaling-3.png" alt="Visualization illustrating the difference between proper scaling and improper scaling. The left panel shows the original data without scaling. The middle panel shows the results of proper scaling. The right panel shows the results of improper scaling." width="50%"><p class="caption">
Figure 7.6: Visualization illustrating the difference between proper scaling and improper scaling. The left panel shows the original data without scaling. The middle panel shows the results of proper scaling. The right panel shows the results of improper scaling.
</p>
</div>
<blockquote>
<p><strong>Key Insight:</strong> Proper scaling ensures that distance metrics remain valid, while improper scaling creates inconsistencies that undermine the kNN algorithm’s performance. <em>Scaling parameters should always be derived from the training set and applied consistently to the test set</em>. Neglecting this principle introduces data leakage, which distorts model evaluation and leads to overly optimistic performance estimates.</p>
</blockquote>
</div>
<div id="one-hot-encoding-1" class="section level3" number="7.5.3">
<h3>
<span class="header-section-number">7.5.3</span> One-Hot Encoding<a class="anchor" aria-label="anchor" href="#one-hot-encoding-1"><i class="fas fa-link"></i></a>
</h3>
<p>Categorical features, such as <em>marital status</em> or <em>subscription type</em>, cannot be directly used in distance calculations because distance metrics like Euclidean distance only work with numerical data. To overcome this, we use <em>one-hot encoding</em>, which converts categorical variables into binary (dummy) variables.</p>
<p>For example, the categorical variable <code>voice.plan</code>, with levels <code>yes</code> and <code>no</code>, can be encoded as:</p>
<p><span class="math display">\[
\text{voice.plan-yes} =
\begin{cases}
1 \quad \text{if voice plan = yes}  \\
0 \quad \text{if voice plan = no}
\end{cases}
\]</span></p>
<p>For categorical variables with more than two categories, one-hot encoding creates multiple binary columns—one for each category except one, to avoid redundancy. This approach ensures that the categorical variable is fully represented without introducing unnecessary correlations.</p>
<p>The <strong>liver</strong> package in R provides the <code><a href="https://rdrr.io/pkg/liver/man/one.hot.html">one.hot()</a></code> function to perform one-hot encoding automatically. It identifies categorical variables and encodes them into binary columns, leaving numerical features unchanged. Applying one-hot encoding to the <em>marital</em> variable in the <em>bank</em> dataset, for instance, adds binary columns for the encoded categories:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="chapter-knn.html#cb133-1" tabindex="-1"></a><span class="fu">data</span>(bank)</span>
<span id="cb133-2"><a href="chapter-knn.html#cb133-2" tabindex="-1"></a></span>
<span id="cb133-3"><a href="chapter-knn.html#cb133-3" tabindex="-1"></a><span class="co"># To perform one-hot encoding on the "marital" variable</span></span>
<span id="cb133-4"><a href="chapter-knn.html#cb133-4" tabindex="-1"></a>bank_encoded <span class="ot">&lt;-</span> <span class="fu">one.hot</span>(bank, <span class="at">cols =</span> <span class="fu">c</span>(<span class="st">"marital"</span>), <span class="at">dropCols =</span> <span class="cn">FALSE</span>)</span>
<span id="cb133-5"><a href="chapter-knn.html#cb133-5" tabindex="-1"></a></span>
<span id="cb133-6"><a href="chapter-knn.html#cb133-6" tabindex="-1"></a><span class="fu">str</span>(bank_encoded)</span>
<span id="cb133-7"><a href="chapter-knn.html#cb133-7" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">4521</span> obs. of  <span class="dv">20</span> variables<span class="sc">:</span></span>
<span id="cb133-8"><a href="chapter-knn.html#cb133-8" tabindex="-1"></a>    <span class="er">$</span> age             <span class="sc">:</span> int  <span class="dv">30</span> <span class="dv">33</span> <span class="dv">35</span> <span class="dv">30</span> <span class="dv">59</span> <span class="dv">35</span> <span class="dv">36</span> <span class="dv">39</span> <span class="dv">41</span> <span class="dv">43</span> ...</span>
<span id="cb133-9"><a href="chapter-knn.html#cb133-9" tabindex="-1"></a>    <span class="sc">$</span> job             <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">12</span> levels <span class="st">"admin."</span>,<span class="st">"blue-collar"</span>,..<span class="sc">:</span> <span class="dv">11</span> <span class="dv">8</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">2</span> <span class="dv">5</span> <span class="dv">7</span> <span class="dv">10</span> <span class="dv">3</span> <span class="dv">8</span> ...</span>
<span id="cb133-10"><a href="chapter-knn.html#cb133-10" tabindex="-1"></a>    <span class="sc">$</span> marital         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"divorced"</span>,<span class="st">"married"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb133-11"><a href="chapter-knn.html#cb133-11" tabindex="-1"></a>    <span class="sc">$</span> marital_divorced<span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb133-12"><a href="chapter-knn.html#cb133-12" tabindex="-1"></a>    <span class="sc">$</span> marital_married <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb133-13"><a href="chapter-knn.html#cb133-13" tabindex="-1"></a>    <span class="sc">$</span> marital_single  <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb133-14"><a href="chapter-knn.html#cb133-14" tabindex="-1"></a>    <span class="sc">$</span> education       <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">4</span> levels <span class="st">"primary"</span>,<span class="st">"secondary"</span>,..<span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">1</span> ...</span>
<span id="cb133-15"><a href="chapter-knn.html#cb133-15" tabindex="-1"></a>    <span class="sc">$</span> default         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb133-16"><a href="chapter-knn.html#cb133-16" tabindex="-1"></a>    <span class="sc">$</span> balance         <span class="sc">:</span> int  <span class="dv">1787</span> <span class="dv">4789</span> <span class="dv">1350</span> <span class="dv">1476</span> <span class="dv">0</span> <span class="dv">747</span> <span class="dv">307</span> <span class="dv">147</span> <span class="dv">221</span> <span class="sc">-</span><span class="dv">88</span> ...</span>
<span id="cb133-17"><a href="chapter-knn.html#cb133-17" tabindex="-1"></a>    <span class="sc">$</span> housing         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb133-18"><a href="chapter-knn.html#cb133-18" tabindex="-1"></a>    <span class="sc">$</span> loan            <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> ...</span>
<span id="cb133-19"><a href="chapter-knn.html#cb133-19" tabindex="-1"></a>    <span class="sc">$</span> contact         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"cellular"</span>,<span class="st">"telephone"</span>,..<span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">1</span> ...</span>
<span id="cb133-20"><a href="chapter-knn.html#cb133-20" tabindex="-1"></a>    <span class="sc">$</span> day             <span class="sc">:</span> int  <span class="dv">19</span> <span class="dv">11</span> <span class="dv">16</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">23</span> <span class="dv">14</span> <span class="dv">6</span> <span class="dv">14</span> <span class="dv">17</span> ...</span>
<span id="cb133-21"><a href="chapter-knn.html#cb133-21" tabindex="-1"></a>    <span class="sc">$</span> month           <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">12</span> levels <span class="st">"apr"</span>,<span class="st">"aug"</span>,<span class="st">"dec"</span>,..<span class="sc">:</span> <span class="dv">11</span> <span class="dv">9</span> <span class="dv">1</span> <span class="dv">7</span> <span class="dv">9</span> <span class="dv">4</span> <span class="dv">9</span> <span class="dv">9</span> <span class="dv">9</span> <span class="dv">1</span> ...</span>
<span id="cb133-22"><a href="chapter-knn.html#cb133-22" tabindex="-1"></a>    <span class="sc">$</span> duration        <span class="sc">:</span> int  <span class="dv">79</span> <span class="dv">220</span> <span class="dv">185</span> <span class="dv">199</span> <span class="dv">226</span> <span class="dv">141</span> <span class="dv">341</span> <span class="dv">151</span> <span class="dv">57</span> <span class="dv">313</span> ...</span>
<span id="cb133-23"><a href="chapter-knn.html#cb133-23" tabindex="-1"></a>    <span class="sc">$</span> campaign        <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> ...</span>
<span id="cb133-24"><a href="chapter-knn.html#cb133-24" tabindex="-1"></a>    <span class="sc">$</span> pdays           <span class="sc">:</span> int  <span class="sc">-</span><span class="dv">1</span> <span class="dv">339</span> <span class="dv">330</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">-</span><span class="dv">1</span> <span class="dv">176</span> <span class="dv">330</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">-</span><span class="dv">1</span> <span class="dv">147</span> ...</span>
<span id="cb133-25"><a href="chapter-knn.html#cb133-25" tabindex="-1"></a>    <span class="sc">$</span> previous        <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">2</span> ...</span>
<span id="cb133-26"><a href="chapter-knn.html#cb133-26" tabindex="-1"></a>    <span class="sc">$</span> poutcome        <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">4</span> levels <span class="st">"failure"</span>,<span class="st">"other"</span>,..<span class="sc">:</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">1</span> ...</span>
<span id="cb133-27"><a href="chapter-knn.html#cb133-27" tabindex="-1"></a>    <span class="sc">$</span> deposit         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> ...</span></code></pre></div>
<p>Setting <code>dropCols = FALSE</code> retains the original categorical column in the dataset, which may be useful for reference or debugging. However, in most cases, it is recommended to remove the original column after encoding to avoid redundancy.</p>
<blockquote>
<p><strong>Note:</strong> One-hot encoding is unnecessary for ordinal features, where the categories have a natural order (e.g., <code>low</code>, <code>medium</code>, <code>high</code>). Ordinal variables should instead be assigned numerical values that preserve their order (e.g., <code>low = 1</code>, <code>medium = 2</code>, <code>high = 3</code>), enabling the kNN algorithm to treat them as numerical features. For instance, if <code>education.level</code> has values {<code>low</code>, <code>medium</code>, <code>high</code>}, one-hot encoding would lose the natural progression between these categories. Instead, assigning numerical values (<code>low = 1</code>, <code>medium = 2</code>, <code>high = 3</code>) allows the algorithm to recognize the ordinal nature of the feature, preserving its relationship in distance calculations.</p>
</blockquote>
</div>
</div>
<div id="sec-kNN-churn" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> Applying kNN Algorithm in Practice<a class="anchor" aria-label="anchor" href="#sec-kNN-churn"><i class="fas fa-link"></i></a>
</h2>
<p>Applying the kNN algorithm involves several key steps, from preparing the data to training the model, making predictions, and evaluating its performance. In this section, we demonstrate the entire workflow using the <em>churn</em> dataset from the <strong>liver</strong> package in R. The target variable, <code>churn</code>, indicates whether a customer has churned (<code>yes</code>) or not (<code>no</code>), while the predictors include customer characteristics such as account length, international plan status, and call details. For details on exploratory data analysis, problem understanding, and data preparation for this dataset, refer to Section <a href="chapter-EDA.html#EDA-sec-churn">4.3</a>.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="chapter-knn.html#cb134-1" tabindex="-1"></a><span class="fu">str</span>(churn)</span>
<span id="cb134-2"><a href="chapter-knn.html#cb134-2" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">5000</span> obs. of  <span class="dv">20</span> variables<span class="sc">:</span></span>
<span id="cb134-3"><a href="chapter-knn.html#cb134-3" tabindex="-1"></a>    <span class="er">$</span> state         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">51</span> levels <span class="st">"AK"</span>,<span class="st">"AL"</span>,<span class="st">"AR"</span>,..<span class="sc">:</span> <span class="dv">17</span> <span class="dv">36</span> <span class="dv">32</span> <span class="dv">36</span> <span class="dv">37</span> <span class="dv">2</span> <span class="dv">20</span> <span class="dv">25</span> <span class="dv">19</span> <span class="dv">50</span> ...</span>
<span id="cb134-4"><a href="chapter-knn.html#cb134-4" tabindex="-1"></a>    <span class="sc">$</span> area.code     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"area_code_408"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> ...</span>
<span id="cb134-5"><a href="chapter-knn.html#cb134-5" tabindex="-1"></a>    <span class="sc">$</span> account.length<span class="sc">:</span> int  <span class="dv">128</span> <span class="dv">107</span> <span class="dv">137</span> <span class="dv">84</span> <span class="dv">75</span> <span class="dv">118</span> <span class="dv">121</span> <span class="dv">147</span> <span class="dv">117</span> <span class="dv">141</span> ...</span>
<span id="cb134-6"><a href="chapter-knn.html#cb134-6" tabindex="-1"></a>    <span class="sc">$</span> voice.plan    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> ...</span>
<span id="cb134-7"><a href="chapter-knn.html#cb134-7" tabindex="-1"></a>    <span class="sc">$</span> voice.messages<span class="sc">:</span> int  <span class="dv">25</span> <span class="dv">26</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">24</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">37</span> ...</span>
<span id="cb134-8"><a href="chapter-knn.html#cb134-8" tabindex="-1"></a>    <span class="sc">$</span> intl.plan     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> ...</span>
<span id="cb134-9"><a href="chapter-knn.html#cb134-9" tabindex="-1"></a>    <span class="sc">$</span> intl.mins     <span class="sc">:</span> num  <span class="dv">10</span> <span class="fl">13.7</span> <span class="fl">12.2</span> <span class="fl">6.6</span> <span class="fl">10.1</span> <span class="fl">6.3</span> <span class="fl">7.5</span> <span class="fl">7.1</span> <span class="fl">8.7</span> <span class="fl">11.2</span> ...</span>
<span id="cb134-10"><a href="chapter-knn.html#cb134-10" tabindex="-1"></a>    <span class="sc">$</span> intl.calls    <span class="sc">:</span> int  <span class="dv">3</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">7</span> <span class="dv">3</span> <span class="dv">6</span> <span class="dv">7</span> <span class="dv">6</span> <span class="dv">4</span> <span class="dv">5</span> ...</span>
<span id="cb134-11"><a href="chapter-knn.html#cb134-11" tabindex="-1"></a>    <span class="sc">$</span> intl.charge   <span class="sc">:</span> num  <span class="fl">2.7</span> <span class="fl">3.7</span> <span class="fl">3.29</span> <span class="fl">1.78</span> <span class="fl">2.73</span> <span class="fl">1.7</span> <span class="fl">2.03</span> <span class="fl">1.92</span> <span class="fl">2.35</span> <span class="fl">3.02</span> ...</span>
<span id="cb134-12"><a href="chapter-knn.html#cb134-12" tabindex="-1"></a>    <span class="sc">$</span> day.mins      <span class="sc">:</span> num  <span class="dv">265</span> <span class="dv">162</span> <span class="dv">243</span> <span class="dv">299</span> <span class="dv">167</span> ...</span>
<span id="cb134-13"><a href="chapter-knn.html#cb134-13" tabindex="-1"></a>    <span class="sc">$</span> day.calls     <span class="sc">:</span> int  <span class="dv">110</span> <span class="dv">123</span> <span class="dv">114</span> <span class="dv">71</span> <span class="dv">113</span> <span class="dv">98</span> <span class="dv">88</span> <span class="dv">79</span> <span class="dv">97</span> <span class="dv">84</span> ...</span>
<span id="cb134-14"><a href="chapter-knn.html#cb134-14" tabindex="-1"></a>    <span class="sc">$</span> day.charge    <span class="sc">:</span> num  <span class="fl">45.1</span> <span class="fl">27.5</span> <span class="fl">41.4</span> <span class="fl">50.9</span> <span class="fl">28.3</span> ...</span>
<span id="cb134-15"><a href="chapter-knn.html#cb134-15" tabindex="-1"></a>    <span class="sc">$</span> eve.mins      <span class="sc">:</span> num  <span class="fl">197.4</span> <span class="fl">195.5</span> <span class="fl">121.2</span> <span class="fl">61.9</span> <span class="fl">148.3</span> ...</span>
<span id="cb134-16"><a href="chapter-knn.html#cb134-16" tabindex="-1"></a>    <span class="sc">$</span> eve.calls     <span class="sc">:</span> int  <span class="dv">99</span> <span class="dv">103</span> <span class="dv">110</span> <span class="dv">88</span> <span class="dv">122</span> <span class="dv">101</span> <span class="dv">108</span> <span class="dv">94</span> <span class="dv">80</span> <span class="dv">111</span> ...</span>
<span id="cb134-17"><a href="chapter-knn.html#cb134-17" tabindex="-1"></a>    <span class="sc">$</span> eve.charge    <span class="sc">:</span> num  <span class="fl">16.78</span> <span class="fl">16.62</span> <span class="fl">10.3</span> <span class="fl">5.26</span> <span class="fl">12.61</span> ...</span>
<span id="cb134-18"><a href="chapter-knn.html#cb134-18" tabindex="-1"></a>    <span class="sc">$</span> night.mins    <span class="sc">:</span> num  <span class="dv">245</span> <span class="dv">254</span> <span class="dv">163</span> <span class="dv">197</span> <span class="dv">187</span> ...</span>
<span id="cb134-19"><a href="chapter-knn.html#cb134-19" tabindex="-1"></a>    <span class="sc">$</span> night.calls   <span class="sc">:</span> int  <span class="dv">91</span> <span class="dv">103</span> <span class="dv">104</span> <span class="dv">89</span> <span class="dv">121</span> <span class="dv">118</span> <span class="dv">118</span> <span class="dv">96</span> <span class="dv">90</span> <span class="dv">97</span> ...</span>
<span id="cb134-20"><a href="chapter-knn.html#cb134-20" tabindex="-1"></a>    <span class="sc">$</span> night.charge  <span class="sc">:</span> num  <span class="fl">11.01</span> <span class="fl">11.45</span> <span class="fl">7.32</span> <span class="fl">8.86</span> <span class="fl">8.41</span> ...</span>
<span id="cb134-21"><a href="chapter-knn.html#cb134-21" tabindex="-1"></a>    <span class="sc">$</span> customer.calls<span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">0</span> <span class="dv">3</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> ...</span>
<span id="cb134-22"><a href="chapter-knn.html#cb134-22" tabindex="-1"></a>    <span class="sc">$</span> churn         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span></code></pre></div>
<p>The dataset is a <em>data.frame</em> in <strong>R</strong> with 5000 observations and 19 predictor variables. The target variable, <em>churn</em>, indicates whether a customer has churned (<code>yes</code>) or not (<code>no</code>).</p>
<p>Based on insights gained in Section <a href="chapter-EDA.html#EDA-sec-churn">4.3</a>, we select the following features for building the kNN model:</p>
<p><code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>day.mins</code>, <code>eve.mins</code>, <code>night.mins</code>, and <code>customer.calls</code>.</p>
<p>The next steps involve preparing the data through feature scaling and one-hot encoding, followed by selecting an optimal <span class="math inline">\(k\)</span>, training the kNN model, and evaluating its performance.</p>
<div id="step-1-preparing-the-data" class="section level3" number="7.6.1">
<h3>
<span class="header-section-number">7.6.1</span> Step 1: Preparing the Data<a class="anchor" aria-label="anchor" href="#step-1-preparing-the-data"><i class="fas fa-link"></i></a>
</h3>
<p>The first step in applying kNN is to partition the dataset into training and test sets, followed by preprocessing tasks like feature scaling and one-hot encoding. Since the dataset is already cleaned and free of missing values, we can proceed directly with partitioning before applying these transformations.</p>
<p>We split the dataset into an 80% training set and a 20% test set using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function from the <strong>liver</strong> package:</p>
<div class="sourceCode" id="cb135"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">43</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_sets</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/partition.html">partition</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">churn</span>, ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_set</span> <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part1</span></span>
<span><span class="va">test_set</span>  <span class="op">=</span> <span class="va">data_sets</span><span class="op">$</span><span class="va">part2</span></span>
<span></span>
<span><span class="va">test_labels</span> <span class="op">=</span> <span class="va">test_set</span><span class="op">$</span><span class="va">churn</span></span></code></pre></div>
<p>The <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function randomly splits the dataset while maintaining the class distribution of the target variable, ensuring a representative training and test set. As we validated the partition in Section <a href="chapter-modeling.html#sec-validate-partition">6.4</a>, we can now proceed with feature scaling and one-hot encoding to ensure compatibility with the kNN algorithm.</p>
<div id="one-hot-encoding-2" class="section level4 unnumbered">
<h4>One-Hot Encoding<a class="anchor" aria-label="anchor" href="#one-hot-encoding-2"><i class="fas fa-link"></i></a>
</h4>
<p>Since kNN relies on distance calculations, categorical variables like <code>voice.plan</code> and <code>intl.plan</code> must be converted into numerical representations. One-hot encoding achieves this by creating binary (dummy) variables for each category. We apply the <code><a href="https://rdrr.io/pkg/liver/man/one.hot.html">one.hot()</a></code> function from the <strong>liver</strong> package to transform categorical features into a numerical format suitable for kNN:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="chapter-knn.html#cb136-1" tabindex="-1"></a>categorical_vars <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"voice.plan"</span>, <span class="st">"intl.plan"</span>)</span>
<span id="cb136-2"><a href="chapter-knn.html#cb136-2" tabindex="-1"></a></span>
<span id="cb136-3"><a href="chapter-knn.html#cb136-3" tabindex="-1"></a>train_onehot <span class="ot">=</span> <span class="fu">one.hot</span>(train_set, <span class="at">cols =</span> categorical_vars)</span>
<span id="cb136-4"><a href="chapter-knn.html#cb136-4" tabindex="-1"></a>test_onehot  <span class="ot">=</span> <span class="fu">one.hot</span>(test_set,  <span class="at">cols =</span> categorical_vars)</span>
<span id="cb136-5"><a href="chapter-knn.html#cb136-5" tabindex="-1"></a></span>
<span id="cb136-6"><a href="chapter-knn.html#cb136-6" tabindex="-1"></a><span class="fu">str</span>(test_onehot)</span>
<span id="cb136-7"><a href="chapter-knn.html#cb136-7" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">1000</span> obs. of  <span class="dv">22</span> variables<span class="sc">:</span></span>
<span id="cb136-8"><a href="chapter-knn.html#cb136-8" tabindex="-1"></a>    <span class="er">$</span> state         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">51</span> levels <span class="st">"AK"</span>,<span class="st">"AL"</span>,<span class="st">"AR"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">50</span> <span class="dv">14</span> <span class="dv">46</span> <span class="dv">10</span> <span class="dv">4</span> <span class="dv">25</span> <span class="dv">15</span> <span class="dv">11</span> <span class="dv">32</span> ...</span>
<span id="cb136-9"><a href="chapter-knn.html#cb136-9" tabindex="-1"></a>    <span class="sc">$</span> area.code     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"area_code_408"</span>,..<span class="sc">:</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> ...</span>
<span id="cb136-10"><a href="chapter-knn.html#cb136-10" tabindex="-1"></a>    <span class="sc">$</span> account.length<span class="sc">:</span> int  <span class="dv">118</span> <span class="dv">141</span> <span class="dv">85</span> <span class="dv">76</span> <span class="dv">147</span> <span class="dv">130</span> <span class="dv">20</span> <span class="dv">142</span> <span class="dv">72</span> <span class="dv">149</span> ...</span>
<span id="cb136-11"><a href="chapter-knn.html#cb136-11" tabindex="-1"></a>    <span class="sc">$</span> voice.plan_yes<span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> ...</span>
<span id="cb136-12"><a href="chapter-knn.html#cb136-12" tabindex="-1"></a>    <span class="sc">$</span> voice.plan_no <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> ...</span>
<span id="cb136-13"><a href="chapter-knn.html#cb136-13" tabindex="-1"></a>    <span class="sc">$</span> voice.messages<span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">37</span> <span class="dv">27</span> <span class="dv">33</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">37</span> <span class="dv">0</span> ...</span>
<span id="cb136-14"><a href="chapter-knn.html#cb136-14" tabindex="-1"></a>    <span class="sc">$</span> intl.plan_yes <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb136-15"><a href="chapter-knn.html#cb136-15" tabindex="-1"></a>    <span class="sc">$</span> intl.plan_no  <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb136-16"><a href="chapter-knn.html#cb136-16" tabindex="-1"></a>    <span class="sc">$</span> intl.mins     <span class="sc">:</span> num  <span class="fl">6.3</span> <span class="fl">11.2</span> <span class="fl">13.8</span> <span class="dv">10</span> <span class="fl">10.6</span> <span class="fl">9.5</span> <span class="fl">6.3</span> <span class="fl">14.2</span> <span class="fl">14.7</span> <span class="fl">11.1</span> ...</span>
<span id="cb136-17"><a href="chapter-knn.html#cb136-17" tabindex="-1"></a>    <span class="sc">$</span> intl.calls    <span class="sc">:</span> int  <span class="dv">6</span> <span class="dv">5</span> <span class="dv">4</span> <span class="dv">5</span> <span class="dv">4</span> <span class="dv">19</span> <span class="dv">6</span> <span class="dv">6</span> <span class="dv">6</span> <span class="dv">9</span> ...</span>
<span id="cb136-18"><a href="chapter-knn.html#cb136-18" tabindex="-1"></a>    <span class="sc">$</span> intl.charge   <span class="sc">:</span> num  <span class="fl">1.7</span> <span class="fl">3.02</span> <span class="fl">3.73</span> <span class="fl">2.7</span> <span class="fl">2.86</span> <span class="fl">2.57</span> <span class="fl">1.7</span> <span class="fl">3.83</span> <span class="fl">3.97</span> <span class="dv">3</span> ...</span>
<span id="cb136-19"><a href="chapter-knn.html#cb136-19" tabindex="-1"></a>    <span class="sc">$</span> day.mins      <span class="sc">:</span> num  <span class="dv">223</span> <span class="dv">259</span> <span class="dv">196</span> <span class="dv">190</span> <span class="dv">155</span> ...</span>
<span id="cb136-20"><a href="chapter-knn.html#cb136-20" tabindex="-1"></a>    <span class="sc">$</span> day.calls     <span class="sc">:</span> int  <span class="dv">98</span> <span class="dv">84</span> <span class="dv">139</span> <span class="dv">66</span> <span class="dv">117</span> <span class="dv">112</span> <span class="dv">109</span> <span class="dv">95</span> <span class="dv">80</span> <span class="dv">94</span> ...</span>
<span id="cb136-21"><a href="chapter-knn.html#cb136-21" tabindex="-1"></a>    <span class="sc">$</span> day.charge    <span class="sc">:</span> num  <span class="dv">38</span> <span class="dv">44</span> <span class="fl">33.4</span> <span class="fl">32.2</span> <span class="fl">26.4</span> ...</span>
<span id="cb136-22"><a href="chapter-knn.html#cb136-22" tabindex="-1"></a>    <span class="sc">$</span> eve.mins      <span class="sc">:</span> num  <span class="dv">221</span> <span class="dv">222</span> <span class="dv">281</span> <span class="dv">213</span> <span class="dv">240</span> ...</span>
<span id="cb136-23"><a href="chapter-knn.html#cb136-23" tabindex="-1"></a>    <span class="sc">$</span> eve.calls     <span class="sc">:</span> int  <span class="dv">101</span> <span class="dv">111</span> <span class="dv">90</span> <span class="dv">65</span> <span class="dv">93</span> <span class="dv">99</span> <span class="dv">84</span> <span class="dv">63</span> <span class="dv">102</span> <span class="dv">92</span> ...</span>
<span id="cb136-24"><a href="chapter-knn.html#cb136-24" tabindex="-1"></a>    <span class="sc">$</span> eve.charge    <span class="sc">:</span> num  <span class="fl">18.8</span> <span class="fl">18.9</span> <span class="fl">23.9</span> <span class="fl">18.1</span> <span class="fl">20.4</span> ...</span>
<span id="cb136-25"><a href="chapter-knn.html#cb136-25" tabindex="-1"></a>    <span class="sc">$</span> night.mins    <span class="sc">:</span> num  <span class="fl">203.9</span> <span class="fl">326.4</span> <span class="fl">89.3</span> <span class="fl">165.7</span> <span class="fl">208.8</span> ...</span>
<span id="cb136-26"><a href="chapter-knn.html#cb136-26" tabindex="-1"></a>    <span class="sc">$</span> night.calls   <span class="sc">:</span> int  <span class="dv">118</span> <span class="dv">97</span> <span class="dv">75</span> <span class="dv">108</span> <span class="dv">133</span> <span class="dv">78</span> <span class="dv">102</span> <span class="dv">148</span> <span class="dv">71</span> <span class="dv">108</span> ...</span>
<span id="cb136-27"><a href="chapter-knn.html#cb136-27" tabindex="-1"></a>    <span class="sc">$</span> night.charge  <span class="sc">:</span> num  <span class="fl">9.18</span> <span class="fl">14.69</span> <span class="fl">4.02</span> <span class="fl">7.46</span> <span class="fl">9.4</span> ...</span>
<span id="cb136-28"><a href="chapter-knn.html#cb136-28" tabindex="-1"></a>    <span class="sc">$</span> customer.calls<span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">1</span> ...</span>
<span id="cb136-29"><a href="chapter-knn.html#cb136-29" tabindex="-1"></a>    <span class="sc">$</span> churn         <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"yes"</span>,<span class="st">"no"</span><span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span></code></pre></div>
<p>For binary categorical variables, one-hot encoding produces two columns (e.g., <code>voice.plan_yes</code> and <code>voice.plan_no</code>). Since one variable is always the complement of the other, we retain only one (e.g., <code>voice.plan_yes</code>) to avoid redundancy.</p>
</div>
<div id="feature-scaling-2" class="section level4 unnumbered">
<h4>Feature Scaling<a class="anchor" aria-label="anchor" href="#feature-scaling-2"><i class="fas fa-link"></i></a>
</h4>
<p>Since kNN calculates distances between data points, features with larger numerical ranges can disproportionately influence the results. Scaling ensures that all features contribute equally to distance calculations, preventing dominance by high-magnitude features.</p>
<p>To standardize the numerical variables, we apply min-max scaling using the <code><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax()</a></code> function from the <strong>liver</strong> package. Scaling parameters (minimum and maximum values) must be computed from the training set and then applied consistently to both the training and test sets. This prevents data leakage, which occurs when test data influences the training process, leading to misleadingly high performance estimates.</p>
<div class="sourceCode" id="cb137"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">numeric_vars</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"account.length"</span>, <span class="st">"voice.messages"</span>, <span class="st">"intl.mins"</span>, <span class="st">"intl.calls"</span>, </span>
<span>                 <span class="st">"day.mins"</span>, <span class="st">"day.calls"</span>, <span class="st">"eve.mins"</span>, <span class="st">"eve.calls"</span>, </span>
<span>                 <span class="st">"night.mins"</span>, <span class="st">"night.calls"</span>, <span class="st">"customer.calls"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">min_train</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">train_set</span><span class="op">[</span>, <span class="va">numeric_vars</span><span class="op">]</span>, <span class="va">min</span><span class="op">)</span></span>
<span><span class="va">max_train</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">train_set</span><span class="op">[</span>, <span class="va">numeric_vars</span><span class="op">]</span>, <span class="va">max</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_scaled</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax</a></span><span class="op">(</span><span class="va">train_onehot</span>, col <span class="op">=</span> <span class="va">numeric_vars</span>, min <span class="op">=</span> <span class="va">min_train</span>, max <span class="op">=</span> <span class="va">max_train</span><span class="op">)</span></span>
<span><span class="va">test_scaled</span>  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax</a></span><span class="op">(</span><span class="va">test_onehot</span>,  col <span class="op">=</span> <span class="va">numeric_vars</span>, min <span class="op">=</span> <span class="va">min_train</span>, max <span class="op">=</span> <span class="va">max_train</span><span class="op">)</span></span></code></pre></div>
<p>The <code><a href="https://rdrr.io/pkg/liver/man/minmax.html">minmax()</a></code> function normalizes the numerical features to the range <span class="math inline">\([0, 1]\)</span>, ensuring they have comparable scales while preserving relative differences. This transformation prevents any single feature from dominating the kNN distance calculations, leading to more balanced and accurate predictions.</p>
</div>
</div>
<div id="step-2-choosing-an-optimal-k" class="section level3" number="7.6.2">
<h3>
<span class="header-section-number">7.6.2</span> Step 2: Choosing an Optimal <span class="math inline">\(k\)</span><a class="anchor" aria-label="anchor" href="#step-2-choosing-an-optimal-k"><i class="fas fa-link"></i></a>
</h3>
<p>The choice of <span class="math inline">\(k\)</span> determines the trade-off between capturing local patterns and generalizing well to unseen data. Selecting an inappropriate <span class="math inline">\(k\)</span> may result in overfitting (if <span class="math inline">\(k\)</span> is too small) or oversmoothing (if <span class="math inline">\(k\)</span> is too large). To identify the optimal <span class="math inline">\(k\)</span>, we evaluate the model’s accuracy for different values of <span class="math inline">\(k\)</span> using the <code><a href="https://rdrr.io/pkg/liver/man/kNN.plot.html">kNN.plot()</a></code> function:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="chapter-knn.html#cb138-1" tabindex="-1"></a>formula <span class="ot">=</span> churn <span class="sc">~</span> account.length <span class="sc">+</span> voice.plan_yes <span class="sc">+</span> voice.messages <span class="sc">+</span> </span>
<span id="cb138-2"><a href="chapter-knn.html#cb138-2" tabindex="-1"></a>                  intl.plan_yes <span class="sc">+</span> intl.mins <span class="sc">+</span> intl.calls <span class="sc">+</span> </span>
<span id="cb138-3"><a href="chapter-knn.html#cb138-3" tabindex="-1"></a>                  day.mins <span class="sc">+</span> day.calls <span class="sc">+</span> eve.mins <span class="sc">+</span> eve.calls <span class="sc">+</span> </span>
<span id="cb138-4"><a href="chapter-knn.html#cb138-4" tabindex="-1"></a>                  night.mins <span class="sc">+</span> night.calls <span class="sc">+</span> customer.calls</span>
<span id="cb138-5"><a href="chapter-knn.html#cb138-5" tabindex="-1"></a></span>
<span id="cb138-6"><a href="chapter-knn.html#cb138-6" tabindex="-1"></a><span class="fu">kNN.plot</span>(<span class="at">formula =</span> formula, <span class="at">train =</span> train_scaled, <span class="at">test =</span> test_scaled, </span>
<span id="cb138-7"><a href="chapter-knn.html#cb138-7" tabindex="-1"></a>         <span class="at">k.max =</span> <span class="dv">30</span>, <span class="at">set.seed =</span> <span class="dv">43</span>)</span>
<span id="cb138-8"><a href="chapter-knn.html#cb138-8" tabindex="-1"></a>   Setting levels<span class="sc">:</span> reference <span class="ot">=</span> <span class="st">"yes"</span>, case <span class="ot">=</span> <span class="st">"no"</span></span></code></pre></div>
<div class="inline-figure"><img src="knn_files/figure-html/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The <code><a href="https://rdrr.io/pkg/liver/man/kNN.plot.html">kNN.plot()</a></code> function visualizes the relationship between <span class="math inline">\(k\)</span> and model accuracy, helping us determine the value of <span class="math inline">\(k\)</span> that balances model complexity and generalization. By examining the plot, we observe that the highest accuracy is achieved when <span class="math inline">\(k = 5\)</span>. This choice maintains sufficient flexibility to capture meaningful patterns while avoiding excessive sensitivity to outliers.</p>
</div>
<div id="step-3-training-the-model-and-making-predictions" class="section level3" number="7.6.3">
<h3>
<span class="header-section-number">7.6.3</span> Step 3: Training the Model and Making Predictions<a class="anchor" aria-label="anchor" href="#step-3-training-the-model-and-making-predictions"><i class="fas fa-link"></i></a>
</h3>
<p>Since we identified <span class="math inline">\(k = 5\)</span> as the optimal value in Step 2, we now proceed to train the kNN model and make predictions on the test set. To apply the kNN algorithm in <strong>R</strong>, we use the <code><a href="https://rdrr.io/pkg/liver/man/kNN.html">kNN()</a></code> function from the <strong>liver</strong> package as follows:</p>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">kNN_predict</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/liver/man/kNN.html">kNN</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula</span>, train <span class="op">=</span> <span class="va">train_scaled</span>, test <span class="op">=</span> <span class="va">test_scaled</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code></pre></div>
<p>The <code><a href="https://rdrr.io/pkg/liver/man/kNN.html">kNN()</a></code> function automates the kNN classification process by computing distances between each test observation and all training data points. It then selects the 5 closest neighbors based on the chosen distance metric and assigns the most frequently occurring class among them as the predicted label. Since test data was not used during training, these predictions provide an unbiased estimate of how well the model generalizes to new observations.</p>
</div>
<div id="step-4-evaluating-the-model" class="section level3" number="7.6.4">
<h3>
<span class="header-section-number">7.6.4</span> Step 4: Evaluating the Model<a class="anchor" aria-label="anchor" href="#step-4-evaluating-the-model"><i class="fas fa-link"></i></a>
</h3>
<p>Evaluating model performance is crucial to ensure that the kNN algorithm generalizes well to unseen data and makes reliable predictions. A confusion matrix provides a summary of correct and incorrect predictions by comparing the predicted labels to the actual labels in the test set. We compute it using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> function from the <strong>liver</strong> package:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="chapter-knn.html#cb140-1" tabindex="-1"></a><span class="fu">conf.mat</span>(kNN_predict, test_labels, <span class="at">reference =</span> <span class="st">"yes"</span>)</span>
<span id="cb140-2"><a href="chapter-knn.html#cb140-2" tabindex="-1"></a>          Actual</span>
<span id="cb140-3"><a href="chapter-knn.html#cb140-3" tabindex="-1"></a>   Predict yes  no</span>
<span id="cb140-4"><a href="chapter-knn.html#cb140-4" tabindex="-1"></a>       yes  <span class="dv">54</span>   <span class="dv">7</span></span>
<span id="cb140-5"><a href="chapter-knn.html#cb140-5" tabindex="-1"></a>       no   <span class="dv">83</span> <span class="dv">856</span></span></code></pre></div>
<p>From the confusion matrix, we see that the model correctly classified 910 instances, while 90 instances were misclassified. This summary helps assess model performance and identify areas for improvement.</p>
</div>
<div id="final-remarks" class="section level3 unnumbered">
<h3>Final Remarks<a class="anchor" aria-label="anchor" href="#final-remarks"><i class="fas fa-link"></i></a>
</h3>
<p>This step-by-step implementation of kNN highlighted the crucial role of data preprocessing, parameter tuning, and model evaluation in achieving reliable predictions. Key factors such as the choice of <span class="math inline">\(k\)</span>, feature scaling, and encoding categorical data significantly influence the accuracy and generalization of kNN models.</p>
<p>While the confusion matrix provides an initial assessment of model performance, additional evaluation metrics such as accuracy, precision, recall, and F1-score offer deeper insights. These aspects will be explored in detail in the next chapter (Chapter <a href="chapter-evaluation.html#chapter-evaluation">8</a>).</p>
</div>
</div>
<div id="key-takeaways-from-knn" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> Key Takeaways from kNN<a class="anchor" aria-label="anchor" href="#key-takeaways-from-knn"><i class="fas fa-link"></i></a>
</h2>
<p>In this chapter, we explored the k-Nearest Neighbors (kNN) algorithm, a simple yet effective method for solving classification problems. We began by revisiting the concept of classification and its real-world applications, highlighting the difference between binary and multi-class problems. We then examined the mechanics of kNN, emphasizing its reliance on distance metrics to identify the most similar data points. Essential preprocessing steps, such as feature scaling and one-hot encoding, were discussed to ensure accurate and meaningful distance calculations. We also covered the importance of selecting an optimal <span class="math inline">\(k\)</span> value and demonstrated the implementation of kNN using the <strong>liver</strong> package in R with the <em>churn</em> dataset. Through practical examples, we reinforced the significance of proper data preparation and parameter tuning for building reliable classification models.</p>
<p>The simplicity and interpretability of kNN make it an excellent starting point for understanding classification and exploring dataset structures. However, the algorithm has notable limitations, including sensitivity to noise, computational inefficiency with large datasets, and the necessity for proper scaling and feature selection. These challenges make kNN less practical for large-scale applications, but it remains a valuable tool for small to medium-sized datasets and serves as a benchmark for evaluating more advanced algorithms.</p>
<p>While kNN is intuitive and easy to implement, its prediction speed and scalability constraints often limit its use in modern, large-scale datasets. Nonetheless, it is a useful baseline method and a stepping stone to more sophisticated techniques. In the upcoming chapters, we will explore advanced classification algorithms, such as Decision Trees, Random Forests, and Logistic Regression, which address the limitations of kNN and provide enhanced performance and scalability for a wide range of applications.</p>
</div>
<div id="exercises-5" class="section level2" number="7.8">
<h2>
<span class="header-section-number">7.8</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-5"><i class="fas fa-link"></i></a>
</h2>
<div id="conceptual-questions-3" class="section level3 unnumbered">
<h3>Conceptual Questions<a class="anchor" aria-label="anchor" href="#conceptual-questions-3"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>Explain the fundamental difference between classification and regression. Provide an example of each.<br>
</li>
<li>What are the key steps in applying the kNN algorithm?<br>
</li>
<li>Why is the choice of <span class="math inline">\(k\)</span> important in kNN, and what happens when <span class="math inline">\(k\)</span> is too small or too large?<br>
</li>
<li>Describe the role of distance metrics in kNN classification. Why is Euclidean distance commonly used?<br>
</li>
<li>What are the limitations of kNN compared to other classification algorithms?<br>
</li>
<li>How does feature scaling impact the performance of kNN? Why is it necessary?<br>
</li>
<li>Describe how one-hot encoding is used in kNN. Why is it necessary for categorical variables?<br>
</li>
<li>How does kNN handle missing values? What strategies can be used to deal with missing data?<br>
</li>
<li>Explain the difference between <em>lazy learning</em> (such as kNN) and <em>eager learning</em> (such as decision trees or logistic regression).<br>
</li>
<li>Why is kNN considered a non-parametric algorithm? What advantages and disadvantages does this bring?</li>
</ol>
</div>
<div id="hands-on-practice-applying-knn-to-the-bank-dataset" class="section level3 unnumbered">
<h3>Hands-On Practice: Applying kNN to the Bank Dataset<a class="anchor" aria-label="anchor" href="#hands-on-practice-applying-knn-to-the-bank-dataset"><i class="fas fa-link"></i></a>
</h3>
<p>Here, we want to apply the concepts covered in this chapter using the <em>bank</em> dataset from the <strong>liver</strong> package. The <em>bank</em> dataset contains customer information, including demographics and financial details, and the target variable <em>deposit</em> indicates whether a customer subscribed to a term deposit. This dataset is well-suited for classification problems and provides an opportunity to practice kNN in real-world scenarios.</p>
<p>To begin, load the necessary package and dataset:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="chapter-knn.html#cb141-1" tabindex="-1"></a><span class="fu">library</span>(liver)</span>
<span id="cb141-2"><a href="chapter-knn.html#cb141-2" tabindex="-1"></a></span>
<span id="cb141-3"><a href="chapter-knn.html#cb141-3" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb141-4"><a href="chapter-knn.html#cb141-4" tabindex="-1"></a><span class="fu">data</span>(bank)</span>
<span id="cb141-5"><a href="chapter-knn.html#cb141-5" tabindex="-1"></a></span>
<span id="cb141-6"><a href="chapter-knn.html#cb141-6" tabindex="-1"></a><span class="co"># View the structure of the dataset</span></span>
<span id="cb141-7"><a href="chapter-knn.html#cb141-7" tabindex="-1"></a><span class="fu">str</span>(bank)</span>
<span id="cb141-8"><a href="chapter-knn.html#cb141-8" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">4521</span> obs. of  <span class="dv">17</span> variables<span class="sc">:</span></span>
<span id="cb141-9"><a href="chapter-knn.html#cb141-9" tabindex="-1"></a>    <span class="er">$</span> age      <span class="sc">:</span> int  <span class="dv">30</span> <span class="dv">33</span> <span class="dv">35</span> <span class="dv">30</span> <span class="dv">59</span> <span class="dv">35</span> <span class="dv">36</span> <span class="dv">39</span> <span class="dv">41</span> <span class="dv">43</span> ...</span>
<span id="cb141-10"><a href="chapter-knn.html#cb141-10" tabindex="-1"></a>    <span class="sc">$</span> job      <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">12</span> levels <span class="st">"admin."</span>,<span class="st">"blue-collar"</span>,..<span class="sc">:</span> <span class="dv">11</span> <span class="dv">8</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">2</span> <span class="dv">5</span> <span class="dv">7</span> <span class="dv">10</span> <span class="dv">3</span> <span class="dv">8</span> ...</span>
<span id="cb141-11"><a href="chapter-knn.html#cb141-11" tabindex="-1"></a>    <span class="sc">$</span> marital  <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"divorced"</span>,<span class="st">"married"</span>,..<span class="sc">:</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb141-12"><a href="chapter-knn.html#cb141-12" tabindex="-1"></a>    <span class="sc">$</span> education<span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">4</span> levels <span class="st">"primary"</span>,<span class="st">"secondary"</span>,..<span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">1</span> ...</span>
<span id="cb141-13"><a href="chapter-knn.html#cb141-13" tabindex="-1"></a>    <span class="sc">$</span> default  <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> ...</span>
<span id="cb141-14"><a href="chapter-knn.html#cb141-14" tabindex="-1"></a>    <span class="sc">$</span> balance  <span class="sc">:</span> int  <span class="dv">1787</span> <span class="dv">4789</span> <span class="dv">1350</span> <span class="dv">1476</span> <span class="dv">0</span> <span class="dv">747</span> <span class="dv">307</span> <span class="dv">147</span> <span class="dv">221</span> <span class="sc">-</span><span class="dv">88</span> ...</span>
<span id="cb141-15"><a href="chapter-knn.html#cb141-15" tabindex="-1"></a>    <span class="sc">$</span> housing  <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">2</span> ...</span>
<span id="cb141-16"><a href="chapter-knn.html#cb141-16" tabindex="-1"></a>    <span class="sc">$</span> loan     <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">2</span> ...</span>
<span id="cb141-17"><a href="chapter-knn.html#cb141-17" tabindex="-1"></a>    <span class="sc">$</span> contact  <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">3</span> levels <span class="st">"cellular"</span>,<span class="st">"telephone"</span>,..<span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">1</span> ...</span>
<span id="cb141-18"><a href="chapter-knn.html#cb141-18" tabindex="-1"></a>    <span class="sc">$</span> day      <span class="sc">:</span> int  <span class="dv">19</span> <span class="dv">11</span> <span class="dv">16</span> <span class="dv">3</span> <span class="dv">5</span> <span class="dv">23</span> <span class="dv">14</span> <span class="dv">6</span> <span class="dv">14</span> <span class="dv">17</span> ...</span>
<span id="cb141-19"><a href="chapter-knn.html#cb141-19" tabindex="-1"></a>    <span class="sc">$</span> month    <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">12</span> levels <span class="st">"apr"</span>,<span class="st">"aug"</span>,<span class="st">"dec"</span>,..<span class="sc">:</span> <span class="dv">11</span> <span class="dv">9</span> <span class="dv">1</span> <span class="dv">7</span> <span class="dv">9</span> <span class="dv">4</span> <span class="dv">9</span> <span class="dv">9</span> <span class="dv">9</span> <span class="dv">1</span> ...</span>
<span id="cb141-20"><a href="chapter-knn.html#cb141-20" tabindex="-1"></a>    <span class="sc">$</span> duration <span class="sc">:</span> int  <span class="dv">79</span> <span class="dv">220</span> <span class="dv">185</span> <span class="dv">199</span> <span class="dv">226</span> <span class="dv">141</span> <span class="dv">341</span> <span class="dv">151</span> <span class="dv">57</span> <span class="dv">313</span> ...</span>
<span id="cb141-21"><a href="chapter-knn.html#cb141-21" tabindex="-1"></a>    <span class="sc">$</span> campaign <span class="sc">:</span> int  <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">2</span> <span class="dv">1</span> ...</span>
<span id="cb141-22"><a href="chapter-knn.html#cb141-22" tabindex="-1"></a>    <span class="sc">$</span> pdays    <span class="sc">:</span> int  <span class="sc">-</span><span class="dv">1</span> <span class="dv">339</span> <span class="dv">330</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">-</span><span class="dv">1</span> <span class="dv">176</span> <span class="dv">330</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">-</span><span class="dv">1</span> <span class="dv">147</span> ...</span>
<span id="cb141-23"><a href="chapter-knn.html#cb141-23" tabindex="-1"></a>    <span class="sc">$</span> previous <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">2</span> ...</span>
<span id="cb141-24"><a href="chapter-knn.html#cb141-24" tabindex="-1"></a>    <span class="sc">$</span> poutcome <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">4</span> levels <span class="st">"failure"</span>,<span class="st">"other"</span>,..<span class="sc">:</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">1</span> <span class="dv">2</span> <span class="dv">4</span> <span class="dv">4</span> <span class="dv">1</span> ...</span>
<span id="cb141-25"><a href="chapter-knn.html#cb141-25" tabindex="-1"></a>    <span class="sc">$</span> deposit  <span class="sc">:</span> Factor w<span class="sc">/</span> <span class="dv">2</span> levels <span class="st">"no"</span>,<span class="st">"yes"</span><span class="sc">:</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> ...</span></code></pre></div>
<div id="data-exploration-and-preparation" class="section level4 unnumbered">
<h4>Data Exploration and Preparation<a class="anchor" aria-label="anchor" href="#data-exploration-and-preparation"><i class="fas fa-link"></i></a>
</h4>
<ol start="11" style="list-style-type: decimal">
<li>Load the <em>bank</em> dataset and display its structure. Identify the target variable and the predictor variables.<br>
</li>
<li>Count the number of instances where a customer subscribed to a term deposit (<em>deposit = “yes”</em>) versus those who did not (<em>deposit = “no”</em>). What does this tell you about the dataset?<br>
</li>
<li>Identify nominal variables in the dataset. Convert them into numerical features using one-hot encoding with the <code><a href="https://rdrr.io/pkg/liver/man/one.hot.html">one.hot()</a></code> function.<br>
</li>
<li>Partition the dataset into 80% training and 20% testing sets using the <code><a href="https://rdrr.io/pkg/liver/man/partition.html">partition()</a></code> function. Ensure the target variable remains proportionally distributed in both sets.<br>
</li>
<li>Validate the partitioning by comparing the class distribution of the target variable in the training and test sets.<br>
</li>
<li>Apply min-max scaling to numerical variables in both training and test sets. Ensure that the scaling parameters are derived from the training set only.</li>
</ol>
</div>
<div id="choosing-the-optimal-k" class="section level4 unnumbered">
<h4>Choosing the Optimal <span class="math inline">\(k\)</span><a class="anchor" aria-label="anchor" href="#choosing-the-optimal-k"><i class="fas fa-link"></i></a>
</h4>
<ol start="17" style="list-style-type: decimal">
<li>Use the <code><a href="https://rdrr.io/pkg/liver/man/kNN.plot.html">kNN.plot()</a></code> function to determine the optimal <span class="math inline">\(k\)</span> value for classifying <code>deposit</code> in the <em>bank</em> dataset.<br>
</li>
<li>What is the best <span class="math inline">\(k\)</span> value based on accuracy? How does accuracy change as <span class="math inline">\(k\)</span> increases?</li>
<li>Interpret the meaning of the accuracy curve generated by <code><a href="https://rdrr.io/pkg/liver/man/kNN.plot.html">kNN.plot()</a></code>. What patterns do you observe?</li>
</ol>
</div>
<div id="building-and-evaluating-the-knn-model" class="section level4 unnumbered">
<h4>Building and Evaluating the kNN Model<a class="anchor" aria-label="anchor" href="#building-and-evaluating-the-knn-model"><i class="fas fa-link"></i></a>
</h4>
<ol start="20" style="list-style-type: decimal">
<li>Train a kNN model using the optimal <span class="math inline">\(k\)</span> and make predictions on the test set.<br>
</li>
<li>Generate a confusion matrix for the kNN model predictions using the <code><a href="https://rdrr.io/pkg/liver/man/conf.mat.html">conf.mat()</a></code> function. Interpret the results.<br>
</li>
<li>Calculate the accuracy of the kNN model. How well does it perform in predicting <em>deposit</em>?<br>
</li>
<li>Besides accuracy, what other evaluation metrics (e.g., precision, recall, F1-score) would be useful for assessing kNN performance in the <em>bank</em> dataset? Compute and interpret these metrics.<br>
</li>
<li>Compare the performance of kNN with different values of <span class="math inline">\(k\)</span> (e.g., <span class="math inline">\(k = 1, 5, 15, 25\)</span>). How does changing <span class="math inline">\(k\)</span> affect the classification results?<br>
</li>
<li>Train a kNN model using only a subset of features: <code>age</code>, <code>balance</code>, <code>duration</code>, and <code>campaign</code>. Compare its accuracy with the full-feature model. What does this tell you about feature selection?<br>
</li>
<li>Compare the accuracy of kNN when using min-max scaling versus z-score standardization. How does the choice of scaling method impact model performance?</li>
</ol>
</div>
</div>
<div id="critical-thinking-and-real-world-applications" class="section level3 unnumbered">
<h3>Critical Thinking and Real-World Applications<a class="anchor" aria-label="anchor" href="#critical-thinking-and-real-world-applications"><i class="fas fa-link"></i></a>
</h3>
<ol start="27" style="list-style-type: decimal">
<li>Suppose you are building a fraud detection system for a bank. Would kNN be a suitable algorithm? What are its advantages and limitations in this context?<br>
</li>
<li>How would you handle imbalanced classes in the <em>bank</em> dataset? What strategies could improve classification performance?<br>
</li>
<li>In a high-dimensional dataset with hundreds of features, would kNN still be an effective approach? Why or why not?<br>
</li>
<li>Imagine you are working with a dataset where new data points arrive in real-time. What challenges would kNN face, and how could they be addressed?<br>
</li>
<li>If a financial institution wants to classify customers into different risk categories for loan approval, what preprocessing steps would be essential before applying kNN?<br>
</li>
<li>In a dataset where some features are irrelevant or redundant, how could you improve kNN’s performance? What feature selection methods would you use?<br>
</li>
<li>If computation time is a concern, what strategies could you apply to make kNN more efficient for large datasets?<br>
</li>
<li>Suppose kNN is performing poorly on the <em>bank</em> dataset. What possible reasons could explain this, and how would you troubleshoot the issue?</li>
</ol>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="chapter-modeling.html"><span class="header-section-number">6</span> Preparing Data for Modeling</a></div>
<div class="next"><a href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-knn"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></li>
<li>
<a class="nav-link" href="#classification"><span class="header-section-number">7.1</span> Classification</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#where-is-classification-used">Where Is Classification Used?</a></li>
<li><a class="nav-link" href="#how-does-classification-work">How Does Classification Work?</a></li>
<li><a class="nav-link" href="#which-classification-algorithm-should-you-use">Which Classification Algorithm Should You Use?</a></li>
<li><a class="nav-link" href="#why-is-classification-important">Why Is Classification Important?</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#how-k-nearest-neighbors-works"><span class="header-section-number">7.2</span> How k-Nearest Neighbors Works</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#how-does-knn-classify-a-new-observation">How Does kNN Classify a New Observation?</a></li>
<li><a class="nav-link" href="#strengths-and-limitations-of-knn">Strengths and Limitations of kNN</a></li>
</ul>
</li>
<li><a class="nav-link" href="#knn-in-action-a-toy-example-for-drug-classification">kNN in Action: A Toy Example for Drug Classification</a></li>
<li>
<a class="nav-link" href="#distance-metrics"><span class="header-section-number">7.3</span> Distance Metrics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#euclidean-distance">Euclidean Distance</a></li>
<li><a class="nav-link" href="#choosing-the-right-distance-metric">Choosing the Right Distance Metric</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#how-to-choose-an-optimal-k"><span class="header-section-number">7.4</span> How to Choose an Optimal \(k\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#balancing-overfitting-and-underfitting">Balancing Overfitting and Underfitting</a></li>
<li><a class="nav-link" href="#choosing-k-through-validation">Choosing \(k\) Through Validation</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#preparing-data-for-knn"><span class="header-section-number">7.5</span> Preparing Data for kNN</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#feature-scaling-1"><span class="header-section-number">7.5.1</span> Feature Scaling</a></li>
<li><a class="nav-link" href="#scaling-training-and-test-data-the-same-way"><span class="header-section-number">7.5.2</span> Scaling Training and Test Data the Same Way</a></li>
<li><a class="nav-link" href="#one-hot-encoding-1"><span class="header-section-number">7.5.3</span> One-Hot Encoding</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-kNN-churn"><span class="header-section-number">7.6</span> Applying kNN Algorithm in Practice</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#step-1-preparing-the-data"><span class="header-section-number">7.6.1</span> Step 1: Preparing the Data</a></li>
<li><a class="nav-link" href="#step-2-choosing-an-optimal-k"><span class="header-section-number">7.6.2</span> Step 2: Choosing an Optimal \(k\)</a></li>
<li><a class="nav-link" href="#step-3-training-the-model-and-making-predictions"><span class="header-section-number">7.6.3</span> Step 3: Training the Model and Making Predictions</a></li>
<li><a class="nav-link" href="#step-4-evaluating-the-model"><span class="header-section-number">7.6.4</span> Step 4: Evaluating the Model</a></li>
<li><a class="nav-link" href="#final-remarks">Final Remarks</a></li>
</ul>
</li>
<li><a class="nav-link" href="#key-takeaways-from-knn"><span class="header-section-number">7.7</span> Key Takeaways from kNN</a></li>
<li>
<a class="nav-link" href="#exercises-5"><span class="header-section-number">7.8</span> Exercises</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#conceptual-questions-3">Conceptual Questions</a></li>
<li><a class="nav-link" href="#hands-on-practice-applying-knn-to-the-bank-dataset">Hands-On Practice: Applying kNN to the Bank Dataset</a></li>
<li><a class="nav-link" href="#critical-thinking-and-real-world-applications">Critical Thinking and Real-World Applications</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/RezaMoammadi/Book-Data-Science/blob/master/knn.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/RezaMoammadi/Book-Data-Science/edit/master/knn.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Science Foundations and Machine Learning Using R</strong>" was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:white">Reza Mohammadi</span></a>. It was last built on 2025-03-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
