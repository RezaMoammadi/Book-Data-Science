<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Regression Analysis: Foundations and Applications | Data Science Foundations and Machine Learning Using R</title>
<meta name="author" content="Reza Mohammadi">
<meta name="description" content="Regression analysis has been a fundamental tool in statistical modeling for centuries and remains one of the most versatile techniques in data science. Its mathematical foundations were...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="Chapter 10 Regression Analysis: Foundations and Applications | Data Science Foundations and Machine Learning Using R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://uncovering-data-science.netlify.app/chapter-regression.html">
<meta property="og:image" content="https://uncovering-data-science.netlify.app/images/logo_black.png">
<meta property="og:description" content="Regression analysis has been a fundamental tool in statistical modeling for centuries and remains one of the most versatile techniques in data science. Its mathematical foundations were...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 10 Regression Analysis: Foundations and Applications | Data Science Foundations and Machine Learning Using R">
<meta name="twitter:description" content="Regression analysis has been a fundamental tool in statistical modeling for centuries and remains one of the most versatile techniques in data science. Its mathematical foundations were...">
<meta name="twitter:image" content="https://uncovering-data-science.netlify.app/images/logo_black.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Data Science Foundations and Machine Learning Using R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="chapter-into-R.html"><span class="header-section-number">1</span> The Basics for R</a></li>
<li><a class="" href="chapter-intro-DS.html"><span class="header-section-number">2</span> Introduction to Data Science</a></li>
<li><a class="" href="chapter-data-prep.html"><span class="header-section-number">3</span> Data Preparation</a></li>
<li><a class="" href="chapter-EDA.html"><span class="header-section-number">4</span> Exploratory Data Analysis</a></li>
<li><a class="" href="chapter-statistics.html"><span class="header-section-number">5</span> Statistical Inference and Hypothesis Testing</a></li>
<li><a class="" href="chapter-modeling.html"><span class="header-section-number">6</span> Preparing Data for Modeling</a></li>
<li><a class="" href="chapter-knn.html"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></li>
<li><a class="" href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></li>
<li><a class="" href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></li>
<li><a class="active" href="chapter-regression.html"><span class="header-section-number">10</span> Regression Analysis: Foundations and Applications</a></li>
<li><a class="" href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></li>
<li><a class="" href="chapter-nn.html"><span class="header-section-number">12</span> Neural Networks: The Building Blocks of Artificial Intelligence</a></li>
<li><a class="" href="chapter-cluster.html"><span class="header-section-number">13</span> Clustering</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/RezaMoammadi/Book-Data-Science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-regression" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Regression Analysis: Foundations and Applications<a class="anchor" aria-label="anchor" href="#chapter-regression"><i class="fas fa-link"></i></a>
</h1>
<p><em>Regression analysis</em> has been a fundamental tool in statistical modeling for centuries and remains one of the most versatile techniques in data science. Its mathematical foundations were established by early statisticians such as Legendre and Gauss, who developed the least squares method. Since then, regression analysis has evolved into a widely used framework for examining relationships between variables. With advancements in computing and programming languages such as R, it is now accessible and scalable for addressing complex real-world problems.</p>
<p>Regression models provide a systematic approach for quantifying relationships, uncovering patterns, and making predictions. These models are applied across diverse fields, including economics, medicine, and engineering, to estimate effects, forecast outcomes, and support data-driven decision-making. Whether predicting the impact of advertising expenditure on sales, modeling housing prices, or identifying risk factors for disease, regression analysis serves as a cornerstone of statistical modeling.<br>
As Charles Wheelan describes in <a href="https://www.goodreads.com/book/show/15786586-naked-statistics"><em>Naked Statistics</em></a><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Charles Wheelan, &lt;em&gt;Naked Statistics: Stripping the Dread from the Data&lt;/em&gt; (WW Norton &amp;amp; Company, 2013).&lt;/p&gt;"><sup>7</sup></a></span>, <em>“Regression modeling is the hydrogen bomb of the statistics arsenal.”</em> This analogy highlights the method’s immense power—when used correctly, it provides a formidable tool for making informed decisions, but its misuse can lead to misleading conclusions.</p>
<p>This chapter provides a structured introduction to regression techniques, beginning with simple linear regression and extending to multiple regression, generalized linear models (GLMs), and non-linear regression approaches. Throughout, we will apply these techniques to real-world datasets, including an <em>online marketing dataset</em> for modeling the impact of digital advertising on revenue and a <em>housing price dataset</em> to explore the relationship between property attributes and market value. By the end, readers will have a solid foundation in both the theoretical principles and practical applications of regression modeling in R, enabling them to analyze and interpret real-world data effectively.</p>
<div id="sec-simple-regression" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Simple Linear Regression<a class="anchor" aria-label="anchor" href="#sec-simple-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Simple linear regression is the most fundamental regression model, allowing us to quantify the relationship between a <em>single predictor</em> and a <em>response variable</em>. It provides a straightforward approach to estimating how changes in one variable influence another. By focusing on a single predictor, we establish a clear understanding of regression mechanics before extending the model to multiple predictors.</p>
<p>To illustrate simple linear regression, we use the <em>marketing</em> dataset from the <strong>liver</strong> package. This dataset captures <em>daily digital marketing activities</em> and their impact on <em>revenue generation</em>, making it an ideal real-world example for regression analysis. It includes key performance indicators of <em>online advertising campaigns</em>, such as expenditure, user engagement metrics, and daily revenue.</p>
<p>The dataset consists of 40 observations and 8 variables:</p>
<ul>
<li>
<code>spend</code>: Daily expenditure on pay-per-click (PPC) advertising.<br>
</li>
<li>
<code>clicks</code>: Number of clicks on advertisements.<br>
</li>
<li>
<code>impressions</code>: Number of times ads were displayed to users.<br>
</li>
<li>
<code>transactions</code>: Number of completed transactions per day.<br>
</li>
<li>
<code>click.rate</code>: Click-through rate (CTR), calculated as the proportion of impressions resulting in clicks.<br>
</li>
<li>
<code>conversion.rate</code>: Conversion rate, representing the proportion of clicks leading to transactions.<br>
</li>
<li>
<code>display</code>: Whether a display campaign was active (<code>yes</code> or <code>no</code>).<br>
</li>
<li>
<code>revenue</code>: Total daily revenue (response variable).</li>
</ul>
<p>We begin by loading the dataset and examining its structure:</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="chapter-regression.html#cb170-1" tabindex="-1"></a><span class="fu">library</span>(liver)</span>
<span id="cb170-2"><a href="chapter-regression.html#cb170-2" tabindex="-1"></a></span>
<span id="cb170-3"><a href="chapter-regression.html#cb170-3" tabindex="-1"></a><span class="fu">data</span>(marketing, <span class="at">package =</span> <span class="st">"liver"</span>)</span>
<span id="cb170-4"><a href="chapter-regression.html#cb170-4" tabindex="-1"></a></span>
<span id="cb170-5"><a href="chapter-regression.html#cb170-5" tabindex="-1"></a><span class="fu">str</span>(marketing)</span>
<span id="cb170-6"><a href="chapter-regression.html#cb170-6" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">40</span> obs. of  <span class="dv">8</span> variables<span class="sc">:</span></span>
<span id="cb170-7"><a href="chapter-regression.html#cb170-7" tabindex="-1"></a>    <span class="er">$</span> spend          <span class="sc">:</span> num  <span class="fl">22.6</span> <span class="fl">37.3</span> <span class="fl">55.6</span> <span class="fl">45.4</span> <span class="fl">50.2</span> ...</span>
<span id="cb170-8"><a href="chapter-regression.html#cb170-8" tabindex="-1"></a>    <span class="sc">$</span> clicks         <span class="sc">:</span> int  <span class="dv">165</span> <span class="dv">228</span> <span class="dv">291</span> <span class="dv">247</span> <span class="dv">290</span> <span class="dv">172</span> <span class="dv">68</span> <span class="dv">112</span> <span class="dv">306</span> <span class="dv">300</span> ...</span>
<span id="cb170-9"><a href="chapter-regression.html#cb170-9" tabindex="-1"></a>    <span class="sc">$</span> impressions    <span class="sc">:</span> int  <span class="dv">8672</span> <span class="dv">11875</span> <span class="dv">14631</span> <span class="dv">11709</span> <span class="dv">14768</span> <span class="dv">8698</span> <span class="dv">2924</span> <span class="dv">5919</span> <span class="dv">14789</span> <span class="dv">14818</span> ...</span>
<span id="cb170-10"><a href="chapter-regression.html#cb170-10" tabindex="-1"></a>    <span class="sc">$</span> display        <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb170-11"><a href="chapter-regression.html#cb170-11" tabindex="-1"></a>    <span class="sc">$</span> transactions   <span class="sc">:</span> int  <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> ...</span>
<span id="cb170-12"><a href="chapter-regression.html#cb170-12" tabindex="-1"></a>    <span class="sc">$</span> click.rate     <span class="sc">:</span> num  <span class="fl">1.9</span> <span class="fl">1.92</span> <span class="fl">1.99</span> <span class="fl">2.11</span> <span class="fl">1.96</span> <span class="fl">1.98</span> <span class="fl">2.33</span> <span class="fl">1.89</span> <span class="fl">2.07</span> <span class="fl">2.02</span> ...</span>
<span id="cb170-13"><a href="chapter-regression.html#cb170-13" tabindex="-1"></a>    <span class="sc">$</span> conversion.rate<span class="sc">:</span> num  <span class="fl">1.21</span> <span class="fl">0.88</span> <span class="fl">1.03</span> <span class="fl">0.81</span> <span class="fl">1.03</span> <span class="fl">1.16</span> <span class="fl">1.47</span> <span class="fl">0.89</span> <span class="fl">0.98</span> <span class="dv">1</span> ...</span>
<span id="cb170-14"><a href="chapter-regression.html#cb170-14" tabindex="-1"></a>    <span class="sc">$</span> revenue        <span class="sc">:</span> num  <span class="fl">58.9</span> <span class="fl">44.9</span> <span class="fl">141.6</span> <span class="fl">209.8</span> <span class="fl">197.7</span> ...</span></code></pre></div>
<p>The dataset contains 8 variables and 40 observations. The response variable, <code>revenue</code>, is continuous, while the remaining 7 variables serve as potential predictors.</p>
<div id="exploring-relationships-in-the-data" class="section level3 unnumbered">
<h3>Exploring Relationships in the Data<a class="anchor" aria-label="anchor" href="#exploring-relationships-in-the-data"><i class="fas fa-link"></i></a>
</h3>
<p>Before constructing a regression model, we first explore the relationships between variables to ensure that our assumptions hold and to identify strong predictors. A useful tool for this is the <code><a href="https://rdrr.io/pkg/psych/man/pairs.panels.html">pairs.panels()</a></code> function from the <strong>psych</strong> package, which provides a comprehensive overview of pairwise relationships:</p>
<div class="sourceCode" id="cb171"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://personality-project.org/r/psych/">psych</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/psych/man/pairs.panels.html">pairs.panels</a></span><span class="op">(</span><span class="va">marketing</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="regression_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>This visualization includes:</p>
<ul>
<li>
<em>Scatter plots</em> (lower triangle), showing how each predictor relates to the response variable.<br>
</li>
<li>
<em>Histograms</em> (diagonal), illustrating the distribution of each variable.<br>
</li>
<li>
<em>Correlation coefficients</em> (upper triangle), quantifying the strength and direction of linear associations.</li>
</ul>
<p>From the correlation matrix, we observe that <code>spend</code> and <code>revenue</code> exhibit a <em>strong positive correlation</em> of 0.79. This suggests that <em>higher advertising expenditure is associated with higher revenue</em>, making <code>spend</code> a strong candidate for predicting <code>revenue</code>.</p>
<p>In the next section, we formalize this relationship using a <em>simple linear regression model</em>.</p>
</div>
<div id="fitting-a-simple-linear-regression-model" class="section level3 unnumbered">
<h3>Fitting a Simple Linear Regression Model<a class="anchor" aria-label="anchor" href="#fitting-a-simple-linear-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p>A logical starting point in regression analysis is examining the relationship between a single predictor and the response variable. This allows for a clearer understanding of how one variable influences another before incorporating additional predictors into more complex models. Here, we investigate how advertising expenditure (<code>spend</code>) affects daily revenue (<code>revenue</code>) using a simple linear regression model.</p>
<p>Before fitting the model, it is essential to visualize the relationship between these variables to assess whether a linear assumption is reasonable. A scatter plot with a fitted least-squares regression line provides insight into the strength and direction of the relationship:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scoter-plot-simple-reg"></span>
<img src="regression_files/figure-html/scoter-plot-simple-reg-1.png" alt="Scatter plot of daily revenue (€) versus daily spend (€) for 40 observations, with the fitted least-squares regression line (blue) showing the linear relationship." width="80%"><p class="caption">
Figure 10.1: Scatter plot of daily revenue (€) versus daily spend (€) for 40 observations, with the fitted least-squares regression line (blue) showing the linear relationship.
</p>
</div>
<p>Figure <a href="chapter-regression.html#fig:scoter-plot-simple-reg">10.1</a> illustrates the relationship between <code>spend</code> and <code>revenue</code> in the <em>marketing</em> dataset. The scatter plot suggests a positive association, indicating that increased advertising expenditure is generally linked to higher revenue.</p>
<p>A simple linear regression model is mathematically expressed as:</p>
<p><span class="math display">\[
\hat{y} = b_0 + b_1x
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{y}\)</span> represents the predicted value of the response variable (<code>revenue</code>).<br>
</li>
<li>
<span class="math inline">\(x\)</span> denotes the predictor variable (<code>spend</code>).<br>
</li>
<li>
<span class="math inline">\(b_0\)</span> is the intercept, indicating the estimated revenue when no advertising expenditure is made.<br>
</li>
<li>
<span class="math inline">\(b_1\)</span> is the slope, representing the expected change in revenue for a one-unit increase in <code>spend</code>.</li>
</ul>
<p>This formulation provides a framework for estimating the relationship between advertising expenditure and revenue, which we will now proceed to quantify.</p>
</div>
<div id="estimating-the-model-in-r" class="section level3 unnumbered">
<h3>Estimating the Model in R<a class="anchor" aria-label="anchor" href="#estimating-the-model-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>To estimate the regression coefficients, we use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function in R, which fits a linear model using the least squares method. The syntax follows the format:</p>
<div class="sourceCode" id="cb172"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response_variable</span> <span class="op">~</span> <span class="va">predictor_variable</span>, data <span class="op">=</span> <span class="va">dataset</span><span class="op">)</span></span></code></pre></div>
<p>For our analysis, we model <code>revenue</code> as a function of <code>spend</code>:</p>
<div class="sourceCode" id="cb173"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">simple_reg</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">revenue</span> <span class="op">~</span> <span class="va">spend</span>, data <span class="op">=</span> <span class="va">marketing</span><span class="op">)</span></span></code></pre></div>
<p>After fitting the model, we summarize the results using the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="chapter-regression.html#cb174-1" tabindex="-1"></a><span class="fu">summary</span>(simple_reg)</span>
<span id="cb174-2"><a href="chapter-regression.html#cb174-2" tabindex="-1"></a>   </span>
<span id="cb174-3"><a href="chapter-regression.html#cb174-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb174-4"><a href="chapter-regression.html#cb174-4" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend, <span class="at">data =</span> marketing)</span>
<span id="cb174-5"><a href="chapter-regression.html#cb174-5" tabindex="-1"></a>   </span>
<span id="cb174-6"><a href="chapter-regression.html#cb174-6" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb174-7"><a href="chapter-regression.html#cb174-7" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb174-8"><a href="chapter-regression.html#cb174-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">175.640</span>  <span class="sc">-</span><span class="fl">56.226</span>    <span class="fl">1.448</span>   <span class="fl">65.235</span>  <span class="fl">210.987</span> </span>
<span id="cb174-9"><a href="chapter-regression.html#cb174-9" tabindex="-1"></a>   </span>
<span id="cb174-10"><a href="chapter-regression.html#cb174-10" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb174-11"><a href="chapter-regression.html#cb174-11" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb174-12"><a href="chapter-regression.html#cb174-12" tabindex="-1"></a>   (Intercept)  <span class="fl">15.7058</span>    <span class="fl">35.1727</span>   <span class="fl">0.447</span>    <span class="fl">0.658</span>    </span>
<span id="cb174-13"><a href="chapter-regression.html#cb174-13" tabindex="-1"></a>   spend         <span class="fl">5.2517</span>     <span class="fl">0.6624</span>   <span class="fl">7.928</span> <span class="fl">1.42e-09</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb174-14"><a href="chapter-regression.html#cb174-14" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb174-15"><a href="chapter-regression.html#cb174-15" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb174-16"><a href="chapter-regression.html#cb174-16" tabindex="-1"></a>   </span>
<span id="cb174-17"><a href="chapter-regression.html#cb174-17" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">93.82</span> on <span class="dv">38</span> degrees of freedom</span>
<span id="cb174-18"><a href="chapter-regression.html#cb174-18" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6232</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6133</span> </span>
<span id="cb174-19"><a href="chapter-regression.html#cb174-19" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">62.86</span> on <span class="dv">1</span> and <span class="dv">38</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.415e-09</span></span></code></pre></div>
<p>The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> output provides key insights into the estimated model. The regression equation based on the estimated coefficients is:</p>
<p><span class="math display">\[
\hat{\text{revenue}} = 15.71 + 5.25 \cdot \text{spend}
\]</span></p>
<p>where:</p>
<ul>
<li>The <strong>intercept</strong> (<span class="math inline">\(b_0\)</span>) is 15.71, representing the estimated daily revenue when no money is spent on advertising (<code>spend = 0</code>).<br>
</li>
<li>The <strong>slope</strong> (<span class="math inline">\(b_1\)</span>) is 5.25, meaning that for each additional €1 spent on advertising, daily revenue is expected to increase by approximately €5.25.</li>
</ul>
<p>Beyond the estimated coefficients, the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> output provides several key metrics for evaluating the regression model:</p>
<ul>
<li>
<strong>Estimate</strong>: The estimated values of the intercept and slope.<br>
</li>
<li>
<strong>Standard error</strong>: Measures the variability of each coefficient estimate. Smaller standard errors indicate more precise estimates.<br>
</li>
<li>
<strong>t-value and p-value</strong>: The t-value quantifies how many standard errors the coefficient is from zero, while the p-value assesses statistical significance. A small p-value (typically &lt; 0.05) suggests that the predictor has a significant impact on the response variable.<br>
</li>
<li>
<strong>Multiple R-squared (<span class="math inline">\(R^2\)</span>)</strong>: Measures the proportion of variance in <code>revenue</code> explained by <code>spend</code>. Here, <span class="math inline">\(R^2 = 0.623\)</span>, meaning that <strong>62.3% of the variation in revenue is explained by advertising spend</strong>.<br>
</li>
<li>
<strong>Residual standard error (RSE)</strong>: Provides an estimate of the typical prediction error. In this case, <span class="math inline">\(RSE = 93.82\)</span>, indicating that, on average, predictions deviate from actual revenue values by approximately €93.82.</li>
</ul>
<p>The results confirm a statistically significant relationship between advertising spend and revenue, supporting the use of regression analysis for business decision-making. In the next section, we explore how this model can be applied for prediction and how residual analysis helps validate model assumptions.</p>
</div>
<div id="interpreting-the-regression-line" class="section level3 unnumbered">
<h3>Interpreting the Regression Line<a class="anchor" aria-label="anchor" href="#interpreting-the-regression-line"><i class="fas fa-link"></i></a>
</h3>
<p>The regression line provides a mathematical approximation of the relationship between advertising spend and revenue. Once the model is estimated, it can be used for prediction. Suppose a company wants to estimate the expected revenue for a day when €25 is spent on pay-per-click (PPC) advertising. Using the regression equation:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\hat{\text{revenue}} &amp; = 15.71 + 5.25 \cdot 25 \\
&amp; = 147
\end{split}
\end{equation}\]</span>
Thus, the predicted daily revenue is approximately €147.</p>
<p>This predictive capability is particularly valuable for marketing teams planning advertising budgets. For example, if the goal is to maximize returns while controlling costs, the model provides an evidence-based estimate of how revenue responds to different levels of spending. Decision-makers can use this information to determine optimal advertising expenditures, set performance targets, and allocate marketing resources efficiently.</p>
</div>
<div id="residuals-and-model-fit" class="section level3 unnumbered">
<h3>Residuals and Model Fit<a class="anchor" aria-label="anchor" href="#residuals-and-model-fit"><i class="fas fa-link"></i></a>
</h3>
<p>Residuals measure the difference between observed and predicted values, providing insight into how well the regression model fits the data. The residual for an observation is calculated as:</p>
<p><span class="math display">\[
\text{Residual} = y - \hat{y}
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the actual observed value, and <span class="math inline">\(\hat{y}\)</span> is the predicted value from the regression model. For example, suppose a day in the dataset has a marketing spend of €25 and an actual revenue of 185.36. The residual for this observation is:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\text{Residual} &amp; = 185.36 - 147 \\
&amp; = 38.36
\end{split}
\end{equation}\]</span></p>
<p>Residuals play a crucial role in assessing model adequacy. Ideally, they should be randomly distributed around zero, indicating that the model captures the relationship between variables well. However, if residuals exhibit systematic patterns—such as curvature or increasing variance—this suggests that the model does not fully capture the relationship and may require adjustments, such as incorporating additional predictors or using a non-linear model.</p>
<p>The regression line is estimated using the least squares method, which finds the line that minimizes the sum of squared residuals, also known as the sum of squared errors (SSE):</p>
<p><span class="math display" id="eq:sse">\[\begin{equation}
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\tag{10.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> represents the observed revenue, <span class="math inline">\(\hat{y}_i\)</span> is the predicted revenue, and <span class="math inline">\(n\)</span> is the number of observations. Minimizing SSE ensures that the estimated regression line optimally represents the relationship between the predictor and response variable, leading to more accurate predictions.</p>
<p>Monitoring residuals is an essential step in regression analysis. If residuals exhibit no discernible pattern and are evenly spread around zero, the linear model is likely appropriate. However, if residuals show trends or increasing variability, further refinement—such as adding interaction terms, transforming variables, or considering a different modeling approach—may be necessary.</p>
<p>In summary, simple linear regression provides an effective way to model and interpret the relationship between two variables. By analyzing the <em>marketing</em> dataset, we demonstrated how to estimate, interpret, and apply a regression model to make predictions. This foundational understanding of simple linear regression sets the stage for evaluating model quality and extending the framework to multiple predictors in the following sections.</p>
</div>
</div>
<div id="hypothesis-testing-in-simple-linear-regression" class="section level2 unnumbered">
<h2>Hypothesis Testing in Simple Linear Regression<a class="anchor" aria-label="anchor" href="#hypothesis-testing-in-simple-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Hypothesis testing in regression analysis helps determine whether a predictor variable has a statistically significant relationship with the response variable. Specifically, we test whether the estimated slope <span class="math inline">\(b_1\)</span> from the sample regression model provides evidence of a true linear relationship in the population, where the unknown slope is denoted as <span class="math inline">\(\beta_1\)</span>.</p>
<p>The population regression equation models the relationship between a predictor <span class="math inline">\(x\)</span> and a response <span class="math inline">\(y\)</span> for the entire population and is expressed as:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1x + \epsilon
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span> represents the population intercept, which is the expected value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>.<br>
</li>
<li>
<span class="math inline">\(\beta_1\)</span> represents the population slope, indicating how <span class="math inline">\(y\)</span> changes for a one-unit increase in <span class="math inline">\(x\)</span>.<br>
</li>
<li>
<span class="math inline">\(\epsilon\)</span> is a random error term accounting for variability in <span class="math inline">\(y\)</span> not explained by the linear model.</li>
</ul>
<p>The primary objective of hypothesis testing in regression is to determine whether the slope <span class="math inline">\(\beta_1\)</span> is significantly different from zero. If <span class="math inline">\(\beta_1 = 0\)</span>, the regression equation simplifies to:</p>
<p><span class="math display">\[
y = \beta_0 + \epsilon
\]</span></p>
<p>This suggests that the predictor <span class="math inline">\(x\)</span> has no linear relationship with the response variable <span class="math inline">\(y\)</span>. Conversely, if <span class="math inline">\(\beta_1 \neq 0\)</span>, there is statistical evidence of an association between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. To formally test this, we set up the following hypotheses:</p>
<p><span class="math display">\[
\begin{cases}
  H_0: \beta_1 =  0, \quad \text{(no linear relationship between \( x \) and \( y \))}  \\
  H_a: \beta_1 \neq 0, \quad \text{(a linear relationship exists between \( x \) and \( y \))}
\end{cases}
\]</span></p>
<p>The estimated slope <span class="math inline">\(b_1\)</span> from the sample data provides an approximation of <span class="math inline">\(\beta_1\)</span>. To assess its significance, we rely on the following key statistical measures:</p>
<ul>
<li>
<strong>Standard error of the slope</strong>: Measures the variability in the estimate <span class="math inline">\(b_1\)</span>.<br>
</li>
<li>
<strong>t-statistic</strong>: Determines how many standard errors the estimated slope is from zero. It is computed as:</li>
</ul>
<p><span class="math display">\[
t = \frac{b_1}{SE(b_1)}
\]</span></p>
<ul>
<li>
<strong>p-value</strong>: Represents the probability of observing a t-statistic as extreme as the one calculated, assuming the null hypothesis is true. A small p-value (typically less than 0.05) provides strong evidence to reject <span class="math inline">\(H_0\)</span>, indicating that the predictor is significantly associated with the response variable.</li>
</ul>
<p>To illustrate hypothesis testing in simple linear regression, we examine the results of the model that predicts <code>revenue</code> (daily revenue) based on <code>spend</code> (advertising expenditure) using the <em>marketing</em> dataset. The estimated slope <span class="math inline">\(b_1\)</span> for <code>spend</code> is:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="chapter-regression.html#cb175-1" tabindex="-1"></a><span class="fu">summary</span>(simple_reg)</span>
<span id="cb175-2"><a href="chapter-regression.html#cb175-2" tabindex="-1"></a>   </span>
<span id="cb175-3"><a href="chapter-regression.html#cb175-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb175-4"><a href="chapter-regression.html#cb175-4" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend, <span class="at">data =</span> marketing)</span>
<span id="cb175-5"><a href="chapter-regression.html#cb175-5" tabindex="-1"></a>   </span>
<span id="cb175-6"><a href="chapter-regression.html#cb175-6" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb175-7"><a href="chapter-regression.html#cb175-7" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb175-8"><a href="chapter-regression.html#cb175-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">175.640</span>  <span class="sc">-</span><span class="fl">56.226</span>    <span class="fl">1.448</span>   <span class="fl">65.235</span>  <span class="fl">210.987</span> </span>
<span id="cb175-9"><a href="chapter-regression.html#cb175-9" tabindex="-1"></a>   </span>
<span id="cb175-10"><a href="chapter-regression.html#cb175-10" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb175-11"><a href="chapter-regression.html#cb175-11" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb175-12"><a href="chapter-regression.html#cb175-12" tabindex="-1"></a>   (Intercept)  <span class="fl">15.7058</span>    <span class="fl">35.1727</span>   <span class="fl">0.447</span>    <span class="fl">0.658</span>    </span>
<span id="cb175-13"><a href="chapter-regression.html#cb175-13" tabindex="-1"></a>   spend         <span class="fl">5.2517</span>     <span class="fl">0.6624</span>   <span class="fl">7.928</span> <span class="fl">1.42e-09</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb175-14"><a href="chapter-regression.html#cb175-14" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb175-15"><a href="chapter-regression.html#cb175-15" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb175-16"><a href="chapter-regression.html#cb175-16" tabindex="-1"></a>   </span>
<span id="cb175-17"><a href="chapter-regression.html#cb175-17" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">93.82</span> on <span class="dv">38</span> degrees of freedom</span>
<span id="cb175-18"><a href="chapter-regression.html#cb175-18" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6232</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6133</span> </span>
<span id="cb175-19"><a href="chapter-regression.html#cb175-19" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">62.86</span> on <span class="dv">1</span> and <span class="dv">38</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.415e-09</span></span></code></pre></div>
<p>From the output:</p>
<ul>
<li>The <strong>t-statistic</strong> for the slope is 7.93.<br>
</li>
<li>The <strong>p-value</strong> is 1.4150362^{-9}, which is very close to zero.</li>
</ul>
<p>Since the p-value is significantly smaller than the commonly used significance level (<span class="math inline">\(\alpha = 0.05\)</span>), we reject the null hypothesis <span class="math inline">\(H_0\)</span>. This confirms that the predictor <code>spend</code> has a statistically significant effect on <code>revenue</code>. Specifically:</p>
<ul>
<li>The slope estimate <span class="math inline">\(b_1 = 5.25\)</span> suggests that for each additional €1 spent on advertising, daily revenue is expected to increase by approximately 5.25.<br>
</li>
<li>The strong statistical significance of <code>spend</code> validates its role as an important predictor for <code>revenue</code>, supporting its inclusion in the model.</li>
</ul>
<p>Hypothesis testing in simple linear regression provides a structured approach for determining whether a predictor variable has a meaningful impact on the response variable. A statistically significant slope (<span class="math inline">\(\beta_1 \neq 0\)</span>) indicates that changes in the predictor <span class="math inline">\(x\)</span> are associated with changes in the response <span class="math inline">\(y\)</span>, allowing for data-driven decision-making.</p>
<p>While statistical significance establishes the presence of a relationship, it does not imply causation. Additional factors, such as potential confounders, omitted variables, and model assumptions, should be considered when interpreting regression results.</p>
<p>In the next sections, we will explore further techniques for evaluating regression model quality, including measures of goodness-of-fit and model diagnostics. We will also extend these concepts to multiple predictors, enabling more comprehensive analyses and better predictions.</p>
</div>
<div id="measuring-the-quality-of-a-regression-model" class="section level2 unnumbered">
<h2>Measuring the Quality of a Regression Model<a class="anchor" aria-label="anchor" href="#measuring-the-quality-of-a-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>Evaluating the effectiveness of a regression model goes beyond determining whether a predictor is statistically significant. While hypothesis testing confirms whether a predictor has a meaningful relationship with the response variable, it does not assess how well the model fits the data. To measure model quality, we rely on additional metrics that quantify predictive accuracy and explanatory power. Two key statistics for this purpose are the <strong>Residual Standard Error (RSE)</strong> and the <strong><span class="math inline">\(R^2\)</span> (R-squared) statistic</strong>.</p>
<div id="residual-standard-error-rse" class="section level3 unnumbered">
<h3>Residual Standard Error (RSE)<a class="anchor" aria-label="anchor" href="#residual-standard-error-rse"><i class="fas fa-link"></i></a>
</h3>
<p>Residual Standard Error (RSE) provides an estimate of the typical prediction error in the model. It measures how much the observed values deviate from the predicted values on average. The formula for RSE is:</p>
<p><span class="math display">\[
RSE = \sqrt{\frac{1}{n-p-1} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},
\]</span>
where <span class="math inline">\(y_i\)</span> represents the observed values of the response variable, <span class="math inline">\(\hat{y}_i\)</span> represents the predicted values, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(p\)</span> is the number of predictors in the model.</p>
<p>A smaller RSE indicates a model with more precise predictions. For example, in the simple linear regression model for the <em>marketing</em> dataset, the RSE is:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="chapter-regression.html#cb176-1" tabindex="-1"></a>rse_value <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(simple_reg<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">summary</span>(simple_reg)<span class="sc">$</span>df[<span class="dv">2</span>])</span>
<span id="cb176-2"><a href="chapter-regression.html#cb176-2" tabindex="-1"></a><span class="fu">round</span>(rse_value, <span class="dv">2</span>)</span>
<span id="cb176-3"><a href="chapter-regression.html#cb176-3" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">93.82</span></span></code></pre></div>
<p>This value represents the average deviation of predicted revenue from actual revenue. A lower RSE suggests a better-fitting model, though it should always be interpreted in the context of the response variable’s scale.</p>
</div>
<div id="r-squared-r2" class="section level3 unnumbered">
<h3>R-squared (<span class="math inline">\(R^2\)</span>)<a class="anchor" aria-label="anchor" href="#r-squared-r2"><i class="fas fa-link"></i></a>
</h3>
<p>The <span class="math inline">\(R^2\)</span> statistic measures how well the regression model explains the variability in the response variable. It is defined as:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{SSE}{SST}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(SST\)</span> (Total Sum of Squares) represents the total variability in the response variable before fitting the model.<br>
</li>
<li>
<span class="math inline">\(SSE\)</span> (Sum of Squared Errors) represents the variability that remains unexplained after fitting the model.</li>
</ul>
<p><span class="math inline">\(R^2\)</span> ranges from 0 to 1, where higher values indicate that the model explains a greater proportion of variability in the response variable. For example, in the <em>marketing</em> dataset, the <span class="math inline">\(R^2\)</span> value is:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="chapter-regression.html#cb177-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(simple_reg)<span class="sc">$</span>r.squared, <span class="dv">3</span>)</span>
<span id="cb177-2"><a href="chapter-regression.html#cb177-2" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.623</span></span></code></pre></div>
<p>This means that 62.3% of the variation in <code>revenue</code> is explained by <code>spend</code>. While higher <span class="math inline">\(R^2\)</span> values suggest a better fit, they do not guarantee that the model generalizes well to new data. It is important to supplement <span class="math inline">\(R^2\)</span> with additional model diagnostics.</p>
</div>
<div id="relationship-between-r2-and-the-correlation-coefficient" class="section level3 unnumbered">
<h3>Relationship Between <span class="math inline">\(R^2\)</span> and the Correlation Coefficient<a class="anchor" aria-label="anchor" href="#relationship-between-r2-and-the-correlation-coefficient"><i class="fas fa-link"></i></a>
</h3>
<p>In simple linear regression, <span class="math inline">\(R^2\)</span> is directly related to the correlation coefficient <span class="math inline">\(r\)</span> between the predictor and response variable:</p>
<p><span class="math display">\[
R^2 = r^2
\]</span></p>
<p>For example, in the <em>marketing</em> dataset, the correlation between <code>spend</code> and <code>revenue</code> is:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="chapter-regression.html#cb178-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(marketing<span class="sc">$</span>spend, marketing<span class="sc">$</span>revenue), <span class="dv">2</span>)</span>
<span id="cb178-2"><a href="chapter-regression.html#cb178-2" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.79</span></span></code></pre></div>
<p>Squaring this value gives:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="chapter-regression.html#cb179-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(marketing<span class="sc">$</span>spend, marketing<span class="sc">$</span>revenue)<span class="sc">^</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb179-2"><a href="chapter-regression.html#cb179-2" tabindex="-1"></a>   [<span class="dv">1</span>] <span class="fl">0.62</span></span></code></pre></div>
<p>which matches the <span class="math inline">\(R^2\)</span> value, reinforcing how <span class="math inline">\(R^2\)</span> quantifies the strength of the linear relationship.</p>
</div>
<div id="adjusted-r-squared" class="section level3 unnumbered">
<h3>Adjusted R-squared<a class="anchor" aria-label="anchor" href="#adjusted-r-squared"><i class="fas fa-link"></i></a>
</h3>
<p>While <span class="math inline">\(R^2\)</span> measures the proportion of variance explained by the model, <strong>Adjusted <span class="math inline">\(R^2\)</span></strong> accounts for the number of predictors, ensuring that adding unnecessary variables does not artificially inflate the statistic. It is calculated as:</p>
<p><span class="math display">\[
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \cdot \frac{n-1}{n-p-1},
\]</span>
where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of predictors.</p>
<p>Adjusted <span class="math inline">\(R^2\)</span> penalizes the inclusion of irrelevant predictors, making it particularly useful in multiple regression settings. In simple linear regression (where <span class="math inline">\(p = 1\)</span>), <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span> are equal, but in multiple regression, Adjusted <span class="math inline">\(R^2\)</span> is often lower and provides a better measure of model performance.</p>
</div>
<div id="interpreting-model-quality" class="section level3 unnumbered">
<h3>Interpreting Model Quality<a class="anchor" aria-label="anchor" href="#interpreting-model-quality"><i class="fas fa-link"></i></a>
</h3>
<p>A good regression model should have:</p>
<ul>
<li>A <strong>low RSE</strong>, indicating that predictions are close to observed values.<br>
</li>
<li>A <strong>high <span class="math inline">\(R^2\)</span></strong>, suggesting that the model explains most of the variability in the response variable.<br>
</li>
<li>A <strong>high Adjusted <span class="math inline">\(R^2\)</span></strong>, ensuring that additional predictors improve the model rather than introducing noise.</li>
</ul>
<p>However, a model should not be judged by these metrics alone. Even a high <span class="math inline">\(R^2\)</span> model may fail if it violates regression assumptions or overfits the data. Additional diagnostics, such as residual analysis and cross-validation, are essential to ensure model reliability.</p>
<p>By understanding these measures of model quality, we gain deeper insight into the effectiveness of regression models and prepare for extending these concepts to multiple predictors in the next sections.</p>
</div>
</div>
<div id="sec-multiple-regression" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Multiple Linear Regression<a class="anchor" aria-label="anchor" href="#sec-multiple-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Simple linear regression is useful for modeling relationships between two variables, but in many real-world applications, multiple factors influence the response variable. Multiple linear regression extends simple regression by incorporating multiple predictors, improving both estimation accuracy and predictive performance.</p>
<p>To illustrate, we expand the previous model, which included only <code>spend</code> as a predictor, by adding <code>display</code>, an indicator of whether a display advertising campaign was active. This additional predictor allows us to assess its impact on revenue. The general equation for a multiple regression model with <span class="math inline">\(p\)</span> predictors is:</p>
<p><span class="math display">\[
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the intercept, and <span class="math inline">\(\beta_1, \beta_2, \dots, \beta_p\)</span> represent the estimated effects of each predictor on the response variable.</p>
<p>For our case, the equation with two predictors, <code>spend</code> and <code>display</code>, is:</p>
<p><span class="math display">\[
\hat{\text{revenue}} = \beta_0 + \beta_1 \cdot \text{spend} + \beta_2 \cdot \text{display}
\]</span></p>
<p>where <code>spend</code> represents daily advertising expenditure and <code>display</code> is a categorical variable (<code>yes/no</code>), which R automatically converts into a binary indicator. Here, <code>display = 1</code> indicates an active display campaign, while <code>display = 0</code> means no display campaign.</p>
<div id="fitting-the-multiple-regression-model" class="section level3 unnumbered">
<h3>Fitting the Multiple Regression Model<a class="anchor" aria-label="anchor" href="#fitting-the-multiple-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p>We fit the multiple regression model using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function in R:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="chapter-regression.html#cb180-1" tabindex="-1"></a>multiple_reg <span class="ot">=</span> <span class="fu">lm</span>(revenue <span class="sc">~</span> spend <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb180-2"><a href="chapter-regression.html#cb180-2" tabindex="-1"></a></span>
<span id="cb180-3"><a href="chapter-regression.html#cb180-3" tabindex="-1"></a><span class="fu">summary</span>(multiple_reg)</span>
<span id="cb180-4"><a href="chapter-regression.html#cb180-4" tabindex="-1"></a>   </span>
<span id="cb180-5"><a href="chapter-regression.html#cb180-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb180-6"><a href="chapter-regression.html#cb180-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb180-7"><a href="chapter-regression.html#cb180-7" tabindex="-1"></a>   </span>
<span id="cb180-8"><a href="chapter-regression.html#cb180-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb180-9"><a href="chapter-regression.html#cb180-9" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb180-10"><a href="chapter-regression.html#cb180-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">189.420</span>  <span class="sc">-</span><span class="fl">45.527</span>    <span class="fl">5.566</span>   <span class="fl">54.943</span>  <span class="fl">154.340</span> </span>
<span id="cb180-11"><a href="chapter-regression.html#cb180-11" tabindex="-1"></a>   </span>
<span id="cb180-12"><a href="chapter-regression.html#cb180-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb180-13"><a href="chapter-regression.html#cb180-13" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb180-14"><a href="chapter-regression.html#cb180-14" tabindex="-1"></a>   (Intercept) <span class="sc">-</span><span class="fl">41.4377</span>    <span class="fl">32.2789</span>  <span class="sc">-</span><span class="fl">1.284</span> <span class="fl">0.207214</span>    </span>
<span id="cb180-15"><a href="chapter-regression.html#cb180-15" tabindex="-1"></a>   spend         <span class="fl">5.3556</span>     <span class="fl">0.5523</span>   <span class="fl">9.698</span> <span class="fl">1.05e-11</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb180-16"><a href="chapter-regression.html#cb180-16" tabindex="-1"></a>   display     <span class="fl">104.2878</span>    <span class="fl">24.7353</span>   <span class="fl">4.216</span> <span class="fl">0.000154</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb180-17"><a href="chapter-regression.html#cb180-17" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb180-18"><a href="chapter-regression.html#cb180-18" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb180-19"><a href="chapter-regression.html#cb180-19" tabindex="-1"></a>   </span>
<span id="cb180-20"><a href="chapter-regression.html#cb180-20" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">78.14</span> on <span class="dv">37</span> degrees of freedom</span>
<span id="cb180-21"><a href="chapter-regression.html#cb180-21" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7455</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7317</span> </span>
<span id="cb180-22"><a href="chapter-regression.html#cb180-22" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">54.19</span> on <span class="dv">2</span> and <span class="dv">37</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.012e-11</span></span></code></pre></div>
<p>The estimated regression equation is:</p>
<p><span class="math display">\[
\hat{\text{revenue}} = -41.44 + 5.36 \cdot \text{spend} + 104.29 \cdot \text{display}
\]</span></p>
<p>where:
- The <strong>intercept</strong> (<span class="math inline">\(\beta_0\)</span>) is -41.44, representing the estimated revenue when both <code>spend</code> is zero and no display campaign is running.
- The <strong>coefficient for <code>spend</code></strong> (<span class="math inline">\(\beta_1\)</span>) is 5.36, indicating that for each additional €1 spent, revenue increases by approximately 5.36, assuming <code>display</code> remains unchanged.
- The <strong>coefficient for <code>display</code></strong> (<span class="math inline">\(\beta_2\)</span>) is 104.29, meaning that when a display campaign is active (<code>display = 1</code>), revenue increases by approximately 104.29, holding <code>spend</code> constant.</p>
</div>
<div id="making-predictions" class="section level3 unnumbered">
<h3>Making Predictions<a class="anchor" aria-label="anchor" href="#making-predictions"><i class="fas fa-link"></i></a>
</h3>
<p>Consider a scenario where the company spends €25 on advertising while running a display campaign (<code>display = 1</code>). Using the regression equation, the predicted revenue is:</p>
<p><span class="math display">\[
\hat{\text{revenue}} = -41.44 + 5.36 \cdot 25 + 104.29 \cdot 1 = 196.74
\]</span></p>
<p>Thus, the predicted revenue for that day is approximately €196.74.</p>
<p>The residual (prediction error) for a specific observation is calculated as the difference between the actual and predicted revenue:</p>
<p><span class="math display">\[
\text{Residual} = y - \hat{y} = 185.36 - 196.74 = -11.49
\]</span></p>
<p>The prediction error is smaller than that of the simple regression model, confirming that including <code>display</code> improves predictive accuracy.</p>
</div>
<div id="evaluating-model-performance" class="section level3 unnumbered">
<h3>Evaluating Model Performance<a class="anchor" aria-label="anchor" href="#evaluating-model-performance"><i class="fas fa-link"></i></a>
</h3>
<p>Adding <code>display</code> enhances the regression model by reducing prediction errors and improving model fit. We compare key performance metrics between the simple and multiple regression models:</p>
<ul>
<li><p><strong>Residual Standard Error (RSE):</strong> In the simple regression model, <span class="math inline">\(RSE = 93.82\)</span>, whereas in the multiple regression model, <span class="math inline">\(RSE = 78.14\)</span>. The reduction in RSE indicates improved prediction accuracy.</p></li>
<li><p><strong><span class="math inline">\(R^2\)</span> (R-squared):</strong> The simple regression model had <span class="math inline">\(R^2 = 62\%\)</span>, whereas the multiple regression model increased to <span class="math inline">\(R^2 = 75\%\)</span>, demonstrating improved explanatory power.</p></li>
<li><p><strong>Adjusted <span class="math inline">\(R^2\)</span>:</strong> Unlike <span class="math inline">\(R^2\)</span>, Adjusted <span class="math inline">\(R^2\)</span> accounts for the number of predictors. In the simple regression model, Adjusted <span class="math inline">\(R^2 = 61\%\)</span>, while in the multiple regression model, Adjusted <span class="math inline">\(R^2 = 73\%\)</span>, confirming that the additional predictor contributes meaningfully to model performance.</p></li>
</ul>
</div>
<div id="key-takeaways-2" class="section level3 unnumbered">
<h3>Key Takeaways<a class="anchor" aria-label="anchor" href="#key-takeaways-2"><i class="fas fa-link"></i></a>
</h3>
<p>The multiple regression model improves upon simple regression by providing a better fit, reducing prediction errors, and enabling more accurate estimation of revenue drivers. Including <code>display</code> alongside <code>spend</code> strengthens the model’s ability to explain revenue variation. However, as models grow more complex, careful evaluation is necessary to prevent issues such as <strong>multicollinearity</strong> (high correlation between predictors) and <strong>overfitting</strong> (adding unnecessary predictors that reduce generalizability).</p>
<p>In the next sections, we will examine model assumptions, conduct diagnostics, and refine regression models to ensure validity and reliability.</p>
</div>
</div>
<div id="generalized-linear-models-glms" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Generalized Linear Models (GLMs)<a class="anchor" aria-label="anchor" href="#generalized-linear-models-glms"><i class="fas fa-link"></i></a>
</h2>
<p>Linear regression provides a useful framework for modeling continuous outcomes, but it is not suitable when the response variable is binary, count-based, or follows a distribution other than normal. <em>Generalized Linear Models (GLMs)</em> extend traditional linear regression by introducing a <em>link function</em>, which transforms the relationship between predictors and the response variable, and a <em>variance function</em>, which accounts for non-constant variability in the response. These extensions allow GLMs to accommodate a broader range of response variable distributions, making them widely applicable in fields such as finance, healthcare, and marketing.</p>
<p>GLMs retain the fundamental principles of linear regression but introduce three key components:
1. <em>Random component</em>: Specifies the probability distribution of the response variable, which can belong to the exponential family (e.g., normal, binomial, or Poisson distributions).
2. <em>Systematic component</em>: Represents the linear combination of predictor variables.
3. <em>Link function</em>: Transforms the expected value of the response variable so that it can be modeled as a linear function of the predictors.</p>
<p>In the following sections, we introduce two commonly used GLMs:</p>
<ul>
<li>
<em>Logistic regression</em>, which models binary outcomes.</li>
<li>
<em>Poisson regression</em>, which is suited for modeling count data.</li>
</ul>
<p>By extending regression beyond continuous responses, these models provide a more flexible and interpretable framework for analyzing data in a variety of applications. The next sections discuss their theoretical foundations and implementation in R.</p>
</div>
<div id="logistic-regression" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Logistic regression is a generalized linear model designed for binary classification, where the response variable takes two values, such as 0/1 or yes/no. Instead of predicting a continuous outcome, logistic regression estimates the probability that an observation belongs to a particular category. To ensure that predicted probabilities remain within the range <span class="math inline">\([0,1]\)</span>, the model applies the <em>logit function</em>, which transforms the linear combination of predictors into a probability scale:</p>
<p><span class="math display">\[
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]</span></p>
<p>Here, <span class="math inline">\(p\)</span> represents the probability that the outcome is 1, and the logit transformation ensures a linear relationship between the predictors and the log-odds of the response variable.</p>
<div id="logistic-regression-in-r" class="section level3 unnumbered">
<h3>Logistic Regression in R<a class="anchor" aria-label="anchor" href="#logistic-regression-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>To illustrate logistic regression, we use the <em>churn</em> dataset, which contains information on customer behavior. The objective is to predict whether a customer will <em>churn</em> (leave the service) based on customer characteristics and service usage patterns. The selected predictors include variables such as <code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>day.mins</code>, <code>eve.mins</code>, <code>night.mins</code>, and <code>customer.calls</code>, which capture aspects of user engagement and service utilization.</p>
<p>In R, logistic regression is implemented using the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function, which fits generalized linear models. The function follows the syntax:</p>
<div class="sourceCode" id="cb181"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">response_variable</span> <span class="op">~</span> <span class="va">predictor_variables</span>, data <span class="op">=</span> <span class="va">dataset</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span></code></pre></div>
<p>where <code>response_variable</code> is the binary outcome, <code>predictor_variables</code> are the independent variables, and <code>family = binomial</code> specifies a logistic regression model.</p>
<p>For the <em>churn</em> dataset, we fit a logistic regression model as follows:</p>
<div class="sourceCode" id="cb182"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span></span>
<span></span>
<span><span class="va">logreg_1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">churn</span> <span class="op">~</span> <span class="va">account.length</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> </span>
<span>                         <span class="va">night.mins</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">customer.calls</span> <span class="op">+</span> <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">voice.plan</span>, </span>
<span>               data <span class="op">=</span> <span class="va">churn</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span></code></pre></div>
<p>The model estimates the relationship between the predictors and the probability of churn. To examine the model’s coefficients and significance levels, we use:</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="chapter-regression.html#cb183-1" tabindex="-1"></a><span class="fu">summary</span>(logreg_1)</span>
<span id="cb183-2"><a href="chapter-regression.html#cb183-2" tabindex="-1"></a>   </span>
<span id="cb183-3"><a href="chapter-regression.html#cb183-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb183-4"><a href="chapter-regression.html#cb183-4" tabindex="-1"></a>   <span class="fu">glm</span>(<span class="at">formula =</span> churn <span class="sc">~</span> account.length <span class="sc">+</span> voice.messages <span class="sc">+</span> day.mins <span class="sc">+</span> </span>
<span id="cb183-5"><a href="chapter-regression.html#cb183-5" tabindex="-1"></a>       eve.mins <span class="sc">+</span> night.mins <span class="sc">+</span> intl.mins <span class="sc">+</span> customer.calls <span class="sc">+</span> intl.plan <span class="sc">+</span> </span>
<span id="cb183-6"><a href="chapter-regression.html#cb183-6" tabindex="-1"></a>       voice.plan, <span class="at">family =</span> binomial, <span class="at">data =</span> churn)</span>
<span id="cb183-7"><a href="chapter-regression.html#cb183-7" tabindex="-1"></a>   </span>
<span id="cb183-8"><a href="chapter-regression.html#cb183-8" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb183-9"><a href="chapter-regression.html#cb183-9" tabindex="-1"></a>                    Estimate Std. Error z value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>z<span class="sc">|</span>)    </span>
<span id="cb183-10"><a href="chapter-regression.html#cb183-10" tabindex="-1"></a>   (Intercept)     <span class="fl">8.8917584</span>  <span class="fl">0.6582188</span>  <span class="fl">13.509</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-11"><a href="chapter-regression.html#cb183-11" tabindex="-1"></a>   account.length <span class="sc">-</span><span class="fl">0.0013811</span>  <span class="fl">0.0011453</span>  <span class="sc">-</span><span class="fl">1.206</span>   <span class="fl">0.2279</span>    </span>
<span id="cb183-12"><a href="chapter-regression.html#cb183-12" tabindex="-1"></a>   voice.messages <span class="sc">-</span><span class="fl">0.0355317</span>  <span class="fl">0.0150397</span>  <span class="sc">-</span><span class="fl">2.363</span>   <span class="fl">0.0182</span> <span class="sc">*</span>  </span>
<span id="cb183-13"><a href="chapter-regression.html#cb183-13" tabindex="-1"></a>   day.mins       <span class="sc">-</span><span class="fl">0.0136547</span>  <span class="fl">0.0009103</span> <span class="sc">-</span><span class="fl">15.000</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-14"><a href="chapter-regression.html#cb183-14" tabindex="-1"></a>   eve.mins       <span class="sc">-</span><span class="fl">0.0071210</span>  <span class="fl">0.0009419</span>  <span class="sc">-</span><span class="fl">7.561</span> <span class="fl">4.02e-14</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-15"><a href="chapter-regression.html#cb183-15" tabindex="-1"></a>   night.mins     <span class="sc">-</span><span class="fl">0.0040518</span>  <span class="fl">0.0009048</span>  <span class="sc">-</span><span class="fl">4.478</span> <span class="fl">7.53e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-16"><a href="chapter-regression.html#cb183-16" tabindex="-1"></a>   intl.mins      <span class="sc">-</span><span class="fl">0.0882514</span>  <span class="fl">0.0170578</span>  <span class="sc">-</span><span class="fl">5.174</span> <span class="fl">2.30e-07</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-17"><a href="chapter-regression.html#cb183-17" tabindex="-1"></a>   customer.calls <span class="sc">-</span><span class="fl">0.5183958</span>  <span class="fl">0.0328652</span> <span class="sc">-</span><span class="fl">15.773</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-18"><a href="chapter-regression.html#cb183-18" tabindex="-1"></a>   intl.planno     <span class="fl">2.0958198</span>  <span class="fl">0.1214476</span>  <span class="fl">17.257</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-19"><a href="chapter-regression.html#cb183-19" tabindex="-1"></a>   voice.planno   <span class="sc">-</span><span class="fl">2.1637477</span>  <span class="fl">0.4836735</span>  <span class="sc">-</span><span class="fl">4.474</span> <span class="fl">7.69e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb183-20"><a href="chapter-regression.html#cb183-20" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb183-21"><a href="chapter-regression.html#cb183-21" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb183-22"><a href="chapter-regression.html#cb183-22" tabindex="-1"></a>   </span>
<span id="cb183-23"><a href="chapter-regression.html#cb183-23" tabindex="-1"></a>   (Dispersion parameter <span class="cf">for</span> binomial family taken to be <span class="dv">1</span>)</span>
<span id="cb183-24"><a href="chapter-regression.html#cb183-24" tabindex="-1"></a>   </span>
<span id="cb183-25"><a href="chapter-regression.html#cb183-25" tabindex="-1"></a>       Null deviance<span class="sc">:</span> <span class="fl">4075.0</span>  on <span class="dv">4999</span>  degrees of freedom</span>
<span id="cb183-26"><a href="chapter-regression.html#cb183-26" tabindex="-1"></a>   Residual deviance<span class="sc">:</span> <span class="fl">3174.3</span>  on <span class="dv">4990</span>  degrees of freedom</span>
<span id="cb183-27"><a href="chapter-regression.html#cb183-27" tabindex="-1"></a>   AIC<span class="sc">:</span> <span class="fl">3194.3</span></span>
<span id="cb183-28"><a href="chapter-regression.html#cb183-28" tabindex="-1"></a>   </span>
<span id="cb183-29"><a href="chapter-regression.html#cb183-29" tabindex="-1"></a>   Number of Fisher Scoring iterations<span class="sc">:</span> <span class="dv">6</span></span></code></pre></div>
<p>The output provides key information, including estimated coefficients, standard errors, z-statistics, and p-values. A small p-value (typically less than 0.05) suggests that the corresponding predictor has a statistically significant effect on the probability of churn. If a variable such as <code>account.length</code> has a large p-value, it suggests that the predictor does not contribute significantly to explaining churn and may be removed from the model. Refining the model by removing non-significant predictors and re-evaluating improves both interpretability and predictive performance.</p>
</div>
</div>
<div id="poisson-regression" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Poisson Regression<a class="anchor" aria-label="anchor" href="#poisson-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Poisson regression is a generalized linear model designed for modeling count data, where the response variable represents the number of occurrences of an event within a fixed interval. Examples include the number of customer service calls received daily, website visits per hour, or purchases made per customer. Unlike linear regression, which assumes normally distributed residuals, Poisson regression assumes that the response variable follows a <em>Poisson distribution</em> and that its mean equals its variance. This assumption makes Poisson regression particularly useful for data with non-negative integer counts.</p>
<p>The model is formulated as:</p>
<p><span class="math display">\[
\ln(\lambda) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> represents the expected count (mean) of the response variable, and the predictors <span class="math inline">\(x_1, x_2, \dots, x_p\)</span> influence the log of <span class="math inline">\(\lambda\)</span>. The logarithmic transformation ensures that predicted values remain positive, preventing the model from producing negative counts.</p>
<div id="poisson-regression-in-r" class="section level3 unnumbered">
<h3>Poisson Regression in R<a class="anchor" aria-label="anchor" href="#poisson-regression-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>To illustrate Poisson regression, we analyze customer service call frequency using the <em>churn</em> dataset. The objective is to model the number of customer service calls (<code>customer.calls</code>) based on customer attributes and service usage. Since <code>customer.calls</code> is an integer-valued response variable, Poisson regression is more appropriate than linear regression.</p>
<p>In R, Poisson regression is implemented using the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function, similar to logistic regression. The syntax follows:</p>
<div class="sourceCode" id="cb184"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">response_variable</span> <span class="op">~</span> <span class="va">predictor_variables</span>, data <span class="op">=</span> <span class="va">dataset</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span></span></code></pre></div>
<p>For our example, we fit a Poisson regression model as follows:</p>
<div class="sourceCode" id="cb185"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">customer.calls</span> <span class="op">~</span> <span class="va">churn</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> </span>
<span>                           <span class="va">night.mins</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">voice.plan</span></span>
<span></span>
<span><span class="va">reg_pois</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">churn</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span></span></code></pre></div>
<p>Here, <code>customer.calls</code> is the response variable, while predictors such as <code>churn</code>, <code>intl.plan</code>, and <code>day.mins</code> help explain variations in call frequency. The <code>family = poisson</code> argument specifies that the model follows a Poisson distribution.</p>
<p>Once the model is fitted, we examine the results:</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="chapter-regression.html#cb186-1" tabindex="-1"></a><span class="fu">summary</span>(reg_pois)</span>
<span id="cb186-2"><a href="chapter-regression.html#cb186-2" tabindex="-1"></a>   </span>
<span id="cb186-3"><a href="chapter-regression.html#cb186-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb186-4"><a href="chapter-regression.html#cb186-4" tabindex="-1"></a>   <span class="fu">glm</span>(<span class="at">formula =</span> formula, <span class="at">family =</span> poisson, <span class="at">data =</span> churn)</span>
<span id="cb186-5"><a href="chapter-regression.html#cb186-5" tabindex="-1"></a>   </span>
<span id="cb186-6"><a href="chapter-regression.html#cb186-6" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb186-7"><a href="chapter-regression.html#cb186-7" tabindex="-1"></a>                    Estimate Std. Error z value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>z<span class="sc">|</span>)    </span>
<span id="cb186-8"><a href="chapter-regression.html#cb186-8" tabindex="-1"></a>   (Intercept)     <span class="fl">0.9957186</span>  <span class="fl">0.1323004</span>   <span class="fl">7.526</span> <span class="fl">5.22e-14</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb186-9"><a href="chapter-regression.html#cb186-9" tabindex="-1"></a>   churnno        <span class="sc">-</span><span class="fl">0.5160641</span>  <span class="fl">0.0304013</span> <span class="sc">-</span><span class="fl">16.975</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb186-10"><a href="chapter-regression.html#cb186-10" tabindex="-1"></a>   voice.messages  <span class="fl">0.0034062</span>  <span class="fl">0.0028294</span>   <span class="fl">1.204</span> <span class="fl">0.228646</span>    </span>
<span id="cb186-11"><a href="chapter-regression.html#cb186-11" tabindex="-1"></a>   day.mins       <span class="sc">-</span><span class="fl">0.0006875</span>  <span class="fl">0.0002078</span>  <span class="sc">-</span><span class="fl">3.309</span> <span class="fl">0.000938</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb186-12"><a href="chapter-regression.html#cb186-12" tabindex="-1"></a>   eve.mins       <span class="sc">-</span><span class="fl">0.0005649</span>  <span class="fl">0.0002237</span>  <span class="sc">-</span><span class="fl">2.525</span> <span class="fl">0.011554</span> <span class="sc">*</span>  </span>
<span id="cb186-13"><a href="chapter-regression.html#cb186-13" tabindex="-1"></a>   night.mins     <span class="sc">-</span><span class="fl">0.0003602</span>  <span class="fl">0.0002245</span>  <span class="sc">-</span><span class="fl">1.604</span> <span class="fl">0.108704</span>    </span>
<span id="cb186-14"><a href="chapter-regression.html#cb186-14" tabindex="-1"></a>   intl.mins      <span class="sc">-</span><span class="fl">0.0075034</span>  <span class="fl">0.0040886</span>  <span class="sc">-</span><span class="fl">1.835</span> <span class="fl">0.066475</span> .  </span>
<span id="cb186-15"><a href="chapter-regression.html#cb186-15" tabindex="-1"></a>   intl.planno     <span class="fl">0.2085330</span>  <span class="fl">0.0407760</span>   <span class="fl">5.114</span> <span class="fl">3.15e-07</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb186-16"><a href="chapter-regression.html#cb186-16" tabindex="-1"></a>   voice.planno    <span class="fl">0.0735515</span>  <span class="fl">0.0878175</span>   <span class="fl">0.838</span> <span class="fl">0.402284</span>    </span>
<span id="cb186-17"><a href="chapter-regression.html#cb186-17" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb186-18"><a href="chapter-regression.html#cb186-18" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb186-19"><a href="chapter-regression.html#cb186-19" tabindex="-1"></a>   </span>
<span id="cb186-20"><a href="chapter-regression.html#cb186-20" tabindex="-1"></a>   (Dispersion parameter <span class="cf">for</span> poisson family taken to be <span class="dv">1</span>)</span>
<span id="cb186-21"><a href="chapter-regression.html#cb186-21" tabindex="-1"></a>   </span>
<span id="cb186-22"><a href="chapter-regression.html#cb186-22" tabindex="-1"></a>       Null deviance<span class="sc">:</span> <span class="fl">5991.1</span>  on <span class="dv">4999</span>  degrees of freedom</span>
<span id="cb186-23"><a href="chapter-regression.html#cb186-23" tabindex="-1"></a>   Residual deviance<span class="sc">:</span> <span class="fl">5719.5</span>  on <span class="dv">4991</span>  degrees of freedom</span>
<span id="cb186-24"><a href="chapter-regression.html#cb186-24" tabindex="-1"></a>   AIC<span class="sc">:</span> <span class="dv">15592</span></span>
<span id="cb186-25"><a href="chapter-regression.html#cb186-25" tabindex="-1"></a>   </span>
<span id="cb186-26"><a href="chapter-regression.html#cb186-26" tabindex="-1"></a>   Number of Fisher Scoring iterations<span class="sc">:</span> <span class="dv">5</span></span></code></pre></div>
<p>The summary output provides estimated coefficients, standard errors, z-statistics, and p-values. A small p-value (typically &lt; 0.05) suggests that a predictor significantly influences the expected number of customer calls. If predictors such as <code>voice.messages</code> or <code>night.mins</code> have large p-values, they may not contribute meaningfully and can be removed in subsequent model refinements.</p>
<p>Interpreting the coefficients in a Poisson regression model differs from linear regression. A coefficient represents the expected percentage change in the response variable for a one-unit increase in the predictor. For instance, if the coefficient of <code>intl.plan</code> is 0.3, it implies that customers with an international plan make approximately <span class="math inline">\(e^{0.3} - 1 \approx 35\%\)</span> more service calls than those without one, holding all other predictors constant.</p>
<p>In summary, Poisson regression extends the linear regression framework to count data, making it a valuable tool for event frequency modeling. Like logistic regression, it belongs to the broader family of generalized linear models, enabling flexible modeling beyond continuous response variables. By iteratively refining the model and excluding non-significant predictors, we ensure an interpretable and effective model for practical applications.</p>
<p>In the next sections, we will explore techniques for validating and improving regression models to enhance their predictive reliability.</p>
</div>
</div>
<div id="sec-stepwise-regression" class="section level2" number="10.6">
<h2>
<span class="header-section-number">10.6</span> Model Selection Using Stepwise Regression<a class="anchor" aria-label="anchor" href="#sec-stepwise-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Selecting the right predictors is essential for building a regression model that is both accurate and interpretable. This process, known as <em>model specification</em>, helps the model retain essential relationships while preventing overfitting and excluding irrelevant predictors. Proper model specification enhances predictive accuracy and ensures that insights derived from the model remain meaningful.</p>
<p>In practice, datasets—especially in business and data science applications—often contain numerous potential predictors. Managing this complexity requires systematic methods for identifying the most relevant variables. One such approach is <em>stepwise regression</em>, an iterative algorithm that evaluates predictors based on their statistical contribution to the model. Stepwise regression iteratively adds or removes predictors based on their statistical significance, ensuring that only the most relevant variables are retained.</p>
<p>Due to its structured approach, stepwise regression is particularly useful for small to medium-sized datasets where automated predictor selection improves model interpretability without excessive computational burden.</p>
<div id="the-role-of-aic-in-model-selection" class="section level3 unnumbered">
<h3>The Role of AIC in Model Selection<a class="anchor" aria-label="anchor" href="#the-role-of-aic-in-model-selection"><i class="fas fa-link"></i></a>
</h3>
<p>To evaluate model quality during the selection process, we use criteria such as the <em>Akaike Information Criterion (AIC)</em>. AIC provides a trade-off between model complexity and goodness of fit, where lower values indicate a more optimal balance between explanatory power and parsimony. It is defined as:<br><span class="math display">\[
AIC = 2p + n \log\left(\frac{SSE}{n}\right),
\]</span>
where <span class="math inline">\(p\)</span> represents the number of estimated parameters in the model, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(SSE\)</span> is the sum of squared errors, representing the total unexplained variability in the response variable and measuring the extent to which the model fails to account for observed data.</p>
<p>Unlike <span class="math inline">\(R^2\)</span>, which always increases when additional predictors are included, AIC accounts for overfitting by introducing a penalty for model complexity. This prevents overly complex models that fit the training data well but fail to generalize to new observations. By prioritizing models with a lower AIC, we select those that achieve the best balance between simplicity and predictive accuracy.</p>
</div>
<div id="implementing-stepwise-regression-in-r" class="section level3 unnumbered">
<h3>Implementing Stepwise Regression in R<a class="anchor" aria-label="anchor" href="#implementing-stepwise-regression-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>Stepwise regression is implemented in R using the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function, which automates the selection of predictors to find an optimal model. The function iteratively evaluates variables and makes inclusion or exclusion decisions based on statistical criteria. Three approaches can be specified using the <code>direction</code> argument: <code>"forward"</code>, which starts with no predictors and adds them incrementally; <code>"backward"</code>, which begins with all predictors and removes the least significant ones; and <code>"both"</code>, which combines forward selection and backward elimination to refine the model in an iterative process.</p>
<div class="example">
<p><span id="exm:ex-stepwise-regression" class="example"><strong>Example 10.1  </strong></span>To illustrate stepwise regression, we apply it to the <em>marketing</em> dataset, which contains seven predictors. The objective is to identify the best regression model for predicting <code>revenue</code> while ensuring a balance between model complexity and interpretability.</p>
<p>We begin by fitting a regression model that includes all available predictors:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="chapter-regression.html#cb187-1" tabindex="-1"></a>ml_all <span class="ot">=</span> <span class="fu">lm</span>(revenue <span class="sc">~</span> ., <span class="at">data =</span> marketing)</span>
<span id="cb187-2"><a href="chapter-regression.html#cb187-2" tabindex="-1"></a></span>
<span id="cb187-3"><a href="chapter-regression.html#cb187-3" tabindex="-1"></a><span class="fu">summary</span>(ml_all)</span>
<span id="cb187-4"><a href="chapter-regression.html#cb187-4" tabindex="-1"></a>   </span>
<span id="cb187-5"><a href="chapter-regression.html#cb187-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb187-6"><a href="chapter-regression.html#cb187-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> ., <span class="at">data =</span> marketing)</span>
<span id="cb187-7"><a href="chapter-regression.html#cb187-7" tabindex="-1"></a>   </span>
<span id="cb187-8"><a href="chapter-regression.html#cb187-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb187-9"><a href="chapter-regression.html#cb187-9" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb187-10"><a href="chapter-regression.html#cb187-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">138.00</span>  <span class="sc">-</span><span class="fl">59.12</span>   <span class="fl">15.16</span>   <span class="fl">54.58</span>  <span class="fl">106.99</span> </span>
<span id="cb187-11"><a href="chapter-regression.html#cb187-11" tabindex="-1"></a>   </span>
<span id="cb187-12"><a href="chapter-regression.html#cb187-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb187-13"><a href="chapter-regression.html#cb187-13" tabindex="-1"></a>                     Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)</span>
<span id="cb187-14"><a href="chapter-regression.html#cb187-14" tabindex="-1"></a>   (Intercept)     <span class="sc">-</span><span class="fl">25.260020</span> <span class="fl">246.988978</span>  <span class="sc">-</span><span class="fl">0.102</span>    <span class="fl">0.919</span></span>
<span id="cb187-15"><a href="chapter-regression.html#cb187-15" tabindex="-1"></a>   spend            <span class="sc">-</span><span class="fl">0.025807</span>   <span class="fl">2.605645</span>  <span class="sc">-</span><span class="fl">0.010</span>    <span class="fl">0.992</span></span>
<span id="cb187-16"><a href="chapter-regression.html#cb187-16" tabindex="-1"></a>   clicks            <span class="fl">1.211912</span>   <span class="fl">1.630953</span>   <span class="fl">0.743</span>    <span class="fl">0.463</span></span>
<span id="cb187-17"><a href="chapter-regression.html#cb187-17" tabindex="-1"></a>   impressions      <span class="sc">-</span><span class="fl">0.005308</span>   <span class="fl">0.021588</span>  <span class="sc">-</span><span class="fl">0.246</span>    <span class="fl">0.807</span></span>
<span id="cb187-18"><a href="chapter-regression.html#cb187-18" tabindex="-1"></a>   display          <span class="fl">79.835729</span> <span class="fl">117.558849</span>   <span class="fl">0.679</span>    <span class="fl">0.502</span></span>
<span id="cb187-19"><a href="chapter-regression.html#cb187-19" tabindex="-1"></a>   transactions     <span class="sc">-</span><span class="fl">7.012069</span>  <span class="fl">66.383251</span>  <span class="sc">-</span><span class="fl">0.106</span>    <span class="fl">0.917</span></span>
<span id="cb187-20"><a href="chapter-regression.html#cb187-20" tabindex="-1"></a>   click.rate      <span class="sc">-</span><span class="fl">10.951493</span> <span class="fl">106.833894</span>  <span class="sc">-</span><span class="fl">0.103</span>    <span class="fl">0.919</span></span>
<span id="cb187-21"><a href="chapter-regression.html#cb187-21" tabindex="-1"></a>   conversion.rate  <span class="fl">19.926588</span> <span class="fl">135.746632</span>   <span class="fl">0.147</span>    <span class="fl">0.884</span></span>
<span id="cb187-22"><a href="chapter-regression.html#cb187-22" tabindex="-1"></a>   </span>
<span id="cb187-23"><a href="chapter-regression.html#cb187-23" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">77.61</span> on <span class="dv">32</span> degrees of freedom</span>
<span id="cb187-24"><a href="chapter-regression.html#cb187-24" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7829</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7354</span> </span>
<span id="cb187-25"><a href="chapter-regression.html#cb187-25" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">16.48</span> on <span class="dv">7</span> and <span class="dv">32</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">5.498e-09</span></span></code></pre></div>
<p>The initial model includes all predictors, but some may not contribute meaningfully to explaining <code>revenue</code>. Evaluating model fit using the Akaike Information Criterion (AIC) helps balance predictive accuracy with model simplicity.</p>
<p>Next, we apply stepwise regression using the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function, setting <code>direction = "both"</code> to allow for both forward selection and backward elimination:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="chapter-regression.html#cb188-1" tabindex="-1"></a>ml_stepwise <span class="ot">=</span> <span class="fu">step</span>(ml_all, <span class="at">direction =</span> <span class="st">"both"</span>)</span>
<span id="cb188-2"><a href="chapter-regression.html#cb188-2" tabindex="-1"></a>   Start<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">355.21</span></span>
<span id="cb188-3"><a href="chapter-regression.html#cb188-3" tabindex="-1"></a>   revenue <span class="sc">~</span> spend <span class="sc">+</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> </span>
<span id="cb188-4"><a href="chapter-regression.html#cb188-4" tabindex="-1"></a>       click.rate <span class="sc">+</span> conversion.rate</span>
<span id="cb188-5"><a href="chapter-regression.html#cb188-5" tabindex="-1"></a>   </span>
<span id="cb188-6"><a href="chapter-regression.html#cb188-6" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb188-7"><a href="chapter-regression.html#cb188-7" tabindex="-1"></a>   <span class="sc">-</span> spend            <span class="dv">1</span>       <span class="fl">0.6</span> <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb188-8"><a href="chapter-regression.html#cb188-8" tabindex="-1"></a>   <span class="sc">-</span> click.rate       <span class="dv">1</span>      <span class="fl">63.3</span> <span class="dv">192822</span> <span class="fl">353.23</span></span>
<span id="cb188-9"><a href="chapter-regression.html#cb188-9" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">67.2</span> <span class="dv">192826</span> <span class="fl">353.23</span></span>
<span id="cb188-10"><a href="chapter-regression.html#cb188-10" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">129.8</span> <span class="dv">192889</span> <span class="fl">353.24</span></span>
<span id="cb188-11"><a href="chapter-regression.html#cb188-11" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">364.2</span> <span class="dv">193123</span> <span class="fl">353.29</span></span>
<span id="cb188-12"><a href="chapter-regression.html#cb188-12" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">2778.1</span> <span class="dv">195537</span> <span class="fl">353.79</span></span>
<span id="cb188-13"><a href="chapter-regression.html#cb188-13" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3326.0</span> <span class="dv">196085</span> <span class="fl">353.90</span></span>
<span id="cb188-14"><a href="chapter-regression.html#cb188-14" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192759</span> <span class="fl">355.21</span></span>
<span id="cb188-15"><a href="chapter-regression.html#cb188-15" tabindex="-1"></a>   </span>
<span id="cb188-16"><a href="chapter-regression.html#cb188-16" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">353.21</span></span>
<span id="cb188-17"><a href="chapter-regression.html#cb188-17" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> click.rate <span class="sc">+</span> </span>
<span id="cb188-18"><a href="chapter-regression.html#cb188-18" tabindex="-1"></a>       conversion.rate</span>
<span id="cb188-19"><a href="chapter-regression.html#cb188-19" tabindex="-1"></a>   </span>
<span id="cb188-20"><a href="chapter-regression.html#cb188-20" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb188-21"><a href="chapter-regression.html#cb188-21" tabindex="-1"></a>   <span class="sc">-</span> click.rate       <span class="dv">1</span>      <span class="fl">67.9</span> <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb188-22"><a href="chapter-regression.html#cb188-22" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">75.1</span> <span class="dv">192835</span> <span class="fl">351.23</span></span>
<span id="cb188-23"><a href="chapter-regression.html#cb188-23" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">151.5</span> <span class="dv">192911</span> <span class="fl">351.24</span></span>
<span id="cb188-24"><a href="chapter-regression.html#cb188-24" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">380.8</span> <span class="dv">193141</span> <span class="fl">351.29</span></span>
<span id="cb188-25"><a href="chapter-regression.html#cb188-25" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">2787.2</span> <span class="dv">195547</span> <span class="fl">351.79</span></span>
<span id="cb188-26"><a href="chapter-regression.html#cb188-26" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3325.6</span> <span class="dv">196085</span> <span class="fl">351.90</span></span>
<span id="cb188-27"><a href="chapter-regression.html#cb188-27" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb188-28"><a href="chapter-regression.html#cb188-28" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>       <span class="fl">0.6</span> <span class="dv">192759</span> <span class="fl">355.21</span></span>
<span id="cb188-29"><a href="chapter-regression.html#cb188-29" tabindex="-1"></a>   </span>
<span id="cb188-30"><a href="chapter-regression.html#cb188-30" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">351.23</span></span>
<span id="cb188-31"><a href="chapter-regression.html#cb188-31" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> conversion.rate</span>
<span id="cb188-32"><a href="chapter-regression.html#cb188-32" tabindex="-1"></a>   </span>
<span id="cb188-33"><a href="chapter-regression.html#cb188-33" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb188-34"><a href="chapter-regression.html#cb188-34" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">47.4</span> <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb188-35"><a href="chapter-regression.html#cb188-35" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">129.0</span> <span class="dv">192957</span> <span class="fl">349.25</span></span>
<span id="cb188-36"><a href="chapter-regression.html#cb188-36" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">312.9</span> <span class="dv">193141</span> <span class="fl">349.29</span></span>
<span id="cb188-37"><a href="chapter-regression.html#cb188-37" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3425.7</span> <span class="dv">196253</span> <span class="fl">349.93</span></span>
<span id="cb188-38"><a href="chapter-regression.html#cb188-38" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">3747.1</span> <span class="dv">196575</span> <span class="fl">350.00</span></span>
<span id="cb188-39"><a href="chapter-regression.html#cb188-39" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb188-40"><a href="chapter-regression.html#cb188-40" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>      <span class="fl">67.9</span> <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb188-41"><a href="chapter-regression.html#cb188-41" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>       <span class="fl">5.2</span> <span class="dv">192822</span> <span class="fl">353.23</span></span>
<span id="cb188-42"><a href="chapter-regression.html#cb188-42" tabindex="-1"></a>   </span>
<span id="cb188-43"><a href="chapter-regression.html#cb188-43" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">349.24</span></span>
<span id="cb188-44"><a href="chapter-regression.html#cb188-44" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> conversion.rate</span>
<span id="cb188-45"><a href="chapter-regression.html#cb188-45" tabindex="-1"></a>   </span>
<span id="cb188-46"><a href="chapter-regression.html#cb188-46" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb188-47"><a href="chapter-regression.html#cb188-47" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>      <span class="fl">89.6</span> <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb188-48"><a href="chapter-regression.html#cb188-48" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">480.9</span> <span class="dv">193356</span> <span class="fl">347.34</span></span>
<span id="cb188-49"><a href="chapter-regression.html#cb188-49" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">5437.2</span> <span class="dv">198312</span> <span class="fl">348.35</span></span>
<span id="cb188-50"><a href="chapter-regression.html#cb188-50" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb188-51"><a href="chapter-regression.html#cb188-51" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>      <span class="fl">47.4</span> <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb188-52"><a href="chapter-regression.html#cb188-52" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>      <span class="fl">40.2</span> <span class="dv">192835</span> <span class="fl">351.23</span></span>
<span id="cb188-53"><a href="chapter-regression.html#cb188-53" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>      <span class="fl">13.6</span> <span class="dv">192861</span> <span class="fl">351.23</span></span>
<span id="cb188-54"><a href="chapter-regression.html#cb188-54" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>   <span class="fl">30863.2</span> <span class="dv">223738</span> <span class="fl">353.17</span></span>
<span id="cb188-55"><a href="chapter-regression.html#cb188-55" tabindex="-1"></a>   </span>
<span id="cb188-56"><a href="chapter-regression.html#cb188-56" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">347.26</span></span>
<span id="cb188-57"><a href="chapter-regression.html#cb188-57" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display</span>
<span id="cb188-58"><a href="chapter-regression.html#cb188-58" tabindex="-1"></a>   </span>
<span id="cb188-59"><a href="chapter-regression.html#cb188-59" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb188-60"><a href="chapter-regression.html#cb188-60" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>       <span class="dv">399</span> <span class="dv">193364</span> <span class="fl">345.34</span></span>
<span id="cb188-61"><a href="chapter-regression.html#cb188-61" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb188-62"><a href="chapter-regression.html#cb188-62" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>     <span class="dv">14392</span> <span class="dv">207357</span> <span class="fl">348.13</span></span>
<span id="cb188-63"><a href="chapter-regression.html#cb188-63" tabindex="-1"></a>   <span class="sc">+</span> conversion.rate  <span class="dv">1</span>        <span class="dv">90</span> <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb188-64"><a href="chapter-regression.html#cb188-64" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>        <span class="dv">52</span> <span class="dv">192913</span> <span class="fl">349.24</span></span>
<span id="cb188-65"><a href="chapter-regression.html#cb188-65" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>        <span class="dv">33</span> <span class="dv">192932</span> <span class="fl">349.25</span></span>
<span id="cb188-66"><a href="chapter-regression.html#cb188-66" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>         <span class="dv">8</span> <span class="dv">192957</span> <span class="fl">349.25</span></span>
<span id="cb188-67"><a href="chapter-regression.html#cb188-67" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>     <span class="dv">35038</span> <span class="dv">228002</span> <span class="fl">351.93</span></span>
<span id="cb188-68"><a href="chapter-regression.html#cb188-68" tabindex="-1"></a>   </span>
<span id="cb188-69"><a href="chapter-regression.html#cb188-69" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">345.34</span></span>
<span id="cb188-70"><a href="chapter-regression.html#cb188-70" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> display</span>
<span id="cb188-71"><a href="chapter-regression.html#cb188-71" tabindex="-1"></a>   </span>
<span id="cb188-72"><a href="chapter-regression.html#cb188-72" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb188-73"><a href="chapter-regression.html#cb188-73" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">193364</span> <span class="fl">345.34</span></span>
<span id="cb188-74"><a href="chapter-regression.html#cb188-74" tabindex="-1"></a>   <span class="sc">+</span> impressions      <span class="dv">1</span>       <span class="dv">399</span> <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb188-75"><a href="chapter-regression.html#cb188-75" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>       <span class="dv">215</span> <span class="dv">193149</span> <span class="fl">347.29</span></span>
<span id="cb188-76"><a href="chapter-regression.html#cb188-76" tabindex="-1"></a>   <span class="sc">+</span> conversion.rate  <span class="dv">1</span>         <span class="dv">8</span> <span class="dv">193356</span> <span class="fl">347.34</span></span>
<span id="cb188-77"><a href="chapter-regression.html#cb188-77" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>         <span class="dv">6</span> <span class="dv">193358</span> <span class="fl">347.34</span></span>
<span id="cb188-78"><a href="chapter-regression.html#cb188-78" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>         <span class="dv">2</span> <span class="dv">193362</span> <span class="fl">347.34</span></span>
<span id="cb188-79"><a href="chapter-regression.html#cb188-79" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>     <span class="dv">91225</span> <span class="dv">284589</span> <span class="fl">358.80</span></span>
<span id="cb188-80"><a href="chapter-regression.html#cb188-80" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="dv">606800</span> <span class="dv">800164</span> <span class="fl">400.15</span></span></code></pre></div>
<p>The algorithm iteratively assesses each predictor’s contribution, removing those that do not improve model performance or adding those that enhance it, based on AIC. For example, <code>spend</code> is removed in the first iteration as it does not significantly enhance the model. The stepwise process continues until no further improvements can be made, terminating after 6 iterations.</p>
<p>Tracking AIC values throughout the selection process allows us to quantify model improvements. The initial full model, which includes all predictors, has an AIC value of 355.21. After multiple iterations, the final model achieves a lower AIC value of 345.34, indicating a more efficient model with improved fit.</p>
<p>To examine the final selected model, we use:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="chapter-regression.html#cb189-1" tabindex="-1"></a><span class="fu">summary</span>(ml_stepwise)</span>
<span id="cb189-2"><a href="chapter-regression.html#cb189-2" tabindex="-1"></a>   </span>
<span id="cb189-3"><a href="chapter-regression.html#cb189-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb189-4"><a href="chapter-regression.html#cb189-4" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> clicks <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb189-5"><a href="chapter-regression.html#cb189-5" tabindex="-1"></a>   </span>
<span id="cb189-6"><a href="chapter-regression.html#cb189-6" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb189-7"><a href="chapter-regression.html#cb189-7" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb189-8"><a href="chapter-regression.html#cb189-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">141.89</span>  <span class="sc">-</span><span class="fl">55.92</span>   <span class="fl">16.44</span>   <span class="fl">52.70</span>  <span class="fl">115.46</span> </span>
<span id="cb189-9"><a href="chapter-regression.html#cb189-9" tabindex="-1"></a>   </span>
<span id="cb189-10"><a href="chapter-regression.html#cb189-10" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb189-11"><a href="chapter-regression.html#cb189-11" tabindex="-1"></a>                Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb189-12"><a href="chapter-regression.html#cb189-12" tabindex="-1"></a>   (Intercept) <span class="sc">-</span><span class="fl">33.63248</span>   <span class="fl">28.68893</span>  <span class="sc">-</span><span class="fl">1.172</span> <span class="fl">0.248564</span>    </span>
<span id="cb189-13"><a href="chapter-regression.html#cb189-13" tabindex="-1"></a>   clicks        <span class="fl">0.89517</span>    <span class="fl">0.08308</span>  <span class="fl">10.775</span> <span class="fl">5.76e-13</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb189-14"><a href="chapter-regression.html#cb189-14" tabindex="-1"></a>   display      <span class="fl">95.51462</span>   <span class="fl">22.86126</span>   <span class="fl">4.178</span> <span class="fl">0.000172</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb189-15"><a href="chapter-regression.html#cb189-15" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb189-16"><a href="chapter-regression.html#cb189-16" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb189-17"><a href="chapter-regression.html#cb189-17" tabindex="-1"></a>   </span>
<span id="cb189-18"><a href="chapter-regression.html#cb189-18" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">72.29</span> on <span class="dv">37</span> degrees of freedom</span>
<span id="cb189-19"><a href="chapter-regression.html#cb189-19" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7822</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7704</span> </span>
<span id="cb189-20"><a href="chapter-regression.html#cb189-20" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">66.44</span> on <span class="dv">2</span> and <span class="dv">37</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">5.682e-13</span></span></code></pre></div>
<p>Stepwise regression results in a more parsimonious model with only two predictors: <code>clicks</code> and <code>display</code>. The refined regression equation is:</p>
<p><span class="math display">\[
\hat{\text{revenue}} = -33.63 + 0.9 \cdot \text{clicks} + 95.51 \cdot \text{display}
\]</span></p>
<p>The final model demonstrates an improved fit compared to the initial full model. The <strong>Residual Standard Error (RSE)</strong>, which measures typical prediction error, has decreased from approximately 93.82 to 72.29, indicating improved accuracy. The <strong>R-squared (<span class="math inline">\(R^2\)</span>)</strong> value has increased from 62% to 77%, suggesting that a greater proportion of the variability in <code>revenue</code> is now explained by the selected predictors.</p>
</div>
</div>
<div id="strengths-limitations-and-considerations-for-stepwise-regression" class="section level3 unnumbered">
<h3>Strengths, Limitations, and Considerations for Stepwise Regression<a class="anchor" aria-label="anchor" href="#strengths-limitations-and-considerations-for-stepwise-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Stepwise regression offers a systematic approach to model selection, balancing interpretability and efficiency. By iteratively refining the set of predictors, it helps identify an optimal model without manually testing every possible combination. However, stepwise regression also has important limitations that should be considered.</p>
<p>One key limitation is that the algorithm evaluates predictors sequentially rather than exhaustively considering all possible subsets of variables. This can sometimes result in suboptimal models, especially when strong predictor interactions are ignored. Additionally, stepwise regression is prone to <strong>overfitting</strong>, particularly in small datasets with many predictors. Overfitting occurs when the model captures random noise rather than meaningful relationships, reducing its generalizability to new data. Furthermore, the presence of <strong>multicollinearity</strong> among predictors can distort coefficient estimates and p-values, leading to misleading conclusions.</p>
<p>For high-dimensional datasets or cases where predictor selection must be more robust, alternative methods such as <em>LASSO</em> (Least Absolute Shrinkage and Selection Operator) and <em>Ridge Regression</em> are often preferred. These techniques introduce regularization, which helps stabilize model estimates and improve predictive accuracy by penalizing overly complex models. For further exploration, refer to <a href="https://www.statlearning.com">An Introduction to Statistical Learning with Applications in R</a>.<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;James Gareth et al., &lt;em&gt;An Introduction to Statistical Learning: With Applications in r&lt;/em&gt; (Spinger, 2013).&lt;/p&gt;"><sup>8</sup></a></span></p>
<p>Careful model specification is a crucial step in regression analysis. By selecting predictors systematically and evaluating model performance with appropriate criteria, we can construct models that are both accurate and interpretable. While stepwise regression has its limitations, it remains a widely used method for predictor selection in datasets of moderate size. Its ability to enhance predictive performance while maintaining simplicity makes it a valuable tool in data-driven decision-making.</p>
</div>
</div>
<div id="extending-linear-models-to-capture-non-linear-relationships" class="section level2" number="10.7">
<h2>
<span class="header-section-number">10.7</span> Extending Linear Models to Capture Non-Linear Relationships<a class="anchor" aria-label="anchor" href="#extending-linear-models-to-capture-non-linear-relationships"><i class="fas fa-link"></i></a>
</h2>
<p>Thus far, we have focused on linear regression models, which are simple, interpretable, and easy to implement. While these models work well when relationships between predictors and response variables are approximately linear, their predictive power is limited when the relationships exhibit curvature or other forms of non-linearity. In such cases, assuming a strictly linear relationship can lead to poor model performance and inaccurate predictions.</p>
<p>Earlier, we explored techniques such as stepwise regression (Section <a href="chapter-regression.html#sec-stepwise-regression">10.6</a>) to refine model selection by reducing complexity and addressing multicollinearity. However, these methods do not account for non-linearity in relationships between predictors and the response variable. To address this limitation while maintaining model interpretability, we turn to <em>polynomial regression</em>, an extension of linear regression that introduces non-linear terms.</p>
<div id="the-need-for-non-linear-regression" class="section level3 unnumbered">
<h3>The Need for Non-Linear Regression<a class="anchor" aria-label="anchor" href="#the-need-for-non-linear-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Linear regression assumes a constant rate of change between predictors and the response variable, resulting in a straight-line relationship. However, many real-world datasets exhibit more complex patterns. Consider the scatter plot in Figure <a href="chapter-regression.html#fig:scoter-plot-non-reg">10.2</a>, which depicts the relationship between <code>unit.price</code> (house price per unit area) and <code>house.age</code> (age of the house) from the <em>house</em> dataset. The orange line represents a simple linear regression fit, which does not adequately capture the curvature in the data.</p>
<p>To better model this relationship, we can introduce non-linear terms into the regression equation. If the data suggests a quadratic trend, the model can be expressed as:</p>
<p><span class="math display">\[
unit.price = b_0 + b_1 \cdot house.age + b_2 \cdot house.age^2
\]</span></p>
<p>This equation incorporates both <code>house.age</code> and its squared term (<code>house.age^2</code>), allowing for a curved relationship between the predictor and response variable. Although polynomial regression introduces non-linear predictors, the model remains a <em>linear regression model</em> because the coefficients (<span class="math inline">\(b_0, b_1, b_2\)</span>) are estimated using standard least squares methods. The blue curve in Figure <a href="chapter-regression.html#fig:scoter-plot-non-reg">10.2</a> illustrates the improved fit of a quadratic regression model, which captures the pattern in the data more effectively than the simple linear model.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scoter-plot-non-reg"></span>
<img src="regression_files/figure-html/scoter-plot-non-reg-1.png" alt="Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in orange and the quadratic regression curve in blue." width="100%"><p class="caption">
Figure 10.2: Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in orange and the quadratic regression curve in blue.
</p>
</div>
<p>This example highlights the need for non-linear regression techniques when the assumption of linearity does not hold. By incorporating polynomial terms, we can improve model accuracy while retaining interpretability, ensuring that predictions align more closely with real-world data patterns.</p>
</div>
</div>
<div id="polynomial-regression" class="section level2" number="10.8">
<h2>
<span class="header-section-number">10.8</span> Polynomial Regression<a class="anchor" aria-label="anchor" href="#polynomial-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Polynomial regression extends linear regression by incorporating higher-degree terms of the predictor variable, such as squared (<span class="math inline">\(x^2\)</span>) or cubic (<span class="math inline">\(x^3\)</span>) terms. This allows the model to capture non-linear relationships while remaining <em>linear in the coefficients</em>, meaning it can still be estimated using least squares. The general polynomial regression model is given by:</p>
<p><span class="math display">\[
\hat{y} = b_0 + b_1 \cdot x + b_2 \cdot x^2 + \dots + b_d \cdot x^d
\]</span></p>
<p>where <span class="math inline">\(d\)</span> represents the degree of the polynomial. While polynomial regression provides flexibility, higher-degree polynomials (<span class="math inline">\(d &gt; 3\)</span>) can lead to overfitting, capturing noise rather than meaningful patterns, particularly at the boundaries of the predictor range.</p>
<div class="example">
<p><span id="exm:ex-polynomial-regression" class="example"><strong>Example 10.2  </strong></span>To illustrate polynomial regression, we use the <em>house</em> dataset from the <strong>liver</strong> package. This dataset includes housing prices and features such as age, proximity to public transport, and local amenities. Our goal is to model <code>unit.price</code> (house price per unit area) as a function of <code>house.age</code> and compare the performance of simple linear regression to polynomial regression.</p>
<p>First, we load the dataset and examine its structure:</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="chapter-regression.html#cb190-1" tabindex="-1"></a><span class="fu">data</span>(house)</span>
<span id="cb190-2"><a href="chapter-regression.html#cb190-2" tabindex="-1"></a></span>
<span id="cb190-3"><a href="chapter-regression.html#cb190-3" tabindex="-1"></a><span class="fu">str</span>(house)</span>
<span id="cb190-4"><a href="chapter-regression.html#cb190-4" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">414</span> obs. of  <span class="dv">6</span> variables<span class="sc">:</span></span>
<span id="cb190-5"><a href="chapter-regression.html#cb190-5" tabindex="-1"></a>    <span class="er">$</span> house.age      <span class="sc">:</span> num  <span class="dv">32</span> <span class="fl">19.5</span> <span class="fl">13.3</span> <span class="fl">13.3</span> <span class="dv">5</span> <span class="fl">7.1</span> <span class="fl">34.5</span> <span class="fl">20.3</span> <span class="fl">31.7</span> <span class="fl">17.9</span> ...</span>
<span id="cb190-6"><a href="chapter-regression.html#cb190-6" tabindex="-1"></a>    <span class="sc">$</span> distance.to.MRT<span class="sc">:</span> num  <span class="fl">84.9</span> <span class="fl">306.6</span> <span class="dv">562</span> <span class="dv">562</span> <span class="fl">390.6</span> ...</span>
<span id="cb190-7"><a href="chapter-regression.html#cb190-7" tabindex="-1"></a>    <span class="sc">$</span> stores.number  <span class="sc">:</span> int  <span class="dv">10</span> <span class="dv">9</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">7</span> <span class="dv">6</span> <span class="dv">1</span> <span class="dv">3</span> ...</span>
<span id="cb190-8"><a href="chapter-regression.html#cb190-8" tabindex="-1"></a>    <span class="sc">$</span> latitude       <span class="sc">:</span> num  <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> ...</span>
<span id="cb190-9"><a href="chapter-regression.html#cb190-9" tabindex="-1"></a>    <span class="sc">$</span> longitude      <span class="sc">:</span> num  <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> ...</span>
<span id="cb190-10"><a href="chapter-regression.html#cb190-10" tabindex="-1"></a>    <span class="sc">$</span> unit.price     <span class="sc">:</span> num  <span class="fl">37.9</span> <span class="fl">42.2</span> <span class="fl">47.3</span> <span class="fl">54.8</span> <span class="fl">43.1</span> <span class="fl">32.1</span> <span class="fl">40.3</span> <span class="fl">46.7</span> <span class="fl">18.8</span> <span class="fl">22.1</span> ...</span></code></pre></div>
<p>The dataset consists of 414 observations and 6 variables. The target variable is <code>unit.price</code>, while predictors include <code>house.age</code> (years), <code>distance.to.MRT</code> (distance to the nearest MRT station), <code>stores.number</code> (number of nearby convenience stores), <code>latitude</code>, and <code>longitude</code>.</p>
<p>We begin by fitting a simple linear regression model:</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="chapter-regression.html#cb191-1" tabindex="-1"></a>simple_reg_house <span class="ot">=</span> <span class="fu">lm</span>(unit.price <span class="sc">~</span> house.age, <span class="at">data =</span> house)</span>
<span id="cb191-2"><a href="chapter-regression.html#cb191-2" tabindex="-1"></a></span>
<span id="cb191-3"><a href="chapter-regression.html#cb191-3" tabindex="-1"></a><span class="fu">summary</span>(simple_reg_house)</span>
<span id="cb191-4"><a href="chapter-regression.html#cb191-4" tabindex="-1"></a>   </span>
<span id="cb191-5"><a href="chapter-regression.html#cb191-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb191-6"><a href="chapter-regression.html#cb191-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> unit.price <span class="sc">~</span> house.age, <span class="at">data =</span> house)</span>
<span id="cb191-7"><a href="chapter-regression.html#cb191-7" tabindex="-1"></a>   </span>
<span id="cb191-8"><a href="chapter-regression.html#cb191-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb191-9"><a href="chapter-regression.html#cb191-9" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb191-10"><a href="chapter-regression.html#cb191-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">31.113</span> <span class="sc">-</span><span class="fl">10.738</span>   <span class="fl">1.626</span>   <span class="fl">8.199</span>  <span class="fl">77.781</span> </span>
<span id="cb191-11"><a href="chapter-regression.html#cb191-11" tabindex="-1"></a>   </span>
<span id="cb191-12"><a href="chapter-regression.html#cb191-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb191-13"><a href="chapter-regression.html#cb191-13" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb191-14"><a href="chapter-regression.html#cb191-14" tabindex="-1"></a>   (Intercept) <span class="fl">42.43470</span>    <span class="fl">1.21098</span>  <span class="fl">35.042</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb191-15"><a href="chapter-regression.html#cb191-15" tabindex="-1"></a>   house.age   <span class="sc">-</span><span class="fl">0.25149</span>    <span class="fl">0.05752</span>  <span class="sc">-</span><span class="fl">4.372</span> <span class="fl">1.56e-05</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb191-16"><a href="chapter-regression.html#cb191-16" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb191-17"><a href="chapter-regression.html#cb191-17" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb191-18"><a href="chapter-regression.html#cb191-18" tabindex="-1"></a>   </span>
<span id="cb191-19"><a href="chapter-regression.html#cb191-19" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">13.32</span> on <span class="dv">412</span> degrees of freedom</span>
<span id="cb191-20"><a href="chapter-regression.html#cb191-20" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.04434</span>,    Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.04202</span> </span>
<span id="cb191-21"><a href="chapter-regression.html#cb191-21" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">19.11</span> on <span class="dv">1</span> and <span class="dv">412</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.56e-05</span></span></code></pre></div>
<p>The <em>R-squared (<span class="math inline">\(R^2\)</span>)</em> value for this model is 0.04, indicating that only 4.43% of the variability in house prices is explained by <code>house.age</code>. This suggests that the linear model does not fully capture the relationship.</p>
<p>Next, we fit a quadratic polynomial regression model to introduce curvature:</p>
<p><span class="math display">\[
unit.price = b_0 + b_1 \cdot house.age + b_2 \cdot house.age^2
\]</span></p>
<p>This can be implemented in R using the <code><a href="https://rdrr.io/r/stats/poly.html">poly()</a></code> function:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="chapter-regression.html#cb192-1" tabindex="-1"></a>reg_nonlinear_house <span class="ot">=</span> <span class="fu">lm</span>(unit.price <span class="sc">~</span> <span class="fu">poly</span>(house.age, <span class="dv">2</span>), <span class="at">data =</span> house)</span>
<span id="cb192-2"><a href="chapter-regression.html#cb192-2" tabindex="-1"></a></span>
<span id="cb192-3"><a href="chapter-regression.html#cb192-3" tabindex="-1"></a><span class="fu">summary</span>(reg_nonlinear_house)</span>
<span id="cb192-4"><a href="chapter-regression.html#cb192-4" tabindex="-1"></a>   </span>
<span id="cb192-5"><a href="chapter-regression.html#cb192-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb192-6"><a href="chapter-regression.html#cb192-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> unit.price <span class="sc">~</span> <span class="fu">poly</span>(house.age, <span class="dv">2</span>), <span class="at">data =</span> house)</span>
<span id="cb192-7"><a href="chapter-regression.html#cb192-7" tabindex="-1"></a>   </span>
<span id="cb192-8"><a href="chapter-regression.html#cb192-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb192-9"><a href="chapter-regression.html#cb192-9" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb192-10"><a href="chapter-regression.html#cb192-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">26.542</span>  <span class="sc">-</span><span class="fl">9.085</span>  <span class="sc">-</span><span class="fl">0.445</span>   <span class="fl">8.260</span>  <span class="fl">79.961</span> </span>
<span id="cb192-11"><a href="chapter-regression.html#cb192-11" tabindex="-1"></a>   </span>
<span id="cb192-12"><a href="chapter-regression.html#cb192-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb192-13"><a href="chapter-regression.html#cb192-13" tabindex="-1"></a>                       Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb192-14"><a href="chapter-regression.html#cb192-14" tabindex="-1"></a>   (Intercept)           <span class="fl">37.980</span>      <span class="fl">0.599</span>  <span class="fl">63.406</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb192-15"><a href="chapter-regression.html#cb192-15" tabindex="-1"></a>   <span class="fu">poly</span>(house.age, <span class="dv">2</span>)<span class="dv">1</span>  <span class="sc">-</span><span class="fl">58.225</span>     <span class="fl">12.188</span>  <span class="sc">-</span><span class="fl">4.777</span> <span class="fl">2.48e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb192-16"><a href="chapter-regression.html#cb192-16" tabindex="-1"></a>   <span class="fu">poly</span>(house.age, <span class="dv">2</span>)<span class="dv">2</span>  <span class="fl">109.635</span>     <span class="fl">12.188</span>   <span class="fl">8.995</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb192-17"><a href="chapter-regression.html#cb192-17" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb192-18"><a href="chapter-regression.html#cb192-18" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb192-19"><a href="chapter-regression.html#cb192-19" tabindex="-1"></a>   </span>
<span id="cb192-20"><a href="chapter-regression.html#cb192-20" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">12.19</span> on <span class="dv">411</span> degrees of freedom</span>
<span id="cb192-21"><a href="chapter-regression.html#cb192-21" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.2015</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.1977</span> </span>
<span id="cb192-22"><a href="chapter-regression.html#cb192-22" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">51.87</span> on <span class="dv">2</span> and <span class="dv">411</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="er">&lt;</span> <span class="fl">2.2e-16</span></span></code></pre></div>
<p>The quadratic model achieves a significantly higher <em>R-squared (<span class="math inline">\(R^2\)</span>)</em> value of 0.2, compared to the simple regression model. Additionally, the <em>Residual Standard Error (RSE)</em> is lower, indicating smaller prediction errors. These improvements confirm that incorporating a quadratic term better captures the non-linear relationship between house age and price.</p>
</div>
<p>Polynomial regression effectively extends linear regression by allowing for curvature in the data. However, selecting an appropriate polynomial degree is crucial to avoid overfitting. More advanced techniques, such as splines and generalized additive models, provide additional flexibility while addressing some of the limitations of polynomial regression. These techniques are discussed in Chapter 7 of <a href="https://www.statlearning.com">An Introduction to Statistical Learning with Applications in R</a>.<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Gareth et al.&lt;/p&gt;"><sup>9</sup></a></span></p>
</div>
<div id="diagnosing-and-validating-regression-models" class="section level2" number="10.9">
<h2>
<span class="header-section-number">10.9</span> Diagnosing and Validating Regression Models<a class="anchor" aria-label="anchor" href="#diagnosing-and-validating-regression-models"><i class="fas fa-link"></i></a>
</h2>
<p>Before deploying a regression model, it is essential to validate its assumptions. Ignoring these assumptions is akin to constructing a house on an unstable foundation—predictions based on an invalid model can lead to misleading conclusions and costly mistakes. Model diagnostics ensure that the model is robust, reliable, and appropriate for making predictions.</p>
<p>Linear regression models rely on several key assumptions:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Independence</strong>: Observations should be independent, meaning the response for one observation does not depend on another.<br>
</li>
<li>
<strong>Linearity</strong>: The relationship between the predictor(s) and the response variable should be approximately linear. Scatter plots of predictors against the response variable help assess this assumption.<br>
</li>
<li>
<strong>Normality</strong>: The residuals (errors) should follow a normal distribution, which can be assessed visually using a Q-Q plot.<br>
</li>
<li>
<strong>Constant Variance (Homoscedasticity)</strong>: The residuals should exhibit constant variance across all levels of the predictor(s). A residuals vs. fitted values plot is typically used to check this assumption.</li>
</ol>
<p>Violations of these assumptions can undermine the validity of statistical inferences, leading to unreliable predictions and inaccurate parameter estimates.</p>
<div class="example">
<p><span id="exm:ex-diagnosing-regression" class="example"><strong>Example 10.3  </strong></span>To demonstrate model diagnostics, we evaluate the assumptions of the multiple regression model constructed in Example <a href="chapter-regression.html#exm:ex-stepwise-regression">10.1</a> using the <em>marketing</em> dataset. The fitted model predicts daily revenue (<code>revenue</code>) based on <code>clicks</code> and <code>display</code>.</p>
<p>We generate diagnostic plots for the model as follows:</p>
<div class="sourceCode" id="cb193"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ml_stepwise</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">revenue</span> <span class="op">~</span> <span class="va">clicks</span> <span class="op">+</span> <span class="va">display</span>, data <span class="op">=</span> <span class="va">marketing</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ml_stepwise</span><span class="op">)</span>  </span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:model-diagnostics"></span>
<img src="regression_files/figure-html/model-diagnostics-1.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><img src="regression_files/figure-html/model-diagnostics-2.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><img src="regression_files/figure-html/model-diagnostics-3.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><img src="regression_files/figure-html/model-diagnostics-4.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><p class="caption">
Figure 10.3: Diagnostic plots for assessing regression model assumptions.
</p>
</div>
<p>These diagnostic plots provide insights into the validity of the model’s assumptions.</p>
<ul>
<li>The <strong>Normal Q-Q plot</strong> (upper-right) assesses whether residuals follow a normal distribution. If the points lie approximately along a straight line, the assumption of normality is satisfied. In this case, the residuals closely follow the theoretical normal distribution, supporting the assumption.<br>
</li>
<li>The <strong>Residuals vs. Fitted plot</strong> (upper-left) checks for both linearity and homoscedasticity. A random scatter pattern without discernible structure supports the assumption of linearity, while an even vertical spread across fitted values confirms constant variance. Here, the residuals appear randomly distributed, suggesting that these assumptions hold.<br>
</li>
<li>The <strong>Independence assumption</strong> is not explicitly tested with diagnostic plots but depends on the dataset structure. In the <em>marketing</em> dataset, daily revenue is unlikely to be influenced by prior days’ revenue, making the independence assumption reasonable.</li>
</ul>
<p>Based on these diagnostics, the regression model satisfies the required assumptions, confirming its suitability for inference and prediction. Failing to check these assumptions could result in unreliable results, underscoring the importance of model validation.</p>
</div>
<p>When assumptions are violated, alternative approaches may be necessary. <strong>Robust regression</strong> methods can be employed when normality or homoscedasticity assumptions do not hold. <strong>Non-linear regression</strong> techniques, including polynomial regression and splines, can address cases where relationships deviate from linearity. <strong>Transformations of variables</strong>, such as logarithmic or square root transformations, can also help stabilize variance and improve model fit.</p>
<p>Beyond assumption checks, cross-validation and out-of-sample testing provide additional validation by assessing how well the model generalizes to new data. These techniques prevent overfitting and ensure that model performance is not driven by noise in the training data.</p>
<p>Validating regression models is fundamental to producing reliable, interpretable, and actionable results. By following best practices in model diagnostics, we strengthen the statistical foundation of our analyses and enhance the trustworthiness of predictions.</p>
</div>
<div id="regression-exercises" class="section level2" number="10.10">
<h2>
<span class="header-section-number">10.10</span> Exercises<a class="anchor" aria-label="anchor" href="#regression-exercises"><i class="fas fa-link"></i></a>
</h2>
<p>The exercises are structured to test theoretical understanding, interpretation of regression outputs, and practical implementation in <strong>R</strong> using datasets from the <strong>liver</strong> package.</p>
<div id="simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets" class="section level3 unnumbered">
<h3>Simple and Multiple Linear Regression (House, Insurance, and Cereal Datasets)<a class="anchor" aria-label="anchor" href="#simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets"><i class="fas fa-link"></i></a>
</h3>
<div id="conceptual-questions-6" class="section level4 unnumbered">
<h4>Conceptual Questions<a class="anchor" aria-label="anchor" href="#conceptual-questions-6"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>Explain the difference between <em>simple linear regression</em> and <em>multiple linear regression</em>.<br>
</li>
<li>What are the key assumptions of linear regression? How do these assumptions impact model performance?<br>
</li>
<li>Define and interpret the <em>R-squared (<span class="math inline">\(R^2\)</span>)</em> value in a regression model.<br>
</li>
<li>Explain the purpose of the <em>Residual Standard Error (RSE)</em> and how it differs from <span class="math inline">\(R^2\)</span>.<br>
</li>
<li>How does <em>multicollinearity</em> affect a multiple regression model? How can it be detected?<br>
</li>
<li>What is the difference between <em>Adjusted <span class="math inline">\(R^2\)</span></em> and <em><span class="math inline">\(R^2\)</span></em>? Why is Adjusted <span class="math inline">\(R^2\)</span> preferred in multiple regression?</li>
<li>What are the advantages of using <em>categorical variables</em> in a regression model? How does R handle categorical variables?</li>
</ol>
</div>
<div id="practical-exercises-using-the-house-dataset" class="section level4 unnumbered">
<h4>Practical Exercises Using the House Dataset<a class="anchor" aria-label="anchor" href="#practical-exercises-using-the-house-dataset"><i class="fas fa-link"></i></a>
</h4>
<p>Load the <em>house</em> dataset:</p>
<div class="sourceCode" id="cb194"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">house</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code></pre></div>
<ol start="8" style="list-style-type: decimal">
<li>Fit a <em>simple linear regression</em> model to predict <code>unit.price</code> based on <code>house.age</code>. Display and interpret the summary of the model.<br>
</li>
<li>Extend the model by fitting a <em>multiple linear regression</em> model using <code>house.age</code>, <code>distance.to.MRT</code>, and <code>stores.number</code> as predictors. Interpret the coefficient estimates.<br>
</li>
<li>Use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to estimate house prices for properties with an age of 10, 20, and 30 years.<br>
</li>
<li>Assess whether <code>latitude</code> and <code>longitude</code> improve the model’s predictive ability.<br>
</li>
<li>Evaluate the <em>Residual Standard Error (RSE)</em> and <em><span class="math inline">\(R^2\)</span></em> of the model. What do these values tell you about model performance?<br>
</li>
<li>Create a <em>residual plot</em> for the model and analyze whether the residuals appear randomly distributed.<br>
</li>
<li>Generate a <em>Q-Q plot</em> for the residuals. What does it reveal about the normality assumption?</li>
</ol>
</div>
<div id="practical-exercises-using-the-insurance-dataset" class="section level4 unnumbered">
<h4>Practical Exercises Using the Insurance Dataset<a class="anchor" aria-label="anchor" href="#practical-exercises-using-the-insurance-dataset"><i class="fas fa-link"></i></a>
</h4>
<p>Load the <em>insurance</em> dataset:</p>
<div class="sourceCode" id="cb195"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">insurance</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code></pre></div>
<ol start="15" style="list-style-type: decimal">
<li>Fit a multiple linear regression model predicting <code>charges</code> based on <code>age</code>, <code>bmi</code>, <code>children</code>, and <code>smoker</code>.<br>
</li>
<li>Interpret the coefficient of <code>smoker</code>. What does it suggest about the impact of smoking on insurance charges?<br>
</li>
<li>Assess whether <em>interaction effects</em> exist between <code>age</code> and <code>bmi</code>.<br>
</li>
<li>Evaluate the model’s Adjusted <span class="math inline">\(R^2\)</span>. Does adding <code>region</code> as a predictor improve the model?<br>
</li>
<li>Perform a <em>stepwise regression</em> to determine the best subset of predictors.</li>
</ol>
</div>
<div id="practical-exercises-using-the-cereal-dataset" class="section level4 unnumbered">
<h4>Practical Exercises Using the Cereal Dataset<a class="anchor" aria-label="anchor" href="#practical-exercises-using-the-cereal-dataset"><i class="fas fa-link"></i></a>
</h4>
<p>Load the <em>cereal</em> dataset:</p>
<div class="sourceCode" id="cb196"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">cereal</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code></pre></div>
<ol start="20" style="list-style-type: decimal">
<li>Fit a multiple linear regression model predicting <code>rating</code> based on <code>calories</code>, <code>protein</code>, <code>sugars</code>, and <code>fiber</code>.<br>
</li>
<li>Based on the model summary, which predictor has the strongest impact on <code>rating</code>?<br>
</li>
<li>Does <code>sodium</code> significantly affect <code>rating</code>? Should it be included in the model?<br>
</li>
<li>Compare the effects of <code>fiber</code> and <code>sugars</code>. Which has a larger impact on <code>rating</code>?<br>
</li>
<li>Apply <em>stepwise regression</em> to refine the model and identify the most relevant predictors.</li>
</ol>
</div>
</div>
<div id="polynomial-regression-house-dataset" class="section level3 unnumbered">
<h3>Polynomial Regression (House Dataset)<a class="anchor" aria-label="anchor" href="#polynomial-regression-house-dataset"><i class="fas fa-link"></i></a>
</h3>
<div id="conceptual-questions-7" class="section level4 unnumbered">
<h4>Conceptual Questions<a class="anchor" aria-label="anchor" href="#conceptual-questions-7"><i class="fas fa-link"></i></a>
</h4>
<ol start="25" style="list-style-type: decimal">
<li>What is <em>polynomial regression</em>, and how does it differ from multiple linear regression?<br>
</li>
<li>Why does polynomial regression remain a <em>linear model</em> even though it includes non-linear terms?<br>
</li>
<li>What is the risk of using <em>high-degree polynomial regression</em>?<br>
</li>
<li>How do you determine the optimal degree for a polynomial regression model?<br>
</li>
<li>How can overfitting be detected in polynomial regression?</li>
</ol>
</div>
<div id="practical-exercises-using-the-house-dataset-1" class="section level4 unnumbered">
<h4>Practical Exercises Using the House Dataset<a class="anchor" aria-label="anchor" href="#practical-exercises-using-the-house-dataset-1"><i class="fas fa-link"></i></a>
</h4>
<ol start="30" style="list-style-type: decimal">
<li>Fit a <em>quadratic polynomial regression</em> model predicting <code>unit.price</code> using <code>house.age</code>. Compare it to a simple linear regression model.<br>
</li>
<li>Fit a <em>cubic polynomial regression</em> model. Does it perform better than the quadratic model?<br>
</li>
<li>Plot the simple, quadratic, and cubic regression fits on the same graph.<br>
</li>
<li>Use <em>cross-validation</em> to determine the best polynomial degree.<br>
</li>
<li>Interpret the coefficients of the quadratic regression model.</li>
</ol>
</div>
</div>
<div id="logistic-regression-bank-dataset" class="section level3 unnumbered">
<h3>Logistic Regression (Bank Dataset)<a class="anchor" aria-label="anchor" href="#logistic-regression-bank-dataset"><i class="fas fa-link"></i></a>
</h3>
<div id="conceptual-questions-8" class="section level4 unnumbered">
<h4>Conceptual Questions<a class="anchor" aria-label="anchor" href="#conceptual-questions-8"><i class="fas fa-link"></i></a>
</h4>
<ol start="35" style="list-style-type: decimal">
<li>What is the difference between <em>linear regression</em> and <em>logistic regression</em>?<br>
</li>
<li>Why does logistic regression use the <em>logit function</em> instead of fitting a linear model directly?<br>
</li>
<li>How do you interpret the <em>odds ratio</em> in a logistic regression model?<br>
</li>
<li>What is the <em>confusion matrix</em>, and how is it used to evaluate logistic regression?<br>
</li>
<li>What is the difference between <em>precision</em> and <em>recall</em> in classification models?</li>
</ol>
</div>
<div id="practical-exercises-using-the-bank-dataset" class="section level4 unnumbered">
<h4>Practical Exercises Using the Bank Dataset<a class="anchor" aria-label="anchor" href="#practical-exercises-using-the-bank-dataset"><i class="fas fa-link"></i></a>
</h4>
<p>Load the <em>bank</em> dataset:</p>
<div class="sourceCode" id="cb197"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">bank</span>, package <span class="op">=</span> <span class="st">"liver"</span><span class="op">)</span></span></code></pre></div>
<ol start="40" style="list-style-type: decimal">
<li>Fit a <em>logistic regression</em> model predicting whether a customer subscribed to a term deposit (<code>y</code>) based on <code>age</code>, <code>balance</code>, and <code>duration</code>.<br>
</li>
<li>Interpret the coefficients in terms of <em>odds ratios</em>.<br>
</li>
<li>Use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to estimate the probability of subscription for a new customer.<br>
</li>
<li>Create a <em>confusion matrix</em> to evaluate the model’s performance.<br>
</li>
<li>Compute the <em>accuracy, precision, recall, and F1-score</em> of the model.<br>
</li>
<li>Use <em>stepwise regression</em> to refine the logistic model.<br>
</li>
<li>Evaluate the <em>receiver operating characteristic (ROC) curve</em> for the model.</li>
</ol>
</div>
</div>
<div id="stepwise-regression-house-dataset" class="section level3 unnumbered">
<h3>Stepwise Regression (House Dataset)<a class="anchor" aria-label="anchor" href="#stepwise-regression-house-dataset"><i class="fas fa-link"></i></a>
</h3>
<ol start="47" style="list-style-type: decimal">
<li>Apply <em>stepwise regression</em> to the <code>house</code> dataset to identify the most relevant predictors for <code>unit.price</code>.<br>
</li>
<li>Compare the stepwise regression model to the full multiple regression model. Does it perform better?<br>
</li>
<li>Assess whether <em>interaction terms</em> improve the stepwise regression model.</li>
</ol>
</div>
<div id="model-diagnostics-and-validation" class="section level3 unnumbered">
<h3>Model Diagnostics and Validation<a class="anchor" aria-label="anchor" href="#model-diagnostics-and-validation"><i class="fas fa-link"></i></a>
</h3>
<ol start="50" style="list-style-type: decimal">
<li>Check the <em>assumptions of linear regression</em> for the multiple regression model on the <em>house</em> dataset.<br>
</li>
<li>Generate <em>diagnostic plots</em> (residuals vs. fitted, Q-Q plot, scale-location plot).<br>
</li>
<li>Use <em>cross-validation</em> to assess model performance.<br>
</li>
<li>Compare the <em>mean squared error (MSE)</em> of different models.<br>
</li>
<li>Assess whether a <em>log-transformation</em> improves model performance.</li>
</ol>
</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></div>
<div class="next"><a href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-regression"><span class="header-section-number">10</span> Regression Analysis: Foundations and Applications</a></li>
<li>
<a class="nav-link" href="#sec-simple-regression"><span class="header-section-number">10.1</span> Simple Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#exploring-relationships-in-the-data">Exploring Relationships in the Data</a></li>
<li><a class="nav-link" href="#fitting-a-simple-linear-regression-model">Fitting a Simple Linear Regression Model</a></li>
<li><a class="nav-link" href="#estimating-the-model-in-r">Estimating the Model in R</a></li>
<li><a class="nav-link" href="#interpreting-the-regression-line">Interpreting the Regression Line</a></li>
<li><a class="nav-link" href="#residuals-and-model-fit">Residuals and Model Fit</a></li>
</ul>
</li>
<li><a class="nav-link" href="#hypothesis-testing-in-simple-linear-regression">Hypothesis Testing in Simple Linear Regression</a></li>
<li>
<a class="nav-link" href="#measuring-the-quality-of-a-regression-model">Measuring the Quality of a Regression Model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#residual-standard-error-rse">Residual Standard Error (RSE)</a></li>
<li><a class="nav-link" href="#r-squared-r2">R-squared (\(R^2\))</a></li>
<li><a class="nav-link" href="#relationship-between-r2-and-the-correlation-coefficient">Relationship Between \(R^2\) and the Correlation Coefficient</a></li>
<li><a class="nav-link" href="#adjusted-r-squared">Adjusted R-squared</a></li>
<li><a class="nav-link" href="#interpreting-model-quality">Interpreting Model Quality</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-multiple-regression"><span class="header-section-number">10.2</span> Multiple Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#fitting-the-multiple-regression-model">Fitting the Multiple Regression Model</a></li>
<li><a class="nav-link" href="#making-predictions">Making Predictions</a></li>
<li><a class="nav-link" href="#evaluating-model-performance">Evaluating Model Performance</a></li>
<li><a class="nav-link" href="#key-takeaways-2">Key Takeaways</a></li>
</ul>
</li>
<li><a class="nav-link" href="#generalized-linear-models-glms"><span class="header-section-number">10.3</span> Generalized Linear Models (GLMs)</a></li>
<li>
<a class="nav-link" href="#logistic-regression"><span class="header-section-number">10.4</span> Logistic Regression</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#logistic-regression-in-r">Logistic Regression in R</a></li></ul>
</li>
<li>
<a class="nav-link" href="#poisson-regression"><span class="header-section-number">10.5</span> Poisson Regression</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#poisson-regression-in-r">Poisson Regression in R</a></li></ul>
</li>
<li>
<a class="nav-link" href="#sec-stepwise-regression"><span class="header-section-number">10.6</span> Model Selection Using Stepwise Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-role-of-aic-in-model-selection">The Role of AIC in Model Selection</a></li>
<li><a class="nav-link" href="#implementing-stepwise-regression-in-r">Implementing Stepwise Regression in R</a></li>
<li><a class="nav-link" href="#strengths-limitations-and-considerations-for-stepwise-regression">Strengths, Limitations, and Considerations for Stepwise Regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#extending-linear-models-to-capture-non-linear-relationships"><span class="header-section-number">10.7</span> Extending Linear Models to Capture Non-Linear Relationships</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#the-need-for-non-linear-regression">The Need for Non-Linear Regression</a></li></ul>
</li>
<li><a class="nav-link" href="#polynomial-regression"><span class="header-section-number">10.8</span> Polynomial Regression</a></li>
<li><a class="nav-link" href="#diagnosing-and-validating-regression-models"><span class="header-section-number">10.9</span> Diagnosing and Validating Regression Models</a></li>
<li>
<a class="nav-link" href="#regression-exercises"><span class="header-section-number">10.10</span> Exercises</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets">Simple and Multiple Linear Regression (House, Insurance, and Cereal Datasets)</a></li>
<li><a class="nav-link" href="#polynomial-regression-house-dataset">Polynomial Regression (House Dataset)</a></li>
<li><a class="nav-link" href="#logistic-regression-bank-dataset">Logistic Regression (Bank Dataset)</a></li>
<li><a class="nav-link" href="#stepwise-regression-house-dataset">Stepwise Regression (House Dataset)</a></li>
<li><a class="nav-link" href="#model-diagnostics-and-validation">Model Diagnostics and Validation</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/RezaMoammadi/Book-Data-Science/blob/master/regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/RezaMoammadi/Book-Data-Science/edit/master/regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Science Foundations and Machine Learning Using R</strong>" was written by <a href="https://www.uva.nl/profile/a.mohammadi"><span style="color:white">Reza Mohammadi</span></a>. It was last built on 2025-03-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
