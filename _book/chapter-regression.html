<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Regression Modeling: From Basics to Advanced Techniques | Uncovering Data Science with R</title>
<meta name="author" content="Reza Mohammadi">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Chapter 10 Regression Modeling: From Basics to Advanced Techniques | Uncovering Data Science with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://uncovering-data-science.netlify.app/chapter-regression.html">
<meta property="og:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 10 Regression Modeling: From Basics to Advanced Techniques | Uncovering Data Science with R">
<meta name="twitter:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="Regression modelling has been a cornerstone of statistical analysis for centuries, evolving into one of the most powerful and versatile tools in data science. Its origins trace back to Isaac...">
<meta property="og:description" content="Regression modelling has been a cornerstone of statistical analysis for centuries, evolving into one of the most powerful and versatile tools in data science. Its origins trace back to Isaac...">
<meta name="twitter:description" content="Regression modelling has been a cornerstone of statistical analysis for centuries, evolving into one of the most powerful and versatile tools in data science. Its origins trace back to Isaac...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Uncovering Data Science with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="chapter-into-R.html"><span class="header-section-number">1</span> The Basics for R</a></li>
<li><a class="" href="chapter-intro-DS.html"><span class="header-section-number">2</span> Introduction to Data Science</a></li>
<li><a class="" href="chapter-data-prep.html"><span class="header-section-number">3</span> Data Preparation</a></li>
<li><a class="" href="chapter-EDA.html"><span class="header-section-number">4</span> Exploratory Data Analysis</a></li>
<li><a class="" href="chapter-statistics.html"><span class="header-section-number">5</span> Statistical Inference and Hypothesis Testing</a></li>
<li><a class="" href="chapter-modeling.html"><span class="header-section-number">6</span> Preparing Data for Modeling</a></li>
<li><a class="" href="chapter-knn.html"><span class="header-section-number">7</span> Classification using k-Nearest Neighbors</a></li>
<li><a class="" href="chapter-evaluation.html"><span class="header-section-number">8</span> Model Evaluation</a></li>
<li><a class="" href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></li>
<li><a class="active" href="chapter-regression.html"><span class="header-section-number">10</span> Regression Modeling: From Basics to Advanced Techniques</a></li>
<li><a class="" href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></li>
<li><a class="" href="chapter-nn.html"><span class="header-section-number">12</span> Neural Networks: The Building Blocks of Artificial Intelligence</a></li>
<li><a class="" href="chapter-cluster.html"><span class="header-section-number">13</span> Clustering</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/RezaMoammadi/Book-Data-Science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-regression" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Regression Modeling: From Basics to Advanced Techniques<a class="anchor" aria-label="anchor" href="#chapter-regression"><i class="fas fa-link"></i></a>
</h1>
<p>Regression modelling has been a cornerstone of statistical analysis for centuries, evolving into one of the most powerful and versatile tools in data science. Its origins trace back to Isaac Newton’s work in the 1700s, with the term “regression” later introduced by Francis Galton in the 19th century to describe biological phenomena. Early pioneers like Legendre and Gauss laid the mathematical groundwork with the development of the least squares method, and today, thanks to advancements in computing and programming languages like <strong>R</strong>, regression analysis is accessible, scalable, and integral to solving real-world problems.</p>
<p>As Charles Wheelan eloquently puts it in his book <a href="https://www.goodreads.com/book/show/15786586-naked-statistics">“Naked Statistics”</a>:</p>
<blockquote>
<p><em>Regression modelling is the hydrogen bomb of the statistics arsenal.</em></p>
</blockquote>
<p>What makes regression so powerful is its ability to quantify relationships between variables, uncover patterns, and make predictions. Whether you’re estimating the impact of advertising spend on sales, forecasting housing prices, or identifying the risk factors for a disease, regression modelling provides the foundation for evidence-based decision-making.</p>
<p>In this chapter, we will explore the essentials of regression, from its simplest form—<strong>simple linear regression</strong>—to more advanced techniques like <strong>generalized linear models (GLMs)</strong> and <strong>non-linear regression</strong>. By the end, you will not only understand the mathematical principles behind regression models but also gain practical experience applying them in R to analyze and interpret real-world data.</p>
<div id="sec-simple-regression" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Simple Linear Regression<a class="anchor" aria-label="anchor" href="#sec-simple-regression"><i class="fas fa-link"></i></a>
</h2>
<p>To explore regression methods for estimation and prediction, we will use the <em>marketing</em> dataset from the <strong>liver</strong> package. This dataset provides a straightforward and practical example of a real-world scenario, where a company seeks to optimize advertising spending to maximize revenue. Its small size and clean structure make it an ideal learning tool for understanding regression concepts. The dataset contains information on advertising campaigns, including spending, clicks, impressions, transactions, and daily revenue. The target variable, <code>revenue</code>, represents the daily revenue generated, while the other seven variables serve as predictors.</p>
<p>The <em>marketing</em> dataset, formatted as a <code>data.frame</code>, consists of 40 observations (rows) and 8 variables (columns):</p>
<ul>
<li>
<strong><code>spend</code></strong>: Daily spending on pay-per-click (PPC) advertising.<br>
</li>
<li>
<strong><code>clicks</code></strong>: Number of clicks on the ads.<br>
</li>
<li>
<strong><code>impressions</code></strong>: Number of ad impressions per day.<br>
</li>
<li>
<strong><code>display</code></strong>: Whether a display campaign was running (<code>yes</code> or <code>no</code>).<br>
</li>
<li>
<strong><code>transactions</code></strong>: Number of transactions per day.<br>
</li>
<li>
<strong><code>click.rate</code></strong>: Click-through rate (CTR).<br>
</li>
<li>
<strong><code>conversion.rate</code></strong>: Conversion rate.<br>
</li>
<li>
<strong><code>revenue</code></strong>: Daily revenue (target variable).</li>
</ul>
<p>Let’s load and examine the structure of the dataset:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="chapter-regression.html#cb141-1" tabindex="-1"></a><span class="fu">data</span>(marketing, <span class="at">package =</span> <span class="st">"liver"</span>)</span>
<span id="cb141-2"><a href="chapter-regression.html#cb141-2" tabindex="-1"></a></span>
<span id="cb141-3"><a href="chapter-regression.html#cb141-3" tabindex="-1"></a><span class="fu">str</span>(marketing)</span>
<span id="cb141-4"><a href="chapter-regression.html#cb141-4" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">40</span> obs. of  <span class="dv">8</span> variables<span class="sc">:</span></span>
<span id="cb141-5"><a href="chapter-regression.html#cb141-5" tabindex="-1"></a>    <span class="er">$</span> spend          <span class="sc">:</span> num  <span class="fl">22.6</span> <span class="fl">37.3</span> <span class="fl">55.6</span> <span class="fl">45.4</span> <span class="fl">50.2</span> ...</span>
<span id="cb141-6"><a href="chapter-regression.html#cb141-6" tabindex="-1"></a>    <span class="sc">$</span> clicks         <span class="sc">:</span> int  <span class="dv">165</span> <span class="dv">228</span> <span class="dv">291</span> <span class="dv">247</span> <span class="dv">290</span> <span class="dv">172</span> <span class="dv">68</span> <span class="dv">112</span> <span class="dv">306</span> <span class="dv">300</span> ...</span>
<span id="cb141-7"><a href="chapter-regression.html#cb141-7" tabindex="-1"></a>    <span class="sc">$</span> impressions    <span class="sc">:</span> int  <span class="dv">8672</span> <span class="dv">11875</span> <span class="dv">14631</span> <span class="dv">11709</span> <span class="dv">14768</span> <span class="dv">8698</span> <span class="dv">2924</span> <span class="dv">5919</span> <span class="dv">14789</span> <span class="dv">14818</span> ...</span>
<span id="cb141-8"><a href="chapter-regression.html#cb141-8" tabindex="-1"></a>    <span class="sc">$</span> display        <span class="sc">:</span> int  <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> ...</span>
<span id="cb141-9"><a href="chapter-regression.html#cb141-9" tabindex="-1"></a>    <span class="sc">$</span> transactions   <span class="sc">:</span> int  <span class="dv">2</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">3</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">3</span> <span class="dv">3</span> ...</span>
<span id="cb141-10"><a href="chapter-regression.html#cb141-10" tabindex="-1"></a>    <span class="sc">$</span> click.rate     <span class="sc">:</span> num  <span class="fl">1.9</span> <span class="fl">1.92</span> <span class="fl">1.99</span> <span class="fl">2.11</span> <span class="fl">1.96</span> <span class="fl">1.98</span> <span class="fl">2.33</span> <span class="fl">1.89</span> <span class="fl">2.07</span> <span class="fl">2.02</span> ...</span>
<span id="cb141-11"><a href="chapter-regression.html#cb141-11" tabindex="-1"></a>    <span class="sc">$</span> conversion.rate<span class="sc">:</span> num  <span class="fl">1.21</span> <span class="fl">0.88</span> <span class="fl">1.03</span> <span class="fl">0.81</span> <span class="fl">1.03</span> <span class="fl">1.16</span> <span class="fl">1.47</span> <span class="fl">0.89</span> <span class="fl">0.98</span> <span class="dv">1</span> ...</span>
<span id="cb141-12"><a href="chapter-regression.html#cb141-12" tabindex="-1"></a>    <span class="sc">$</span> revenue        <span class="sc">:</span> num  <span class="fl">58.9</span> <span class="fl">44.9</span> <span class="fl">141.6</span> <span class="fl">209.8</span> <span class="fl">197.7</span> ...</span></code></pre></div>
<p>The dataset includes 8 variables and 40 observations, with 7 predictors and one numerical-continuous target variable (<code>revenue</code>). This clean dataset serves as a perfect starting point for regression analysis.</p>
<p>To understand the relationships between variables, we use the <code>pairs.panels()</code> function from the <strong>psych</strong> package to create a visualization:</p>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">pairs.panels</span><span class="op">(</span><span class="va">marketing</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="regression_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>This plot includes: <em>Bivariate scatter plots</em> (bottom-left) showing relationships between pairs of variables; <em>Histograms</em> (diagonal) showing the distribution of each variable; <em>Correlation coefficients</em> (top-right) quantifying the strength of linear relationships. For example, the variables <code>spend</code> and <code>revenue</code> exhibit a strong positive linear relationship, with a correlation coefficient of 0.79. This indicates that higher spending is generally associated with higher revenue, supporting the hypothesis of a linear relationship between these variables.</p>
<div id="fitting-a-simple-linear-regression-model" class="section level3 unnumbered">
<h3>Fitting a Simple Linear Regression Model<a class="anchor" aria-label="anchor" href="#fitting-a-simple-linear-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p>To begin our analysis, we focus on a simple linear regression model that examines the relationship between a single predictor (<code>spend</code>) and the target variable (<code>revenue</code>). This provides a foundational understanding of regression before expanding to more complex models involving multiple predictors. First, let’s visualize the relationship with a scatter plot and overlay a regression line:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scoter-plot-simple-reg"></span>
<img src="regression_files/figure-html/scoter-plot-simple-reg-1.png" alt="Scatter plot of daily revenue (€) versus daily spend (€) for 40 observations, with the fitted least-squares regression line (blue) showing the linear relationship." width="80%"><p class="caption">
Figure 10.1: Scatter plot of daily revenue (€) versus daily spend (€) for 40 observations, with the fitted least-squares regression line (blue) showing the linear relationship.
</p>
</div>
<p>Figure <a href="chapter-regression.html#fig:scoter-plot-simple-reg">10.1</a> displays the scatter plot of <code>spend</code> versus <code>revenue</code> for the <em>marketing</em> dataset, with the fitted least-squares regression line.</p>
<p>The <strong>regression equation</strong> is:<br><span class="math display">\[
\hat{y} = b_0 + b_1x
\]</span>
where:</p>
<ul>
<li>
<span class="math inline">\(b_0\)</span>: Intercept with y-axis (estimated revenue when spending is zero).<br>
</li>
<li>
<span class="math inline">\(b_1\)</span>: Slope of the line (change in revenue for a one-unit increase in spending).<br>
</li>
<li>
<span class="math inline">\(\hat{y}\)</span>: Predicted value of the dependent variable (<code>revenue</code>) for a given independent variable (<code>spend</code>).<br>
</li>
<li>
<span class="math inline">\(x\)</span>: Independent variable (<code>spend</code>).</li>
</ul>
</div>
<div id="estimating-the-model-in-r" class="section level3 unnumbered">
<h3>Estimating the Model in R<a class="anchor" aria-label="anchor" href="#estimating-the-model-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>We use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to estimate the regression coefficients:</p>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">simple_reg</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">revenue</span> <span class="op">~</span> <span class="va">spend</span>, data <span class="op">=</span> <span class="va">marketing</span><span class="op">)</span></span></code></pre></div>
<p>The regression results are summarized using the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="chapter-regression.html#cb144-1" tabindex="-1"></a><span class="fu">summary</span>(simple_reg)</span>
<span id="cb144-2"><a href="chapter-regression.html#cb144-2" tabindex="-1"></a>   </span>
<span id="cb144-3"><a href="chapter-regression.html#cb144-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb144-4"><a href="chapter-regression.html#cb144-4" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend, <span class="at">data =</span> marketing)</span>
<span id="cb144-5"><a href="chapter-regression.html#cb144-5" tabindex="-1"></a>   </span>
<span id="cb144-6"><a href="chapter-regression.html#cb144-6" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb144-7"><a href="chapter-regression.html#cb144-7" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb144-8"><a href="chapter-regression.html#cb144-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">175.640</span>  <span class="sc">-</span><span class="fl">56.226</span>    <span class="fl">1.448</span>   <span class="fl">65.235</span>  <span class="fl">210.987</span> </span>
<span id="cb144-9"><a href="chapter-regression.html#cb144-9" tabindex="-1"></a>   </span>
<span id="cb144-10"><a href="chapter-regression.html#cb144-10" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb144-11"><a href="chapter-regression.html#cb144-11" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb144-12"><a href="chapter-regression.html#cb144-12" tabindex="-1"></a>   (Intercept)  <span class="fl">15.7058</span>    <span class="fl">35.1727</span>   <span class="fl">0.447</span>    <span class="fl">0.658</span>    </span>
<span id="cb144-13"><a href="chapter-regression.html#cb144-13" tabindex="-1"></a>   spend         <span class="fl">5.2517</span>     <span class="fl">0.6624</span>   <span class="fl">7.928</span> <span class="fl">1.42e-09</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb144-14"><a href="chapter-regression.html#cb144-14" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb144-15"><a href="chapter-regression.html#cb144-15" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb144-16"><a href="chapter-regression.html#cb144-16" tabindex="-1"></a>   </span>
<span id="cb144-17"><a href="chapter-regression.html#cb144-17" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">93.82</span> on <span class="dv">38</span> degrees of freedom</span>
<span id="cb144-18"><a href="chapter-regression.html#cb144-18" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6232</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6133</span> </span>
<span id="cb144-19"><a href="chapter-regression.html#cb144-19" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">62.86</span> on <span class="dv">1</span> and <span class="dv">38</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.415e-09</span></span></code></pre></div>
<p>The output includes the estimated coefficients, standard errors, t-statistics, p-values, and goodness-of-fit metrics. The estimated regression equation is:</p>
<p><span class="math display">\[
\text{revenue} = 15.71 + 5.25 \cdot \text{spend}
\]</span></p>
<p>This means that:</p>
<ul>
<li>The <strong>intercept</strong> (<span class="math inline">\(b_0\)</span>) is 15.71, representing the estimated daily revenue when no money is spent on PPC advertising.<br>
</li>
<li>The <strong>slope</strong> (<span class="math inline">\(b_1\)</span>) is 5.25, meaning that for every additional €1 spent, daily revenue increases by approximately 5.25.</li>
</ul>
</div>
<div id="interpreting-the-regression-line" class="section level3 unnumbered">
<h3>Interpreting the Regression Line<a class="anchor" aria-label="anchor" href="#interpreting-the-regression-line"><i class="fas fa-link"></i></a>
</h3>
<p>The regression line serves as a linear approximation of the relationship between spending and revenue. We can use it to make predictions. For instance, suppose we want to predict the revenue for a day with €25 spent on advertising. Using the regression equation:</p>
<p><span class="math display">\[
\hat{y} = 15.71 + 5.25 \cdot 25 = 147
\]</span></p>
<p>Thus, the estimated daily revenue is €147. Suppose a marketing team wants to plan their budget for an upcoming campaign. If they spend €25 on PPC advertising, the model predicts an estimated revenue of €147. This insight can help guide spending decisions to maximize returns.</p>
</div>
<div id="residuals-and-model-fit" class="section level3 unnumbered">
<h3>Residuals and Model Fit<a class="anchor" aria-label="anchor" href="#residuals-and-model-fit"><i class="fas fa-link"></i></a>
</h3>
<p>Residuals (<span class="math inline">\(y - \hat{y}\)</span>) represent the vertical distances between observed data points and the regression line. For example, one day in the dataset with a spend of €25 has an actual revenue of 185.36. The prediction error (residual) is:</p>
<p><span class="math display">\[
\text{Residual} = y - \hat{y} = 185.36 - 147 = 38.36
\]</span></p>
<p>Residuals help us identify potential patterns or deviations that the linear model may not capture, ensuring the model’s assumptions hold. For example, if residuals exhibit systematic patterns, such as curvature, it may indicate that the relationship between the variables is not truly linear, signaling the need for model adjustments.</p>
<p>The <strong>least-squares method</strong> minimizes the sum of squared residuals (SSE):</p>
<p><span class="math display" id="eq:sse">\[\begin{equation}
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
\tag{10.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the observed value, <span class="math inline">\(\hat{y}_i\)</span> is the predicted value, and <span class="math inline">\(n\)</span> is the number of observations. Minimizing SSE ensures that the regression line is the best linear fit to the data, providing the most accurate predictions. This approach remains the most commonly used for linear regression due to its simplicity and efficiency.</p>
</div>
<div id="key-insights" class="section level3 unnumbered">
<h3>Key Insights<a class="anchor" aria-label="anchor" href="#key-insights"><i class="fas fa-link"></i></a>
</h3>
<p>Here are some important takeaways from our simple linear regression analysis:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Intercept (<span class="math inline">\(b_0\)</span>)</strong>: The intercept represents the estimated revenue when spending is zero. In our dataset, <span class="math inline">\(b_0 = 15.71\)</span>, which makes sense as there are days with no spending.<br>
</li>
<li>
<strong>Slope (<span class="math inline">\(b_1\)</span>)</strong>: The slope indicates that for every €1 increase in spending, revenue is expected to increase by approximately 5.25.<br>
</li>
<li>
<strong>Prediction Accuracy</strong>: While the regression line provides an excellent linear approximation, individual residuals reveal prediction errors for specific data points.</li>
</ol>
<p>In summary, simple linear regression provides an effective way to model and interpret the relationship between two variables. By analyzing the <em>marketing</em> dataset, we demonstrated how to estimate, interpret, and apply a regression model to make predictions. The next sections will expand on these concepts by exploring techniques to assess model quality and extend regression to multiple predictors.</p>
<p>With this foundational understanding of simple linear regression, we are now ready to explore techniques to assess the quality of regression models and extend these methods to multiple predictors in the next sections.</p>
</div>
</div>
<div id="hypothesis-testing-in-simple-linear-regression" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Hypothesis Testing in Simple Linear Regression<a class="anchor" aria-label="anchor" href="#hypothesis-testing-in-simple-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In regression analysis, we use the estimated slope <span class="math inline">\(b_1\)</span> from the sample regression equation to draw inferences about the unknown slope <span class="math inline">\(\beta_1\)</span> in the population regression equation. The population regression equation provides a linear approximation of the relationship between a predictor (<span class="math inline">\(x\)</span>) and a response (<span class="math inline">\(y\)</span>) for the entire population, not just the sample data. It is expressed as:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1x + \epsilon
\]</span></p>
<p>In this equation:</p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span>: The population <strong>intercept</strong>, representing the expected value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>.<br>
</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The population <strong>slope</strong>, representing the change in <span class="math inline">\(y\)</span> for a one-unit increase in <span class="math inline">\(x\)</span>.<br>
</li>
<li>
<span class="math inline">\(\epsilon\)</span>: A random error term that captures the variability in <span class="math inline">\(y\)</span> not explained by the linear relationship.</li>
</ul>
<p>The goal of hypothesis testing in regression is to determine whether <span class="math inline">\(\beta_1\)</span>, the slope of the population regression line, is significantly different from zero. If <span class="math inline">\(\beta_1 = 0\)</span>, the regression equation simplifies to:</p>
<p><span class="math display">\[
y = \beta_0 + \epsilon
\]</span></p>
<p>This indicates that there is <strong>no linear relationship</strong> between the predictor <span class="math inline">\(x\)</span> and the response <span class="math inline">\(y\)</span>. Conversely, if <span class="math inline">\(\beta_1 \neq 0\)</span>, a linear relationship exists between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. To formally test this, we conduct the following hypothesis test:</p>
<p><span class="math display">\[
\bigg\{
\begin{matrix}
  H_0: \beta_1 =  0 \quad \text{(No linear relationship between \(x\) and \(y\))} \qquad  \\
  H_a: \beta_1 \neq 0 \quad \text{(A linear relationship exists between \(x\) and \(y\))}
\end{matrix}
\]</span></p>
<p>In simple linear regression, the summary output of the model provides all the necessary information to test the hypotheses. Specifically, the slope of the regression line is estimated from the sample data as <span class="math inline">\(b_1\)</span>, along with the following key components:</p>
<ul>
<li>
<strong>Standard error</strong>: Measures the variability of the slope estimate <span class="math inline">\(b_1\)</span>.<br>
</li>
<li>
<strong>t-statistic</strong>: Quantifies how many standard errors <span class="math inline">\(b_1\)</span> is away from 0.<br>
</li>
<li>
<strong>p-value</strong>: Indicates the probability of observing a t-statistic as extreme as the one calculated, assuming <span class="math inline">\(H_0\)</span> (i.e., <span class="math inline">\(\beta_1 = 0\)</span>) is true.</li>
</ul>
<p>Let’s revisit the simple linear regression results for the <em>marketing</em> dataset, where we modeled <code>revenue</code> (daily revenue) as a function of <code>spend</code> (daily advertising spend). The estimated slope <span class="math inline">\(b_1\)</span> for <code>spend</code> is 5.25, with the following corresponding hypothesis test results:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="chapter-regression.html#cb145-1" tabindex="-1"></a><span class="fu">summary</span>(simple_reg)</span>
<span id="cb145-2"><a href="chapter-regression.html#cb145-2" tabindex="-1"></a>   </span>
<span id="cb145-3"><a href="chapter-regression.html#cb145-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb145-4"><a href="chapter-regression.html#cb145-4" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend, <span class="at">data =</span> marketing)</span>
<span id="cb145-5"><a href="chapter-regression.html#cb145-5" tabindex="-1"></a>   </span>
<span id="cb145-6"><a href="chapter-regression.html#cb145-6" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb145-7"><a href="chapter-regression.html#cb145-7" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb145-8"><a href="chapter-regression.html#cb145-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">175.640</span>  <span class="sc">-</span><span class="fl">56.226</span>    <span class="fl">1.448</span>   <span class="fl">65.235</span>  <span class="fl">210.987</span> </span>
<span id="cb145-9"><a href="chapter-regression.html#cb145-9" tabindex="-1"></a>   </span>
<span id="cb145-10"><a href="chapter-regression.html#cb145-10" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb145-11"><a href="chapter-regression.html#cb145-11" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb145-12"><a href="chapter-regression.html#cb145-12" tabindex="-1"></a>   (Intercept)  <span class="fl">15.7058</span>    <span class="fl">35.1727</span>   <span class="fl">0.447</span>    <span class="fl">0.658</span>    </span>
<span id="cb145-13"><a href="chapter-regression.html#cb145-13" tabindex="-1"></a>   spend         <span class="fl">5.2517</span>     <span class="fl">0.6624</span>   <span class="fl">7.928</span> <span class="fl">1.42e-09</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb145-14"><a href="chapter-regression.html#cb145-14" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb145-15"><a href="chapter-regression.html#cb145-15" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb145-16"><a href="chapter-regression.html#cb145-16" tabindex="-1"></a>   </span>
<span id="cb145-17"><a href="chapter-regression.html#cb145-17" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">93.82</span> on <span class="dv">38</span> degrees of freedom</span>
<span id="cb145-18"><a href="chapter-regression.html#cb145-18" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6232</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.6133</span> </span>
<span id="cb145-19"><a href="chapter-regression.html#cb145-19" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">62.86</span> on <span class="dv">1</span> and <span class="dv">38</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.415e-09</span></span></code></pre></div>
<p>From the output:</p>
<ul>
<li>The <strong>t-statistic</strong> for the slope is 7.93.<br>
</li>
<li>The <strong>p-value</strong> is 1.4150362^{-9} (close to zero).</li>
</ul>
<p>The p-value, 1.4150362^{-9}, is extremely small (near zero). This represents the probability of observing a t-statistic as extreme as the one calculated, assuming there is no relationship between <code>spend</code> and <code>revenue</code> (<span class="math inline">\(\beta_1 = 0\)</span>).</p>
<p>Since the p-value is much smaller than the commonly used significance level (<span class="math inline">\(\alpha = 0.05\)</span>), we reject the null hypothesis <span class="math inline">\(H_0\)</span>. This leads to the conclusion that the slope <span class="math inline">\(\beta_1\)</span> is significantly different from zero, providing strong evidence of a linear relationship between <code>spend</code> and <code>revenue</code>.</p>
<p>The hypothesis test confirms that the predictor <code>spend</code> has a statistically significant impact on the response <code>revenue</code>. Specifically:</p>
<ul>
<li>The slope estimate <span class="math inline">\(b_1 = 5.25\)</span> suggests that for each additional €1 spent on advertising, daily revenue increases by approximately 5.25 units.<br>
</li>
<li>This result validates the use of <code>spend</code> as a predictor for <code>revenue</code>, providing a quantitative measure of their relationship.</li>
</ul>
<p>Hypothesis testing in simple linear regression provides a formal way to evaluate the significance of the relationship between a predictor and a response variable. A statistically significant slope (<span class="math inline">\(\beta_1\)</span>) indicates that changes in the predictor <span class="math inline">\(x\)</span> are associated with changes in the response <span class="math inline">\(y\)</span>.</p>
<p>In the next sections, we will explore additional techniques to evaluate regression model quality and extend these concepts to multiple predictors, enabling more comprehensive analyses and better predictions.</p>
</div>
<div id="measuring-the-quality-of-a-regression-model" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Measuring the Quality of a Regression Model<a class="anchor" aria-label="anchor" href="#measuring-the-quality-of-a-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>How can we evaluate the effectiveness of a regression model? If we cannot reject the null hypothesis (<span class="math inline">\(H_0: \beta_1 = 0\)</span>) in the hypothesis test, the model provides no evidence of a linear relationship between the predictor and response variable, making it ineffective. However, when we establish that <span class="math inline">\(\beta_1 \neq 0\)</span>, we rely on additional metrics to assess the quality of the regression model. Two key statistics for this purpose are the <strong>Residual Standard Error (SSE)</strong> and the <strong><span class="math inline">\(R^2\)</span> (R-squared) statistic</strong>.</p>
<p>The <strong>Residual Standard Error (RSE)</strong> indicates a measure of the size of the “typical” prediction error. It is defined as:</p>
<p><span class="math display">\[
RSE = \sqrt{\frac{1}{n-p-1} SSE}
\]</span>
where SSE is the sum of squared errors as defined in Equation <a href="chapter-regression.html#eq:sse">(10.1)</a>, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(p\)</span> is the number of predictors in the model. The RSE provides an estimate of the typical prediction error, with smaller values indicating a better fit.</p>
<p>A smaller SSE indicates that the regression model more accurately captures the variability in the response variable, producing better predictions. For example, in the simple linear regression model for the <em>marketing</em> dataset, the SSE is 3.3447117^{5}. This value represents the total squared error in predicting <code>revenue</code> from <code>spend</code>. The closer SSE is to zero, the smaller the typical prediction error, which indicates a higher-quality model.</p>
<p>The <strong><span class="math inline">\(R^2\)</span> (R-squared)</strong> statistic evaluates how well the regression model explains the variability in the response variable. It is defined as:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{SSE}{SST}
\]</span></p>
<p>where <span class="math inline">\(SST\)</span> is the total variability in the response variable (<span class="math inline">\(y\)</span>) before fitting the model, and <span class="math inline">\(SSE\)</span> is the variability that remains unexplained after fitting the model. <span class="math inline">\(R^2\)</span> represents the proportion of variability in <span class="math inline">\(y\)</span> accounted for by the linear relationship with <span class="math inline">\(x\)</span>. For instance, if <span class="math inline">\(R^2 = 0.80\)</span>, it means that 80% of the variability in <span class="math inline">\(y\)</span> is explained by the model, while the remaining 20% is due to factors not captured by the model.</p>
<p>In the simple linear regression model for the <em>marketing</em> dataset, <span class="math inline">\(R^2 = 62\)</span>% (or 0.62 as a decimal). This means that 62% of the variability in daily revenue is explained by the linear relationship with daily spend. A higher <span class="math inline">\(R^2\)</span> value suggests a better fit, with values close to 100% indicating an excellent fit. However, <span class="math inline">\(R^2\)</span> alone cannot assess whether the model will generalize well to new data, so it is important to combine it with other diagnostics and validation techniques.</p>
<p>It is also important to understand the relationship between <span class="math inline">\(R^2\)</span> and the correlation coefficient (<span class="math inline">\(r\)</span>). In simple linear regression, the square of the correlation coefficient (<span class="math inline">\(r^2\)</span>) is equal to <span class="math inline">\(R^2\)</span>. For example, the correlation coefficient between <code>spend</code> and <code>revenue</code> in the <em>marketing</em> dataset is 0.79, and squaring this value gives 0.62—a value that equals <span class="math inline">\(R^2\)</span>. This highlights how <span class="math inline">\(R^2\)</span> reflects the strength of the linear relationship between the predictor and response variables.</p>
<p>You might also wonder about the difference between <em>Multiple R-squared</em> and <em>Adjusted R-squared</em>. While <span class="math inline">\(R^2\)</span> measures the proportion of variance in <span class="math inline">\(y\)</span> explained by the model, <strong>Adjusted <span class="math inline">\(R^2\)</span></strong> accounts for the number of predictors in the model. This adjustment prevents <span class="math inline">\(R^2\)</span> from artificially inflating when irrelevant predictors are added to the model. Adjusted <span class="math inline">\(R^2\)</span> is calculated as:</p>
<p><span class="math display">\[
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \cdot \frac{n-1}{n-p-1}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(p\)</span> is the number of predictors. Adjusted <span class="math inline">\(R^2\)</span> penalizes models with unnecessary predictors, providing a more reliable measure of model performance.</p>
<p>In simple linear regression (with one predictor), <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span> are the same because there is only one predictor. However, in multiple linear regression, Adjusted <span class="math inline">\(R^2\)</span> is typically lower and is a better indicator of how well the model generalizes to new data. It is particularly helpful for comparing models with different numbers of predictors, as it adjusts for model complexity.</p>
<p>In conclusion, the Residual Standard Error (SSE), <span class="math inline">\(R^2\)</span>, and Adjusted <span class="math inline">\(R^2\)</span> are essential metrics for evaluating the quality of a regression model. SSE provides a measure of the model’s prediction accuracy, while <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span> assess how well the model explains the variability in the response variable. These metrics help guide model interpretation and selection, especially when extending to more complex models.</p>
</div>
<div id="sec-multiple-regression" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> Multiple Linear Regression<a class="anchor" aria-label="anchor" href="#sec-multiple-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In many real-world scenarios, datasets often include numerous variables, many of which may have a linear relationship with the target (response) variable. <strong>Multiple regression modeling</strong> provides a method to analyze and quantify these relationships by incorporating multiple predictors into a single model. By doing so, multiple regression offers greater precision for estimation and prediction compared to simple regression, much like how regression itself improves upon univariate estimates.</p>
<p>To illustrate multiple regression using the <em>marketing</em> dataset, we will add the predictor <code>display</code> to the simple regression model (which previously only used <code>spend</code> as a predictor) and evaluate whether this improves the model’s quality. The general equation for a multiple regression model is:</p>
<p><span class="math display">\[
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of predictors, <span class="math inline">\(\beta_0\)</span> is the intercept, and <span class="math inline">\(\beta_1, \beta_2, \dots, \beta_p\)</span> are the coefficients (slopes) representing the relationship between each predictor and the response variable.</p>
<p>For our case, the equation with two predictors (<code>spend</code> and <code>display</code>) becomes:</p>
<p><span class="math display">\[
\hat{y} = b_0 + b_1 \cdot \text{spend} + b_2 \cdot \text{display}
\]</span></p>
<p>In R, we fit this model using the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="chapter-regression.html#cb146-1" tabindex="-1"></a>multiple_reg <span class="ot">=</span> <span class="fu">lm</span>(revenue <span class="sc">~</span> spend <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb146-2"><a href="chapter-regression.html#cb146-2" tabindex="-1"></a></span>
<span id="cb146-3"><a href="chapter-regression.html#cb146-3" tabindex="-1"></a><span class="fu">summary</span>(multiple_reg)</span>
<span id="cb146-4"><a href="chapter-regression.html#cb146-4" tabindex="-1"></a>   </span>
<span id="cb146-5"><a href="chapter-regression.html#cb146-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb146-6"><a href="chapter-regression.html#cb146-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> spend <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb146-7"><a href="chapter-regression.html#cb146-7" tabindex="-1"></a>   </span>
<span id="cb146-8"><a href="chapter-regression.html#cb146-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb146-9"><a href="chapter-regression.html#cb146-9" tabindex="-1"></a>        Min       <span class="dv">1</span>Q   Median       <span class="dv">3</span>Q      Max </span>
<span id="cb146-10"><a href="chapter-regression.html#cb146-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">189.420</span>  <span class="sc">-</span><span class="fl">45.527</span>    <span class="fl">5.566</span>   <span class="fl">54.943</span>  <span class="fl">154.340</span> </span>
<span id="cb146-11"><a href="chapter-regression.html#cb146-11" tabindex="-1"></a>   </span>
<span id="cb146-12"><a href="chapter-regression.html#cb146-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb146-13"><a href="chapter-regression.html#cb146-13" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb146-14"><a href="chapter-regression.html#cb146-14" tabindex="-1"></a>   (Intercept) <span class="sc">-</span><span class="fl">41.4377</span>    <span class="fl">32.2789</span>  <span class="sc">-</span><span class="fl">1.284</span> <span class="fl">0.207214</span>    </span>
<span id="cb146-15"><a href="chapter-regression.html#cb146-15" tabindex="-1"></a>   spend         <span class="fl">5.3556</span>     <span class="fl">0.5523</span>   <span class="fl">9.698</span> <span class="fl">1.05e-11</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb146-16"><a href="chapter-regression.html#cb146-16" tabindex="-1"></a>   display     <span class="fl">104.2878</span>    <span class="fl">24.7353</span>   <span class="fl">4.216</span> <span class="fl">0.000154</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb146-17"><a href="chapter-regression.html#cb146-17" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb146-18"><a href="chapter-regression.html#cb146-18" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb146-19"><a href="chapter-regression.html#cb146-19" tabindex="-1"></a>   </span>
<span id="cb146-20"><a href="chapter-regression.html#cb146-20" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">78.14</span> on <span class="dv">37</span> degrees of freedom</span>
<span id="cb146-21"><a href="chapter-regression.html#cb146-21" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7455</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7317</span> </span>
<span id="cb146-22"><a href="chapter-regression.html#cb146-22" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">54.19</span> on <span class="dv">2</span> and <span class="dv">37</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.012e-11</span></span></code></pre></div>
<p>The estimated regression equation from the output is:</p>
<p><span class="math display">\[
\text{revenue} = -41.44 + 5.36 \cdot \text{spend} + 104.29 \cdot \text{display}
\]</span></p>
<p>In this model:</p>
<ul>
<li>The <strong>intercept</strong> (<span class="math inline">\(b_0\)</span>) is -41.44, representing the estimated revenue when both predictors (<code>spend</code> and <code>display</code>) are zero.<br>
</li>
<li>The <strong>coefficient for <code>spend</code></strong> (<span class="math inline">\(b_1\)</span>) is 5.36, indicating that for every additional €1 spent, revenue increases by approximately 5.36 euros, holding <code>display</code> constant.<br>
</li>
<li>The <strong>coefficient for <code>display</code></strong> (<span class="math inline">\(b_2\)</span>) is 104.29, meaning that running a display campaign (<code>display = 1</code>) increases revenue by approximately 104.29 euros, holding <code>spend</code> constant.</li>
</ul>
<p>To illustrate, consider a day where the company spends €25 on advertising and runs a display campaign (<code>display = 1</code>). Using the regression equation, the predicted revenue is:</p>
<p><span class="math display">\[
\hat{y} = -41.44 + 5.36 \cdot 25 + 104.29 \cdot 1 = 196.74
\]</span></p>
<p>Thus, the predicted revenue for that day is approximately €196.74.</p>
<p>The prediction error (or residual) for this specific day is calculated as the difference between the actual revenue <span class="math inline">\(y\)</span> and the predicted revenue <span class="math inline">\(\hat{y}\)</span>:</p>
<p><span class="math display">\[
\text{Residual} = y - \hat{y} = 185.36 - 196.74 = -11.49
\]</span></p>
<p>Interestingly, this prediction error is smaller than the one produced by the simpler regression model, which only used <code>spend</code> as a predictor. In the simple regression model, the residual was 38.26. The improvement in prediction accuracy is due to the inclusion of <code>display</code> as an additional predictor, which provides more information about the variability in revenue.</p>
<p>Adding the predictor <code>display</code> not only reduces the prediction error but also improves the overall quality of the regression model. For example:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Residual Standard Error (RSE)</strong>: The RSE measures the typical size of prediction errors. In the simple regression model, the RSE was approximately 93.82. With the multiple regression model, the RSE has decreased to approximately 78.14, indicating smaller, more accurate prediction errors.</p></li>
<li><p><strong><span class="math inline">\(R^2\)</span> (R-squared)</strong>: The <span class="math inline">\(R^2\)</span> statistic measures the proportion of variability in the response variable (<code>revenue</code>) explained by the predictors. In the simple regression model, <span class="math inline">\(R^2 = 62\%\)</span>. After adding <code>display</code>, <span class="math inline">\(R^2 = 75\%\)</span>. This indicates that a larger proportion of the variability in revenue is now explained by the model.</p></li>
<li><p><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>: Unlike <span class="math inline">\(R^2\)</span>, Adjusted <span class="math inline">\(R^2\)</span> accounts for the number of predictors, making it a more reliable metric for comparing models with different numbers of predictors. The Adjusted <span class="math inline">\(R^2\)</span> has increased from 61% in the simple regression model to 73% in the multiple regression model. This confirms that the additional predictor contributes meaningfully to the model.</p></li>
</ol>
<p>In summary, the multiple regression model demonstrates clear improvements over the simple regression model. By including <code>display</code> as an additional predictor, we achieve:</p>
<ul>
<li>
<strong>Better Fit</strong>: The model explains a larger proportion of the variability in revenue, as reflected by the higher <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span>.<br>
</li>
<li>
<strong>Reduced Prediction Error</strong>: The smaller RSE indicates that the model provides more accurate predictions.<br>
</li>
<li>
<strong>Enhanced Interpretability</strong>: The coefficients allow us to quantify the effect of running a display campaign and increasing spending on revenue.</li>
</ul>
<p>Multiple regression modeling offers a robust framework for analyzing relationships between a response variable and multiple predictors. This added flexibility makes it an indispensable tool for understanding complex datasets and improving predictive performance. In the next sections, we will explore how to evaluate model assumptions, perform diagnostics, and refine regression models further to ensure their validity and reliability.</p>
</div>
<div id="generalized-linear-models-glms" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Generalized Linear Models (GLMs)<a class="anchor" aria-label="anchor" href="#generalized-linear-models-glms"><i class="fas fa-link"></i></a>
</h2>
<p>While linear regression is a powerful tool for modeling continuous outcomes, it is limited when dealing with non-continuous response variables, such as binary or count data. <strong>Generalized Linear Models (GLMs)</strong> extend the linear regression framework by introducing a <strong>link function</strong> and a <strong>variance function</strong>, enabling them to model a wide range of response variable distributions. This makes GLMs versatile and widely applicable in data science, as they can handle binary, count, and other non-continuous outcomes.</p>
<p>In this section, we will focus on two widely used GLMs:</p>
<ul>
<li>
<strong>Logistic regression</strong>, which is used for binary classification tasks.</li>
<li>
<strong>Poisson regression</strong>, which is used for modeling count data.</li>
</ul>
<div id="logistic-regression" class="section level3" number="10.5.1">
<h3>
<span class="header-section-number">10.5.1</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Logistic regression</strong> is a GLM designed to model binary outcomes, where the response variable takes two values, such as 0/1 or yes/no. Instead of predicting the response variable directly, logistic regression estimates the probability of the outcome being in one class versus the other. To ensure the predicted probabilities lie between 0 and 1, the model uses the <strong>logit function</strong>, which is defined as:</p>
<p><span class="math display">\[
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]</span></p>
<p>Here, <span class="math inline">\(p\)</span> represents the probability of the outcome being 1, and the logit function transforms the linear combination of predictors into a probability.</p>
</div>
<div id="logistic-regression-in-r" class="section level3" number="10.5.2">
<h3>
<span class="header-section-number">10.5.2</span> Logistic Regression in R<a class="anchor" aria-label="anchor" href="#logistic-regression-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>To illustrate logistic regression, consider a task where we want to predict whether a customer will <strong>churn</strong> (leave the service) based on a set of predictors. The <em>churn</em> dataset, which we will use for this example, contains several variables related to customer behavior. However, based on prior knowledge, we will select the following predictors for our model:</p>
<p><code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>day.mins</code>, <code>eve.mins</code>, <code>night.mins</code>, and <code>customer.calls</code>.</p>
<p>We use the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function to fit the logistic regression model in R, specifying the response variable as <code>churn</code> (a binary variable) and the predictors as specified above:</p>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">churn</span><span class="op">)</span></span>
<span></span>
<span><span class="va">formula</span> <span class="op">=</span> <span class="va">churn</span> <span class="op">~</span> <span class="va">account.length</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> </span>
<span>                  <span class="va">night.mins</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">customer.calls</span> <span class="op">+</span> <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">voice.plan</span></span>
<span></span>
<span><span class="va">logreg_1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">churn</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span></code></pre></div>
<p>Here:</p>
<ul>
<li>
<code>churn</code> is the binary response variable.<br>
</li>
<li>The predictors are specified in the <code>formula</code>.<br>
</li>
<li>
<code>family = binomial</code> indicates a logistic regression model.</li>
</ul>
<p>To view the model’s summary and interpret the results, use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="chapter-regression.html#cb148-1" tabindex="-1"></a><span class="fu">summary</span>(logreg_1)</span>
<span id="cb148-2"><a href="chapter-regression.html#cb148-2" tabindex="-1"></a>   </span>
<span id="cb148-3"><a href="chapter-regression.html#cb148-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb148-4"><a href="chapter-regression.html#cb148-4" tabindex="-1"></a>   <span class="fu">glm</span>(<span class="at">formula =</span> formula, <span class="at">family =</span> binomial, <span class="at">data =</span> churn)</span>
<span id="cb148-5"><a href="chapter-regression.html#cb148-5" tabindex="-1"></a>   </span>
<span id="cb148-6"><a href="chapter-regression.html#cb148-6" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb148-7"><a href="chapter-regression.html#cb148-7" tabindex="-1"></a>                    Estimate Std. Error z value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>z<span class="sc">|</span>)    </span>
<span id="cb148-8"><a href="chapter-regression.html#cb148-8" tabindex="-1"></a>   (Intercept)     <span class="fl">8.8917584</span>  <span class="fl">0.6582188</span>  <span class="fl">13.509</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-9"><a href="chapter-regression.html#cb148-9" tabindex="-1"></a>   account.length <span class="sc">-</span><span class="fl">0.0013811</span>  <span class="fl">0.0011453</span>  <span class="sc">-</span><span class="fl">1.206</span>   <span class="fl">0.2279</span>    </span>
<span id="cb148-10"><a href="chapter-regression.html#cb148-10" tabindex="-1"></a>   voice.messages <span class="sc">-</span><span class="fl">0.0355317</span>  <span class="fl">0.0150397</span>  <span class="sc">-</span><span class="fl">2.363</span>   <span class="fl">0.0182</span> <span class="sc">*</span>  </span>
<span id="cb148-11"><a href="chapter-regression.html#cb148-11" tabindex="-1"></a>   day.mins       <span class="sc">-</span><span class="fl">0.0136547</span>  <span class="fl">0.0009103</span> <span class="sc">-</span><span class="fl">15.000</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-12"><a href="chapter-regression.html#cb148-12" tabindex="-1"></a>   eve.mins       <span class="sc">-</span><span class="fl">0.0071210</span>  <span class="fl">0.0009419</span>  <span class="sc">-</span><span class="fl">7.561</span> <span class="fl">4.02e-14</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-13"><a href="chapter-regression.html#cb148-13" tabindex="-1"></a>   night.mins     <span class="sc">-</span><span class="fl">0.0040518</span>  <span class="fl">0.0009048</span>  <span class="sc">-</span><span class="fl">4.478</span> <span class="fl">7.53e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-14"><a href="chapter-regression.html#cb148-14" tabindex="-1"></a>   intl.mins      <span class="sc">-</span><span class="fl">0.0882514</span>  <span class="fl">0.0170578</span>  <span class="sc">-</span><span class="fl">5.174</span> <span class="fl">2.30e-07</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-15"><a href="chapter-regression.html#cb148-15" tabindex="-1"></a>   customer.calls <span class="sc">-</span><span class="fl">0.5183958</span>  <span class="fl">0.0328652</span> <span class="sc">-</span><span class="fl">15.773</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-16"><a href="chapter-regression.html#cb148-16" tabindex="-1"></a>   intl.planno     <span class="fl">2.0958198</span>  <span class="fl">0.1214476</span>  <span class="fl">17.257</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-17"><a href="chapter-regression.html#cb148-17" tabindex="-1"></a>   voice.planno   <span class="sc">-</span><span class="fl">2.1637477</span>  <span class="fl">0.4836735</span>  <span class="sc">-</span><span class="fl">4.474</span> <span class="fl">7.69e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb148-18"><a href="chapter-regression.html#cb148-18" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb148-19"><a href="chapter-regression.html#cb148-19" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb148-20"><a href="chapter-regression.html#cb148-20" tabindex="-1"></a>   </span>
<span id="cb148-21"><a href="chapter-regression.html#cb148-21" tabindex="-1"></a>   (Dispersion parameter <span class="cf">for</span> binomial family taken to be <span class="dv">1</span>)</span>
<span id="cb148-22"><a href="chapter-regression.html#cb148-22" tabindex="-1"></a>   </span>
<span id="cb148-23"><a href="chapter-regression.html#cb148-23" tabindex="-1"></a>       Null deviance<span class="sc">:</span> <span class="fl">4075.0</span>  on <span class="dv">4999</span>  degrees of freedom</span>
<span id="cb148-24"><a href="chapter-regression.html#cb148-24" tabindex="-1"></a>   Residual deviance<span class="sc">:</span> <span class="fl">3174.3</span>  on <span class="dv">4990</span>  degrees of freedom</span>
<span id="cb148-25"><a href="chapter-regression.html#cb148-25" tabindex="-1"></a>   AIC<span class="sc">:</span> <span class="fl">3194.3</span></span>
<span id="cb148-26"><a href="chapter-regression.html#cb148-26" tabindex="-1"></a>   </span>
<span id="cb148-27"><a href="chapter-regression.html#cb148-27" tabindex="-1"></a>   Number of Fisher Scoring iterations<span class="sc">:</span> <span class="dv">6</span></span></code></pre></div>
<p>The summary output provides the estimated coefficients, standard errors, z-statistics, and p-values for each predictor. Predictors with high p-values (typically &gt; 0.05) are not statistically significant and should be reconsidered for removal. For instance, if <code>account.length</code> has a high p-value, we would exclude it from the model and re-run the regression. This iterative process ensures that only meaningful predictors are included in the final model.</p>
</div>
<div id="poisson-regression" class="section level3" number="10.5.3">
<h3>
<span class="header-section-number">10.5.3</span> Poisson Regression<a class="anchor" aria-label="anchor" href="#poisson-regression"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Poisson regression</strong> is a GLM used for modeling count data, where the response variable represents the number of occurrences of an event within a fixed interval. Examples include the number of customer service calls, website visits, or product purchases. The Poisson regression model assumes that:</p>
<ol style="list-style-type: decimal">
<li>The response variable follows a Poisson distribution.<br>
</li>
<li>The mean of the distribution equals its variance.<br>
</li>
<li>The log of the mean (<span class="math inline">\(\ln(\lambda)\)</span>) can be expressed as a linear combination of predictors.</li>
</ol>
<p>The Poisson regression model is expressed as:</p>
<p><span class="math display">\[
\ln(\lambda) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]</span></p>
<p>Here, <span class="math inline">\(\lambda\)</span> is the expected count (mean) of the response variable, and the predictors (<span class="math inline">\(x_1, x_2, \dots, x_p\)</span>) influence the log of <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="poisson-regression-in-r" class="section level3" number="10.5.4">
<h3>
<span class="header-section-number">10.5.4</span> Poisson Regression in R<a class="anchor" aria-label="anchor" href="#poisson-regression-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>To demonstrate Poisson regression, consider a task where we want to predict the <strong>number of customer service calls</strong> (<code>customer.calls</code>) based on the following predictors from the <em>churn</em> dataset:</p>
<p><code>churn</code>, <code>account.length</code>, <code>voice.plan</code>, <code>voice.messages</code>, <code>intl.plan</code>, <code>intl.mins</code>, <code>day.mins</code>, <code>eve.mins</code>, and <code>night.mins</code>.</p>
<p>Since <code>customer.calls</code> is an integer-valued variable, Poisson regression is more appropriate than linear regression for this task. We fit the Poisson regression model using the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function:</p>
<div class="sourceCode" id="cb149"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">formula</span> <span class="op">=</span> <span class="va">customer.calls</span> <span class="op">~</span> <span class="va">churn</span> <span class="op">+</span> <span class="va">voice.messages</span> <span class="op">+</span> <span class="va">day.mins</span> <span class="op">+</span> <span class="va">eve.mins</span> <span class="op">+</span> </span>
<span>                           <span class="va">night.mins</span> <span class="op">+</span> <span class="va">intl.mins</span> <span class="op">+</span> <span class="va">intl.plan</span> <span class="op">+</span> <span class="va">voice.plan</span></span>
<span></span>
<span><span class="va">reg_pois</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">churn</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span></span></code></pre></div>
<p>Here:
- <code>customer.calls</code> is the response variable.<br>
- The predictors are specified in the <code>formula</code>.<br>
- <code>family = poisson</code> indicates a Poisson regression model.</p>
<p>To evaluate the model, view the summary of the regression output:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="chapter-regression.html#cb150-1" tabindex="-1"></a><span class="fu">summary</span>(reg_pois)</span>
<span id="cb150-2"><a href="chapter-regression.html#cb150-2" tabindex="-1"></a>   </span>
<span id="cb150-3"><a href="chapter-regression.html#cb150-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb150-4"><a href="chapter-regression.html#cb150-4" tabindex="-1"></a>   <span class="fu">glm</span>(<span class="at">formula =</span> formula, <span class="at">family =</span> poisson, <span class="at">data =</span> churn)</span>
<span id="cb150-5"><a href="chapter-regression.html#cb150-5" tabindex="-1"></a>   </span>
<span id="cb150-6"><a href="chapter-regression.html#cb150-6" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb150-7"><a href="chapter-regression.html#cb150-7" tabindex="-1"></a>                    Estimate Std. Error z value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>z<span class="sc">|</span>)    </span>
<span id="cb150-8"><a href="chapter-regression.html#cb150-8" tabindex="-1"></a>   (Intercept)     <span class="fl">0.9957186</span>  <span class="fl">0.1323004</span>   <span class="fl">7.526</span> <span class="fl">5.22e-14</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb150-9"><a href="chapter-regression.html#cb150-9" tabindex="-1"></a>   churnno        <span class="sc">-</span><span class="fl">0.5160641</span>  <span class="fl">0.0304013</span> <span class="sc">-</span><span class="fl">16.975</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb150-10"><a href="chapter-regression.html#cb150-10" tabindex="-1"></a>   voice.messages  <span class="fl">0.0034062</span>  <span class="fl">0.0028294</span>   <span class="fl">1.204</span> <span class="fl">0.228646</span>    </span>
<span id="cb150-11"><a href="chapter-regression.html#cb150-11" tabindex="-1"></a>   day.mins       <span class="sc">-</span><span class="fl">0.0006875</span>  <span class="fl">0.0002078</span>  <span class="sc">-</span><span class="fl">3.309</span> <span class="fl">0.000938</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb150-12"><a href="chapter-regression.html#cb150-12" tabindex="-1"></a>   eve.mins       <span class="sc">-</span><span class="fl">0.0005649</span>  <span class="fl">0.0002237</span>  <span class="sc">-</span><span class="fl">2.525</span> <span class="fl">0.011554</span> <span class="sc">*</span>  </span>
<span id="cb150-13"><a href="chapter-regression.html#cb150-13" tabindex="-1"></a>   night.mins     <span class="sc">-</span><span class="fl">0.0003602</span>  <span class="fl">0.0002245</span>  <span class="sc">-</span><span class="fl">1.604</span> <span class="fl">0.108704</span>    </span>
<span id="cb150-14"><a href="chapter-regression.html#cb150-14" tabindex="-1"></a>   intl.mins      <span class="sc">-</span><span class="fl">0.0075034</span>  <span class="fl">0.0040886</span>  <span class="sc">-</span><span class="fl">1.835</span> <span class="fl">0.066475</span> .  </span>
<span id="cb150-15"><a href="chapter-regression.html#cb150-15" tabindex="-1"></a>   intl.planno     <span class="fl">0.2085330</span>  <span class="fl">0.0407760</span>   <span class="fl">5.114</span> <span class="fl">3.15e-07</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb150-16"><a href="chapter-regression.html#cb150-16" tabindex="-1"></a>   voice.planno    <span class="fl">0.0735515</span>  <span class="fl">0.0878175</span>   <span class="fl">0.838</span> <span class="fl">0.402284</span>    </span>
<span id="cb150-17"><a href="chapter-regression.html#cb150-17" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb150-18"><a href="chapter-regression.html#cb150-18" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb150-19"><a href="chapter-regression.html#cb150-19" tabindex="-1"></a>   </span>
<span id="cb150-20"><a href="chapter-regression.html#cb150-20" tabindex="-1"></a>   (Dispersion parameter <span class="cf">for</span> poisson family taken to be <span class="dv">1</span>)</span>
<span id="cb150-21"><a href="chapter-regression.html#cb150-21" tabindex="-1"></a>   </span>
<span id="cb150-22"><a href="chapter-regression.html#cb150-22" tabindex="-1"></a>       Null deviance<span class="sc">:</span> <span class="fl">5991.1</span>  on <span class="dv">4999</span>  degrees of freedom</span>
<span id="cb150-23"><a href="chapter-regression.html#cb150-23" tabindex="-1"></a>   Residual deviance<span class="sc">:</span> <span class="fl">5719.5</span>  on <span class="dv">4991</span>  degrees of freedom</span>
<span id="cb150-24"><a href="chapter-regression.html#cb150-24" tabindex="-1"></a>   AIC<span class="sc">:</span> <span class="dv">15592</span></span>
<span id="cb150-25"><a href="chapter-regression.html#cb150-25" tabindex="-1"></a>   </span>
<span id="cb150-26"><a href="chapter-regression.html#cb150-26" tabindex="-1"></a>   Number of Fisher Scoring iterations<span class="sc">:</span> <span class="dv">5</span></span></code></pre></div>
<p>The summary output provides the estimated coefficients, standard errors, z-statistics, and p-values for each predictor. Predictors with high p-values (typically &gt; 0.05) are not statistically significant and should be reconsidered. For instance, if the predictors <code>voice.messages</code>, <code>night.mins</code>, and <code>voice.plan</code> have high p-values, they can be excluded from the model.</p>
<p>For example, the coefficient of a significant predictor (e.g., <code>intl.plan</code>) can be interpreted as the expected percentage change in the mean of <code>customer.calls</code> for a one-unit change in the predictor, holding all other variables constant.</p>
<p><strong>Generalized Linear Models (GLMs)</strong> provide a flexible framework for modeling response variables with non-normal distributions. Logistic regression and Poisson regression are two common GLMs that allow us to model binary and count data, respectively. These models extend the applicability of regression techniques to a wide range of practical problems in data science, from customer churn prediction to event counts. By iteratively refining the model and excluding non-significant predictors, we ensure that the final model is both interpretable and predictive.</p>
<p>In the next sections, we will explore further techniques for validating and improving regression models to ensure their robustness and practical utility.</p>
</div>
</div>
<div id="sec-stepwise-regression" class="section level2" number="10.6">
<h2>
<span class="header-section-number">10.6</span> Model Selection Using Stepwise Regression<a class="anchor" aria-label="anchor" href="#sec-stepwise-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Selecting the appropriate predictors is a crucial step in building a regression model that is both accurate and interpretable. This process, known as <strong>model specification</strong>, ensures that the model captures the underlying relationships in the data without overfitting or including irrelevant predictors. Proper model specification enhances the model’s ability to make reliable predictions and provides meaningful insights for decision-making.</p>
<p>In many real-world applications, particularly in business and data science, datasets often contain dozens or even hundreds of potential predictors. Managing this complexity requires systematic methods to identify the most relevant predictors. One such method is <strong>stepwise regression</strong>, a popular algorithm for iterative model selection. Stepwise regression begins by evaluating predictors one at a time, adding those that improve the model and removing those that do not contribute meaningfully. This iterative process helps handle issues such as multicollinearity and ensures that only the most helpful predictors remain in the final model. Additionally, stepwise regression is computationally efficient for small to medium-sized datasets, making it a practical choice in many scenarios.</p>
<p>To assess model quality during the selection process, we use metrics like the <strong>Akaike Information Criterion (AIC)</strong>. The AIC provides a balance between model complexity and goodness of fit, with a lower AIC value indicating a better model. The AIC is defined as:</p>
<p><span class="math display">\[
AIC = 2p + n \log\left(\frac{SSE}{n}\right)
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of estimated predictors in the model, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(SSE\)</span> is the sum of squared errors, which quantifies the unexplained variability in the response variable. Using the AIC ensures that we select a model that explains the data well while penalizing unnecessary complexity. Unlike <span class="math inline">\(R^2\)</span>, which always increases as predictors are added, AIC introduces a penalty for model complexity, favoring simpler models that generalize better to new data. This makes AIC a robust criterion for selecting parsimonious and interpretable models.</p>
<p>To demonstrate the process of model specification, we will apply <strong>stepwise regression</strong> to the <em>marketing</em> dataset, which contains seven predictors. Our goal is to identify the best regression model for predicting <code>revenue</code> (daily revenue) based on these predictors.</p>
<div class="example">
<p><span id="exm:ex-stepwise-regression" class="example"><strong>Example 10.1  </strong></span>We start by building a linear regression model that includes all available predictors. This allows us to assess the initial model and use its results as a baseline for the stepwise regression process. The formula <code>revenue ~ .</code> in the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function specifies that all predictors in the dataset should be included in the initial model:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="chapter-regression.html#cb151-1" tabindex="-1"></a>ml_all <span class="ot">=</span> <span class="fu">lm</span>(revenue <span class="sc">~</span> ., <span class="at">data =</span> marketing)</span>
<span id="cb151-2"><a href="chapter-regression.html#cb151-2" tabindex="-1"></a></span>
<span id="cb151-3"><a href="chapter-regression.html#cb151-3" tabindex="-1"></a><span class="fu">summary</span>(ml_all)</span>
<span id="cb151-4"><a href="chapter-regression.html#cb151-4" tabindex="-1"></a>   </span>
<span id="cb151-5"><a href="chapter-regression.html#cb151-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb151-6"><a href="chapter-regression.html#cb151-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> ., <span class="at">data =</span> marketing)</span>
<span id="cb151-7"><a href="chapter-regression.html#cb151-7" tabindex="-1"></a>   </span>
<span id="cb151-8"><a href="chapter-regression.html#cb151-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb151-9"><a href="chapter-regression.html#cb151-9" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb151-10"><a href="chapter-regression.html#cb151-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">138.00</span>  <span class="sc">-</span><span class="fl">59.12</span>   <span class="fl">15.16</span>   <span class="fl">54.58</span>  <span class="fl">106.99</span> </span>
<span id="cb151-11"><a href="chapter-regression.html#cb151-11" tabindex="-1"></a>   </span>
<span id="cb151-12"><a href="chapter-regression.html#cb151-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb151-13"><a href="chapter-regression.html#cb151-13" tabindex="-1"></a>                     Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)</span>
<span id="cb151-14"><a href="chapter-regression.html#cb151-14" tabindex="-1"></a>   (Intercept)     <span class="sc">-</span><span class="fl">25.260020</span> <span class="fl">246.988978</span>  <span class="sc">-</span><span class="fl">0.102</span>    <span class="fl">0.919</span></span>
<span id="cb151-15"><a href="chapter-regression.html#cb151-15" tabindex="-1"></a>   spend            <span class="sc">-</span><span class="fl">0.025807</span>   <span class="fl">2.605645</span>  <span class="sc">-</span><span class="fl">0.010</span>    <span class="fl">0.992</span></span>
<span id="cb151-16"><a href="chapter-regression.html#cb151-16" tabindex="-1"></a>   clicks            <span class="fl">1.211912</span>   <span class="fl">1.630953</span>   <span class="fl">0.743</span>    <span class="fl">0.463</span></span>
<span id="cb151-17"><a href="chapter-regression.html#cb151-17" tabindex="-1"></a>   impressions      <span class="sc">-</span><span class="fl">0.005308</span>   <span class="fl">0.021588</span>  <span class="sc">-</span><span class="fl">0.246</span>    <span class="fl">0.807</span></span>
<span id="cb151-18"><a href="chapter-regression.html#cb151-18" tabindex="-1"></a>   display          <span class="fl">79.835729</span> <span class="fl">117.558849</span>   <span class="fl">0.679</span>    <span class="fl">0.502</span></span>
<span id="cb151-19"><a href="chapter-regression.html#cb151-19" tabindex="-1"></a>   transactions     <span class="sc">-</span><span class="fl">7.012069</span>  <span class="fl">66.383251</span>  <span class="sc">-</span><span class="fl">0.106</span>    <span class="fl">0.917</span></span>
<span id="cb151-20"><a href="chapter-regression.html#cb151-20" tabindex="-1"></a>   click.rate      <span class="sc">-</span><span class="fl">10.951493</span> <span class="fl">106.833894</span>  <span class="sc">-</span><span class="fl">0.103</span>    <span class="fl">0.919</span></span>
<span id="cb151-21"><a href="chapter-regression.html#cb151-21" tabindex="-1"></a>   conversion.rate  <span class="fl">19.926588</span> <span class="fl">135.746632</span>   <span class="fl">0.147</span>    <span class="fl">0.884</span></span>
<span id="cb151-22"><a href="chapter-regression.html#cb151-22" tabindex="-1"></a>   </span>
<span id="cb151-23"><a href="chapter-regression.html#cb151-23" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">77.61</span> on <span class="dv">32</span> degrees of freedom</span>
<span id="cb151-24"><a href="chapter-regression.html#cb151-24" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7829</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7354</span> </span>
<span id="cb151-25"><a href="chapter-regression.html#cb151-25" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">16.48</span> on <span class="dv">7</span> and <span class="dv">32</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">5.498e-09</span></span></code></pre></div>
<p>The output of this model reveals that several predictors have high p-values (much greater than the significance level <span class="math inline">\(\alpha = 0.05\)</span>), suggesting that these variables may not significantly contribute to the model. High multicollinearity among predictors could also be affecting the model’s performance.</p>
<p>Next, we apply <strong>stepwise regression</strong> using the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function. By setting the <code>direction</code> argument to <code>"both"</code>, the algorithm iteratively adds and removes predictors, evaluating their contribution to the model at each step:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="chapter-regression.html#cb152-1" tabindex="-1"></a>ml_stepwise <span class="ot">=</span> <span class="fu">step</span>(ml_all, <span class="at">direction =</span> <span class="st">"both"</span>)</span>
<span id="cb152-2"><a href="chapter-regression.html#cb152-2" tabindex="-1"></a>   Start<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">355.21</span></span>
<span id="cb152-3"><a href="chapter-regression.html#cb152-3" tabindex="-1"></a>   revenue <span class="sc">~</span> spend <span class="sc">+</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> </span>
<span id="cb152-4"><a href="chapter-regression.html#cb152-4" tabindex="-1"></a>       click.rate <span class="sc">+</span> conversion.rate</span>
<span id="cb152-5"><a href="chapter-regression.html#cb152-5" tabindex="-1"></a>   </span>
<span id="cb152-6"><a href="chapter-regression.html#cb152-6" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb152-7"><a href="chapter-regression.html#cb152-7" tabindex="-1"></a>   <span class="sc">-</span> spend            <span class="dv">1</span>       <span class="fl">0.6</span> <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb152-8"><a href="chapter-regression.html#cb152-8" tabindex="-1"></a>   <span class="sc">-</span> click.rate       <span class="dv">1</span>      <span class="fl">63.3</span> <span class="dv">192822</span> <span class="fl">353.23</span></span>
<span id="cb152-9"><a href="chapter-regression.html#cb152-9" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">67.2</span> <span class="dv">192826</span> <span class="fl">353.23</span></span>
<span id="cb152-10"><a href="chapter-regression.html#cb152-10" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">129.8</span> <span class="dv">192889</span> <span class="fl">353.24</span></span>
<span id="cb152-11"><a href="chapter-regression.html#cb152-11" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">364.2</span> <span class="dv">193123</span> <span class="fl">353.29</span></span>
<span id="cb152-12"><a href="chapter-regression.html#cb152-12" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">2778.1</span> <span class="dv">195537</span> <span class="fl">353.79</span></span>
<span id="cb152-13"><a href="chapter-regression.html#cb152-13" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3326.0</span> <span class="dv">196085</span> <span class="fl">353.90</span></span>
<span id="cb152-14"><a href="chapter-regression.html#cb152-14" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192759</span> <span class="fl">355.21</span></span>
<span id="cb152-15"><a href="chapter-regression.html#cb152-15" tabindex="-1"></a>   </span>
<span id="cb152-16"><a href="chapter-regression.html#cb152-16" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">353.21</span></span>
<span id="cb152-17"><a href="chapter-regression.html#cb152-17" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> click.rate <span class="sc">+</span> </span>
<span id="cb152-18"><a href="chapter-regression.html#cb152-18" tabindex="-1"></a>       conversion.rate</span>
<span id="cb152-19"><a href="chapter-regression.html#cb152-19" tabindex="-1"></a>   </span>
<span id="cb152-20"><a href="chapter-regression.html#cb152-20" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb152-21"><a href="chapter-regression.html#cb152-21" tabindex="-1"></a>   <span class="sc">-</span> click.rate       <span class="dv">1</span>      <span class="fl">67.9</span> <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb152-22"><a href="chapter-regression.html#cb152-22" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">75.1</span> <span class="dv">192835</span> <span class="fl">351.23</span></span>
<span id="cb152-23"><a href="chapter-regression.html#cb152-23" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">151.5</span> <span class="dv">192911</span> <span class="fl">351.24</span></span>
<span id="cb152-24"><a href="chapter-regression.html#cb152-24" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">380.8</span> <span class="dv">193141</span> <span class="fl">351.29</span></span>
<span id="cb152-25"><a href="chapter-regression.html#cb152-25" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">2787.2</span> <span class="dv">195547</span> <span class="fl">351.79</span></span>
<span id="cb152-26"><a href="chapter-regression.html#cb152-26" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3325.6</span> <span class="dv">196085</span> <span class="fl">351.90</span></span>
<span id="cb152-27"><a href="chapter-regression.html#cb152-27" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb152-28"><a href="chapter-regression.html#cb152-28" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>       <span class="fl">0.6</span> <span class="dv">192759</span> <span class="fl">355.21</span></span>
<span id="cb152-29"><a href="chapter-regression.html#cb152-29" tabindex="-1"></a>   </span>
<span id="cb152-30"><a href="chapter-regression.html#cb152-30" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">351.23</span></span>
<span id="cb152-31"><a href="chapter-regression.html#cb152-31" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> transactions <span class="sc">+</span> conversion.rate</span>
<span id="cb152-32"><a href="chapter-regression.html#cb152-32" tabindex="-1"></a>   </span>
<span id="cb152-33"><a href="chapter-regression.html#cb152-33" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb152-34"><a href="chapter-regression.html#cb152-34" tabindex="-1"></a>   <span class="sc">-</span> transactions     <span class="dv">1</span>      <span class="fl">47.4</span> <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb152-35"><a href="chapter-regression.html#cb152-35" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>     <span class="fl">129.0</span> <span class="dv">192957</span> <span class="fl">349.25</span></span>
<span id="cb152-36"><a href="chapter-regression.html#cb152-36" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">312.9</span> <span class="dv">193141</span> <span class="fl">349.29</span></span>
<span id="cb152-37"><a href="chapter-regression.html#cb152-37" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="fl">3425.7</span> <span class="dv">196253</span> <span class="fl">349.93</span></span>
<span id="cb152-38"><a href="chapter-regression.html#cb152-38" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">3747.1</span> <span class="dv">196575</span> <span class="fl">350.00</span></span>
<span id="cb152-39"><a href="chapter-regression.html#cb152-39" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb152-40"><a href="chapter-regression.html#cb152-40" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>      <span class="fl">67.9</span> <span class="dv">192760</span> <span class="fl">353.21</span></span>
<span id="cb152-41"><a href="chapter-regression.html#cb152-41" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>       <span class="fl">5.2</span> <span class="dv">192822</span> <span class="fl">353.23</span></span>
<span id="cb152-42"><a href="chapter-regression.html#cb152-42" tabindex="-1"></a>   </span>
<span id="cb152-43"><a href="chapter-regression.html#cb152-43" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">349.24</span></span>
<span id="cb152-44"><a href="chapter-regression.html#cb152-44" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display <span class="sc">+</span> conversion.rate</span>
<span id="cb152-45"><a href="chapter-regression.html#cb152-45" tabindex="-1"></a>   </span>
<span id="cb152-46"><a href="chapter-regression.html#cb152-46" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb152-47"><a href="chapter-regression.html#cb152-47" tabindex="-1"></a>   <span class="sc">-</span> conversion.rate  <span class="dv">1</span>      <span class="fl">89.6</span> <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb152-48"><a href="chapter-regression.html#cb152-48" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>     <span class="fl">480.9</span> <span class="dv">193356</span> <span class="fl">347.34</span></span>
<span id="cb152-49"><a href="chapter-regression.html#cb152-49" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>    <span class="fl">5437.2</span> <span class="dv">198312</span> <span class="fl">348.35</span></span>
<span id="cb152-50"><a href="chapter-regression.html#cb152-50" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb152-51"><a href="chapter-regression.html#cb152-51" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>      <span class="fl">47.4</span> <span class="dv">192828</span> <span class="fl">351.23</span></span>
<span id="cb152-52"><a href="chapter-regression.html#cb152-52" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>      <span class="fl">40.2</span> <span class="dv">192835</span> <span class="fl">351.23</span></span>
<span id="cb152-53"><a href="chapter-regression.html#cb152-53" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>      <span class="fl">13.6</span> <span class="dv">192861</span> <span class="fl">351.23</span></span>
<span id="cb152-54"><a href="chapter-regression.html#cb152-54" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>   <span class="fl">30863.2</span> <span class="dv">223738</span> <span class="fl">353.17</span></span>
<span id="cb152-55"><a href="chapter-regression.html#cb152-55" tabindex="-1"></a>   </span>
<span id="cb152-56"><a href="chapter-regression.html#cb152-56" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">347.26</span></span>
<span id="cb152-57"><a href="chapter-regression.html#cb152-57" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> impressions <span class="sc">+</span> display</span>
<span id="cb152-58"><a href="chapter-regression.html#cb152-58" tabindex="-1"></a>   </span>
<span id="cb152-59"><a href="chapter-regression.html#cb152-59" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb152-60"><a href="chapter-regression.html#cb152-60" tabindex="-1"></a>   <span class="sc">-</span> impressions      <span class="dv">1</span>       <span class="dv">399</span> <span class="dv">193364</span> <span class="fl">345.34</span></span>
<span id="cb152-61"><a href="chapter-regression.html#cb152-61" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb152-62"><a href="chapter-regression.html#cb152-62" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>     <span class="dv">14392</span> <span class="dv">207357</span> <span class="fl">348.13</span></span>
<span id="cb152-63"><a href="chapter-regression.html#cb152-63" tabindex="-1"></a>   <span class="sc">+</span> conversion.rate  <span class="dv">1</span>        <span class="dv">90</span> <span class="dv">192875</span> <span class="fl">349.24</span></span>
<span id="cb152-64"><a href="chapter-regression.html#cb152-64" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>        <span class="dv">52</span> <span class="dv">192913</span> <span class="fl">349.24</span></span>
<span id="cb152-65"><a href="chapter-regression.html#cb152-65" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>        <span class="dv">33</span> <span class="dv">192932</span> <span class="fl">349.25</span></span>
<span id="cb152-66"><a href="chapter-regression.html#cb152-66" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>         <span class="dv">8</span> <span class="dv">192957</span> <span class="fl">349.25</span></span>
<span id="cb152-67"><a href="chapter-regression.html#cb152-67" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>     <span class="dv">35038</span> <span class="dv">228002</span> <span class="fl">351.93</span></span>
<span id="cb152-68"><a href="chapter-regression.html#cb152-68" tabindex="-1"></a>   </span>
<span id="cb152-69"><a href="chapter-regression.html#cb152-69" tabindex="-1"></a>   Step<span class="sc">:</span>  AIC<span class="ot">=</span><span class="fl">345.34</span></span>
<span id="cb152-70"><a href="chapter-regression.html#cb152-70" tabindex="-1"></a>   revenue <span class="sc">~</span> clicks <span class="sc">+</span> display</span>
<span id="cb152-71"><a href="chapter-regression.html#cb152-71" tabindex="-1"></a>   </span>
<span id="cb152-72"><a href="chapter-regression.html#cb152-72" tabindex="-1"></a>                     Df Sum of Sq    RSS    AIC</span>
<span id="cb152-73"><a href="chapter-regression.html#cb152-73" tabindex="-1"></a>   <span class="sc">&lt;</span>none<span class="sc">&gt;</span>                         <span class="dv">193364</span> <span class="fl">345.34</span></span>
<span id="cb152-74"><a href="chapter-regression.html#cb152-74" tabindex="-1"></a>   <span class="sc">+</span> impressions      <span class="dv">1</span>       <span class="dv">399</span> <span class="dv">192965</span> <span class="fl">347.26</span></span>
<span id="cb152-75"><a href="chapter-regression.html#cb152-75" tabindex="-1"></a>   <span class="sc">+</span> transactions     <span class="dv">1</span>       <span class="dv">215</span> <span class="dv">193149</span> <span class="fl">347.29</span></span>
<span id="cb152-76"><a href="chapter-regression.html#cb152-76" tabindex="-1"></a>   <span class="sc">+</span> conversion.rate  <span class="dv">1</span>         <span class="dv">8</span> <span class="dv">193356</span> <span class="fl">347.34</span></span>
<span id="cb152-77"><a href="chapter-regression.html#cb152-77" tabindex="-1"></a>   <span class="sc">+</span> click.rate       <span class="dv">1</span>         <span class="dv">6</span> <span class="dv">193358</span> <span class="fl">347.34</span></span>
<span id="cb152-78"><a href="chapter-regression.html#cb152-78" tabindex="-1"></a>   <span class="sc">+</span> spend            <span class="dv">1</span>         <span class="dv">2</span> <span class="dv">193362</span> <span class="fl">347.34</span></span>
<span id="cb152-79"><a href="chapter-regression.html#cb152-79" tabindex="-1"></a>   <span class="sc">-</span> display          <span class="dv">1</span>     <span class="dv">91225</span> <span class="dv">284589</span> <span class="fl">358.80</span></span>
<span id="cb152-80"><a href="chapter-regression.html#cb152-80" tabindex="-1"></a>   <span class="sc">-</span> clicks           <span class="dv">1</span>    <span class="dv">606800</span> <span class="dv">800164</span> <span class="fl">400.15</span></span></code></pre></div>
<p>During the first iteration, the stepwise algorithm evaluates the contribution of each predictor and removes the variable <code>spend</code>, which has the highest p-value in the initial model. Subsequent iterations continue this process, adding or removing variables based on their impact on model performance. The stepwise process concludes when no further changes improve the model.</p>
<p>To view the final selected model, we use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="chapter-regression.html#cb153-1" tabindex="-1"></a><span class="fu">summary</span>(ml_stepwise)</span>
<span id="cb153-2"><a href="chapter-regression.html#cb153-2" tabindex="-1"></a>   </span>
<span id="cb153-3"><a href="chapter-regression.html#cb153-3" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb153-4"><a href="chapter-regression.html#cb153-4" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> revenue <span class="sc">~</span> clicks <span class="sc">+</span> display, <span class="at">data =</span> marketing)</span>
<span id="cb153-5"><a href="chapter-regression.html#cb153-5" tabindex="-1"></a>   </span>
<span id="cb153-6"><a href="chapter-regression.html#cb153-6" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb153-7"><a href="chapter-regression.html#cb153-7" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb153-8"><a href="chapter-regression.html#cb153-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">141.89</span>  <span class="sc">-</span><span class="fl">55.92</span>   <span class="fl">16.44</span>   <span class="fl">52.70</span>  <span class="fl">115.46</span> </span>
<span id="cb153-9"><a href="chapter-regression.html#cb153-9" tabindex="-1"></a>   </span>
<span id="cb153-10"><a href="chapter-regression.html#cb153-10" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb153-11"><a href="chapter-regression.html#cb153-11" tabindex="-1"></a>                Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb153-12"><a href="chapter-regression.html#cb153-12" tabindex="-1"></a>   (Intercept) <span class="sc">-</span><span class="fl">33.63248</span>   <span class="fl">28.68893</span>  <span class="sc">-</span><span class="fl">1.172</span> <span class="fl">0.248564</span>    </span>
<span id="cb153-13"><a href="chapter-regression.html#cb153-13" tabindex="-1"></a>   clicks        <span class="fl">0.89517</span>    <span class="fl">0.08308</span>  <span class="fl">10.775</span> <span class="fl">5.76e-13</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb153-14"><a href="chapter-regression.html#cb153-14" tabindex="-1"></a>   display      <span class="fl">95.51462</span>   <span class="fl">22.86126</span>   <span class="fl">4.178</span> <span class="fl">0.000172</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb153-15"><a href="chapter-regression.html#cb153-15" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb153-16"><a href="chapter-regression.html#cb153-16" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb153-17"><a href="chapter-regression.html#cb153-17" tabindex="-1"></a>   </span>
<span id="cb153-18"><a href="chapter-regression.html#cb153-18" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">72.29</span> on <span class="dv">37</span> degrees of freedom</span>
<span id="cb153-19"><a href="chapter-regression.html#cb153-19" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7822</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.7704</span> </span>
<span id="cb153-20"><a href="chapter-regression.html#cb153-20" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">66.44</span> on <span class="dv">2</span> and <span class="dv">37</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">5.682e-13</span></span></code></pre></div>
<p>The stepwise regression process selects a simpler model with only two predictors: <code>clicks</code> and <code>display</code>. The final estimated regression equation is:</p>
<p><span class="math display">\[
\text{revenue} = -33.63 + 0.9 \cdot \text{clicks} + 95.51 \cdot \text{display}
\]</span></p>
<p>This model demonstrates a better fit compared to the initial full model. Specifically:</p>
<ul>
<li>The <strong>Residual Standard Error (RSE)</strong> has decreased from approximately 93.82 to 72.29, indicating that the typical prediction error is now smaller.<br>
</li>
<li>The <strong>R-squared (<span class="math inline">\(R^2\)</span>)</strong> value has increased from 62% to 77%, meaning that a greater proportion of the variability in <code>revenue</code> is explained by the final model.</li>
</ul>
<p>The stepwise regression approach identifies the most relevant predictors (<code>clicks</code> and <code>display</code>) while discarding others that do not significantly improve model performance. This ensures that the final model is both parsimonious and interpretable.</p>
</div>
<p>In this example, stepwise regression helped us build a more efficient regression model for the <em>marketing</em> dataset by iteratively adding and removing predictors. By using the AIC as a selection criterion, we identified the model with the best trade-off between complexity and goodness of fit.</p>
<p>While stepwise regression is a popular and intuitive method for model selection, it is important to recognize its limitations. First, the algorithm evaluates predictors sequentially rather than considering all possible combinations, which can sometimes result in suboptimal models. Second, stepwise regression is prone to overfitting, particularly when working with small datasets or those containing many predictors. Overfitting occurs when the model captures noise rather than meaningful patterns, reducing its generalizability to new data. Finally, multicollinearity can distort the p-values used in stepwise regression, leading to misleading results. For these reasons, alternative methods like <strong>LASSO</strong> (Least Absolute Shrinkage and Selection Operator) or <strong>Ridge Regression</strong> are often preferred in more complex scenarios, particularly with high-dimensional datasets. These methods are beyond the scope of this book, but for further exploration, refer to <a href="https://www.statlearning.com">An Introduction to Statistical Learning with Applications in R</a>.</p>
<p>In conclusion, <strong>model specification</strong> is a critical step in regression analysis. By selecting the right predictors and using systematic techniques like stepwise regression, we can build models that are both accurate and interpretable. While stepwise regression is not without its limitations, it remains a practical and widely used method for selecting predictors in regression models, particularly for datasets with a manageable number of variables. The process improves the model’s predictive performance while maintaining simplicity and interpretability, making it a valuable tool for data-driven decision-making.</p>
</div>
<div id="extending-linear-models-to-capture-non-linear-relationships" class="section level2" number="10.7">
<h2>
<span class="header-section-number">10.7</span> Extending Linear Models to Capture Non-Linear Relationships<a class="anchor" aria-label="anchor" href="#extending-linear-models-to-capture-non-linear-relationships"><i class="fas fa-link"></i></a>
</h2>
<p>Thus far, we have focused on linear regression models, which are simple, interpretable, and easy to implement. While these models work well when relationships between predictors and response variables are approximately linear, their predictive power is limited in cases where these relationships are inherently non-linear. In such scenarios, relying on the linearity assumption can lead to suboptimal results.</p>
<p>In earlier sections, we explored techniques like stepwise regression (Section <a href="chapter-regression.html#sec-stepwise-regression">10.6</a>) to improve linear models by reducing complexity and addressing multicollinearity. However, these techniques operate within the constraints of a linear framework. To capture non-linear patterns while retaining interpretability, we turn to <strong>polynomial regression</strong>, a simple yet powerful extension of linear regression that incorporates non-linear terms.</p>
<div id="the-need-for-non-linear-regression" class="section level3 unnumbered">
<h3>The Need for Non-Linear Regression<a class="anchor" aria-label="anchor" href="#the-need-for-non-linear-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Linear regression assumes a straight-line relationship between predictors and the response variable. However, in many real-world datasets, relationships are more complex. For example, consider the scatter plot in Figure <a href="chapter-regression.html#fig:scoter-plot-non-reg">10.2</a>, which depicts the relationship between <code>unit.price</code> (house price per unit area) and <code>house.age</code> (age of the house) from the <em>house</em> dataset. The orange line represents the fit of a simple linear regression model. Clearly, the data exhibits a curved, non-linear pattern, which the linear fit fails to capture.</p>
<p>To address this limitation, we can enhance the model by incorporating non-linear terms. For instance, if the data suggests a quadratic relationship, we can model it as:</p>
<p><span class="math display">\[
unit.price = b_0 + b_1 \cdot house.age + b_2 \cdot house.age^2
\]</span></p>
<p>This equation predicts <code>unit.price</code> using both <code>house.age</code> and its square (<code>house.age^2</code>). While this model accommodates non-linear relationships, it remains a <strong>linear model</strong> because the coefficients (<span class="math inline">\(b_0, b_1, b_2\)</span>) are estimated using linear least squares. The blue curve in Figure <a href="chapter-regression.html#fig:scoter-plot-non-reg">10.2</a> illustrates the quadratic fit, which captures the data’s pattern far better than the linear fit.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scoter-plot-non-reg"></span>
<img src="regression_files/figure-html/scoter-plot-non-reg-1.png" alt="Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in orange and the quadratic regression curve in blue." width="100%"><p class="caption">
Figure 10.2: Scatter plot of house price ($) versus house age (years) for the house dataset, with the fitted simple linear regression line in orange and the quadratic regression curve in blue.
</p>
</div>
</div>
<div id="polynomial-regression" class="section level3" number="10.7.1">
<h3>
<span class="header-section-number">10.7.1</span> Polynomial Regression<a class="anchor" aria-label="anchor" href="#polynomial-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Polynomial regression extends the linear model by adding higher-order terms of the predictors, such as squared (<span class="math inline">\(x^2\)</span>), cubic (<span class="math inline">\(x^3\)</span>), or higher-degree terms. For example, a cubic regression model takes the form:</p>
<p><span class="math display">\[
\hat{y} = b_0 + b_1 \cdot x + b_2 \cdot x^2 + b_3 \cdot x^3
\]</span></p>
<p>This allows polynomial regression to capture increasingly complex patterns. Importantly, although the model is non-linear in terms of the predictors, it remains <strong>linear in the coefficients</strong> (<span class="math inline">\(b_0, b_1, b_2, b_3\)</span>), making it compatible with standard least squares estimation. However, care must be taken when using high-degree polynomials (<span class="math inline">\(d &gt; 3\)</span>) as they can become overly flexible, fitting noise in the data rather than meaningful patterns (overfitting), especially near the boundaries of the predictor range.</p>
</div>
<div id="example-polynomial-regression-on-the-house-dataset" class="section level3" number="10.7.2">
<h3>
<span class="header-section-number">10.7.2</span> Example: Polynomial Regression on the <em>House</em> Dataset<a class="anchor" aria-label="anchor" href="#example-polynomial-regression-on-the-house-dataset"><i class="fas fa-link"></i></a>
</h3>
<p>To illustrate polynomial regression, we use the <em>house</em> dataset, which contains information about house prices and features such as house age, distance to MRT stations, and the number of convenience stores. Our objective is to model <code>unit.price</code> (house price per unit area) as a function of <code>house.age</code> and compare the performance of simple linear regression to polynomial regression.</p>
<p>The dataset consists of six features and 414 observations. The target variable is <code>unit.price</code>, while the predictors include:</p>
<ul>
<li>
<strong><code>house.age</code></strong>: Age of the house (years).<br>
</li>
<li>
<strong><code>distance.to.MRT</code></strong>: Distance to the nearest MRT station.<br>
</li>
<li>
<strong><code>stores.number</code></strong>: Number of convenience stores.<br>
</li>
<li>
<strong><code>latitude</code></strong>: Latitude.<br>
</li>
<li>
<strong><code>longitude</code></strong>: Longitude.<br>
</li>
<li>
<strong><code>unit.price</code></strong>: House price per unit area (target variable).</li>
</ul>
<p>First, we examine the structure of the dataset:</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="chapter-regression.html#cb154-1" tabindex="-1"></a><span class="fu">data</span>(house)</span>
<span id="cb154-2"><a href="chapter-regression.html#cb154-2" tabindex="-1"></a></span>
<span id="cb154-3"><a href="chapter-regression.html#cb154-3" tabindex="-1"></a><span class="fu">str</span>(house)</span>
<span id="cb154-4"><a href="chapter-regression.html#cb154-4" tabindex="-1"></a>   <span class="st">'data.frame'</span><span class="sc">:</span>    <span class="dv">414</span> obs. of  <span class="dv">6</span> variables<span class="sc">:</span></span>
<span id="cb154-5"><a href="chapter-regression.html#cb154-5" tabindex="-1"></a>    <span class="er">$</span> house.age      <span class="sc">:</span> num  <span class="dv">32</span> <span class="fl">19.5</span> <span class="fl">13.3</span> <span class="fl">13.3</span> <span class="dv">5</span> <span class="fl">7.1</span> <span class="fl">34.5</span> <span class="fl">20.3</span> <span class="fl">31.7</span> <span class="fl">17.9</span> ...</span>
<span id="cb154-6"><a href="chapter-regression.html#cb154-6" tabindex="-1"></a>    <span class="sc">$</span> distance.to.MRT<span class="sc">:</span> num  <span class="fl">84.9</span> <span class="fl">306.6</span> <span class="dv">562</span> <span class="dv">562</span> <span class="fl">390.6</span> ...</span>
<span id="cb154-7"><a href="chapter-regression.html#cb154-7" tabindex="-1"></a>    <span class="sc">$</span> stores.number  <span class="sc">:</span> int  <span class="dv">10</span> <span class="dv">9</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">5</span> <span class="dv">3</span> <span class="dv">7</span> <span class="dv">6</span> <span class="dv">1</span> <span class="dv">3</span> ...</span>
<span id="cb154-8"><a href="chapter-regression.html#cb154-8" tabindex="-1"></a>    <span class="sc">$</span> latitude       <span class="sc">:</span> num  <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> <span class="dv">25</span> ...</span>
<span id="cb154-9"><a href="chapter-regression.html#cb154-9" tabindex="-1"></a>    <span class="sc">$</span> longitude      <span class="sc">:</span> num  <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> <span class="dv">122</span> ...</span>
<span id="cb154-10"><a href="chapter-regression.html#cb154-10" tabindex="-1"></a>    <span class="sc">$</span> unit.price     <span class="sc">:</span> num  <span class="fl">37.9</span> <span class="fl">42.2</span> <span class="fl">47.3</span> <span class="fl">54.8</span> <span class="fl">43.1</span> <span class="fl">32.1</span> <span class="fl">40.3</span> <span class="fl">46.7</span> <span class="fl">18.8</span> <span class="fl">22.1</span> ...</span></code></pre></div>
<p>The dataset includes 414 observations and 6 variables, with 5 predictors and one numerical target variable (<code>unit.price</code>).</p>
<div id="fitting-simple-linear-regression" class="section level4 unnumbered">
<h4>Fitting Simple Linear Regression<a class="anchor" aria-label="anchor" href="#fitting-simple-linear-regression"><i class="fas fa-link"></i></a>
</h4>
<p>We begin by fitting a simple linear regression model to predict <code>unit.price</code> using <code>house.age</code>:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="chapter-regression.html#cb155-1" tabindex="-1"></a>simple_reg_house <span class="ot">=</span> <span class="fu">lm</span>(unit.price <span class="sc">~</span> house.age, <span class="at">data =</span> house)</span>
<span id="cb155-2"><a href="chapter-regression.html#cb155-2" tabindex="-1"></a></span>
<span id="cb155-3"><a href="chapter-regression.html#cb155-3" tabindex="-1"></a><span class="fu">summary</span>(simple_reg_house)</span>
<span id="cb155-4"><a href="chapter-regression.html#cb155-4" tabindex="-1"></a>   </span>
<span id="cb155-5"><a href="chapter-regression.html#cb155-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb155-6"><a href="chapter-regression.html#cb155-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> unit.price <span class="sc">~</span> house.age, <span class="at">data =</span> house)</span>
<span id="cb155-7"><a href="chapter-regression.html#cb155-7" tabindex="-1"></a>   </span>
<span id="cb155-8"><a href="chapter-regression.html#cb155-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb155-9"><a href="chapter-regression.html#cb155-9" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb155-10"><a href="chapter-regression.html#cb155-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">31.113</span> <span class="sc">-</span><span class="fl">10.738</span>   <span class="fl">1.626</span>   <span class="fl">8.199</span>  <span class="fl">77.781</span> </span>
<span id="cb155-11"><a href="chapter-regression.html#cb155-11" tabindex="-1"></a>   </span>
<span id="cb155-12"><a href="chapter-regression.html#cb155-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb155-13"><a href="chapter-regression.html#cb155-13" tabindex="-1"></a>               Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb155-14"><a href="chapter-regression.html#cb155-14" tabindex="-1"></a>   (Intercept) <span class="fl">42.43470</span>    <span class="fl">1.21098</span>  <span class="fl">35.042</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb155-15"><a href="chapter-regression.html#cb155-15" tabindex="-1"></a>   house.age   <span class="sc">-</span><span class="fl">0.25149</span>    <span class="fl">0.05752</span>  <span class="sc">-</span><span class="fl">4.372</span> <span class="fl">1.56e-05</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb155-16"><a href="chapter-regression.html#cb155-16" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb155-17"><a href="chapter-regression.html#cb155-17" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb155-18"><a href="chapter-regression.html#cb155-18" tabindex="-1"></a>   </span>
<span id="cb155-19"><a href="chapter-regression.html#cb155-19" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">13.32</span> on <span class="dv">412</span> degrees of freedom</span>
<span id="cb155-20"><a href="chapter-regression.html#cb155-20" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.04434</span>,    Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.04202</span> </span>
<span id="cb155-21"><a href="chapter-regression.html#cb155-21" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">19.11</span> on <span class="dv">1</span> and <span class="dv">412</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="fl">1.56e-05</span></span></code></pre></div>
<p>The <strong>R-squared (<span class="math inline">\(R^2\)</span>)</strong> value for this model is 0.04, indicating that only 4.43% of the variability in house prices is explained by <code>house.age</code>. This low <span class="math inline">\(R^2\)</span> reflects a poor fit to the data.</p>
</div>
<div id="fitting-polynomial-regression" class="section level4 unnumbered">
<h4>Fitting Polynomial Regression<a class="anchor" aria-label="anchor" href="#fitting-polynomial-regression"><i class="fas fa-link"></i></a>
</h4>
<p>Next, we fit a quadratic polynomial regression model to better capture the non-linear relationship:</p>
<p><span class="math display">\[
unit.price = b_0 + b_1 \cdot house.age + b_2 \cdot house.age^2
\]</span></p>
<p>We implement the model in R:</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="chapter-regression.html#cb156-1" tabindex="-1"></a>reg_nonlinear_house <span class="ot">=</span> <span class="fu">lm</span>(unit.price <span class="sc">~</span> <span class="fu">poly</span>(house.age, <span class="dv">2</span>), <span class="at">data =</span> house)</span>
<span id="cb156-2"><a href="chapter-regression.html#cb156-2" tabindex="-1"></a></span>
<span id="cb156-3"><a href="chapter-regression.html#cb156-3" tabindex="-1"></a><span class="fu">summary</span>(reg_nonlinear_house)</span>
<span id="cb156-4"><a href="chapter-regression.html#cb156-4" tabindex="-1"></a>   </span>
<span id="cb156-5"><a href="chapter-regression.html#cb156-5" tabindex="-1"></a>   Call<span class="sc">:</span></span>
<span id="cb156-6"><a href="chapter-regression.html#cb156-6" tabindex="-1"></a>   <span class="fu">lm</span>(<span class="at">formula =</span> unit.price <span class="sc">~</span> <span class="fu">poly</span>(house.age, <span class="dv">2</span>), <span class="at">data =</span> house)</span>
<span id="cb156-7"><a href="chapter-regression.html#cb156-7" tabindex="-1"></a>   </span>
<span id="cb156-8"><a href="chapter-regression.html#cb156-8" tabindex="-1"></a>   Residuals<span class="sc">:</span></span>
<span id="cb156-9"><a href="chapter-regression.html#cb156-9" tabindex="-1"></a>       Min      <span class="dv">1</span>Q  Median      <span class="dv">3</span>Q     Max </span>
<span id="cb156-10"><a href="chapter-regression.html#cb156-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">26.542</span>  <span class="sc">-</span><span class="fl">9.085</span>  <span class="sc">-</span><span class="fl">0.445</span>   <span class="fl">8.260</span>  <span class="fl">79.961</span> </span>
<span id="cb156-11"><a href="chapter-regression.html#cb156-11" tabindex="-1"></a>   </span>
<span id="cb156-12"><a href="chapter-regression.html#cb156-12" tabindex="-1"></a>   Coefficients<span class="sc">:</span></span>
<span id="cb156-13"><a href="chapter-regression.html#cb156-13" tabindex="-1"></a>                       Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb156-14"><a href="chapter-regression.html#cb156-14" tabindex="-1"></a>   (Intercept)           <span class="fl">37.980</span>      <span class="fl">0.599</span>  <span class="fl">63.406</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb156-15"><a href="chapter-regression.html#cb156-15" tabindex="-1"></a>   <span class="fu">poly</span>(house.age, <span class="dv">2</span>)<span class="dv">1</span>  <span class="sc">-</span><span class="fl">58.225</span>     <span class="fl">12.188</span>  <span class="sc">-</span><span class="fl">4.777</span> <span class="fl">2.48e-06</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb156-16"><a href="chapter-regression.html#cb156-16" tabindex="-1"></a>   <span class="fu">poly</span>(house.age, <span class="dv">2</span>)<span class="dv">2</span>  <span class="fl">109.635</span>     <span class="fl">12.188</span>   <span class="fl">8.995</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb156-17"><a href="chapter-regression.html#cb156-17" tabindex="-1"></a>   <span class="sc">---</span></span>
<span id="cb156-18"><a href="chapter-regression.html#cb156-18" tabindex="-1"></a>   Signif. codes<span class="sc">:</span>  <span class="dv">0</span> <span class="st">'***'</span> <span class="fl">0.001</span> <span class="st">'**'</span> <span class="fl">0.01</span> <span class="st">'*'</span> <span class="fl">0.05</span> <span class="st">'.'</span> <span class="fl">0.1</span> <span class="st">' '</span> <span class="dv">1</span></span>
<span id="cb156-19"><a href="chapter-regression.html#cb156-19" tabindex="-1"></a>   </span>
<span id="cb156-20"><a href="chapter-regression.html#cb156-20" tabindex="-1"></a>   Residual standard error<span class="sc">:</span> <span class="fl">12.19</span> on <span class="dv">411</span> degrees of freedom</span>
<span id="cb156-21"><a href="chapter-regression.html#cb156-21" tabindex="-1"></a>   Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.2015</span>, Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.1977</span> </span>
<span id="cb156-22"><a href="chapter-regression.html#cb156-22" tabindex="-1"></a>   F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">51.87</span> on <span class="dv">2</span> and <span class="dv">411</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="er">&lt;</span> <span class="fl">2.2e-16</span></span></code></pre></div>
<p>The quadratic regression model achieves a significantly higher <strong>R-squared (<span class="math inline">\(R^2\)</span>)</strong> value of 0.2, compared to the simple linear model. This indicates that the quadratic model explains more variability in the data. Additionally, the <strong>Residual Standard Error (RSE)</strong> is lower, reflecting smaller prediction errors.</p>
<p>In this example, polynomial regression outperforms simple linear regression, as evidenced by higher <span class="math inline">\(R^2\)</span> and lower RSE. By incorporating non-linear terms, polynomial regression provides greater flexibility in modeling complex relationships. However, care must be taken to avoid overfitting when adding higher-degree terms.</p>
<p>Polynomial regression offers a straightforward way to extend linear models to non-linear relationships. For more advanced techniques, such as splines and generalized additive models, refer to Chapter 7 of <a href="https://www.statlearning.com">An Introduction to Statistical Learning with Applications in R</a>.</p>
</div>
</div>
</div>
<div id="diagnosing-and-validating-regression-models" class="section level2" number="10.8">
<h2>
<span class="header-section-number">10.8</span> Diagnosing and Validating Regression Models<a class="anchor" aria-label="anchor" href="#diagnosing-and-validating-regression-models"><i class="fas fa-link"></i></a>
</h2>
<p>Before deploying a regression model, it is essential to validate its assumptions. Ignoring these assumptions is akin to building a house on an unstable foundation: predictions from a poorly validated model can lead to erroneous, overoptimistic, or misleading results with costly consequences. Model diagnostics ensure that the model is robust, reliable, and appropriate for making predictions.</p>
<p>Linear regression models are built on the following key assumptions:</p>
<ul>
<li>
<strong>Independence (Random Sampling):</strong> The observations are independent of each other, meaning the response for one observation does not depend on the response of another.<br>
</li>
<li>
<strong>Linearity:</strong> The relationship between the predictor(s) and the response variable is linear. A scatter plot of the predictor(s) against the response can help verify this assumption.<br>
</li>
<li>
<strong>Normality:</strong> The residuals (errors) of the model follow a normal distribution. This can be assessed visually using a Q-Q plot.<br>
</li>
<li>
<strong>Constant Variance (Homoscedasticity):</strong> The residuals have a constant variance at every level of the predictor(s). A residuals vs. fitted values plot is used to check this assumption.</li>
</ul>
<p>Violations of these assumptions may compromise the validity of the model, leading to inaccurate estimates, unreliable predictions, and invalid statistical inferences.</p>
<div id="example-diagnosing-the-regression-model-for-the-marketing-dataset" class="section level3" number="10.8.1">
<h3>
<span class="header-section-number">10.8.1</span> Example: Diagnosing the Regression Model for the <em>Marketing</em> Dataset<a class="anchor" aria-label="anchor" href="#example-diagnosing-the-regression-model-for-the-marketing-dataset"><i class="fas fa-link"></i></a>
</h3>
<p>To demonstrate model diagnostics, we will evaluate the assumptions of the multiple regression model created in Example <a href="chapter-regression.html#exm:ex-stepwise-regression">10.1</a> using the <em>marketing</em> dataset. The fitted model predicts daily revenue (<code>revenue</code>) based on <code>clicks</code> and <code>display</code>.</p>
<p>We generate diagnostic plots for the model as follows:</p>
<div class="sourceCode" id="cb157"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ml_stepwise</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">revenue</span> <span class="op">~</span> <span class="va">clicks</span> <span class="op">+</span> <span class="va">display</span>, data <span class="op">=</span> <span class="va">marketing</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ml_stepwise</span><span class="op">)</span>  </span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:model-diagnostics"></span>
<img src="regression_files/figure-html/model-diagnostics-1.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><img src="regression_files/figure-html/model-diagnostics-2.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><img src="regression_files/figure-html/model-diagnostics-3.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><img src="regression_files/figure-html/model-diagnostics-4.png" alt="Diagnostic plots for assessing regression model assumptions." width="50%"><p class="caption">
Figure 10.3: Diagnostic plots for assessing regression model assumptions.
</p>
</div>
<p>The diagnostic plots help us assess each of the four key assumptions:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Normality of Residuals:</strong> The <strong>Normal Q-Q plot</strong> (upper-right) shows how closely the residuals align with a theoretical normal distribution. If the points fall along a straight line, the residuals are approximately normally distributed. In this example, the majority of points lie close to the line, indicating that the <em>normality assumption</em> is satisfied.</p></li>
<li>
<p><strong>Linearity and Constant Variance (Homoscedasticity): </strong>The <strong>Residuals vs. Fitted plot</strong> (upper-left) helps check for both linearity and homoscedasticity:</p>
<ul>
<li>If the points form a random scatter with no discernible pattern, the <em>linearity assumption</em> holds.<br>
</li>
<li>If the vertical spread of residuals is roughly uniform across all fitted values, the <em>constant variance assumption</em> is satisfied.<br>
</li>
<li>In this case, the residuals appear randomly scattered without systematic curvature or changes in variance, supporting the validity of both assumptions.</li>
</ul>
</li>
<li><p><strong>Independence:</strong> The independence assumption pertains to the dataset itself rather than diagnostic plots. For the <em>marketing</em> dataset, the revenue for one day is unlikely to depend on the revenue for another day, making the <em>independence assumption</em> reasonable for this example.</p></li>
</ol>
<p>Based on the diagnostic plots and the characteristics of the dataset:</p>
<ul>
<li>
<strong>Normality Assumption:</strong> The residuals are approximately normally distributed, as evidenced by the Normal Q-Q plot.<br>
</li>
<li>
<strong>Linearity and Constant Variance:</strong> The residuals vs. fitted values plot shows no visible patterns or heteroscedasticity, validating these assumptions.<br>
</li>
<li>
<strong>Independence:</strong> The nature of the data (daily revenue) suggests that observations are independent of one another.</li>
</ul>
<p>By validating these assumptions, we confirm that the regression model for the <em>marketing</em> dataset is appropriate for inference and prediction. Failing to check these assumptions could lead to unreliable results, emphasizing the importance of diagnostics in regression modeling.</p>
<p>Model diagnostics and validation are fundamental to ensuring the integrity of a regression model. For models that violate one or more assumptions, alternative approaches such as <strong>robust regression</strong>, <strong>non-linear regression</strong>, or <strong>transformations of variables</strong> may be considered. Additionally, validation techniques such as cross-validation or out-of-sample testing can be used to evaluate the model’s performance and generalizability to unseen data.</p>
<p>By adhering to diagnostic best practices, we ensure that the models we deploy are built on solid statistical foundations, capable of providing accurate and trustworthy predictions.</p>
</div>
</div>
<div id="exercises-5" class="section level2" number="10.9">
<h2>
<span class="header-section-number">10.9</span> <strong>Exercises</strong><a class="anchor" aria-label="anchor" href="#exercises-5"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Hands-on exercises for each section to reinforce concepts.</li>
</ul>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="chapter-bayes.html"><span class="header-section-number">9</span> Naive Bayes Classifier</a></div>
<div class="next"><a href="chapter-tree.html"><span class="header-section-number">11</span> Decision Trees and Random Forests</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-regression"><span class="header-section-number">10</span> Regression Modeling: From Basics to Advanced Techniques</a></li>
<li>
<a class="nav-link" href="#sec-simple-regression"><span class="header-section-number">10.1</span> Simple Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#fitting-a-simple-linear-regression-model">Fitting a Simple Linear Regression Model</a></li>
<li><a class="nav-link" href="#estimating-the-model-in-r">Estimating the Model in R</a></li>
<li><a class="nav-link" href="#interpreting-the-regression-line">Interpreting the Regression Line</a></li>
<li><a class="nav-link" href="#residuals-and-model-fit">Residuals and Model Fit</a></li>
<li><a class="nav-link" href="#key-insights">Key Insights</a></li>
</ul>
</li>
<li><a class="nav-link" href="#hypothesis-testing-in-simple-linear-regression"><span class="header-section-number">10.2</span> Hypothesis Testing in Simple Linear Regression</a></li>
<li><a class="nav-link" href="#measuring-the-quality-of-a-regression-model"><span class="header-section-number">10.3</span> Measuring the Quality of a Regression Model</a></li>
<li><a class="nav-link" href="#sec-multiple-regression"><span class="header-section-number">10.4</span> Multiple Linear Regression</a></li>
<li>
<a class="nav-link" href="#generalized-linear-models-glms"><span class="header-section-number">10.5</span> Generalized Linear Models (GLMs)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#logistic-regression"><span class="header-section-number">10.5.1</span> Logistic Regression</a></li>
<li><a class="nav-link" href="#logistic-regression-in-r"><span class="header-section-number">10.5.2</span> Logistic Regression in R</a></li>
<li><a class="nav-link" href="#poisson-regression"><span class="header-section-number">10.5.3</span> Poisson Regression</a></li>
<li><a class="nav-link" href="#poisson-regression-in-r"><span class="header-section-number">10.5.4</span> Poisson Regression in R</a></li>
</ul>
</li>
<li><a class="nav-link" href="#sec-stepwise-regression"><span class="header-section-number">10.6</span> Model Selection Using Stepwise Regression</a></li>
<li>
<a class="nav-link" href="#extending-linear-models-to-capture-non-linear-relationships"><span class="header-section-number">10.7</span> Extending Linear Models to Capture Non-Linear Relationships</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-need-for-non-linear-regression">The Need for Non-Linear Regression</a></li>
<li><a class="nav-link" href="#polynomial-regression"><span class="header-section-number">10.7.1</span> Polynomial Regression</a></li>
<li><a class="nav-link" href="#example-polynomial-regression-on-the-house-dataset"><span class="header-section-number">10.7.2</span> Example: Polynomial Regression on the House Dataset</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#diagnosing-and-validating-regression-models"><span class="header-section-number">10.8</span> Diagnosing and Validating Regression Models</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example-diagnosing-the-regression-model-for-the-marketing-dataset"><span class="header-section-number">10.8.1</span> Example: Diagnosing the Regression Model for the Marketing Dataset</a></li></ul>
</li>
<li><a class="nav-link" href="#exercises-5"><span class="header-section-number">10.9</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/RezaMoammadi/Book-Data-Science/blob/master/regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/RezaMoammadi/Book-Data-Science/edit/master/regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Uncovering Data Science with R</strong>" was written by Reza Mohammadi. It was last built on 2025-02-06.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
