<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chapter 9 Sensitivity and Specificity | Uncovering Data Science with R</title>

  
   
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Chapter 9 Sensitivity and Specificity | Uncovering Data Science with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://uncovering-data-science.netlify.app" />
  <meta property="og:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Sensitivity and Specificity | Uncovering Data Science with R" />
  
  
  <meta name="twitter:image" content="https://uncovering-data-science.netlify.app/images/cover.jpg" />
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.5.1/transition.js"></script>
    <script src="libs/bs3compat-0.5.1/tabs.js"></script>
    <script src="libs/bs3compat-0.5.1/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
  <style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
      <link rel="stylesheet" href="style.css" />
  
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Uncovering Data Science with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="sensitivity-and-specificity" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Sensitivity and Specificity</h1>
<p>A good classification model should be sensitive, meaning that it should identify a high proportion of the customers who are positive (have high income). <strong>Sensitivity</strong> measures the model’s ability to correctly identify positive cases. It answers the question: <em>“Out of all the actual positives, how many did the model correctly predict?”</em>. Sensitivity is defined as
<span class="math display">\[
\text{Sensitivity} = \frac{\text{Number of true positive}}{\text{Number of actual positive}} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]</span></p>
<p>For instance, from Example @ref(<a href="example:ex-confusion-matrix-kNN" class="uri">example:ex-confusion-matrix-kNN</a>), if we’re predicting churner customers, sensitivity tells us how many churner customers the model correctly identified. In this case it would be:
<span class="math display">\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{54}{54 + 7} = 0.885
\]</span>
This statistic is interpreted as follows: our classification model has correctly classified 88.5% of the actual who left the company.</p>
<p>In some fields, such as information retrieval, sensitivity is referred to as recall. Of course, a perfect classification model would have sensitivity = 1.0 = 100%. However, a null model which simply classified all customers as positive would also have sensitivity = 1.0. Clearly, it is not sufficient to identify the positive responses alone.</p>
<p>A classification model also needs to be specific, meaning that it should identify a high proportion of the customers who are negative (<code>churn = no</code>). <strong>Specificity</strong> measures the model’s ability to correctly identify negative cases. It answers the question: <em>“Out of all the actual negatives, how many did the model correctly predict?”</em> Specificity is defined as</p>
<p><span class="math display">\[
\text{Specificity} = \frac{\text{Number of true negatives}}{\text{Number of actual negatives}} = \frac{\text{TN}}{\text{TN} + \text{FP}}
\]</span>
Specificity is crucial in tasks like spam detection, where avoiding false positives (e.g., marking an important email as spam) is more important than catching every piece of spam.</p>
<p>For instance, from Example <a href="#ex:ex-confusion-matrix-kNN"><strong>??</strong></a>, we compute the specificity of our kNN classification model to be:
<span class="math display">\[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}} = \frac{83}{83 + 856} = 0.088
\]</span></p>
<p>Thus, our classification model has correctly classified 8.8% of the actual non-churner customers as not churnering the company.</p>
<p>Of course, a perfect classification model would have specificity = 1.0. But so would a model which classifies all customers as low income. A good classification model should have acceptable levels of both sensitivity and specificity, but what constitutes acceptable varies greatly from domain to domain. Our model specificity of 0.088 is higher than our model sensitivity of 0.885, which is probably okay in this instance. In the credit application domain, it may be more important to correctly identify the customers who will default rather than those who will not default, as we discuss next.</p>
<div id="precision-and-recall" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Precision and Recall</h2>
<div id="precision" class="section level3" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Precision</h3>
<p><strong>Precision</strong> measures how many of the model’s predicted positives are actually positive. It answers the question: <em>“When the model predicts positive, how often is it correct?”</em></p>
<p>The formula is:<br />
<span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]</span></p>
<p>Precision is particularly important in scenarios like fraud detection, where false positives can be costly (e.g., flagging legitimate transactions as fraudulent).</p>
</div>
<div id="recall" class="section level3" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Recall</h3>
<p><strong>Recall</strong> (same as sensitivity) measures the model’s ability to identify positive cases. It answers the question: <em>“Out of all the actual positives, how many did the model correctly predict?”</em></p>
<p>The formula is:<br />
<span class="math display">\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]</span></p>
</div>
<div id="f1-score" class="section level3" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> F1-Score</h3>
<p>The <strong>F1-Score</strong> combines precision and recall into a single metric, providing a balanced measure when there’s a trade-off between the two. It’s the harmonic mean of precision and recall:<br />
<span class="math display">\[
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]</span></p>
<p>The F1-Score is especially useful in imbalanced datasets, where accuracy alone can be misleading.</p>
</div>
</div>
<div id="roc-curve-and-auc" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> ROC Curve and AUC</h2>
<div id="what-is-an-roc-curve" class="section level3" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> What Is an ROC Curve?</h3>
<p>The <strong>Receiver Operating Characteristic (ROC) curve</strong> is a graphical representation of a classification model’s performance across different thresholds. It plots the <strong>True Positive Rate (Sensitivity)</strong> against the <strong>False Positive Rate (1 - Specificity)</strong>.</p>
<p>A perfect model has an ROC curve that hugs the top-left corner, while a random guess forms a diagonal line.</p>
</div>
<div id="area-under-the-curve-auc" class="section level3" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Area Under the Curve (AUC)</h3>
<p>The <strong>AUC</strong> summarizes the ROC curve into a single number, representing the model’s ability to rank positive cases higher than negative ones.<br />
- AUC = 1: Perfect model.<br />
- AUC = 0.5: No better than random guessing.</p>
<p>AUC is particularly valuable for imbalanced datasets, as it evaluates the model’s ranking performance rather than its absolute classification.</p>
</div>
</div>
<div id="choosing-the-right-metric-for-your-problem" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Choosing the Right Metric for Your Problem</h2>
<p>There’s no one-size-fits-all metric for model evaluation. The right choice depends on the problem’s priorities:
- Use <strong>Sensitivity</strong> if false negatives are costly (e.g., missed cancer diagnoses).<br />
- Use <strong>Specificity</strong> if false positives are costly (e.g., spam filters).<br />
- Use <strong>Precision</strong> if the cost of false positives outweighs false negatives (e.g., fraud detection).<br />
- Use <strong>Recall</strong> if catching all positives is more important (e.g., disease screening).<br />
- Use <strong>ROC Curve and AUC</strong> for a broader evaluation of ranking-based models, particularly in imbalanced datasets.</p>
</div>
<div id="summary-1" class="section level2" number="9.4">
<h2><span class="header-section-number">9.4</span> Summary</h2>
<p>In this chapter, we explored key metrics for evaluating classification models, starting with the confusion matrix and deriving metrics like sensitivity, specificity, precision, and recall. We also introduced the ROC curve and AUC for assessing probabilistic models. Each metric has its strengths and is suited to specific tasks, emphasizing the importance of aligning evaluation with the problem’s goals.</p>
<p>As we move forward, we’ll apply these metrics to evaluate different models introduced in upcoming chapters. Remember, model evaluation isn’t just about numbers—it’s about understanding the trade-offs and ensuring the model aligns with the real-world objectives of your problem.</p>

</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Uncovering Data Science with R</strong>" was written by . It was last built on 2024-12-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
