% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[a4paper]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.21,0.21,0.21}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.36,0.36,0.36}{#1}}
\newcommand{\AttributeTok}[1]{#1}
\newcommand{\BaseNTok}[1]{#1}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.39,0.39,0.39}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.36,0.36,0.36}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.36,0.36,0.36}{#1}}
\newcommand{\ConstantTok}[1]{#1}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\DataTypeTok}[1]{#1}
\newcommand{\DecValTok}[1]{#1}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.36,0.36,0.36}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.21,0.21,0.21}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{#1}
\newcommand{\FunctionTok}[1]{#1}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.36,0.36,0.36}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{#1}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.39,0.39,0.39}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.39,0.39,0.39}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.39,0.39,0.39}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.39,0.39,0.39}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.39,0.39,0.39}{#1}}
\newcommand{\VariableTok}[1]{#1}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.39,0.39,0.39}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.36,0.36,0.36}{\textbf{#1}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{bbm}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{listings}
\usepackage{xcolor}

% Define colors for code blocks (optional)
\definecolor{codegray}{gray}{0.9}

\lstset{
    basicstyle=\ttfamily\small,   % Typewriter font, smaller size
    breaklines=true,              % Enable line breaking
    breakatwhitespace=true,       % Break at spaces when possible
    backgroundcolor=\color{codegray}, % Light gray background (optional)
    frame=single,                 % Add a border around code blocks
    keepspaces=true,              % Keep spaces in code
    showspaces=false,             % Hide spaces
    showstringspaces=false        % Hide spaces in strings
}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\usepackage{hyperref}

\usepackage{float}
\floatplacement{figure}{H}
\floatplacement{table}{H}
\setlength{\textfloatsep}{0.1cm}
\usepackage{array}
\usepackage{multirow}
%\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{kotex}

\usepackage{fontspec}
\newfontfamily\koreanfont{NanumGothic} % Replace with your Korean font

% Changed via Rob Hyndman's suggestions
% https://robjhyndman.com/hyndsight/latex-floats/
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}
% Force images to appear after their definition
\usepackage{flafter}

\renewenvironment{quote}{\begin{quotation}}{\end{quotation}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

%% Need to clean up
\newenvironment{rmdblock}[1]
  {\begin{shaded*}
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
  %    {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \item
  }
  {
  \end{itemize}
  \end{shaded*}
  }

\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{learncheck}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{review}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{announcement}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

% No widow lines
% Copied from https://github.com/hadley/adv-r/blob/master/latex/preamble.tex
%\widowpenalty=10000
%\clubpenalty=10000

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\listfiles
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Data Science Foundations and Machine Learning Using R},
  pdfauthor={Reza Mohammadi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Data Science Foundations and Machine Learning Using R}
\author{\href{https://www.uva.nl/profile/a.mohammadi}{{Reza Mohammadi}}}
\date{March 12, 2025}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\chapter*{Preface}\label{preface}


Data science is transforming the way we solve problems, make decisions, and uncover insights from data. From generative AI chatbots---such as ChatGPT, DeepSeek, and Gemini---to personalized recommendations on Netflix and fraud detection in banking, data-driven techniques are revolutionizing industries. Whether you are new to data science or an experienced professional, \emph{Data Science Foundations and Machine Learning Using R} provides an accessible, hands-on introduction to this exciting field---no prior analytics or programming experience required.

This book is designed for those who want to understand data science and machine learning, as well as develop data-driven solutions for real-world applications. It provides a structured and accessible introduction to the fundamental concepts, making it suitable for beginners while also equipping readers with the skills to apply machine learning algorithms effectively. Whether you are a student, a business professional, or a researcher, this book offers a practical and intuitive approach to learning data science using \textbf{R}. Having taught these topics myself, I have found this hands-on approach to be highly effective and well-received by students.

Unlike many theoretical texts, this book emphasizes a \emph{learning-by-doing} approach. Each topic is introduced through step-by-step explanations, practical examples, and exercises---some worked out manually to build intuition, while others involve coding implementations in \textbf{R}. Readers will not only develop a solid theoretical foundation in machine learning and intelligent systems but also gain experience with widely used tools and techniques for applying these concepts in practice.

The remainder of this preface outlines the book's structure, how it can be used in different learning environments, and the foundational knowledge needed to get the most out of it.

\section*{Why This Book?}\label{why-this-book}


Data science is a rapidly evolving field that integrates machine learning, statistical techniques, and computational tools to extract meaningful insights from data. This book provides a structured introduction to data analysis and machine learning using \textbf{R}, a powerful open-source programming language widely adopted for statistical computing, visualization, and predictive modeling.

Unlike many textbooks that assume prior programming experience, this book is designed to be \textbf{intuitive and hands-on}, engaging readers through real-world applications to accelerate the learning curve. Concepts are introduced step by step and immediately applied to real datasets, reinforcing both theoretical foundations and practical skills. Readers will not only learn key data science techniques but also gain hands-on experience in solving analytical problems.

Compared to proprietary software like SAS or SPSS, \textbf{R} offers a free, flexible, and highly extensible environment for data science. With its vast ecosystem of packages, \textbf{R} is a preferred choice for data mining, machine learning, and exploratory analysis across academia, industry, and research.

\section*{Who Should Read This Book?}\label{who-should-read-this-book}


This book is designed for anyone interested in learning data science and machine learning, particularly those new to the field. It is suitable for:

\begin{itemize}
\tightlist
\item
  Business professionals seeking to leverage data for better decision-making,\\
\item
  Students and researchers applying data analysis in their studies or projects,\\
\item
  Beginners with no prior experience in programming or analytics,\\
\item
  Anyone interested in exploring data science and machine learning using \textbf{R}.
\end{itemize}

Whether used for self-study, in a classroom setting, or as a reference for professionals, this book provides a practical and structured approach to applying data science techniques in real-world scenarios.

\section*{What You Will Learn}\label{what-you-will-learn}


This book provides a structured, hands-on introduction to data science and machine learning using \textbf{R}. Through practical examples and real-world datasets, you will:

\begin{itemize}
\tightlist
\item
  \emph{Gain proficiency in R programming} -- Master the core syntax, data structures, and essential functions for data analysis.\\
\item
  \emph{Implement the data science workflow} -- Apply a structured approach to data cleaning, transformation, and exploration.\\
\item
  \emph{Uncover patterns through exploratory data analysis (EDA)} -- Use statistical and graphical techniques to identify insights.\\
\item
  \emph{Build predictive models} -- Develop machine learning models for classification, regression, and clustering.\\
\item
  \emph{Assess and refine models} -- Evaluate performance using key metrics and optimize algorithms for better accuracy.\\
\item
  \emph{Apply data science techniques in real-world scenarios} -- Work with datasets from marketing, finance, and other domains to solve practical problems.
\end{itemize}

By the end of this book, you will have a strong foundation in data science and machine learning, equipped with the skills to analyze complex datasets and develop effective data-driven solutions.

\section*{How This Book Is Structured}\label{how-this-book-is-structured}


This book is designed as a \emph{hands-on guide}, progressing from foundational concepts to more advanced machine learning techniques. Each chapter builds upon previous topics, ensuring a logical learning experience and reinforcing practical skills.

To illustrate concepts effectively, we use \emph{real-world datasets} (see Table \ref{tab:data-table}), which are available in the \textbf{liver} package. These datasets make it easy for readers to follow along with examples and exercises. Below is an overview of the book's chapters:

\begin{itemize}
\tightlist
\item
  \emph{Chapter \ref{chapter-into-R}} -- Introduction to \textbf{R}, including installation and essential functions.\\
\item
  \emph{Chapter \ref{chapter-intro-DS}} -- The foundations of data science and its methodology.\\
\item
  \emph{Chapter \ref{chapter-data-prep}} -- Techniques for cleaning and transforming data.\\
\item
  \emph{Chapter \ref{chapter-EDA}} -- Exploratory Data Analysis (EDA) using visualization and summary statistics.\\
\item
  \emph{Chapter \ref{chapter-statistics}} -- Fundamentals of statistical analysis and hypothesis testing.\\
\item
  \emph{Chapter \ref{chapter-modeling}} -- Overview of machine learning models.\\
\item
  \emph{Chapter \ref{chapter-knn}} -- The k-Nearest Neighbors (k-NN) algorithm.\\
\item
  \emph{Chapter \ref{chapter-evaluation}} -- Model evaluation techniques and performance metrics.\\
\item
  \emph{Chapter \ref{chapter-bayes}} -- The NaÃ¯ve Bayes classifier for probabilistic modeling.\\
\item
  \emph{Chapter \ref{chapter-regression}} -- Linear regression for predictive modeling.\\
\item
  \emph{Chapter \ref{chapter-tree}} -- Decision trees and ensemble methods like Random Forests.\\
\item
  \emph{Chapter \ref{chapter-nn}} -- Introduction to neural networks.\\
\item
  \emph{Chapter \ref{chapter-cluster}} -- Clustering techniques such as k-means.
\end{itemize}

Each chapter concludes with \emph{practical exercises} that reinforce key concepts and provide hands-on experience with real-world datasets. These exercises are designed to help readers apply what they've learned, deepening their understanding of data science and machine learning.

\section*{How to Use This Book}\label{how-to-use-this-book}


This book is designed for \emph{self-study, classroom instruction, and professional learning}. Readers can follow the chapters sequentially for a structured learning experience or refer to specific topics as needed.

To maximize learning and practical engagement:

\begin{itemize}
\tightlist
\item
  \emph{Run the Code Examples} -- All examples are designed for interactive execution in \textbf{R} to reinforce key concepts.\\
\item
  \emph{Complete the Exercises} -- Practical exercises at the end of each chapter strengthen understanding and problem-solving skills.\\
\item
  \emph{Modify and Experiment} -- Tweaking code and testing variations enhance comprehension and adaptability.\\
\item
  \emph{Use It as a Reference} -- Once familiar with the fundamentals, this book serves as a valuable resource for real-world applications.
\end{itemize}

This book has been successfully used in \emph{data science courses at the University of Amsterdam} and is well-suited for similar academic programs and professional training initiatives.

\section*{Datasets Used in This Book}\label{datasets-used-in-this-book}


This book incorporates real-world datasets to illustrate key data science and machine learning concepts. Table \ref{tab:data-table} provides an overview of the datasets used throughout the book, all of which are included in the \textbf{liver} package.

\begin{table}
\centering
\caption{\label{tab:data-table}List of datasets used for case studies in different chapters. Available in the R package liver.}
\centering
\begin{tabular}[t]{>{}l>{\raggedright\arraybackslash}p{20em}l}
\toprule
Name & Description & Chapter\\
\midrule
\textcolor{black}{\textbf{churn}} & Customer churn dataset. & Chapters 4, 6, 7, 8, 10\\
\textcolor{black}{\textbf{bank}} & Direct marketing data from a Portuguese bank. & Chapters 7, 12\\
\textcolor{black}{\textbf{adult}} & US Census data for income prediction. & Chapters 3, 11\\
\textcolor{black}{\textbf{risk}} & Credit risk dataset. & Chapter 9\\
\textcolor{black}{\textbf{marketing}} & Marketing campaign performance data. & Chapter 10\\
\addlinespace
\textcolor{black}{\textbf{house}} & House price prediction dataset. & Chapter 10\\
\textcolor{black}{\textbf{diamonds}} & Diamond pricing dataset. & Chapter 3\\
\textcolor{black}{\textbf{cereal}} & Nutritional information for 77 breakfast cereals. & Chapter 13\\
\bottomrule
\end{tabular}
\end{table}

These datasets were selected to provide hands-on exposure to real-world problems across domains such as marketing, finance, and predictive modeling. By working with these datasets, readers will develop the practical skills necessary to apply data science techniques effectively in professional settings.

The datasets listed in Table \ref{tab:data-table} are primarily used in case studies throughout the book. Additionally, the \textbf{liver} package contains over 15 datasets, providing readers with further opportunities for exploration and hands-on practice. While the main text focuses on a subset of these datasets, additional datasets are integrated into the exercises at the end of each chapter, allowing readers to extend their learning beyond the core examples presented in the book.

\section*{Using This Book for Teaching}\label{using-this-book-for-teaching}


This book is well-suited for \emph{introductory data science and machine learning courses} or as a \emph{supplementary resource} for analytics training. Its structured approach, hands-on exercises, and real-world case studies make it an effective learning tool for students at various levels.

To support structured learning, the book includes \emph{over 500 exercises}, categorized into three levels:

\begin{itemize}
\tightlist
\item
  \emph{Conceptual Exercises} -- Reinforce theoretical understanding and fundamental principles.\\
\item
  \emph{Applied Exercises} -- Require hands-on data analysis using real-world datasets.\\
\item
  \emph{Advanced Exercises} -- Explore complex applications and machine learning techniques, encouraging deeper engagement with the material.
\end{itemize}

Instructors using this book will have access to supplementary teaching materials, including an instructor's manual, slides, and test banks. These resources provide a comprehensive framework for delivering data science and machine learning courses, whether in academic settings or professional training programs.

\chapter{The Basics for R}\label{chapter-into-R}

Before you can analyze data, you need a way to communicate with your computer. That's where programming languages like R and Python come in. Many data science teams use a mix of languages, but R is a great starting point because it is designed specifically for data analysis and statistical computing.

\subsection*{Why Choose R for Data Science?}\label{why-choose-r-for-data-science}


R is widely used in statistics, data analysis, and visualization due to its rich ecosystem of libraries and tools tailored for data science. Unlike general-purpose programming languages, R was built for statistical analysis, allowing data scientists to perform everything from basic calculations to advanced machine learning with just a few lines of code.

While Python is another popular language for data science, R is particularly well-suited for:
- \emph{Statistical Computing} -- R has built-in statistical functions and methods for hypothesis testing, regression modeling, and machine learning.
- \emph{Data Visualization} -- Packages like \texttt{ggplot2} provide powerful tools for creating high-quality plots and graphs with minimal effort.
- \emph{Reproducible Research} -- R Markdown and Shiny make it easy to generate reports and interactive dashboards directly from R code.
- \emph{Bioinformatics \& Finance} -- Many researchers and analysts in these fields use R due to its robust statistical libraries and domain-specific packages.

Beyond its capabilities, R is:

\begin{itemize}
\tightlist
\item
  \emph{Free \& Open Source} -- Available to everyone, with a vibrant community of contributors.
\item
  \emph{Cross-Platform} -- Runs on Windows, macOS, and Linux.
\item
  \emph{Flexible \& Powerful} -- Supports interactive data exploration, visualization, and machine learning.
\end{itemize}

While R is the language, RStudio is the tool that makes working with R easier. RStudio is an integrated development environment (IDE) that provides:

\begin{itemize}
\tightlist
\item
  A console for running R commands,\\
\item
  A script editor with syntax highlighting and auto-completion,\\
\item
  Built-in tools for data visualization, debugging, and package management.
\end{itemize}

In this chapter, you will learn the fundamental skills needed to work with R, from installation to running your first commands. Let's begin! ðŸš€

\section{How to Install R}\label{how-to-install-r}

To get started with R, you first need to install it on your computer. Follow these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to the \href{https://cran.r-project.org}{CRAN website} -- The Comprehensive R Archive Network.\\
\item
  Select your operating system -- Click the link for Windows, macOS, or Linux.\\
\item
  Download and install R -- Follow the on-screen instructions to complete the installation.
\end{enumerate}

\subsection*{Keeping R Up to Date}\label{keeping-r-up-to-date}


R receives a major update once a year, along with 2-3 minor updates annually. While updating R---especially major versions---requires reinstalling your packages, staying up to date ensures you:

âœ… Access the latest features and improvements,\\
âœ… Maintain compatibility with new packages,\\
âœ… Benefit from security patches and performance enhancements.

Keeping R updated might feel like a hassle, but postponing updates can make the process more cumbersome later. It's best to update regularly to ensure smooth performance and compatibility.

\section{How to Install RStudio}\label{how-to-install-rstudio}

RStudio is an open-source integrated development environment (IDE) that makes working with R easier, more interactive, and more efficient. It provides a user-friendly interface, an advanced script editor, and various tools for plotting, debugging, and workspace management---all of which significantly enhance the R programming experience.

\subsection*{Installing RStudio}\label{installing-rstudio}


Follow these steps to install RStudio:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to the \href{http://www.rstudio.com/download}{RStudio website}.\\
\item
  Download the latest version of RStudio Desktop (the free, open-source edition).\\
\item
  Run the installer and follow the on-screen instructions.\\
\item
  Launch RStudio, and you're ready to start coding in R!
\end{enumerate}

RStudio is updated several times a year, and it will notify you when a new version is available. Keeping RStudio up to date is recommended to take advantage of new features and performance improvements.

\subsection*{Exploring the RStudio Interface}\label{exploring-the-rstudio-interface}


When you open RStudio, you will see a window similar to Figure \ref{fig:RStudio-window-1}.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/ch1_RStudio-window-1} 

}

\caption{The RStudio window when you first launch the program.}\label{fig:RStudio-window-1}
\end{figure}

If you see only three panels, add a fourth by selecting \emph{File \textgreater{} New File \textgreater{} R Script}. This opens a script editor where you can write and save R code. Here's a quick overview of RStudio's panels:

\begin{itemize}
\tightlist
\item
  Top-left: Script Editor -- Write and save your R code.\\
\item
  Bottom-left: Console -- Run R commands and see output.\\
\item
  Top-right: Environment \& History -- View variables, datasets, and past commands.\\
\item
  Bottom-right: Plots, Help, \& Files -- Display graphs, access documentation, and manage files.
\end{itemize}

For now, just know that you can type R code into the console and press Enter to run it. As you progress through the book, you'll become more familiar with RStudio's features and learn how to efficiently write, run, and debug R code.

\subsection*{Customizing RStudio}\label{customizing-rstudio}


RStudio is highly customizable, allowing you to tailor it to your workflow. To adjust settings, go to:

\begin{itemize}
\tightlist
\item
  Tools \textgreater{} Global Options -- Access general settings.\\
\item
  Appearance \textgreater{} Editor Theme -- Change the editor's theme (e.g., ``Tomorrow Night 80'' for a dark mode).\\
\item
  Font \& Layout Settings -- Modify font size, panel positions, and other interface options.\\
  A comfortable coding environment enhances productivity---so feel free to explore and tweak the settings to suit your preferences!
\end{itemize}

\section{How to Learn R}\label{how-to-learn-r}

Learning R is an exciting and rewarding journey that opens doors to data science, statistics, and machine learning. Fortunately, there are numerous resources---books, online courses, tutorials, and forums---that can help you get started and advance your skills.

\subsection*{1. Video Tutorials}\label{video-tutorials}


If you prefer learning by watching, YouTube offers a wealth of R tutorials, ranging from beginner to advanced levels:

\begin{itemize}
\tightlist
\item
  \href{https://www.youtube.com/channel/UCJ7w9dVjTOJi8Z7j0y9v6Qw}{R Programming} -- Covers R basics and data science concepts.\\
\item
  \href{https://www.youtube.com/user/dataschool}{Data School} -- Focuses on data analysis, machine learning, and practical R applications.
\end{itemize}

\subsection*{2. Books}\label{books}


Books are a great way to build a deep understanding of R. Here are some top recommendations:

\begin{itemize}
\tightlist
\item
  For Absolute Beginners: \href{https://rstudio-education.github.io/hopr/}{\emph{Hands-On Programming with R}} by Garrett Grolemund\citep{grolemund2014hands} -- A practical introduction for those new to programming.\\
\item
  For Data Science with R: \href{https://r4ds.had.co.nz}{\emph{R for Data Science}} by Hadley Wickham and Garrett Grolemund \citep{wickham2017r} -- Covers data visualization, wrangling, and modeling.\\
\item
  For Machine Learning: \href{https://www.packtpub.com/product/machine-learning-with-r/9781782162148}{\emph{Machine Learning with R}} by Brett Lantz\citep{lantz2013machine} -- A comprehensive guide to machine learning techniques using R.
\end{itemize}

\subsection*{3. Online Courses}\label{online-courses}


If you prefer structured learning with hands-on exercises, online courses offer interactive experiences:

\begin{itemize}
\tightlist
\item
  \href{https://www.datacamp.com}{DataCamp} -- Features beginner-friendly courses like \href{https://learn.datacamp.com/courses/free-introduction-to-r}{\emph{Introduction to R}}.\\
\item
  \href{https://www.coursera.org}{Coursera} -- Offers courses such as \href{https://www.coursera.org/learn/r-programming}{\emph{R Programming}} and the \href{https://www.coursera.org/specializations/jhu-data-science}{\emph{Data Science Specialization}}.
\end{itemize}

\subsection*{4. R Communities \& Forums}\label{r-communities-forums}


Engaging with online communities is a great way to learn from others, ask questions, and get support:

\begin{itemize}
\tightlist
\item
  \href{https://stackoverflow.com/questions/tagged/r}{Stack Overflow} -- Find answers to R-related coding questions.\\
\item
  \href{https://community.rstudio.com/}{RStudio Community} -- Connect with other R users and participate in discussions.
\end{itemize}

\subsection*{5. Practice Regularly}\label{practice-regularly}


The best way to learn R is through consistent practice. Start with simple exercises, explore real-world datasets, and experiment with R code. By combining structured learning with hands-on experience, you'll quickly develop confidence and proficiency in R.

ðŸš€ Start today! Choose one of the resources above and begin your R learning journey.

\section{Getting Help and Learning More}\label{getting-help-and-learning-more}

As you begin your journey with R, you'll likely encounter challenges and questions along the way. Fortunately, there are many resources available to help you troubleshoot problems, deepen your understanding, and continue learning. Whether you're stuck on an error message, exploring a new function, or looking for best practices, a combination of built-in documentation, online communities, and external learning materials can guide you.

R comes with extensive built-in documentation that provides details on functions, packages, and programming techniques. To quickly look up a function, type \texttt{?} followed by the function name in the R console. This will bring up official documentation, including usage examples, argument details, and additional references. You can also use \texttt{help()} or \texttt{example()} to get more context on how a function works.

Beyond R's internal help system, the R community is an invaluable resource. If you have a question, chances are someone has already asked (and answered) it. Platforms like Stack Overflow, RStudio Community, and the R-help mailing list contain thousands of discussions on common and advanced topics in R programming, data science, and machine learning. Searching these forums can often lead you to quick and reliable solutions. If you don't find an existing answer, posting your question with a clear explanation and a reproducible example will increase your chances of getting helpful responses.

A simple Google search is often the fastest way to troubleshoot issues. Searching for an error message or function name will usually direct you to blog posts, documentation, or forum discussions with relevant explanations. Additionally, AI tools like ChatGPT can assist with R programming questions, debugging, and conceptual explanations. While AI-generated solutions aren't always perfect, they can provide useful insights, suggest alternative approaches, and help clarify difficult concepts.

Ultimately, the best way to master R is through hands-on experience. Don't be afraid to experiment---write code, test different functions, and explore new datasets. Mistakes are a natural part of learning, and each one helps reinforce your understanding. The more you practice, the more confident and proficient you'll become in R. Keep coding, keep exploring, and enjoy the journey!

\section{Data Science with R}\label{data-science-with-r}

R provides a strong foundation for data science, but its real power comes from its extensive ecosystem of packages---collections of functions, datasets, and documentation that extend R's capabilities. While the base version of R includes many essential tools, it does not come preloaded with all the statistical and machine learning algorithms you may need. Instead, these algorithms are developed and shared by a large community of researchers and practitioners as free and open-source R packages.

A package is a modular, reusable library that enhances R's functionality. Packages include well-documented functions, usage instructions, and often sample datasets for testing and learning. In this book, we frequently use the \textbf{liver} package, which was developed specifically to accompany this book. It contains datasets and functions designed to illustrate key data science concepts and techniques. Additionally, for each machine learning algorithm covered in this book, we introduce and use the appropriate R packages that implement those methods.

For those interested in exploring further, the Comprehensive R Archive Network (CRAN) hosts thousands of packages for statistical computing, data visualization, and machine learning. The full list of available packages can be browsed on the \href{https://CRAN.R-project.org}{CRAN website}, providing access to tools tailored to various domains in data science and beyond.

\section{How to Install R Packages}\label{install-packages}

There are two ways to install R packages. The first method is through RStudio's graphical interface. Click on the ``Tools'' tab and select ``Install Packages\ldots{}''. In the dialog box that appears, enter the name of the package(s) you wish to install in the ``Packages'' field and click the ``Install'' button. Make sure to check the ``Install dependencies'' option to ensure that all necessary supporting packages are installed as well. See Figure \ref{fig:install-packages} for a visual guide.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/ch1_RStudio-window-install} 

}

\caption{A visual guide to installing R packages using the 'Tools' tab in RStudio.}\label{fig:install-packages}
\end{figure}

The second method is to install packages directly using the \texttt{install.packages()} function. For example, to install the \textbf{liver} package, which provides datasets and functions used throughout this book, enter the following command in the R console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Press ``Enter'' to execute the command. R will connect to \href{https://cran.r-project.org}{CRAN} and download the package in the correct format for your operating system. If you encounter any issues during installation, ensure you are connected to the internet and that your proxy or firewall is not blocking access to CRAN. The first time you install a package, R may ask you to select a CRAN mirror. Choose one that is geographically close to you for faster downloads.

The \texttt{install.packages()} function also allows for customization, such as installing a package from a local file or a specific repository. To learn more, type the following command in the R console:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?}\FunctionTok{install.packages}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Packages only need to be installed once. After installation, they must be loaded into each new R session using the \texttt{library()} function. We will cover how to load packages in the next section.

\section{How to Load R Packages}\label{how-to-load-r-packages}

To optimize memory usage, R does not automatically load all installed packages. Instead, you must explicitly load the necessary packages in each new R session. This ensures that only relevant functions and datasets are available, minimizing resource consumption.\\
To load a package, use the \texttt{library()} or \texttt{require()} function. These functions locate the package on your system and make its functions, datasets, and documentation accessible. For example, to load the \textbf{liver} package, enter the following command in the R console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\end{Highlighting}
\end{Shaded}

Press \emph{Enter} to execute the command. If an error message appears stating that the package is not found (e.g., \texttt{"there\ is\ no\ package\ called\ \textquotesingle{}liver\textquotesingle{}"}), it indicates that the package has not been installed. In such cases, refer to the previous section on installing packages.

Beyond \textbf{liver}, this book utilizes several other R packages, which will be introduced progressively throughout the chapters as needed. However, some R packages contain functions with identical names. For instance, both the \textbf{liver* and }dplyr** packages include a \texttt{select()} function. When multiple packages are loaded, R defaults to using the function from the most recently loaded package.

To explicitly specify which package a function should be sourced from, use the \texttt{::} operator. This ensures clarity and prevents conflicts. For example, to use the \texttt{select()} function from the \textbf{liver} package, enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liver}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This approach is particularly useful in complex projects where multiple packages are required, preventing unintended overwrites of functions with the same name.

\section{Running R Code}\label{running-r-code}

R is an interactive language, allowing you to type commands directly into the console and see the results immediately. For example, you can perform basic arithmetic operations such as addition, subtraction, multiplication, and division. To add two numbers, type the following in the R console:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \SpecialCharTok{+} \DecValTok{3}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

Press \emph{Enter} to execute the command. R will compute the sum and display the result. You can also store this result in a variable for later use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

Here, \texttt{\textless{}-} is the assignment operator in R, used to assign values to variables. Some users prefer the \texttt{=} operator (\texttt{result\ =\ 2\ +\ 3}), which also works in most cases, but \texttt{\textless{}-} remains the recommended convention in R programming.

Variables in R store values for later use, allowing you to perform calculations efficiently. For example, you can multiply \texttt{result} by 4:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\SpecialCharTok{*} \DecValTok{4}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{20}
\end{Highlighting}
\end{Shaded}

R will retrieve the stored value of \texttt{result} and compute the multiplication.

\subsection*{Using Comments in R}\label{using-comments-in-r}


Comments are used to explain your code and make it easier to understand. In R, a comment starts with \texttt{\#}, and everything following it on that line is ignored by the interpreter.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Store the sum of 2 and 3 in the variable \textasciigrave{}result\textasciigrave{}}
\NormalTok{result }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

Comments do not affect the execution of your code but are essential for documentation, especially when working on complex projects or collaborating with others.

\subsection{Functions in R}\label{functions-in-r}

R provides a rich set of built-in functions to perform specific tasks. A function takes \textbf{input(s)} (arguments), processes them, and returns an \textbf{output}. For example, the \texttt{c()} function creates vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)  }\CommentTok{\# Create a vector}
\end{Highlighting}
\end{Shaded}

You can then apply functions to this vector. For example, to compute the average of the numbers in \texttt{x}, use the \texttt{mean()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(x)  }\CommentTok{\# Calculate the mean of x}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

Functions in R follow a simple structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{function\_name}\NormalTok{(arguments)}
\end{Highlighting}
\end{Shaded}

Some functions require arguments, while others are optional. To learn more about a function, use \texttt{?} followed by the function name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?mean  }\CommentTok{\# or help(mean)}
\end{Highlighting}
\end{Shaded}

This will open R's help documentation, providing details about the function's purpose, usage, arguments, and examples.

Functions are essential in R programming, helping to simplify complex operations and making code more reusable and efficient. As you progress, you will also learn how to write your own functions to automate tasks and improve workflow.

\section{How to Import Data into R}\label{how-to-import-data-into-r}

Before performing any analysis, you first need to load data into R. R can read data from multiple sources, including text files, Excel files, and online datasets. Depending on the file format and data source, you can choose from several methods for importing data into R.

\subsection*{Using RStudio's Graphical Interface}\label{using-rstudios-graphical-interface}


The easiest way to import data into R is through RStudio's graphical interface. Click on the \emph{Import Dataset} button in the top-right panel of RStudio (see Figure \ref{fig:load-data} for a visual guide). This will open a dialog box where you can choose the file type:\\
- \textbf{From Text (base)} -- for CSV or tab-delimited files.\\
- \textbf{From Excel} -- for Microsoft Excel files.\\
- Other formats are available, depending on installed packages.

After selecting your file, RStudio will display an import settings window (see Figure \ref{fig:load-data-2}). Here, you can adjust column names, data types, and other options. If the first row contains column names, select \emph{Yes} under the \emph{Heading} option. Click \emph{Import}, and the dataset will appear in RStudio's Environment panel, ready for analysis.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/ch1_RStudio-window-data-1} 

}

\caption{A visual guide to loading a dataset into R using the 'Import Dataset' tab in RStudio.}\label{fig:load-data}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/ch1_RStudio-window-data} 

}

\caption{A visual guide to customizing the import settings when loading a dataset into R using the 'Import Dataset' tab in RStudio.}\label{fig:load-data-2}
\end{figure}

\subsection*{\texorpdfstring{Using \texttt{read.csv()}}{Using read.csv()}}\label{using-read.csv}


You can also import data directly using the \texttt{read.csv()} function, which reads tabular data (such as CSV files) into R as a data frame. If your data file is stored locally, you can load it as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"path/to/your/file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Replace \texttt{"path/to/your/file.csv"} with the actual file path. If your file does not contain column names, use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"path/to/your/file.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection*{Setting the Working Directory}\label{setting-the-working-directory}


By default, R looks for files in the current working directory. If your data is located elsewhere, you can specify the full path in \texttt{read.csv()} or set the working directory.

To check your current working directory:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

To set a new working directory:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setwd}\NormalTok{(}\StringTok{"\textasciitilde{}/Documents"}\NormalTok{)  }\CommentTok{\# Adjust the path based on your system}
\end{Highlighting}
\end{Shaded}

Alternatively, in RStudio, go to \emph{Session \textgreater{} Set Working Directory \textgreater{} Choose Directory\ldots{}} and select the desired folder.

\subsection*{\texorpdfstring{Using \texttt{file.choose()} with \texttt{read.csv()}}{Using file.choose() with read.csv()}}\label{using-file.choose-with-read.csv}


To interactively select a file instead of typing its path manually, use \texttt{file.choose()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\FunctionTok{file.choose}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

This will open a file selection dialog, making it a convenient option when working with multiple datasets.

\subsection*{Loading Data from Online Sources}\label{loading-data-from-online-sources}


R also allows direct import of datasets from web sources. For example, to load a publicly available COVID-19 dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corona\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://opendata.ecdc.europa.eu/covid19/casedistribution/csv"}\NormalTok{, }\AttributeTok{na.strings =} \StringTok{""}\NormalTok{, }\AttributeTok{fileEncoding =} \StringTok{"UTF{-}8{-}BOM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This approach is useful for accessing open datasets from research institutions or government agencies.

\subsection*{\texorpdfstring{Using \texttt{read\_excel()} for Excel Files}{Using read\_excel() for Excel Files}}\label{using-read_excel-for-excel-files}


To import Excel files, use the \texttt{read\_excel()} function from the \textbf{readxl} package. First, install and load the package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"readxl"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

Then, import the Excel file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"path/to/your/file.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Unlike \texttt{read.csv()}, \texttt{read\_excel()} supports multiple sheets within an Excel file, which can be specified using the \texttt{sheet} argument.\\
\#\#\# Loading Data from R Packages \{-\}

Some datasets are available directly in R packages and do not require importing from an external file. For example, the \textbf{liver} package, developed for this book, contains multiple datasets. To access the \emph{churn} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{data}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

Since many of the datasets used in this book are included in the \textbf{liver} package (see Table \ref{tab:data-table}), we will frequently use this package for examples and demonstrations.

This section is well-structured and clearly explains the fundamental data types in R. It is concise and informative, making it accessible to beginners while maintaining a professional tone suitable for a Springer publication. Below are some minor refinements to improve clarity, consistency, and readability.

\section{Data Types in R}\label{data-types-in-r}

Data in R can take various forms, and correctly identifying these types is essential for effective data manipulation, visualization, and analysis. Each data type has specific properties that determine how R processes it, so understanding them helps avoid errors and ensures accurate results.

Here are the most common data types in R:

\begin{itemize}
\tightlist
\item
  \textbf{Numeric}: Represents real numbers, such as \texttt{3.14} or \texttt{-5.67}. This type is used for continuous numerical values, like heights, weights, or temperatures.\\
\item
  \textbf{Integer}: Represents whole numbers without decimals, such as \texttt{1}, \texttt{42}, or \texttt{-10}. This type is useful for count-based data, such as the number of customers or items sold.\\
\item
  \textbf{Character}: Represents text or string data, such as \texttt{"Data\ Science"} or \texttt{"R\ Programming"}. This type is commonly used for categorical labels, names, and descriptive values.\\
\item
  \textbf{Logical}: Represents Boolean values: \texttt{TRUE} or \texttt{FALSE}. Logical data is often used in conditional statements and filtering operations.\\
\item
  \textbf{Factor}: Represents categorical data with predefined levels. Factors are commonly used for storing variables such as \texttt{"Male"} or \texttt{"Female"} in a dataset and are particularly useful in statistical modeling.
\end{itemize}

To check the data type of a variable, use the \texttt{class()} function. For example, to determine the type of the variable \texttt{result}, type:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(result)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"numeric"}
\end{Highlighting}
\end{Shaded}

Press \emph{Enter}, and R will display the variable's data type.

Recognizing different data types is essential for choosing the right analytical and visualization techniques. As we will explore in later chapters (e.g., Chapters \ref{chapter-EDA} and \ref{chapter-statistics}), numerical and categorical variables require different approaches when performing descriptive statistics, hypothesis testing, and data visualization.

\section{Data Structures in R}\label{data-structures-in-r}

Data structures are fundamental to working with data in R. They define how data is stored and manipulated, which directly impacts the efficiency and accuracy of data analysis. The most commonly used data structures in R are vectors, matrices, data frames, and lists, as illustrated in Figure \ref{fig:load-data-2}.

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth]{images/ch1_R-objects} 

}

\caption{A visual guide to different types of data structures in R.}\label{fig:R-objects}
\end{figure}

\subsection*{Vectors in R}\label{vectors-in-r}


A vector is the simplest data structure in R. It is a one-dimensional array that holds elements of the same type (numeric, character, or logical). Vectors are the building blocks of other data structures. You can create a vector using the \texttt{c()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a numeric vector}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}

\CommentTok{\# Display the vector}
\NormalTok{x}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{0} \SpecialCharTok{{-}}\DecValTok{3}  \DecValTok{5}

\CommentTok{\# Check if x is a vector}
\FunctionTok{is.vector}\NormalTok{(x)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\CommentTok{\# Check the length of the vector}
\FunctionTok{length}\NormalTok{(x)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

Here, \texttt{x} is a numeric vector containing five elements. The \texttt{is.vector()} function confirms that \texttt{x} is indeed a vector, while \texttt{length(x)} returns the number of elements in the vector.

\subsection*{Matrices in R}\label{matrices-in-r}


A matrix is a two-dimensional array where all elements must be of the same type. Matrices are useful for mathematical operations and structured numerical data. You can create a matrix using the \texttt{matrix()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a matrix with 2 rows and 3 columns}
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Display the matrix}
\NormalTok{m}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}

\CommentTok{\# Check if m is a matrix}
\FunctionTok{is.matrix}\NormalTok{(m)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\ConstantTok{TRUE}

\CommentTok{\# Check the dimensions of the matrix}
\FunctionTok{dim}\NormalTok{(m)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{2} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

This matrix \texttt{m} consists of two rows and three columns, filled row-wise. The \texttt{dim()} function returns the dimensions of the matrix. To fill the matrix column-wise, set \texttt{byrow\ =\ FALSE}.

\subsection*{Data Frames in R}\label{data-frames-in-r}


A data frame is a two-dimensional table where each column can contain a different data type (numeric, character, or logical). This makes data frames ideal for storing tabular data, similar to spreadsheets. You can create a data frame using the \texttt{data.frame()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create vectors for student data}
\NormalTok{student\_id }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\NormalTok{, }\DecValTok{102}\NormalTok{, }\DecValTok{103}\NormalTok{, }\DecValTok{104}\NormalTok{)}
\NormalTok{name       }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Emma"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Alice"}\NormalTok{, }\StringTok{"Noah"}\NormalTok{)}
\NormalTok{age        }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{22}\NormalTok{)}
\NormalTok{grade      }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{)}

\CommentTok{\# Create a data frame from the vectors}
\NormalTok{students\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(student\_id, name, age, grade)}

\CommentTok{\# Display the data frame}
\NormalTok{students\_df}
\NormalTok{     student\_id  name age grade}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{20}\NormalTok{     A}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Bob  }\DecValTok{21}\NormalTok{     B}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Alice  }\DecValTok{19}\NormalTok{     A}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Noah  }\DecValTok{22}\NormalTok{     C}
\end{Highlighting}
\end{Shaded}

This data frame \texttt{students\_df} consists of four columns: \texttt{student\_id}, \texttt{name}, \texttt{age}, and \texttt{grade}. The \texttt{class()} function confirms that an object is a data frame, while \texttt{is.data.frame()} checks its structure.

To inspect the first few rows of a data frame, use the \texttt{head()} function. For example, to display the first six rows of the \emph{churn} dataset from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)  }\CommentTok{\# Load the liver package}
\FunctionTok{data}\NormalTok{(churn)     }\CommentTok{\# Load the churn dataset}

\CommentTok{\# Check the structure of the dataset}
\FunctionTok{str}\NormalTok{(churn)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{5000}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{17} \DecValTok{36} \DecValTok{32} \DecValTok{36} \DecValTok{37} \DecValTok{2} \DecValTok{20} \DecValTok{25} \DecValTok{19} \DecValTok{50}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area.code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account.length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{128} \DecValTok{107} \DecValTok{137} \DecValTok{84} \DecValTok{75} \DecValTok{118} \DecValTok{121} \DecValTok{147} \DecValTok{117} \DecValTok{141}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.plan    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{26} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{24} \DecValTok{0} \DecValTok{0} \DecValTok{37}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.plan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.mins     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \FloatTok{13.7} \FloatTok{12.2} \FloatTok{6.6} \FloatTok{10.1} \FloatTok{6.3} \FloatTok{7.5} \FloatTok{7.1} \FloatTok{8.7} \FloatTok{11.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{5} \DecValTok{7} \DecValTok{3} \DecValTok{6} \DecValTok{7} \DecValTok{6} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{2.7} \FloatTok{3.7} \FloatTok{3.29} \FloatTok{1.78} \FloatTok{2.73} \FloatTok{1.7} \FloatTok{2.03} \FloatTok{1.92} \FloatTok{2.35} \FloatTok{3.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{265} \DecValTok{162} \DecValTok{243} \DecValTok{299} \DecValTok{167}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{110} \DecValTok{123} \DecValTok{114} \DecValTok{71} \DecValTok{113} \DecValTok{98} \DecValTok{88} \DecValTok{79} \DecValTok{97} \DecValTok{84}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{45.1} \FloatTok{27.5} \FloatTok{41.4} \FloatTok{50.9} \FloatTok{28.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.mins      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{197.4} \FloatTok{195.5} \FloatTok{121.2} \FloatTok{61.9} \FloatTok{148.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{99} \DecValTok{103} \DecValTok{110} \DecValTok{88} \DecValTok{122} \DecValTok{101} \DecValTok{108} \DecValTok{94} \DecValTok{80} \DecValTok{111}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{16.78} \FloatTok{16.62} \FloatTok{10.3} \FloatTok{5.26} \FloatTok{12.61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.mins    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{245} \DecValTok{254} \DecValTok{163} \DecValTok{197} \DecValTok{187}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{91} \DecValTok{103} \DecValTok{104} \DecValTok{89} \DecValTok{121} \DecValTok{118} \DecValTok{118} \DecValTok{96} \DecValTok{90} \DecValTok{97}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{11.01} \FloatTok{11.45} \FloatTok{7.32} \FloatTok{8.86} \FloatTok{8.41}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer.calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{0} \DecValTok{3} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}

\CommentTok{\# Display the first six rows}
\FunctionTok{head}\NormalTok{(churn)}
\NormalTok{     state     area.code account.length voice.plan voice.messages intl.plan}
   \DecValTok{1}\NormalTok{    KS area\_code\_415            }\DecValTok{128}\NormalTok{        yes             }\DecValTok{25}\NormalTok{        no}
   \DecValTok{2}\NormalTok{    OH area\_code\_415            }\DecValTok{107}\NormalTok{        yes             }\DecValTok{26}\NormalTok{        no}
   \DecValTok{3}\NormalTok{    NJ area\_code\_415            }\DecValTok{137}\NormalTok{         no              }\DecValTok{0}\NormalTok{        no}
   \DecValTok{4}\NormalTok{    OH area\_code\_408             }\DecValTok{84}\NormalTok{         no              }\DecValTok{0}\NormalTok{       yes}
   \DecValTok{5}\NormalTok{    OK area\_code\_415             }\DecValTok{75}\NormalTok{         no              }\DecValTok{0}\NormalTok{       yes}
   \DecValTok{6}\NormalTok{    AL area\_code\_510            }\DecValTok{118}\NormalTok{         no              }\DecValTok{0}\NormalTok{       yes}
\NormalTok{     intl.mins intl.calls intl.charge day.mins day.calls day.charge eve.mins}
   \DecValTok{1}      \FloatTok{10.0}          \DecValTok{3}        \FloatTok{2.70}    \FloatTok{265.1}       \DecValTok{110}      \FloatTok{45.07}    \FloatTok{197.4}
   \DecValTok{2}      \FloatTok{13.7}          \DecValTok{3}        \FloatTok{3.70}    \FloatTok{161.6}       \DecValTok{123}      \FloatTok{27.47}    \FloatTok{195.5}
   \DecValTok{3}      \FloatTok{12.2}          \DecValTok{5}        \FloatTok{3.29}    \FloatTok{243.4}       \DecValTok{114}      \FloatTok{41.38}    \FloatTok{121.2}
   \DecValTok{4}       \FloatTok{6.6}          \DecValTok{7}        \FloatTok{1.78}    \FloatTok{299.4}        \DecValTok{71}      \FloatTok{50.90}     \FloatTok{61.9}
   \DecValTok{5}      \FloatTok{10.1}          \DecValTok{3}        \FloatTok{2.73}    \FloatTok{166.7}       \DecValTok{113}      \FloatTok{28.34}    \FloatTok{148.3}
   \DecValTok{6}       \FloatTok{6.3}          \DecValTok{6}        \FloatTok{1.70}    \FloatTok{223.4}        \DecValTok{98}      \FloatTok{37.98}    \FloatTok{220.6}
\NormalTok{     eve.calls eve.charge night.mins night.calls night.charge customer.calls churn}
   \DecValTok{1}        \DecValTok{99}      \FloatTok{16.78}      \FloatTok{244.7}          \DecValTok{91}        \FloatTok{11.01}              \DecValTok{1}\NormalTok{    no}
   \DecValTok{2}       \DecValTok{103}      \FloatTok{16.62}      \FloatTok{254.4}         \DecValTok{103}        \FloatTok{11.45}              \DecValTok{1}\NormalTok{    no}
   \DecValTok{3}       \DecValTok{110}      \FloatTok{10.30}      \FloatTok{162.6}         \DecValTok{104}         \FloatTok{7.32}              \DecValTok{0}\NormalTok{    no}
   \DecValTok{4}        \DecValTok{88}       \FloatTok{5.26}      \FloatTok{196.9}          \DecValTok{89}         \FloatTok{8.86}              \DecValTok{2}\NormalTok{    no}
   \DecValTok{5}       \DecValTok{122}      \FloatTok{12.61}      \FloatTok{186.9}         \DecValTok{121}         \FloatTok{8.41}              \DecValTok{3}\NormalTok{    no}
   \DecValTok{6}       \DecValTok{101}      \FloatTok{18.75}      \FloatTok{203.9}         \DecValTok{118}         \FloatTok{9.18}              \DecValTok{0}\NormalTok{    no}
\end{Highlighting}
\end{Shaded}

This code loads the \textbf{liver} package, retrieves the \emph{churn} dataset, and provides an overview of its structure. The \texttt{str()} function is particularly useful for summarizing data frames, as it displays data types and column values.

\subsection*{Lists in R}\label{lists-in-r}


A list is a flexible data structure that can contain elements of different types, including vectors, matrices, data frames, or even other lists. Lists are useful for storing complex objects in a structured way. You can create a list using the \texttt{list()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a list containing a vector, matrix, and data frame}
\NormalTok{my\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{vector =}\NormalTok{ x, }\AttributeTok{matrix =}\NormalTok{ m, }\AttributeTok{data\_frame =}\NormalTok{ students\_df)}

\CommentTok{\# Display the list}
\NormalTok{my\_list}
   \SpecialCharTok{$}\NormalTok{vector}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{0} \SpecialCharTok{{-}}\DecValTok{3}  \DecValTok{5}
   
   \SpecialCharTok{$}\NormalTok{matrix}
\NormalTok{        [,}\DecValTok{1}\NormalTok{] [,}\DecValTok{2}\NormalTok{] [,}\DecValTok{3}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]    }\DecValTok{1}    \DecValTok{2}    \DecValTok{3}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]    }\DecValTok{4}    \DecValTok{5}    \DecValTok{6}
   
   \SpecialCharTok{$}\NormalTok{data\_frame}
\NormalTok{     student\_id  name age grade}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{20}\NormalTok{     A}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Bob  }\DecValTok{21}\NormalTok{     B}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Alice  }\DecValTok{19}\NormalTok{     A}
   \DecValTok{4}        \DecValTok{104}\NormalTok{  Noah  }\DecValTok{22}\NormalTok{     C}
\end{Highlighting}
\end{Shaded}

This list \texttt{my\_list} stores a vector, a matrix, and a data frame within a single object. Lists allow for efficient organization of heterogeneous data. To explore the structure of a list, use the \texttt{str()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(my\_list)}
\NormalTok{   List of }\DecValTok{3}
    \SpecialCharTok{$}\NormalTok{ vector    }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }\DecValTok{1} \DecValTok{2} \DecValTok{0} \SpecialCharTok{{-}}\DecValTok{3} \DecValTok{5}
    \SpecialCharTok{$}\NormalTok{ matrix    }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{5} \DecValTok{3} \DecValTok{6}
    \SpecialCharTok{$}\NormalTok{ data\_frame}\SpecialCharTok{:}\StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}  \DecValTok{4}\NormalTok{ obs. of  }\DecValTok{4}\NormalTok{ variables}\SpecialCharTok{:}
\NormalTok{     ..}\SpecialCharTok{$}\NormalTok{ student\_id}\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\DecValTok{101} \DecValTok{102} \DecValTok{103} \DecValTok{104}
\NormalTok{     ..}\SpecialCharTok{$}\NormalTok{ name      }\SpecialCharTok{:}\NormalTok{ chr [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\StringTok{"Emma"} \StringTok{"Bob"} \StringTok{"Alice"} \StringTok{"Noah"}
\NormalTok{     ..}\SpecialCharTok{$}\NormalTok{ age       }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\DecValTok{20} \DecValTok{21} \DecValTok{19} \DecValTok{22}
\NormalTok{     ..}\SpecialCharTok{$}\NormalTok{ grade     }\SpecialCharTok{:}\NormalTok{ chr [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\StringTok{"A"} \StringTok{"B"} \StringTok{"A"} \StringTok{"C"}
\end{Highlighting}
\end{Shaded}

Lists are powerful tools in R, especially for handling nested or hierarchical data. For further exploration, use \texttt{?list} to access the documentation and additional examples.

\section{Accessing Records or Variables in R}\label{accessing-records-or-variables-in-r}

Once you've imported data into R, you can easily access specific records or variables using the \texttt{\$} and \texttt{{[}{]}} operators. These tools are essential for extracting data from data frames and lists.

The \texttt{\$} operator allows you to extract a specific column from a data frame or a specific element from a list. For example, to access the \texttt{name} column in the \texttt{students\_df} data frame, you would use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{students\_df}\SpecialCharTok{$}\NormalTok{name}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{"Emma"}  \StringTok{"Bob"}   \StringTok{"Alice"} \StringTok{"Noah"}
\end{Highlighting}
\end{Shaded}

This command retrieves and displays the \texttt{name} column from the \texttt{students\_df} data frame.

Similarly, you can use the \texttt{\$} operator to access elements within a list. For example, to access the \texttt{vector} element in the \texttt{my\_list} list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list}\SpecialCharTok{$}\NormalTok{vector}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{0} \SpecialCharTok{{-}}\DecValTok{3}  \DecValTok{5}
\end{Highlighting}
\end{Shaded}

This command retrieves and displays the \texttt{vector} element from the \texttt{my\_list} list. The \texttt{\$} operator is a straightforward and powerful way to access specific variables or elements within data frames and lists.

Another method for accessing specific records or variables is through the \texttt{{[}{]}} operator, which allows you to subset data frames, matrices, and lists based on specific conditions. For example, to extract the first three rows of the \texttt{students\_df} data frame, you can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{students\_df[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, ]}
\NormalTok{     student\_id  name age grade}
   \DecValTok{1}        \DecValTok{101}\NormalTok{  Emma  }\DecValTok{20}\NormalTok{     A}
   \DecValTok{2}        \DecValTok{102}\NormalTok{   Bob  }\DecValTok{21}\NormalTok{     B}
   \DecValTok{3}        \DecValTok{103}\NormalTok{ Alice  }\DecValTok{19}\NormalTok{     A}
\end{Highlighting}
\end{Shaded}

This command will display the first three rows of the \texttt{students\_df} data frame.

You can also use the \texttt{{[}{]}} operator to extract specific columns. For instance, to select the \texttt{name} and \texttt{grade} columns from the \texttt{students\_df} data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{students\_df[, }\FunctionTok{c}\NormalTok{(}\StringTok{"name"}\NormalTok{, }\StringTok{"grade"}\NormalTok{)]}
\NormalTok{      name grade}
   \DecValTok{1}\NormalTok{  Emma     A}
   \DecValTok{2}\NormalTok{   Bob     B}
   \DecValTok{3}\NormalTok{ Alice     A}
   \DecValTok{4}\NormalTok{  Noah     C}
\end{Highlighting}
\end{Shaded}

This command retrieves and displays only the \texttt{name} and \texttt{grade} columns from the \texttt{students\_df} data frame.

The \texttt{{[}{]}} operator is versatile, enabling you to subset data frames, matrices, and lists with precision. Both the \texttt{\$} and \texttt{{[}{]}} operators are fundamental tools for data manipulation in R, allowing you to efficiently access and manage the data you need.

\section{Visualizing Data in R}\label{visualizing-data-in-r}

Data visualization is a powerful tool for exploring and communicating insights from data. It plays a crucial role in exploratory data analysis (EDA), which we will delve into in Chapter \ref{chapter-EDA}. As the saying goes, ``a picture is worth a thousand words,'' and in data science, this is especially true. R provides a broad array of tools for creating high-quality plots and visualizations, allowing you to effectively present your findings.

In R, there are two primary ways to create plots: using base R graphics and using the \textbf{ggplot2} package. Base R graphics offer a simple and direct way to generate plots, while \textbf{ggplot2} provides greater flexibility and customization. This book primarily uses \textbf{ggplot2}, as it follows a structured approach based on the \emph{grammar of graphics}, which breaks down plots into three key components:

\begin{itemize}
\tightlist
\item
  Data: The dataset to be visualized, which should be in a data frame format when using \textbf{ggplot2}.\\
\item
  Aesthetics: The visual properties of the data points, such as color, shape, and size.\\
\item
  Geometries: The type of plot to be created, such as scatter plots, bar plots, or line plots.
\end{itemize}

To create a plot using \textbf{ggplot2}, first install and load the package. Instructions for installing packages are provided in Section \ref{install-packages}. To load \textbf{ggplot2}, use the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

Next, define the data, aesthetics, and geometries for your plot. For example, to create a scatter plot of miles per gallon (\texttt{mpg}) versus horsepower (\texttt{hp}) using the built-in \emph{mtcars} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-32-1} \end{center}

This code initializes the plot with the \texttt{ggplot()} function, specifying the dataset (\texttt{mtcars}). The \texttt{geom\_point()} function adds points to the plot, and the \texttt{aes()} function maps \texttt{mpg} to the x-axis and \texttt{hp} to the y-axis.

The general template for creating plots with \textbf{ggplot2} follows this structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \SpecialCharTok{\textless{}}\NormalTok{DATA}\SpecialCharTok{\textgreater{}}\NormalTok{) }\SpecialCharTok{+}
  \ErrorTok{\textless{}}\NormalTok{GEOM\_FUNCTION}\SpecialCharTok{\textgreater{}}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\SpecialCharTok{\textless{}}\NormalTok{MAPPINGS}\SpecialCharTok{\textgreater{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Using this template, a variety of visualizations can be created.

\subsection*{Geom Functions in ggplot2}\label{geom-functions-in-ggplot2}


Geom functions determine the type of plot created in \textbf{ggplot2}. Some commonly used geom functions include:

\begin{itemize}
\tightlist
\item
  \texttt{geom\_point()} for scatter plots\\
\item
  \texttt{geom\_bar()} for bar plots\\
\item
  \texttt{geom\_line()} for line plots\\
\item
  \texttt{geom\_boxplot()} for box plots\\
\item
  \texttt{geom\_histogram()} for histograms\\
\item
  \texttt{geom\_density()} for density plots\\
\item
  \texttt{geom\_smooth()} for adding smoothed conditional means to plots
\end{itemize}

For example, to create a smoothed line plot of \texttt{mpg} versus \texttt{hp}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-34-1} \end{center}

Multiple geom functions can be combined in a single plot. To overlay a scatter plot on the smoothed line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-35-1} \end{center}

Alternatively, the \texttt{aes()} function can be placed inside \texttt{ggplot()} to streamline the code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Additional visualization examples can be found in Chapter \ref{chapter-EDA}. For a complete list of geom functions, refer to the \href{https://ggplot2.tidyverse.org}{\textbf{ggplot2} documentation}.

\subsection*{Aesthetics in ggplot2}\label{aesthetics-in-ggplot2}


Aesthetics control the visual properties of data points, such as color, size, and shape. These properties are specified within the \texttt{aes()} function. For example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{color =}\NormalTok{ cyl))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-37-1} \end{center}

Here, \texttt{color\ =\ cyl} maps the color of the points to the number of cylinders (\texttt{cyl}) in the \textbf{mtcars} dataset. \textbf{ggplot2} automatically assigns a unique color to each category and adds a corresponding legend.

In addition to \texttt{color}, other aesthetics such as \texttt{size} and \texttt{alpha} (transparency) can be used:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Left plot: using the size aesthetic}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{size =}\NormalTok{ cyl))}

\CommentTok{\# Right plot: using the alpha aesthetic}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp, }\AttributeTok{alpha =}\NormalTok{ cyl))}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-38-1} \includegraphics[width=0.5\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-38-2}

Aesthetics can also be set directly inside geom functions. For example, to make all points blue triangles of size 3:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mtcars) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ hp), }
             \AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{shape =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-39-1} \end{center}

This section introduced the fundamentals of data visualization in R using \textbf{ggplot2}. The next chapters will explore how visualization plays a crucial role in exploratory data analysis (Chapter \ref{chapter-EDA}) and how to refine plots for communication and reporting. For more details on visualization techniques, see the \href{https://ggplot2.tidyverse.org}{\textbf{ggplot2} documentation}. For interactive graphics, consider exploring the \textbf{plotly} package or \textbf{Shiny} for web applications.

\section{Formula in R}\label{sec-formula-in-R}

Formulas in R provide a concise and intuitive way to specify relationships between variables for statistical modeling. They are widely used in functions for regression, classification, and machine learning to define how a response variable depends on one or more predictors.

In R, formulas use the tilde symbol \texttt{\textasciitilde{}} to express relationships between variables, where the response variable appears on the left-hand side and predictor variables on the right-hand side. For example, the formula \texttt{y\ \textasciitilde{}\ x} specifies that \texttt{y} is modeled as a function of \texttt{x}. When there are multiple predictors, they are separated by \texttt{+}.\\
For instance, using the \texttt{diamonds} dataset, the formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ carat }\SpecialCharTok{+}\NormalTok{ cut }\SpecialCharTok{+}\NormalTok{ color}
\end{Highlighting}
\end{Shaded}

models the \texttt{price} of a diamond based on its \texttt{carat}, \texttt{cut}, and \texttt{color}.

To include all other variables in the dataset as predictors, we can use the shorthand notation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}
\end{Highlighting}
\end{Shaded}

This approach is particularly useful in large datasets where listing all predictors manually would be impractical.

A formula in R acts as a \textbf{quoting operator}, instructing R to interpret the variables symbolically rather than evaluating them immediately. The variable on the left-hand side of \texttt{\textasciitilde{}} is the \textbf{dependent variable} (or response variable), while the variables on the right-hand side are the \textbf{independent variables} (or predictor variables).

\begin{example}
\protect\hypertarget{exm:ex-formula}{}\label{exm:ex-formula}To illustrate, suppose we want to predict the \texttt{price} of a diamond using a linear regression model. We can pass the formula into the \texttt{lm()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ carat }\SpecialCharTok{+}\NormalTok{ cut }\SpecialCharTok{+}\NormalTok{ color, }\AttributeTok{data =}\NormalTok{ diamonds)}
\end{Highlighting}
\end{Shaded}

Here, the formula \texttt{price\ \textasciitilde{}\ carat\ +\ cut\ +\ color} defines the relationship, and the \texttt{data} argument specifies the dataset to use.
\end{example}

Once defined, formulas can be used in various R functions for statistical modeling and machine learning. As you progress through later chapters, you will encounter formulas in functions for regression, classification, and more (e.g., Chapters \ref{chapter-knn}, \ref{chapter-bayes}, and \ref{chapter-regression}). Mastering formula syntax will enable you to efficiently build, customize, and interpret models throughout this book.

\section{Reporting with R Markdown}\label{reporting-with-r-markdown}

Thus far, this book has covered how to interact with R and RStudio for data analysis. This section focuses on an equally important aspect: effectively communicating analytical findings. Data scientists must present results clearly to teams, stakeholders, and clients. Regardless of the depth of an analysis, its impact is limited if it is not communicated effectively. R Markdown facilitates this process by enabling the seamless integration of code, text, and output into dynamic, reproducible reports.

R Markdown allows users to write and execute R code within a document, producing reports, presentations, and dashboards. Unlike traditional notebooks or word processors, R Markdown ensures that text, code, and results remain synchronized as data changes. This book itself is entirely written using R Markdown and generated with the \href{https://bookdown.org}{\textbf{bookdown}} package, ensuring a fully reproducible and dynamic workflow.

R Markdown documents can be exported into multiple formats, including HTML, PDF, Word, and PowerPoint, making it adaptable to various audiences and reporting needs. Furthermore, it supports the creation of interactive documents using Shiny, allowing users to build web applications that facilitate exploratory data analysis.

To get started, the following resources provide useful references:

\begin{itemize}
\tightlist
\item
  \textbf{R Markdown Cheat Sheet}: The \href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{R Markdown Cheat Sheet} offers a concise reference for creating documents, including syntax, formatting, and output options. It is available in RStudio under \emph{Help \textgreater{} Cheatsheets \textgreater{} R Markdown Cheat Sheet}.\\
\item
  \textbf{R Markdown Reference Guide}: The \href{https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}{R Markdown Reference Guide} provides a detailed overview of R Markdown's features, including document structure and customization.
\end{itemize}

\subsection*{R Markdown Basics}\label{r-markdown-basics}


R Markdown follows a literate programming approach, combining text and executable code in a single document. Unlike word processors where formatting is visible during writing, R Markdown requires compilation to generate the final report. This approach ensures automation, as plots and figures are generated dynamically and inserted into the document. Since the code is embedded, analyses are fully reproducible.

To create an R Markdown document in RStudio:

File \textgreater{} New File \textgreater{} R Markdown

A dialog box will appear, allowing the selection of a document type. For a standard report, choose ``Document.'' Other options include ``Presentation'' for slides, ``Shiny'' for interactive applications, and ``From Template'' for predefined formats. After selecting the document type, enter a title and author name. The output format can be set to HTML, PDF, or Word; HTML is often recommended for debugging.

R Markdown files use the \texttt{.Rmd} extension, distinguishing them from \texttt{.R} script files. A newly created file contains a template that can be modified with custom text, code, and formatting.

\subsection*{The Header}\label{the-header}


The header defines metadata such as the document's title, author, date, and output format. It is enclosed within three dashes (\texttt{-\/-\/-}).

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{title}\KeywordTok{:}\AttributeTok{ }\StringTok{"An Analysis of Customer Churn"}
\FunctionTok{author}\KeywordTok{:}\AttributeTok{ }\StringTok{"Reza Mohammadi"}
\FunctionTok{date}\KeywordTok{:}\AttributeTok{ }\StringTok{"Aug 12, 2024"}
\FunctionTok{output}\KeywordTok{:}\AttributeTok{ html\_document}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Title}: The document's title.\\
\item
  \textbf{Author}: The name of the author.\\
\item
  \textbf{Date}: The date of creation.\\
\item
  \textbf{Output format}: The format of the final document (\texttt{html\_document}, \texttt{pdf\_document}, or \texttt{word\_document}).
\end{itemize}

Additional metadata can be included for customization, such as table of contents options and formatting preferences.

\subsection*{Code Chunks and Inline Code}\label{code-chunks-and-inline-code}


R Markdown integrates R code within documents using code chunks, which are enclosed in triple backticks (\texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}) followed by the code. For example:

\begin{Shaded}
\begin{Highlighting}[]

\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{} r}
\DecValTok{2} \SpecialCharTok{+} \DecValTok{3}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{5}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

When compiled, R executes the code and displays the output within the document. Code chunks are used for analysis, visualizations, and modeling. The ``Run'' button in RStudio allows individual execution of chunks. See Figure \ref{fig:run-chunk} for a visual guide.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth]{images/ch1_run-chunk} 

}

\caption{Executing a code chunk in R Markdown using the 'Run' button in RStudio.}\label{fig:run-chunk}
\end{figure}

Common chunk options include:

\begin{itemize}
\tightlist
\item
  \texttt{echo\ =\ FALSE}: Displays output but hides the code.\\
\item
  \texttt{eval\ =\ FALSE}: Shows the code but does not execute it.\\
\item
  \texttt{message\ =\ FALSE}: Suppresses messages.\\
\item
  \texttt{warning\ =\ FALSE}: Suppresses warnings.\\
\item
  \texttt{error\ =\ FALSE}: Hides error messages.\\
\item
  \texttt{include\ =\ FALSE}: Omits both code and output.
\end{itemize}

For inline calculations, use backticks and the \texttt{r} keyword:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{The factorial of 5 is 120.}
\end{Highlighting}
\end{Shaded}

This renders dynamically as:

\begin{verbatim}
The factorial of 5 is 120.
\end{verbatim}

\subsection*{Styling Text}\label{styling-text}


R Markdown supports various text formatting options:

\begin{itemize}
\tightlist
\item
  \textbf{Headings}: Use \texttt{\#} for section titles.\\
\item
  \textbf{Bold}: Enclose text in double asterisks (\texttt{**bold**}).\\
\item
  \textbf{Italic}: Use single asterisks (\texttt{*italic*}).\\
\item
  \textbf{Lists}: Use \texttt{*} for bullet points.\\
\item
  \textbf{Links}: \texttt{{[}R\ Markdown\ website{]}(https://rmarkdown.rstudio.com)}\\
\item
  \textbf{Images}: \texttt{!{[}Alt\ text{]}(path/to/image.png)}
\end{itemize}

For mathematical notation, use LaTeX-style equations:

\begin{Shaded}
\begin{Highlighting}[]
\AnnotationTok{Inline:}\CommentTok{ $y = \textbackslash{}beta\_0 + \textbackslash{}beta\_1 x$  }
\AnnotationTok{Block:}\CommentTok{ $$ y = \textbackslash{}beta\_0 + \textbackslash{}beta\_1 x $$}
\end{Highlighting}
\end{Shaded}

\subsection*{Mastering R Markdown}\label{mastering-r-markdown}


For further learning:

\begin{itemize}
\tightlist
\item
  \textbf{Books}: \href{https://bookdown.org/yihui/rmarkdown/}{\emph{R Markdown: The Definitive Guide}}.\\
\item
  \textbf{Tutorials}: \href{https://rmarkdown.rstudio.com/lesson-1.html}{R Markdown website}.\\
\item
  \textbf{Courses}: \href{https://www.datacamp.com/courses/reporting-with-r-markdown}{DataCamp R Markdown course}.\\
\item
  \textbf{Forums}: \href{https://community.rstudio.com/c/rmarkdown/9}{RStudio Community}.
\end{itemize}

By leveraging R Markdown, data scientists can produce high-quality, reproducible reports that enhance collaboration and communication.

\section{Exercises}\label{intro-R-exercises}

This section provides hands-on exercises to reinforce your understanding of the fundamental concepts covered in this chapter.

\subsection*{Basic Exercises}\label{basic-exercises}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Install \textbf{R} and \textbf{RStudio} on your computer.\\
\item
  Use the \texttt{getwd()} function to check your current working directory. Then, change it to a new directory using \texttt{setwd()}.\\
\item
  Create a numeric vector named \texttt{numbers} containing the values \texttt{5,\ 10,\ 15,\ 20,\ 25}. Then, calculate the mean and standard deviation of the vector.\\
\item
  Create a matrix with 3 rows and 4 columns, filled with numbers from 1 to 12.\\
\item
  Create a data frame containing the following variables:\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \texttt{student\_id} (integer)\\
\item
  \texttt{name} (character)\\
\item
  \texttt{score} (numeric)\\
\item
  \texttt{passed} (logical, where \texttt{TRUE} means the student passed and \texttt{FALSE} means they failed)\\
  Print the first few rows of the data frame using \texttt{head()}.\\
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Install and load the \textbf{liver} and \textbf{ggplot2} packages in R. If you encounter any errors, check your internet connection and ensure CRAN is accessible.\\
\item
  Load the \emph{churn} dataset from the \textbf{liver} package and display the first few rows using the \texttt{head()} function.
\item
  Report the data types of the variables in the \emph{churn} dataset using the \texttt{str()} function.
\item
  Report the dimensions of the \emph{churn} dataset using the \texttt{dim()} function.
\item
  Report the summary statistics of the variables in the \emph{churn} dataset using the \texttt{summary()} function.
\item
  Create a scatter plot using \textbf{ggplot2} that visualizes the relationship between \texttt{day.mins} and \texttt{eve.mins} in the \emph{churn} dataset. \textbf{Hint:} See the code in Section \ref{EDA-sec-multivariate}.
\item
  Create a histogram for the \texttt{day.calls} variable in the \emph{churn} dataset.
\item
  Create a boxplot for the \texttt{day.mins} variable in the \emph{churn} dataset.
\item
  Create a boxplot for the \texttt{day.mins} variable in the \emph{churn} dataset, grouped by the \texttt{churn} variable. \textbf{Hint:} See the code in Section \ref{EDA-sec-numeric}.
\item
  Use the \texttt{mean()} function to compute the mean of the \texttt{customer.calls} variable in the \emph{churn} dataset. Then, calculate the mean of \texttt{customer.calls} for churner \texttt{churn\ ==\ yes}.\\
\item
  Create an R Markdown document that includes a title, author, and a small analysis of the \emph{churn} dataset. Generate an HTML report.
\end{enumerate}

\subsection*{More Challenges Exercise}\label{more-challenges-exercise}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\tightlist
\item
  The following R code generates a simulated dataset with 200 observations. We will use this simulated dataset as a simple toy example in Chapter \ref{chapter-knn} to explain how k-nearest neighbors algorithm works. This simulated data is for patients with three variables:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \texttt{Age}: Age of the patients as numeric variable with range from 15 to 75 years old.\\
\item
  \texttt{Ratio}: Sodium/Potassium ratio in the patient's blood as numeric variable. The ratio is generated based on the \texttt{Type} variable.
\item
  \texttt{Type}: a factor with three levels: \texttt{"A"}, \texttt{"B"}, \texttt{"C"} representing the type of drug the patient is taking.
\end{itemize}

Run the code and report the summary statistics of the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate data for kNN}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{10}\NormalTok{)}

\NormalTok{n  }\OtherTok{=} \DecValTok{200}         \CommentTok{\# Number of patients}
\NormalTok{n1 }\OtherTok{=} \DecValTok{90}          \CommentTok{\# Number of patients with drug A}
\NormalTok{n2 }\OtherTok{=} \DecValTok{60}          \CommentTok{\# Number of patients with drug B }
\NormalTok{n3 }\OtherTok{=}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ n1 }\SpecialCharTok{{-}}\NormalTok{ n2 }\CommentTok{\# Number of patients with drug C}

\CommentTok{\# Generate Age variable between 15 and 75}
\NormalTok{Age }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \DecValTok{15}\SpecialCharTok{:}\DecValTok{75}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Generate Drug Type variable with three levels}
\NormalTok{Type }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{), }\AttributeTok{size =}\NormalTok{ n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{c}\NormalTok{(n1, n2, n3))}

\CommentTok{\# Generate Sodium/Potassium Ratio based on Drug Type}
\NormalTok{Ratio }\OtherTok{=} \FunctionTok{numeric}\NormalTok{(n)}

\NormalTok{Ratio[Type }\SpecialCharTok{==} \StringTok{"A"}\NormalTok{] }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \DecValTok{10}\SpecialCharTok{:}\DecValTok{40}\NormalTok{, }\AttributeTok{size =} \FunctionTok{sum}\NormalTok{(Type }\SpecialCharTok{==} \StringTok{"A"}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{Ratio[Type }\SpecialCharTok{==} \StringTok{"B"}\NormalTok{] }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =}  \DecValTok{5}\SpecialCharTok{:}\DecValTok{15}\NormalTok{, }\AttributeTok{size =} \FunctionTok{sum}\NormalTok{(Type }\SpecialCharTok{==} \StringTok{"B"}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{Ratio[Type }\SpecialCharTok{==} \StringTok{"C"}\NormalTok{] }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =}  \DecValTok{5}\SpecialCharTok{:}\DecValTok{15}\NormalTok{, }\AttributeTok{size =} \FunctionTok{sum}\NormalTok{(Type }\SpecialCharTok{==} \StringTok{"C"}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Create a data frame with the generated variables}
\NormalTok{drug\_data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Age =}\NormalTok{ Age, }\AttributeTok{Ratio =}\NormalTok{ Ratio, }\AttributeTok{Type =}\NormalTok{ Type)}
\end{Highlighting}
\end{Shaded}

Visualize the data using the following \textbf{ggplot2} code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ drug\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y =}\NormalTok{ Ratio)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ Type, }\AttributeTok{shape =}\NormalTok{ Type)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Age vs. Sodium/Potassium Ratio"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Sodium/Potassium Ratio"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{1_Intro_R_files/figure-latex/unnamed-chunk-45-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{17}
\tightlist
\item
  Extend the dataset \texttt{drug\_data} by adding a new variable named \texttt{Outcome}, which is a factor with two levels (\texttt{"Good"} and \texttt{"Bad"}).\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Patients with \texttt{Type\ ==\ "A"} should have a higher probability of \texttt{"Good"} outcomes.\\
\item
  Patients with \texttt{Type\ ==\ "B"} and \texttt{Type\ ==\ "C"} should have a lower probability of \texttt{"Good"} outcomes.\\
\item
  Use \texttt{sample()} with appropriate probabilities to generate the \texttt{Outcome} variable.\\
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{18}
\tightlist
\item
  Create a new scatter plot using \textbf{ggplot2} that visualizes the relationship between \texttt{Age} and \texttt{Ratio}, colored by the \texttt{Outcome} variable.
\item
  Create a new variable \texttt{Age\_group} in the \texttt{drug\_data} dataset that categorizes patients into three groups:\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  ``Young'' (\(\leq 30\) years old)
\item
  ``Middle-aged'' (31-50 years old)
\item
  ``Senior'' (\textgreater50 years old).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Calculate the mean \texttt{Ratio} for each \texttt{Age\_group} category in the \texttt{drug\_data} dataset.\\
\item
  Create a bar chart using \textbf{ggplot2} that displays the average \texttt{Ratio} for each \texttt{Age\_group}.\\
\item
  Modify the \texttt{drug\_data} dataset by adding a \texttt{Risk\_factor} variable, calculated as \texttt{Ratio\ *\ Age\ /\ 10}. Analyze how \texttt{Risk\_factor} differs by \texttt{Type}.\\
\item
  Create a histogram of the \texttt{Risk\_factor} variable, grouped by \texttt{Type}.\\
\item
  Generate a boxplot to visualize the distribution of \texttt{Risk\_factor} across different \texttt{Outcome} categories.
\end{enumerate}

\chapter{Introduction to Data Science}\label{chapter-intro-DS}

\emph{Data Science} is a rapidly evolving field that is transforming industries by leveraging computational, statistical, and analytical techniques. In the 21st century, data has become one of the most valuable resources, often called the \emph{``new oil''} due to its potential to drive innovation and reshape the future.\\
Data science is the key to unlocking this potential. By applying computational, statistical, and analytical techniques, data scientists extract insights from vast amounts of data, enabling organizations to make informed decisions, optimize processes, predict trends, and develop intelligent systems. This has led to groundbreaking advancements in fields such as healthcare, finance, marketing, artificial intelligence (AI), and beyond.

Given its rapid growth and increasing demand, data science is more critical than ever. In this chapter, we'll explore the fundamentals of data science, discuss its significance in modern society, and introduce the Data Science Workflow---a structured approach that data scientists use to transform raw data into actionable insights.

This section is well-structured and provides a clear introduction to data science. It effectively conveys the interdisciplinary nature of the field and highlights its core components. However, there are some areas where clarity, consistency, and flow can be improved. Below are my suggestions:

\section{What is Data Science?}\label{what-is-data-science}

Data science is an interdisciplinary field that integrates computer science, statistics, and domain expertise to extract insights from data. It involves using analytical and computational techniques to process vast amounts of raw data, transforming them into meaningful information that supports decision-making and strategic planning.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth]{images/ch2_data_science} 

}

\caption{Data science is a multidisciplinary field that applies computational and statistical methods to extract insights from data.}\label{fig:Data-Science}
\end{figure}

Although the term ``data science'' is relatively new, its foundations lie in well-established disciplines such as statistics, data analysis, and machine learning. With the exponential growth of digital data, advancements in computational power, and the increasing demand for data-driven decision-making, data science has emerged as a distinct and essential field.

At its core, data science is concerned with extracting knowledge from data using a combination of statistical techniques, machine learning algorithms, and domain-specific methodologies. It helps organizations manage and understand the vast amounts of information generated in the digital age.

\subsection*{Key Components of Data Science}\label{key-components-of-data-science}


The field of data science encompasses three main components:

\begin{itemize}
\tightlist
\item
  \textbf{Data Engineering}: The foundation of data science, responsible for collecting, storing, and structuring large datasets. This includes the development of data pipelines and infrastructure to enable efficient analysis. While crucial, data engineering is beyond the scope of this book.\\
\item
  \textbf{Data Analysis and Statistics}: The application of statistical methods to explore and analyze data. This includes data visualization, hypothesis testing, and predictive modeling. More details on this topic are covered in the \hyperref[chapter-statistics]{Statistical Inference and Hypothesis Testing} and \hyperref[chapter-EDA]{Exploratory Data Analysis} chapters.\\
\item
  \textbf{Machine Learning and Artificial Intelligence}: The use of algorithms to identify patterns, make predictions, and extract deeper insights. This includes supervised and unsupervised learning, deep learning, and natural language processing. These concepts are discussed in the \hyperref[chapter-modeling]{Modeling Process} chapter.
\end{itemize}

\section{Why Data Science Matters}\label{why-data-science-matters}

In the digital age, data has become one of the most valuable resources and is often referred to as the ``new oil'' of the 21st century. This comparison makes sense, as some of the world's most valuable companies today---including OpenAI, Google, and Apple---are driven by artificial intelligence and data science. Just as the wealthiest companies of the 20th century were those that controlled oil and energy, today's leading enterprises leverage data as a key asset for innovation and competitive advantage.

Across industries, data-driven decision-making has become essential. Organizations generate vast amounts of data every day, and without the right tools and techniques, much of this data would remain untapped. Data science helps organizations uncover patterns, detect trends, and make informed decisions that enhance efficiency, reduce costs, and improve customer experiences.\\
Data science plays a crucial role in a wide range of sectors, including:

\begin{itemize}
\tightlist
\item
  \emph{Finance}: Financial institutions leverage data science for risk assessment, fraud detection, and algorithmic trading. Machine learning models identify anomalies in transaction patterns, improving fraud detection and regulatory compliance.\\
\item
  \emph{Marketing}: Businesses use data science to analyze customer behavior, segment audiences, and create targeted marketing campaigns. Platforms such as Facebook and Google Ads leverage sophisticated algorithms to match advertisements with the most relevant audiences, improving engagement and conversion rates.\\
\item
  \emph{Retail and E-commerce}: Companies like Amazon and Walmart use data science to optimize inventory management, predict demand, and personalize recommendations. By analyzing purchase history and browsing behavior, retailers can offer tailored promotions and enhance customer satisfaction.\\
\item
  \emph{Healthcare}: Hospitals and medical researchers use data science for disease diagnosis, patient risk prediction, and personalized treatment plans. By analyzing large datasets of medical records, institutions can identify high-risk patients and take preventative measures to improve health outcomes.
\end{itemize}

For example, Netflix applies data science to analyze viewing patterns and recommend personalized content to users, while supply chain optimization at Amazon ensures faster deliveries by leveraging predictive analytics.

\section{The Data Science Workflow}\label{the-data-science-workflow}

The \emph{data science workflow} follows an \emph{iterative} and \emph{cyclical} approach, where insights gained at each stage inform and refine subsequent steps. Unlike a strictly linear process, data science involves continuous refinement to enhance accuracy and efficiency. This structured approach ensures that data-driven projects are conducted systematically, balancing exploratory analysis, model building, and evaluation to derive meaningful conclusions.

A \emph{data science workflow} follows a phased, adaptive approach within a scientific framework, transforming raw data into actionable knowledge. This transformation is often conceptualized using the \emph{DIKW Pyramid} (Data â†’ Information â†’ Knowledge â†’ Wisdom), as illustrated in Figure \ref{fig:DIKW-Pyramid}.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth]{images/ch2_DIKW-Pyramid} 

}

\caption{The DIKW Pyramid illustrates the transformation of raw data into actionable insights, progressing from data to information, knowledge, and ultimately wisdom.}\label{fig:DIKW-Pyramid}
\end{figure}

While the specifics may vary across projects, most data science workflows follow a common structure. In this book, we adopt the \emph{Data Science Workflow} as a guiding framework for structuring data science projects. This workflow is inspired by the \emph{Cross-Industry Standard Process for Data Mining (CRISP-DM)} model, a widely recognized methodology for data-driven projects. It is a \emph{cyclic} framework that guides data scientists through the following key stages (see Figure \ref{fig:CRISP-DM}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Problem Understanding} -- Defining the business or research question and outlining objectives.\\
\item
  \textbf{Data Preparation} -- Collecting, cleaning, transforming, and organizing data to ensure it is suitable for analysis. This step includes handling missing values, addressing inconsistencies, detecting outliers, and preparing features through scaling, encoding, and transformation.\\
\item
  \textbf{Exploratory Data Analysis (EDA)} -- Identifying patterns, distributions, and relationships within the data.\\
\item
  \textbf{Preparing Data for Modeling} -- Engineering relevant features, normalizing data, and selecting meaningful variables.\\
\item
  \textbf{Modeling} -- Applying machine learning or statistical techniques to develop predictive or descriptive models.\\
\item
  \textbf{Evaluation} -- Assessing model performance using appropriate metrics and validation techniques.\\
\item
  \textbf{Deployment} -- Integrating the model into a production environment and monitoring its performance over time.
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/ch2_DSW} 

}

\caption{The Data Science Workflow is an iterative framework for structuring data science and machine learning projects. Inspired by the CRISP-DM model, it ensures systematic problem-solving and continuous refinement.}\label{fig:CRISP-DM}
\end{figure}

Because data science is inherently \emph{iterative}, these steps are often revisited multiple times within a single project. The \emph{feedback loops} between stages allow for continuous refinement---adjusting data preprocessing, modifying features, or retraining models as new insights emerge. By following a structured workflow, data scientists can ensure rigor, accuracy, and efficiency in transforming data into valuable insights.

\section{Problem Understanding}\label{problem-understanding}

The first step in any data science project is to clearly define the problem---whether it is a business challenge or a research question. This phase is crucial because data science is not just about building models; it is about solving real-world problems using data-driven approaches. A well-defined problem ensures that efforts are aligned with meaningful objectives, improving the likelihood of delivering actionable insights.

At this stage, data scientists work closely with stakeholders to understand the goals, clarify expectations, and define success criteria. The following questions help frame the problem:

\begin{itemize}
\tightlist
\item
  \textbf{Why} is this research or business question important?\\
\item
  \textbf{What} is the desired outcome or impact?\\
\item
  \textbf{How} can data science techniques contribute to addressing this question?
\end{itemize}

Focusing on the \emph{why} and \emph{what} before diving into the \emph{how} is essential. As Simon Sinek emphasizes in his TED talk \href{https://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action?utm_campaign=tedspread&utm_medium=referral&utm_source=tedcomshare}{``How Great Leaders Inspire Action''}, ``People don't buy what you do; they buy why you do it.'' This concept applies to data science as well---understanding the deeper motivation behind a project provides clarity and direction.

For example, a data science team in a business analytics department may be approached by a client who wants a predictive model but lacks clarity on the specific problem they are trying to solve. Without a clear \emph{why}, it becomes difficult to develop a solution that delivers real value. Similarly, students working on research projects often focus on \emph{what} they want to build rather than \emph{why} it is needed.

Suppose a company aims to reduce customer churn. A well-defined objective might be to develop a predictive model that identifies customers at risk of leaving so that targeted retention strategies can be implemented. This initial understanding helps frame the problem and guides the selection of relevant data, modeling techniques, and evaluation metrics.

Problem understanding is both an analytical and creative process. While data science provides tools and methodologies, defining the right problem requires domain expertise and critical thinking. The following steps help ensure a structured approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Clearly articulate the project objectives} and requirements in terms of the overall goals of the business or research entity.
\item
  \textbf{Break down the objectives} to outline specific expectations and desired outcomes.
\item
  \textbf{Translate these objectives into a data science problem} that can be addressed using analytical techniques.
\item
  \textbf{Draft a preliminary strategy} for how to achieve these objectives, considering potential approaches and methodologies.
\end{enumerate}

By thoroughly defining the problem, data scientists set the stage for an effective workflow, ensuring that subsequent analysis and modeling efforts remain aligned with meaningful outcomes.

\section{Data Preparation}\label{data-preparation}

Once the problem is well-defined, the next step is \emph{data preparation}, ensuring the data is accurate, complete, and well-structured. Raw data often contains \emph{missing values, inconsistencies, and outliers}, making this phase critical for reliable analysis. Poorly prepared data can lead to misleading insights, even with sophisticated models.

Data can originate from various sources, including databases, spreadsheets, APIs, and web scraping. It may be \emph{structured} (e.g., numerical data in databases) or \emph{unstructured} (e.g., text, images). Preprocessing is essential before analysis.

Key steps in data preparation include:

\begin{itemize}
\tightlist
\item
  \emph{Data Collection and Integration}: Merging data from multiple sources while ensuring consistency.\\
\item
  \emph{Handling Missing Values}: Removing, imputing, or flagging incomplete data.\\
\item
  \emph{Outlier Detection}: Identifying and managing extreme values using visualization.\\
\item
  \emph{Resolving Inconsistencies}: Standardizing formats, correcting errors, and aligning categorical values.\\
\item
  \emph{Feature Engineering}: Transforming data through encoding, scaling, and normalization for model compatibility.\\
\item
  \emph{Data Summarization}: Checking variable types, computing summary statistics, and detecting duplicates.
\end{itemize}

Though time-consuming, data preparation is essential for accurate modeling and meaningful analysis. In Chapter \ref{chapter-data-prep}, we explore these techniques further with real-world examples.

\section{Exploratory Data Analysis (EDA)}\label{exploratory-data-analysis-eda}

Exploratory Data Analysis (EDA) is a fundamental step in the data science workflow, providing an initial understanding of the dataset before formal modeling. The primary objective of EDA is to uncover patterns, relationships, and anomalies in the data, helping data scientists refine hypotheses and validate assumptions. By systematically examining the data, EDA ensures that the subsequent modeling process is informed by a solid understanding of the dataset's structure and characteristics.

Several key techniques are commonly used in EDA:

\begin{itemize}
\tightlist
\item
  \emph{Summary statistics} -- Measures such as the mean, median, standard deviation, and interquartile range provide insights into the distribution and central tendencies of numerical variables.\\
\item
  \emph{Data visualization} -- Graphical techniques, including histograms, scatter plots, and box plots, reveal data distributions, trends, and potential outliers.\\
\item
  \emph{Correlation analysis} -- Examining relationships between numerical features using correlation coefficients helps identify dependencies that may influence modeling decisions.
\end{itemize}

EDA serves both diagnostic and exploratory functions. It helps detect data quality issues, such as missing values or inconsistencies, while also guiding feature selection and engineering. For instance, if a strong correlation exists between certain features and the target variable, these features may be prioritized in the modeling phase.

A thorough EDA process not only improves the quality of the dataset but also enhances the interpretability and reliability of analytical results. In Chapter \ref{chapter-EDA}, we will explore EDA techniques in greater detail, applying them to real-world datasets to illustrate practical applications.

\section{Preparing Data for Modeling}\label{preparing-data-for-modeling}

With insights from EDA, the next step is to \emph{prepare the data for modeling}. This stage involves \emph{feature engineering}, \emph{feature selection}, and \emph{data splitting}---all of which are crucial for building effective models.

\begin{itemize}
\tightlist
\item
  \emph{Feature Engineering}: Creating new features or transforming existing ones to enhance model performance. For example, deriving new variables by combining existing ones or applying transformations can provide additional predictive power.\\
\item
  \emph{Feature Selection}: Identifying and selecting the most relevant features to improve model efficiency and prevent overfitting. Removing irrelevant or redundant features simplifies the model and enhances interpretability.\\
\item
  \emph{Data Splitting}: Dividing the dataset into training, validation, and testing sets. The training set is used to develop the model, the validation set helps fine-tune parameters, and the test set assesses final model performance.
\end{itemize}

By the end of this stage, the data should be in a structured and well-prepared format, ensuring that models can learn effectively. In Chapter \ref{chapter-modeling}, we will explore these techniques in more detail and apply them to real-world datasets.

\section{Modeling}\label{modeling}

Modeling is the stage where data scientists apply machine learning or statistical techniques to the prepared data to create a predictive or descriptive model. The goal is to build a model that effectively captures relationships within the data and generalizes well to new, unseen data.

The modeling process typically involves:

\begin{itemize}
\tightlist
\item
  \emph{Choosing a Model}: Selecting an appropriate model based on the problem type (e.g., regression, classification, clustering) and the characteristics of the dataset.\\
\item
  \emph{Training the Model}: Fitting the model to the training data to learn patterns and relationships.\\
\item
  \emph{Tuning Hyperparameters}: Adjusting model parameters to optimize performance on the validation set.
\end{itemize}

Common algorithms include linear regression (Chapter \ref{chapter-regression}), decision trees (Chapter \ref{chapter-tree}), NaÃ¯ve Bayes classifier (Chapter \ref{chapter-bayes}), k-Nearest Neighbors (k-NN) algorithm (Chapter \ref{chapter-knn}), and neural networks (Chapter \ref{chapter-nn}). Each method has its strengths and limitations, and selecting the most suitable model depends on the nature of the problem, data quality, and computational constraints. Often, multiple models are tested and compared to determine the best-performing approach.

\section{Evaluation}\label{evaluation}

Once a model is built, it must be rigorously evaluated to ensure its accuracy, generalizability, and robustness before deployment. The evaluation process relies on well-defined performance metrics, which vary depending on the type of problem. For classification models, commonly used metrics include accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (ROC-AUC). In regression tasks, measures such as mean squared error (MSE), mean absolute error (MAE), and the coefficient of determination (\(R^2\)) assess model effectiveness.

To ensure the model is not overfitting to the training data, cross-validation techniques, such as k-fold cross-validation, are employed. These methods provide a more reliable estimate of a model's performance by partitioning the data into multiple subsets for training and validation. Beyond numerical evaluation, error analysis plays a crucial role in diagnosing weaknesses, particularly through confusion matrix interpretation for classification problems and residual analysis for regression. A careful examination of errors often reveals underlying biases, data inconsistencies, or model limitations that require refinement.

If the model fails to meet expectations, adjustments may be necessary, such as feature selection, hyperparameter tuning, or exploring alternative modeling approaches. In Chapter \ref{chapter-evaluation}, we will explore these techniques in detail and apply them to real-world datasets.

\section{Deployment}\label{deployment}

Once the model has been evaluated and meets the project goals, the final step is deployment, where it is integrated into a production environment to generate real-time insights or predictions. This phase is crucial for ensuring that the model contributes tangible value, whether by supporting decision-making processes or by automating tasks within operational systems. Models can be deployed in various ways, such as embedding them in web applications, integrating them into enterprise software, or automating processes in large-scale data pipelines.

Beyond initial integration, continuous monitoring is essential to track the model's performance and detect potential issues. As real-world data evolves, models may experience \emph{concept drift}, where their predictive accuracy deteriorates due to changes in underlying patterns. To mitigate this, periodic model updates and retraining are necessary to maintain reliability. Additionally, implementing robust logging and performance tracking mechanisms helps ensure that discrepancies between predicted and actual outcomes are quickly identified and addressed.

Deployment is not a one-time event but an ongoing process. Effective deployment strategies account for scalability, interpretability, and maintainability, allowing models to remain useful in dynamic environments. As the field of data science advances, the ability to manage deployed models effectively will continue to be a critical factor in transforming analytical insights into real-world impact.

\section{Machine Learning}\label{machine-learning}

Data science relies on machine learning techniques to extract insights from data, make predictions, and uncover patterns. These methods enable data scientists to move beyond descriptive analysis and explore predictive and prescriptive approaches, which are essential for real-world applications. In this section, we provide an overview of machine learning, including its main types---\emph{supervised learning} and \emph{unsupervised learning}---and discuss how machine learning differs from statistical learning.

Machine learning is a branch of artificial intelligence that focuses on developing algorithms that learn from data and make predictions. Rather than being explicitly programmed for each task, machine learning models identify patterns within data and use them to make informed decisions. This approach is particularly useful for complex problems where rule-based programming would be impractical.

For instance, rather than defining a fixed set of rules to detect spam emails, a machine learning model can be trained on a labeled dataset of emails classified as ``spam'' or ``not spam.'' The model learns distinguishing patterns and can classify new emails with high accuracy. This ability to generalize from data makes machine learning invaluable in fields such as finance, healthcare, and marketing.

\subsection*{Machine Learning Tasks: Supervised vs.~Unsupervised Learning}\label{machine-learning-tasks-supervised-vs.-unsupervised-learning}


Machine learning tasks can be broadly categorized into \emph{supervised learning} and \emph{unsupervised learning}, which differ in terms of how models learn from data and the objectives of the analysis.

\textbf{Supervised learning} involves training a model on a labeled dataset, where each data point is associated with a known output. The goal is for the model to learn the relationship between input features and the corresponding output, enabling it to make accurate predictions on new data. Common supervised learning tasks include classification and numeric prediction. In classification, the model assigns data points to predefined categories, such as detecting whether an email is spam or identifying whether a patient has a particular disease. This book covers classification techniques such as decision trees (Chapter \ref{chapter-tree}), the NaÃ¯ve Bayes classifier (Chapter \ref{chapter-bayes}), and the k-Nearest Neighbors (k-NN) algorithm (Chapter \ref{chapter-knn}). Numeric prediction, also known as regression, focuses on estimating continuous values, such as forecasting house prices based on location and size. A detailed discussion of regression techniques is provided in Chapter \ref{chapter-regression}.

\textbf{Unsupervised learning}, on the other hand, is applied to datasets that lack labeled outputs. The objective is to uncover hidden patterns, relationships, or structures within the data. Clustering, a common unsupervised learning technique, groups data points based on similarity, such as segmenting customers according to purchasing behavior. Another important unsupervised learning method is pattern discovery, also known as association rule learning, which identifies relationships between variables. This technique is widely used in market basket analysis to detect frequently co-purchased items. These concepts are explored in further detail in Chapter \ref{chapter-cluster}.

In summary, supervised learning is used when labeled data is available and a specific predictive outcome is required, while unsupervised learning is beneficial for exploratory data analysis, where the goal is to identify underlying structures in unlabeled data. The distinction between these two approaches is fundamental to selecting appropriate machine learning techniques for a given data science problem.

\section{Exercises}\label{exercises}

The following exercises will help reinforce the key concepts covered in this chapter. The questions range from fundamental definitions to applied problem-solving related to data science, the data science workflow, and machine learning.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How does data-driven decision-making impact businesses? Give an example of a real-world application.\\
\item
  \emph{Data Science Workflow} is inspired by the \emph{CRISP-DM} model. What does \emph{CRISP-DM} stand for, and how does it guide data-driven projects? What are the key stages of the \emph{CRISP-DM} model?\\
\item
  \emph{Data Science Workflow} and \emph{CRISP-DM} model are not the only standard processes for data science projects. What are some other methodologies used in the industry?\\
\item
  Do you think we can skip the \emph{Problem Understanding} phase and directly jump to \emph{Data Preparation} in a data science project? Justify your answer.\\
\item
  Why is \emph{Data Preparation} considered one of the most time-consuming steps in a data science project? What are some common challenges faced during this phase?\\
\item
  To what extent can Data Science projects be automated without human intervention? What are the risks and limitations of relying solely on automated tools?\\
\item
  For each of the following scenarios, identify the appropriate stage in the data science workflow:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A company wants to predict customer churn based on historical data.\\
  \item
    A researcher is exploring the relationship between air pollution and respiratory diseases.\\
  \item
    An e-commerce platform is analyzing user behavior to personalize product recommendations.\\
  \item
    A hospital is developing a predictive model for patient readmission rates.\\
  \end{enumerate}
\item
  For each task, classify it as \emph{supervised} or \emph{unsupervised} learning, explain your reasoning, and identify a suitable machine learning algorithm that could be applied.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Identifying fraudulent transactions in a credit card dataset.\\
  \item
    Segmenting customers based on purchasing behavior.\\
  \item
    Predicting stock prices based on historical data.\\
  \item
    Grouping news articles into topics using natural language processing.\\
  \end{enumerate}
\item
  Define a training dataset and a test dataset. Why are they important? How does improper splitting of these datasets affect model performance? Provide an example of a real-world issue caused by poor dataset partitioning.\\
\item
  Many AI-driven systems have been criticized for biased predictions, such as hiring algorithms that favor certain demographics or facial recognition models that misidentify certain racial groups.

  \begin{itemize}
  \tightlist
  \item
    What are some common sources of bias in data science projects?\\
  \item
    How can data scientists ensure fairness and mitigate biases in models?\\
  \item
    Give an example of a real-world case where bias in AI led to negative consequences.\\
  \end{itemize}
\item
  Accuracy is a common metric used to evaluate models, but it is not always the best indicator of success. Consider a binary classification problem where only \emph{2\%} of the cases are positive (e.g., detecting rare diseases or fraud).

  \begin{itemize}
  \tightlist
  \item
    Why might accuracy be misleading in this case?\\
  \item
    What alternative evaluation metrics should be used?\\
  \item
    How would you decide whether a model is truly valuable for decision-making?
  \end{itemize}
\end{enumerate}

\chapter{Data Preparation}\label{chapter-data-prep}

Data preparation is a foundational step in any data science project, ensuring that raw data is transformed into a clean and structured format suitable for analysis. This process is often the most time-consuming yet crucial stage, as the quality of data directly influences the accuracy of insights and the effectiveness of predictive models.

This chapter explores key data preparation techniques, including \emph{handling missing values}, \emph{detecting outliers}, \emph{transforming data}, and \emph{feature engineering}. By the end of this chapter, you will have a clear understanding of how to preprocess raw data, enabling robust statistical modeling and machine learning applications.

To illustrate these concepts, we will use the \emph{diamonds} dataset from the \textbf{ggplot2} package. This dataset contains detailed attributes of diamonds, such as carat, cut, color, clarity, and price, making it an excellent case study for data preprocessing. In this chapter, we focus on the first two steps of the Data Science Workflow---data cleaning and transformation---laying the groundwork for further analysis in subsequent chapters.

\section{Problem Understanding}\label{problem-understanding}

Before preparing data for analysis, it is essential to define the problem and establish clear objectives. In this case, we aim to analyze the \emph{diamonds} dataset to gain insights into \emph{diamond pricing}, a critical factor in industries such as \emph{jewelry retail, gemology, and e-commerce}. The dataset includes attributes that influence diamond value, allowing us to explore the key factors affecting pricing.

\subsection*{Objectives and Key Questions}\label{objectives-and-key-questions}


Our primary objectives with the \emph{diamonds} dataset are to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Examine relationships} between diamond attributes (e.g., carat, cut, color, clarity) and price.\\
\item
  \emph{Identify patterns} that could improve price estimation.\\
\item
  \emph{Assess data quality}, ensuring consistency and detecting missing values or outliers that may affect analysis.
\end{enumerate}

To achieve these objectives, we will address key questions such as:

\begin{itemize}
\tightlist
\item
  Which attributes have the most significant influence on price?\\
\item
  Are there pricing trends based on characteristics such as \emph{carat weight} or \emph{cut quality}?\\
\item
  Are there inconsistencies, errors, or missing values that need to be corrected?
\end{itemize}

\subsection*{Framing the Problem as a Data Science Task}\label{framing-the-problem-as-a-data-science-task}


From a business perspective, understanding diamond pricing can provide valuable insights for \emph{jewelers, e-commerce platforms, and gemologists}. From a \emph{data science} perspective, this problem can be approached in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Predictive modeling}: Developing a model that estimates \emph{diamond price} based on its attributes.\\
\item
  \emph{Exploratory data analysis (EDA)}: Identifying trends and relationships without building a predictive model.
\end{enumerate}

Clearly defining these objectives ensures that our data preparation efforts align with the intended analytical approach, whether for exploratory insights or building robust predictive models that generalize well to unseen data. This structured problem framing will guide decisions during data cleaning, transformation, and feature engineering, ensuring that our analysis remains focused and actionable.

\section{diamonds Dataset Overview}\label{Data-pre-diamonds}

The \emph{diamonds} dataset, included in the \textbf{ggplot2} package, provides structured information on various characteristics of diamonds. Each row represents a unique diamond, with 54,940 entries in total, and contains 10 descriptive variables, including \emph{price}, \emph{carat}, \emph{cut}, \emph{clarity}, and \emph{color}. The goal of our analysis is to gain deeper insights into the factors that influence diamond pricing, understand the distribution of data across these attributes, and explore both quantitative and qualitative relationships between variables.

To use the \emph{diamonds} dataset in \textbf{R}, first ensure that the \textbf{ggplot2} package is installed. If not, install it using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Then, load the package and dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)  }\CommentTok{\# Load ggplot2 package}
\FunctionTok{data}\NormalTok{(diamonds)    }\CommentTok{\# Load diamonds dataset}
\end{Highlighting}
\end{Shaded}

To inspect the dataset structure, use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(diamonds)   }
\NormalTok{   tibble [}\DecValTok{53}\NormalTok{,}\DecValTok{940}\NormalTok{ x }\DecValTok{10}\NormalTok{] (S3}\SpecialCharTok{:}\NormalTok{ tbl\_df}\SpecialCharTok{/}\NormalTok{tbl}\SpecialCharTok{/}\NormalTok{data.frame)}
    \SpecialCharTok{$}\NormalTok{ carat  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{0.23} \FloatTok{0.21} \FloatTok{0.23} \FloatTok{0.29} \FloatTok{0.31} \FloatTok{0.24} \FloatTok{0.24} \FloatTok{0.26} \FloatTok{0.22} \FloatTok{0.23}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cut    }\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Fair"}\SpecialCharTok{\textless{}}\StringTok{"Good"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{4} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ color  }\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"D"}\SpecialCharTok{\textless{}}\StringTok{"E"}\SpecialCharTok{\textless{}}\StringTok{"F"}\SpecialCharTok{\textless{}}\StringTok{"G"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{6} \DecValTok{7} \DecValTok{7} \DecValTok{6} \DecValTok{5} \DecValTok{2} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ clarity}\SpecialCharTok{:}\NormalTok{ Ord.factor w}\SpecialCharTok{/} \DecValTok{8}\NormalTok{ levels }\StringTok{"I1"}\SpecialCharTok{\textless{}}\StringTok{"SI2"}\SpecialCharTok{\textless{}}\StringTok{"SI1"}\SpecialCharTok{\textless{}}\NormalTok{..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{5} \DecValTok{4} \DecValTok{2} \DecValTok{6} \DecValTok{7} \DecValTok{3} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ depth  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{61.5} \FloatTok{59.8} \FloatTok{56.9} \FloatTok{62.4} \FloatTok{63.3} \FloatTok{62.8} \FloatTok{62.3} \FloatTok{61.9} \FloatTok{65.1} \FloatTok{59.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ table  }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\DecValTok{55} \DecValTok{61} \DecValTok{65} \DecValTok{58} \DecValTok{58} \DecValTok{57} \DecValTok{57} \DecValTok{55} \DecValTok{61} \DecValTok{61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ price  }\SpecialCharTok{:}\NormalTok{ int [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\DecValTok{326} \DecValTok{326} \DecValTok{327} \DecValTok{334} \DecValTok{335} \DecValTok{336} \DecValTok{336} \DecValTok{337} \DecValTok{337} \DecValTok{338}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ x      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{3.95} \FloatTok{3.89} \FloatTok{4.05} \FloatTok{4.2} \FloatTok{4.34} \FloatTok{3.94} \FloatTok{3.95} \FloatTok{4.07} \FloatTok{3.87} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ y      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{3.98} \FloatTok{3.84} \FloatTok{4.07} \FloatTok{4.23} \FloatTok{4.35} \FloatTok{3.96} \FloatTok{3.98} \FloatTok{4.11} \FloatTok{3.78} \FloatTok{4.05}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ z      }\SpecialCharTok{:}\NormalTok{ num [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{53940}\NormalTok{] }\FloatTok{2.43} \FloatTok{2.31} \FloatTok{2.31} \FloatTok{2.63} \FloatTok{2.75} \FloatTok{2.48} \FloatTok{2.47} \FloatTok{2.53} \FloatTok{2.49} \FloatTok{2.39}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

This function reveals that the dataset has 53940 observations and 10 variables. Below is a summary of the key attributes:

\begin{itemize}
\tightlist
\item
  \texttt{price}: price in US dollars (\$326--\$18,823).
\item
  \texttt{carat}: weight of the diamond (0.2--5.01).
\item
  \texttt{cut}: quality of the cut (Fair, Good, Very Good, Premium, Ideal).
\item
  \texttt{color}: diamond color, from D (best) to J (worst).
\item
  \texttt{clarity}: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)).
\item
  \texttt{x}: length in mm (0--10.74).
\item
  \texttt{y}: width in mm (0--58.9).
\item
  \texttt{z}: depth in mm (0--31.8).
\item
  \texttt{depth}: total depth percentage = \texttt{2\ *\ z\ /\ (x\ +\ y)}.
\item
  \texttt{table}: width of the top of the diamond relative to its widest point.
\end{itemize}

\subsection*{\texorpdfstring{Types of Features in the \texttt{diamonds} Dataset}{Types of Features in the diamonds Dataset}}\label{types-of-features-in-the-diamonds-dataset}


Understanding the types of features in the dataset is essential for determining the appropriate data preparation steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Quantitative (or Numerical) Variables}: These are represented by numbers and can be continuous or discrete.

  \begin{itemize}
  \tightlist
  \item
    \emph{Continuous Variables}: These variables can take any value within a range. In this dataset, \texttt{carat}, \texttt{price}, \texttt{x}, \texttt{y}, \texttt{z}, and \texttt{depth} are continuous.
  \item
    \emph{Discrete Variables}: These variables take countable values, often integers. For example, a count of customers or the number of purchases would be discrete, though this dataset doesn't include such a variable.
  \end{itemize}
\item
  \emph{Categorical (or Qualitative) Variables}: These describe data that fits into categories rather than having a numerical value. They are divided into three types:

  \begin{itemize}
  \tightlist
  \item
    \emph{Ordinal Variables}: Categorical variables with a meaningful order, but where the intervals between categories are not equal. For instance, \texttt{cut}, \texttt{color}, and \texttt{clarity} are ordinal variables in this dataset. The ordering of levels in these variables (e.g., from ``Fair'' to ``Ideal'' in \texttt{cut}) has meaning.
  \item
    \emph{Nominal Variables}: Categorical variables without any intrinsic ordering among categories. In other datasets, examples might include ``gender'' or ``product type,'' but the \emph{diamonds} dataset does not contain any nominal variables.
  \item
    \emph{Binary Variables}: Variables with only two levels, often coded as 0 and 1. While the \emph{diamonds} dataset doesn't contain binary variables, an example could be a feature like ``has\_certificate'' with values ``yes'' or ``no.''
  \end{itemize}
\end{enumerate}

Knowing the type of each feature guides decisions about data preparation. For instance:
- \emph{Numerical variables} can be normalized or standardized using techniques like Min-Max Scaling or Z-score Scaling.
- \emph{Ordinal variables} may be encoded using ordinal encoding or one-hot encoding, depending on whether the model should recognize the order.
- \emph{Categorical variables} without a meaningful order are typically one-hot encoded.

By understanding the types of variables in the \emph{diamonds} dataset, we can select appropriate transformations and encoding methods to prepare the data effectively for analysis and modeling.

\subsection*{Key Considerations for Data Preparation}\label{key-considerations-for-data-preparation}


With our objectives in mind, here are the main priorities for preparing this dataset:

\begin{itemize}
\tightlist
\item
  \emph{Data Quality}: Ensure that the data is accurate, consistent, and free from major issues. This involves checking for missing values, outliers, and inconsistencies that could bias our analysis.
\item
  \emph{Feature Engineering}: Explore the possibility of creating new features to improve predictive accuracy. For instance, calculating \emph{volume} (using the product of \emph{x}, \emph{y}, and \emph{z} dimensions) could provide an additional measure of a diamond's size.
\item
  \emph{Data Transformation}: Ensure that all features are in appropriate formats. Categorical variables like \emph{cut} and \emph{color} may need to be converted into numeric codes or dummy variables to work with machine learning algorithms effectively.
\end{itemize}

\section{Outliers}\label{Data-pre-outliers}

Outliers are data points that significantly deviate from the general distribution of a dataset. They can arise due to measurement variability, data entry errors, or genuinely unique observations. Identifying and handling outliers is crucial, as they can skew statistical analyses, affect model performance, and lead to misleading insights.

Outliers play a critical role in multiple industries:

\begin{itemize}
\tightlist
\item
  \emph{Finance}: Outliers in transaction data can indicate fraud. Detecting unusually high spending patterns is key to fraud detection models.
\item
  \emph{Healthcare}: Medical records often contain anomalous lab results, which may indicate rare diseases or measurement errors.
\item
  \emph{Manufacturing}: Sensors in factories may detect equipment failures through unusual temperature spikes.
\end{itemize}

In many cases, outliers are not errors but signals of important events. Understanding their role in data analysis ensures that we don't remove valuable insights unintentionally.

\subsection*{Identifying Outliers Using Visualization Techniques}\label{identifying-outliers-using-visualization-techniques}


\subsubsection*{Boxplots: Detecting Extreme Values}\label{boxplots-detecting-extreme-values}


Boxplots are a visual tool for detecting extreme values. Below is a boxplot of the \texttt{y} variable (diamond width) by using the \textbf{ggplot()} and \texttt{geom\_boxplot()} functions from the \textbf{ggplot2} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-4-1} \end{center}

Here, boxplots highlight values beyond the whiskers, which may indicate potential outliers. Since diamonds cannot have a width of 0 mm, values like 32 mm or 59 mm likely result from data entry errors.

\subsubsection*{Histograms: Understanding Outlier Distribution}\label{histograms-understanding-outlier-distribution}


Histograms provide another visual approach to detecting outliers by displaying the frequency distribution of values. Below is a histogram of the \texttt{y} variable by using the \emph{ggplot()} and \emph{geom\_histogram()} functions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-5-1} \end{center}

To enhance visibility, we can zoom in on smaller frequencies by using the \emph{coord\_cartesian()} function from the \textbf{ggplot2} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y), }\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-6-1} \end{center}

Other useful visualization techniques include:

\begin{itemize}
\tightlist
\item
  Violin plots -- Show both outliers and density distributions.
\item
  Density plots -- Provide smoother insights into rare values and multimodal distributions.
\end{itemize}

\subsection*{Handling Outliers: Best Practices}\label{handling-outliers-best-practices}


Once outliers are identified, there are several strategies for handling them:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Removing outliers}: This is appropriate when an outlier is clearly an error (e.g., negative height, duplicate data entry).
\item
  \emph{Transforming values}: Techniques such as log transformation or square root scaling can reduce the influence of extreme values while preserving trends.
\item
  \emph{Winsorization}: Instead of removing outliers, replace them with the nearest percentile-based value (e.g., capping extreme values at the 95th percentile).
\item
  \emph{Using robust statistical methods}: Some algorithms, like median-based regression or random forests, are less sensitive to outliers.
\item
  \emph{Treating outliers as a separate category}: In fraud detection or rare event prediction, outliers may contain valuable insights and should not be removed.
\end{enumerate}

Choosing the right strategy depends on the context of the analysis and the potential impact of the outlier.

\subsection*{Expanded Code Example: Handling Outliers in R}\label{expanded-code-example-handling-outliers-in-r}


After detecting outliers, we can choose to either replace them with \texttt{NA} values or remove them. For this, we could consider using the \texttt{mutate()} function from the \textbf{dplyr} package. Here's an example of treating outliers as missing values using \texttt{mutate()} and \texttt{ifelse()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds\_2 }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(diamonds, }\AttributeTok{y =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ y }\SpecialCharTok{\textgreater{}} \DecValTok{30}\NormalTok{, }\ConstantTok{NA}\NormalTok{, y))}
\end{Highlighting}
\end{Shaded}

Here's how to verify the update:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{      Min. }\DecValTok{1}\NormalTok{st Qu.  Median    Mean }\DecValTok{3}\NormalTok{rd Qu.    Max.    NA}\StringTok{\textquotesingle{}s }
\StringTok{     3.680   4.720   5.710   5.734   6.540  10.540       9}
\end{Highlighting}
\end{Shaded}

This method ensures that outliers do not distort the dataset while allowing for further imputation or analysis.

\section{Missing Values}\label{missing-values}

Missing values pose significant challenges in data analysis, as they can lead to biased results, reduce statistical power, and impact the performance of machine learning models. When handling missing data, we typically consider two approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Imputation: Replacing missing values with estimated values to retain data integrity.\\
\item
  Removal: Deleting records with missing values, though this may lead to data loss and potential bias.
\end{enumerate}

\subsection*{Imputation Techniques}\label{imputation-techniques}


There are several strategies for imputing missing values, each with different use cases:

\begin{itemize}
\tightlist
\item
  Mean, median, or mode imputation: Replaces missing values with the mean, median, or mode of the corresponding column.\\
\item
  Random sampling: Fills missing values with random observations drawn from the existing data distribution.\\
\item
  Predictive imputation: Uses machine learning models such as regression or k-nearest neighbors to estimate missing values.\\
\item
  Multiple imputation: Generates several possible values for missing entries and averages the results to reduce uncertainty.\\
  \#\#\# Example: Random Sampling Imputation in R \{-\}
\end{itemize}

To impute missing values in \texttt{y} using random sampling, we use the \texttt{impute()} function from the \textbf{Hmisc} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds\_2}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(diamonds\_2}\SpecialCharTok{$}\NormalTok{y, }\StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{impute()} function replaces missing values with randomly sampled values from the existing distribution of \texttt{y}, maintaining the overall statistical properties of the dataset.

\subsection*{Best Practices}\label{best-practices}


\begin{itemize}
\tightlist
\item
  Use mean or median imputation for numerical variables when the missing values are missing at random (MAR).\\
\item
  Use mode imputation for categorical variables.\\
\item
  Consider predictive models when the dataset is large and missing values are not completely random.\\
\item
  Always assess the proportion of missing data---if too many values are missing, removing the variable may be a better approach than imputation.
\end{itemize}

\section{Feature Scaling}\label{feature-scaling}

Feature scaling, also known as normalization or standardization, is a crucial step in data preprocessing. It adjusts the range and distribution of numerical features so they are on a similar scale. Many machine learning algorithms, especially those based on distance metrics such as k-nearest neighbors, benefit significantly from scaled input features, as this prevents variables with larger ranges from disproportionately influencing the model's outcome.

For instance, in the \emph{diamonds} dataset, the \texttt{carat} variable ranges from 0.2 to 5, while \texttt{price} ranges from 326 to 18823. Without scaling, variables like \texttt{price} with a wider range can dominate the model's predictions, potentially leading to suboptimal results. To address this, we apply feature scaling techniques to bring all numeric variables onto a comparable scale. In this section, we explore two common scaling methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Min-Max Scaling}: Also known as min-max normalization or min-max transformation.
\item
  \emph{Z-score Scaling}: Also known as standardization or Z-score normalization.
\end{enumerate}

Feature scaling provides several benefits:

\begin{itemize}
\tightlist
\item
  \emph{Improved Model Performance}: Ensures that features contribute equally to the model, preventing features with larger numerical ranges from dominating learning algorithms.
\item
  \emph{Better Model Convergence}: Particularly useful for gradient-based optimization methods such as logistic regression and neural networks.
\item
  \emph{More Effective Distance-Based Learning}: Algorithms such as k-means clustering and support vector machines rely on distance calculations, making feature scaling essential.
\item
  \emph{Consistent Feature Interpretation}: By standardizing numerical values, models become easier to compare and interpret.
\end{itemize}

However, feature scaling also has some drawbacks:

\begin{itemize}
\tightlist
\item
  \emph{Potential Loss of Information}: In some cases, scaling can obscure meaningful differences between data points.
\item
  \emph{Impact on Outliers}: Min-max scaling, in particular, is sensitive to extreme values, which can distort the scaled representation.
\item
  \emph{Additional Computation}: Scaling adds preprocessing overhead, particularly when working with large datasets.
\item
  \emph{Reduced Interpretability}: The original units of measurement are lost, making it harder to relate scaled values to real-world meanings.
\end{itemize}

Selecting the right scaling method depends on the characteristics of the data and the requirements of the model. In the next sections, we will explore these methods in more detail and apply them to the \emph{diamonds} dataset.

\section{Min-Max Scaling}\label{min-max-scaling}

Min-Max Scaling transforms the values of a feature to a fixed range, typically \([0, 1]\). This transformation ensures that the minimum value of each feature becomes 0 and the maximum value becomes 1. It is especially useful for algorithms that rely on distance metrics, as it equalizes the contributions of all features, making comparisons more balanced.

The formula for Min-Max Scaling is:

\[
x_{\text{scaled}} = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}},
\]
where \(x\) is the original feature value, \(x_{\text{min}}\) and \(x_{\text{max}}\) are the minimum and maximum values of the feature, and \(x_{\text{scaled}}\) is the scaled value, ranging between 0 and 1.

Min-Max Scaling is particularly useful for models that require bounded input values, such as neural networks and algorithms relying on gradient-based optimization. However, this method is sensitive to outliers, as extreme values significantly affect the scaled distribution.

\begin{example}
\protect\hypertarget{exm:ex-min-max}{}\label{exm:ex-min-max}To demonstrate Min-Max Scaling, we'll apply it to the \texttt{carat} variable in the \emph{diamonds} dataset, where \texttt{carat} values range from approximately 0.2 to 5. Using the \texttt{minmax()} function from the \textbf{liver} package, we can scale \texttt{carat} values to fit within the range {[}0, 1{]}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carat), }\AttributeTok{bins =} \DecValTok{30}\NormalTok{,}
                 \AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram for \textasciigrave{}carat\textasciigrave{} without scaling"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Values for variable \textasciigrave{}carat\textasciigrave{}"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{minmax}\NormalTok{(carat)), }\AttributeTok{bins =} \DecValTok{30}\NormalTok{,}
                 \AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram for \textasciigrave{}carat\textasciigrave{} with Min{-}Max Scaling"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Values for variable \textasciigrave{}carat\textasciigrave{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-10-1} \includegraphics[width=0.5\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-10-2}

The first histogram (left) shows the distribution of \texttt{carat} without scaling, while the second histogram (right) shows it after Min-Max Scaling. After scaling, the \texttt{carat} values are compressed to a range between 0 and 1, allowing it to be more comparable to other features that may have different original scales. This scaling method is particularly beneficial for distance-based algorithms, as it prevents features with wider ranges from having undue influence.
\end{example}

\section{Z-score Scaling}\label{z-score-scaling}

Z-score Scaling, also known as standardization, transforms feature values so they have a mean of 0 and a standard deviation of 1. This method is particularly useful for algorithms that assume normally distributed data, such as linear regression and logistic regression, because it centers the data around 0 and normalizes the spread of values.

The formula for Z-score Scaling is:

\[
x_{\text{scaled}} = \frac{x - \text{mean}(x)}{\text{sd}(x)}
\]

where \(x\) is the original feature value, \(\text{mean}(x)\) is the mean of the feature, \(\text{sd}(x)\) is the standard deviation of the feature, and \(x_{\text{scaled}}\) is the standardized value, now having a mean of 0 and a standard deviation of 1.

Z-score Scaling is particularly beneficial for models that assume normality or use gradient-based optimization, ensuring that all numerical features contribute equally. However, since it relies on mean and standard deviation, it is \textbf{sensitive to outliers}, which can distort the transformation.

\begin{example}
\protect\hypertarget{exm:ex-zscore}{}\label{exm:ex-zscore}Applying Z-score Scaling to the \texttt{carat} variable in the \emph{diamonds} dataset, where the mean and standard deviation of \texttt{carat} are approximately 0.8 and 0.47, respectively. We use the \texttt{zscore()} function from the \textbf{liver} package to standardize these values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carat), }\AttributeTok{bins =} \DecValTok{30}\NormalTok{,}
                 \AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram for \textasciigrave{}carat\textasciigrave{} without scaling"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Values for variable \textasciigrave{}carat\textasciigrave{}"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{zscore}\NormalTok{(carat)), }\AttributeTok{bins =} \DecValTok{30}\NormalTok{,}
                 \AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Histogram for \textasciigrave{}carat\textasciigrave{} with Z{-}score Scaling"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Values for variable \textasciigrave{}carat\textasciigrave{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-11-1} \includegraphics[width=0.5\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-11-2}

The first histogram (left) displays the distribution of \texttt{carat} without scaling, while the second histogram (right) shows the distribution after Z-score Scaling. This transformation makes feature values comparable across different scales and ensures that each feature contributes equally to distance-based computations and model training.
\end{example}

\begin{quote}
Note: A common misconception is that after Z-score Scaling, the data follows a standard normal distribution. While Z-score Scaling centers the data to a mean of 0 and scales it to a standard deviation of 1, it does not alter the shape of the distribution. If the original distribution is skewed, it will remain skewed after scaling, as seen in the histograms above.
\end{quote}

The choice between Min-Max Scaling and Z-score Scaling depends on the requirements of the model and the characteristics of the data. Min-Max Scaling is preferable for algorithms that require a fixed input range, while Z-score Scaling is better suited for models that assume normally distributed features. By selecting the appropriate scaling method, we ensure balanced feature contributions and improved model performance.

\section{How to Reexpress Categorical Field Values}\label{how-to-reexpress-categorical-field-values}

In data science, categorical features often need to be transformed into a numeric format before they can be used in machine learning models. Algorithms like decision trees, neural networks, and linear regression require numeric inputs to process the data effectively. Converting categorical variables into numerical representations ensures that all features contribute appropriately to the model, rather than being ignored or treated incorrectly.

This process of reexpressing categorical values is a crucial part of data preparation, as it enables us to leverage the full range of features in our dataset. In this section, we explore several methods to convert categorical fields into numeric representations, with a focus on techniques like one-hot encoding and ordinal encoding. We demonstrate these techniques using the \emph{diamonds} dataset, which includes several categorical features such as \texttt{cut}, \texttt{color}, and \texttt{clarity}.

\subsection{Why Reexpress Categorical Fields?}\label{why-reexpress-categorical-fields}

Categorical fields, also known as nominal or ordinal variables, often represent qualitative aspects of data, such as product types, user locations, or levels of satisfaction. In the \emph{diamonds} dataset, for example:

\begin{itemize}
\tightlist
\item
  \texttt{cut} indicates the quality of the diamond's cut (e.g., ``Fair,'' ``Good,'' ``Very Good,'' ``Premium,'' ``Ideal'').
\item
  \texttt{color} represents the diamond's color grade (e.g., ``D,'' ``E,'' ``F,'' with ``D'' being the most colorless and thus most valuable).
\item
  \texttt{clarity} describes the diamond's clarity, reflecting the absence of internal or external flaws.
\end{itemize}

These fields are essential for understanding and predicting diamond pricing, but in their raw form as text labels, they are not suitable for most machine learning algorithms. Transforming them into numeric form allows us to include these valuable insights in our analysis.

\subsection{Techniques for Reexpressing Categorical Variables}\label{techniques-for-reexpressing-categorical-variables}

There are several approaches to converting categorical variables into numeric representations. The method we choose depends on the type of categorical variable and the nature of the data.

\subsubsection*{Ordinal Encoding}\label{ordinal-encoding}


Ordinal encoding is suitable when the categorical variable has a meaningful order. For example, the \texttt{cut} feature in the \emph{diamonds} dataset is ordinal, as there is a natural hierarchy from ``Fair'' to ``Ideal.'' In ordinal encoding, each category is assigned a unique integer based on its rank or level of importance.

In this example, we might assign values as follows:

\begin{itemize}
\tightlist
\item
  ``Fair'' â†’ 1
\item
  ``Good'' â†’ 2
\item
  ``Very Good'' â†’ 3
\item
  ``Premium'' â†’ 4
\item
  ``Ideal'' â†’ 5
\end{itemize}

This approach preserves the order of the categories, which can be useful in models that interpret numeric values in a relative way, such as linear regression. However, it is important to apply ordinal encoding only when the order is meaningful. For non-ordinal variables, other methods like one-hot encoding are more appropriate.

\subsubsection*{One-Hot Encoding}\label{one-hot-encoding}


One-hot encoding is the preferred technique for nominal variables---categorical fields without an intrinsic order. In this approach, each unique category in a field is transformed into a new binary (0 or 1) feature. This method is particularly useful for variables like \texttt{color} and \texttt{clarity} in the \emph{diamonds} dataset, where the categories do not follow a clear sequence.

For example, if we one-hot encode the \texttt{color} feature, we create a set of binary columns, one for each color grade:

\begin{itemize}
\tightlist
\item
  \texttt{color\_D}: 1 if the diamond color is ``D,'' 0 otherwise.
\item
  \texttt{color\_E}: 1 if the diamond color is ``E,'' 0 otherwise.
\item
  \texttt{color\_F}: 1 if the diamond color is ``F,'' 0 otherwise.
\end{itemize}

One-hot encoding avoids introducing false ordinal relationships, ensuring that the model treats each category as an independent entity. However, one downside is that it can significantly increase the dimensionality of the dataset if the categorical field has many unique values.

\begin{quote}
Note: Many machine learning libraries automatically drop one of the binary columns to avoid multicollinearity (perfect correlation among features). For instance, if we have seven color categories, only six binary columns are created, and the missing category is implied when all columns are zero. This approach, known as dummy encoding, helps avoid redundancy and keeps the model simpler.
\end{quote}

\subsubsection*{Frequency Encoding}\label{frequency-encoding}


Another useful approach, especially for high-cardinality categorical variables (those with many unique values), is frequency encoding. This technique replaces each category with its frequency in the dataset, allowing the model to capture information about how common each category is. Frequency encoding can be particularly helpful for fields like \texttt{clarity} if you want to give the model an indication of how prevalent each level is.

For example:

\begin{itemize}
\tightlist
\item
  If ``VS2'' appears 10,000 times in the dataset, it would be encoded as 10,000.
\item
  If ``IF'' appears only 500 times, it would be encoded as 500.
\end{itemize}

Frequency encoding is less commonly used in basic machine learning workflows but can be valuable when dealing with very large datasets, or when one-hot encoding would introduce too many columns. However, be cautious with this approach, as it may inadvertently add an implicit weight to more common categories.

\subsection{Choosing the Right Encoding Technique}\label{choosing-the-right-encoding-technique}

Selecting the appropriate encoding technique depends on the nature of your categorical variable and the requirements of your analysis:

\begin{itemize}
\tightlist
\item
  Ordinal variables (like \texttt{cut}): Use ordinal encoding to preserve the natural order.
\item
  Nominal variables with few unique values (like \texttt{color} and \texttt{clarity}): Use one-hot encoding to represent each category as a binary column.
\item
  High-cardinality categorical variables: Consider frequency encoding if one-hot encoding would introduce too many features.
\end{itemize}

\begin{example}
\protect\hypertarget{exm:ex-encoding}{}\label{exm:ex-encoding}

Applying these techniques to the \emph{diamonds} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Ordinal encoding for \textasciigrave{}cut\textasciigrave{}}
\NormalTok{diamonds }\OtherTok{\textless{}{-}}\NormalTok{ diamonds }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cut\_encoded =} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{factor}\NormalTok{(cut, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Fair"}\NormalTok{, }\StringTok{"Good"}\NormalTok{, }\StringTok{"Very Good"}\NormalTok{, }\StringTok{"Premium"}\NormalTok{, }\StringTok{"Ideal"}\NormalTok{))))}

\CommentTok{\# Example: One{-}hot encoding for \textasciigrave{}color\textasciigrave{}}
\NormalTok{diamonds }\OtherTok{\textless{}{-}}\NormalTok{ diamonds }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{color\_D =} \FunctionTok{ifelse}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"D"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \AttributeTok{color\_E =} \FunctionTok{ifelse}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"E"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \AttributeTok{color\_F =} \FunctionTok{ifelse}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"F"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \AttributeTok{color\_G =} \FunctionTok{ifelse}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"G"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \AttributeTok{color\_H =} \FunctionTok{ifelse}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"H"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \AttributeTok{color\_I =} \FunctionTok{ifelse}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"I"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \AttributeTok{color\_J =} \FunctionTok{ifelse}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"J"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

In this example:

\begin{itemize}
\tightlist
\item
  Ordinal Encoding: We have encoded the \texttt{cut} variable based on its quality hierarchy.
\item
  One-Hot Encoding: We have applied one-hot encoding to \texttt{color}, creating binary columns for each color grade.
\end{itemize}

\end{example}

By encoding the categorical fields in this way, we transform the dataset into a format compatible with most machine learning algorithms while preserving the essential information about each categorical feature.

With our dataset now cleaned, scaled, and encoded, we are ready to move into the next stage of data analysis. In the upcoming chapter, we will explore Exploratory Data Analysis (EDA), where we will use visualizations and summary statistics to gain insights into the structure and relationships within the data. By combining the prepared data with EDA techniques, we can better understand which features may hold predictive value for our model and set the stage for successful machine learning outcomes.

\section{Case Study: Who Can Earn More Than \$50K Per Year?}\label{Data-pre-adult}

In this case study, we will explore the \emph{Adult} dataset, sourced from the \href{https://www.census.gov}{US Census Bureau}. This dataset contains demographic information about individuals, including age, education, occupation, and income. The dataset is available in the \textbf{liver} package. For more details, refer to the \href{https://www.rdocumentation.org/packages/liver/versions/1.3/topics/adult}{documentation}.

The goal of this study is to predict whether an individual earns more than \$50,000 per year based on their attributes. In Section \ref{tree-case-study} of Chapter \ref{chapter-tree}, we will apply decision tree and random forest algorithms to build a predictive model. Before applying these techniques, we need to preprocess the dataset by handling missing values, encoding categorical variables, and scaling numerical features. Let's begin by loading the dataset and examining its structure.

\subsection*{Overview of the Dataset}\label{overview-of-the-dataset}


To use the \emph{Adult} dataset, first ensure that the \textbf{liver} package is installed. If not, install it using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, load the package and dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)  }\CommentTok{\# Load liver package}
\FunctionTok{data}\NormalTok{(adult)     }\CommentTok{\# Load Adult dataset}
\end{Highlighting}
\end{Shaded}

To inspect the dataset structure, use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(adult)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{48598}\NormalTok{ obs. of  }\DecValTok{15}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age           }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{38} \DecValTok{28} \DecValTok{44} \DecValTok{18} \DecValTok{34} \DecValTok{29} \DecValTok{63} \DecValTok{24} \DecValTok{55}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ workclass     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Gov"}\NormalTok{,}\StringTok{"Never{-}worked"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{4} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{5} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ demogweight   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{226802} \DecValTok{89814} \DecValTok{336951} \DecValTok{160323} \DecValTok{103497} \DecValTok{198693} \DecValTok{227026} \DecValTok{104626} \DecValTok{369667} \DecValTok{104996}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{16}\NormalTok{ levels }\StringTok{"10th"}\NormalTok{,}\StringTok{"11th"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{12} \DecValTok{8} \DecValTok{16} \DecValTok{16} \DecValTok{1} \DecValTok{12} \DecValTok{15} \DecValTok{16} \DecValTok{6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education.num }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{7} \DecValTok{9} \DecValTok{12} \DecValTok{10} \DecValTok{10} \DecValTok{6} \DecValTok{9} \DecValTok{15} \DecValTok{10} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital.status}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Divorced"}\NormalTok{,}\StringTok{"Married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ occupation    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{15}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Adm{-}clerical"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{8} \DecValTok{6} \DecValTok{12} \DecValTok{8} \DecValTok{1} \DecValTok{9} \DecValTok{1} \DecValTok{11} \DecValTok{9} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ relationship  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{6}\NormalTok{ levels }\StringTok{"Husband"}\NormalTok{,}\StringTok{"Not{-}in{-}family"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{2} \DecValTok{5} \DecValTok{1} \DecValTok{5} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ race          }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{5}\NormalTok{ levels }\StringTok{"Amer{-}Indian{-}Eskimo"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{5} \DecValTok{5} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ gender        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"Female"}\NormalTok{,}\StringTok{"Male"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital.gain  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{7688} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{3103} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ capital.loss  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ hours.per.week}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{40} \DecValTok{50} \DecValTok{40} \DecValTok{40} \DecValTok{30} \DecValTok{30} \DecValTok{40} \DecValTok{32} \DecValTok{40} \DecValTok{10}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ native.country}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{42}\NormalTok{ levels }\StringTok{"?"}\NormalTok{,}\StringTok{"Cambodia"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{40} \DecValTok{40} \DecValTok{40} \DecValTok{40} \DecValTok{40} \DecValTok{40} \DecValTok{40} \DecValTok{40} \DecValTok{40} \DecValTok{40}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"\textless{}=50K"}\NormalTok{,}\StringTok{"\textgreater{}50K"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 48598 records and 15 variables. Of these, 14 are predictors, while the target variable, \texttt{income}, is a categorical variable with two levels: \texttt{\textless{}=50K} and \texttt{\textgreater{}50K}. The features include both numerical and categorical variables:

\begin{itemize}
\tightlist
\item
  \texttt{age}: Age in years (numerical).\\
\item
  \texttt{workclass}: Employment type (categorical, 6 levels).\\
\item
  \texttt{demogweight}: Census weighting factor (numerical).\\
\item
  \texttt{education}: Highest level of education (categorical, 16 levels).\\
\item
  \texttt{education.num}: Number of years of education (numerical).\\
\item
  \texttt{marital.status}: Marital status (categorical, 5 levels).\\
\item
  \texttt{occupation}: Job category (categorical, 15 levels).\\
\item
  \texttt{relationship}: Family relationship status (categorical, 6 levels).\\
\item
  \texttt{race}: Racial background (categorical, 5 levels).\\
\item
  \texttt{gender}: Gender identity (categorical, Male/Female).\\
\item
  \texttt{capital.gain}: Capital gains (numerical).\\
\item
  \texttt{capital.loss}: Capital losses (numerical).\\
\item
  \texttt{hours.per.week}: Hours worked per week (numerical).\\
\item
  \texttt{native.country}: Country of origin (categorical, 42 levels).\\
\item
  \texttt{income}: Target variable indicating annual income (\texttt{\textless{}=50K} or \texttt{\textgreater{}50K}).
\end{itemize}

For clarity, we categorize the dataset's variables:

\begin{itemize}
\tightlist
\item
  \textbf{Nominal variables}: \texttt{workclass}, \texttt{marital.status}, \texttt{occupation}, \texttt{relationship}, \texttt{race}, \texttt{native.country}, and \texttt{gender}.\\
\item
  \textbf{Ordinal variable}: \texttt{education}.\\
\item
  \textbf{Numerical variables}: \texttt{age}, \texttt{demogweight}, \texttt{education.num}, \texttt{capital.gain}, \texttt{capital.loss}, and \texttt{hours.per.week}.
\end{itemize}

To better understand the dataset, we generate summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(adult)}
\NormalTok{         age              workclass      demogweight             education    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.0}\NormalTok{   ?           }\SpecialCharTok{:} \DecValTok{2794}\NormalTok{   Min.   }\SpecialCharTok{:}  \DecValTok{12285}\NormalTok{   HS}\SpecialCharTok{{-}}\NormalTok{grad     }\SpecialCharTok{:}\DecValTok{15750}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{28.0}\NormalTok{   Gov         }\SpecialCharTok{:} \DecValTok{6536}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{117550}\NormalTok{   Some}\SpecialCharTok{{-}}\NormalTok{college}\SpecialCharTok{:}\DecValTok{10860}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{37.0}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{worked}\SpecialCharTok{:}   \DecValTok{10}\NormalTok{   Median }\SpecialCharTok{:} \DecValTok{178215}\NormalTok{   Bachelors   }\SpecialCharTok{:} \DecValTok{7962}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{38.6}\NormalTok{   Private     }\SpecialCharTok{:}\DecValTok{33780}\NormalTok{   Mean   }\SpecialCharTok{:} \DecValTok{189685}\NormalTok{   Masters     }\SpecialCharTok{:} \DecValTok{2627}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{48.0}\NormalTok{   Self}\SpecialCharTok{{-}}\NormalTok{emp    }\SpecialCharTok{:} \DecValTok{5457}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{237713}\NormalTok{   Assoc}\SpecialCharTok{{-}}\NormalTok{voc   }\SpecialCharTok{:} \DecValTok{2058}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{90.0}\NormalTok{   Without}\SpecialCharTok{{-}}\NormalTok{pay }\SpecialCharTok{:}   \DecValTok{21}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{1490400}   \DecValTok{11}\NormalTok{th        }\SpecialCharTok{:} \DecValTok{1812}  
\NormalTok{                                                          (Other)     }\SpecialCharTok{:} \DecValTok{7529}  
\NormalTok{    education.num         marital.status            occupation   }
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   Divorced     }\SpecialCharTok{:} \DecValTok{6613}\NormalTok{   Craft}\SpecialCharTok{{-}}\NormalTok{repair   }\SpecialCharTok{:} \DecValTok{6096}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{9.00}\NormalTok{   Married      }\SpecialCharTok{:}\DecValTok{22847}\NormalTok{   Prof}\SpecialCharTok{{-}}\NormalTok{specialty }\SpecialCharTok{:} \DecValTok{6071}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{10.00}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{married}\SpecialCharTok{:}\DecValTok{16096}\NormalTok{   Exec}\SpecialCharTok{{-}}\NormalTok{managerial}\SpecialCharTok{:} \DecValTok{6019}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{10.06}\NormalTok{   Separated    }\SpecialCharTok{:} \DecValTok{1526}\NormalTok{   Adm}\SpecialCharTok{{-}}\NormalTok{clerical   }\SpecialCharTok{:} \DecValTok{5603}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{12.00}\NormalTok{   Widowed      }\SpecialCharTok{:} \DecValTok{1516}\NormalTok{   Sales          }\SpecialCharTok{:} \DecValTok{5470}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{16.00}\NormalTok{                         Other}\SpecialCharTok{{-}}\NormalTok{service  }\SpecialCharTok{:} \DecValTok{4920}  
\NormalTok{                                          (Other)        }\SpecialCharTok{:}\DecValTok{14419}  
\NormalTok{            relationship                   race          gender     }
\NormalTok{    Husband       }\SpecialCharTok{:}\DecValTok{19537}\NormalTok{   Amer}\SpecialCharTok{{-}}\NormalTok{Indian}\SpecialCharTok{{-}}\NormalTok{Eskimo}\SpecialCharTok{:}  \DecValTok{470}\NormalTok{   Female}\SpecialCharTok{:}\DecValTok{16156}  
\NormalTok{    Not}\SpecialCharTok{{-}}\ControlFlowTok{in}\SpecialCharTok{{-}}\NormalTok{family }\SpecialCharTok{:}\DecValTok{12546}\NormalTok{   Asian}\SpecialCharTok{{-}}\NormalTok{Pac}\SpecialCharTok{{-}}\NormalTok{Islander}\SpecialCharTok{:} \DecValTok{1504}\NormalTok{   Male  }\SpecialCharTok{:}\DecValTok{32442}  
\NormalTok{    Other}\SpecialCharTok{{-}}\NormalTok{relative}\SpecialCharTok{:} \DecValTok{1506}\NormalTok{   Black             }\SpecialCharTok{:} \DecValTok{4675}                 
\NormalTok{    Own}\SpecialCharTok{{-}}\NormalTok{child     }\SpecialCharTok{:} \DecValTok{7577}\NormalTok{   Other             }\SpecialCharTok{:}  \DecValTok{403}                 
\NormalTok{    Unmarried     }\SpecialCharTok{:} \DecValTok{5118}\NormalTok{   White             }\SpecialCharTok{:}\DecValTok{41546}                 
\NormalTok{    Wife          }\SpecialCharTok{:} \DecValTok{2314}                                            
                                                                    
\NormalTok{     capital.gain      capital.loss     hours.per.week        native.country }
\NormalTok{    Min.   }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   United}\SpecialCharTok{{-}}\NormalTok{States}\SpecialCharTok{:}\DecValTok{43613}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   Mexico       }\SpecialCharTok{:}  \DecValTok{949}  
\NormalTok{    Median }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Median }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   ?            }\SpecialCharTok{:}  \DecValTok{847}  
\NormalTok{    Mean   }\SpecialCharTok{:}  \FloatTok{582.4}\NormalTok{   Mean   }\SpecialCharTok{:}  \FloatTok{87.94}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{40.37}\NormalTok{   Philippines  }\SpecialCharTok{:}  \DecValTok{292}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{45.00}\NormalTok{   Germany      }\SpecialCharTok{:}  \DecValTok{206}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{41310.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{4356.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{99.00}\NormalTok{   Puerto}\SpecialCharTok{{-}}\NormalTok{Rico  }\SpecialCharTok{:}  \DecValTok{184}  
\NormalTok{                                                        (Other)      }\SpecialCharTok{:} \DecValTok{2507}  
\NormalTok{      income     }
    \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K}\SpecialCharTok{:}\DecValTok{37155}  
    \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K }\SpecialCharTok{:}\DecValTok{11443}  
                 
                 
                 
                 
   
\end{Highlighting}
\end{Shaded}

This summary provides insights into the distribution of numerical variables, missing values, and categorical variable levels, guiding us in preparing the data for further analysis.

\subsection{Missing Values}\label{missing-values-1}

The \texttt{summary()} function reveals that the variables \texttt{workclass} and \texttt{native.country} contain missing values, represented by the \texttt{"?"} category. Specifically, 2794 records in \texttt{workclass} and 847 records in \texttt{native.country} have missing values. Since \texttt{"?"} is used as a placeholder for missing data, we first convert these entries to \texttt{NA}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult[adult }\SpecialCharTok{==} \StringTok{"?"}\NormalTok{] }\OtherTok{=} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

After replacing \texttt{"?"} with \texttt{NA}, we remove unused factor levels to clean up the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult }\OtherTok{=} \FunctionTok{droplevels}\NormalTok{(adult)}
\end{Highlighting}
\end{Shaded}

To visualize the distribution of missing values, we use the \texttt{gg\_miss\_var()} function from the \textbf{naniar} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)  }\CommentTok{\# Load package for visualizing missing values}

\FunctionTok{gg\_miss\_var}\NormalTok{(adult, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-18-1} \end{center}

The plot indicates that \texttt{workclass}, \texttt{occupation}, and \texttt{native.country} contain missing values. The percentage of missing values in these variables is relatively low, with \texttt{workclass} and \texttt{occupation} having less than 0.06 percent missing data, while \texttt{native.country} has about 0.02 percent.

\subsubsection*{Imputing Missing Values}\label{imputing-missing-values}


Instead of removing records with missing values, which can lead to information loss, we apply \emph{random imputation}, where missing values are filled with randomly selected values from the existing distribution of each variable. This maintains the natural proportions of each category.

We use the \texttt{impute()} function from the \textbf{Hmisc} package for this purpose:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)  }\CommentTok{\# Load package for imputation}

\CommentTok{\# Impute missing values using random sampling from existing categories}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass      }\OtherTok{=} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass,      }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{=} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country, }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{occupation     }\OtherTok{=} \FunctionTok{impute}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{occupation,     }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To confirm that missing values have been successfully imputed, we generate another missing values plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gg\_miss\_var}\NormalTok{(adult, }\AttributeTok{show\_pct =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-20-1} \end{center}

The updated plot should show no missing values, indicating successful imputation.

\subsubsection*{Alternative Approaches}\label{alternative-approaches}


The \texttt{impute()} function allows for different statistical methods such as mean, median, or mode imputation. Its default behavior is \emph{median} imputation. For more advanced techniques, the \texttt{aregImpute()} function from the \textbf{Hmisc} package offers predictive imputation using additive regression, bootstrapping, and predictive mean matching.

Although removing records with missing values using \texttt{na.omit()} is an option, it is generally discouraged unless missing values are excessive or biased in a way that could distort analysis.

By properly handling missing values, we ensure data completeness and maintain the integrity of the dataset for subsequent preprocessing steps, such as recoding categorical variables and grouping country-level data into broader regions.

\subsection{Encoding Categorical Variables}\label{encoding-categorical-variables}

Categorical variables often contain a large number of unique values, making them challenging to use in predictive models. In the \emph{Adult} dataset, \texttt{native.country} and \texttt{workclass} have multiple categories, which can introduce complexity and redundancy. To simplify these variables, we group similar categories together while preserving their interpretability.

\subsubsection*{\texorpdfstring{Grouping \texttt{native.country} by Continent}{Grouping native.country by Continent}}\label{grouping-native.country-by-continent}


The \texttt{native.country} variable contains 41 distinct countries. To make it more manageable, we categorize countries into broader geographical regions:

\begin{itemize}
\tightlist
\item
  \emph{Europe}: England, France, Germany, Greece, Netherlands, Hungary, Ireland, Italy, Poland, Portugal, Scotland, Yugoslavia\\
\item
  \emph{Asia}: China, Hong Kong, India, Iran, Cambodia, Japan, Laos, Philippines, Vietnam, Taiwan, Thailand\\
\item
  \emph{North America}: Canada, United States, Puerto Rico\\
\item
  \emph{South America}: Colombia, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Mexico, Nicaragua, Outlying US territories, Peru, Jamaica, Trinidad \& Tobago\\
\item
  \emph{Other}: This includes the ambiguous ``South'' category, as its meaning is unclear in the dataset documentation.\\
  We use the \texttt{fct\_collapse()} function from the \textbf{forcats} package to reassign categories:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forcats)  }\CommentTok{\# Load package for categorical variable transformation}

\CommentTok{\# To create a new factor variable with fewer levels for \textasciigrave{}native.country\textasciigrave{}}
\NormalTok{Europe }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"England"}\NormalTok{, }\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Greece"}\NormalTok{, }\StringTok{"Holand{-}Netherlands"}\NormalTok{, }\StringTok{"Hungary"}\NormalTok{, }\StringTok{"Ireland"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Poland"}\NormalTok{, }\StringTok{"Portugal"}\NormalTok{, }\StringTok{"Scotland"}\NormalTok{, }\StringTok{"Yugoslavia"}\NormalTok{)}

\NormalTok{Asia }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"China"}\NormalTok{, }\StringTok{"Hong"}\NormalTok{, }\StringTok{"India"}\NormalTok{, }\StringTok{"Iran"}\NormalTok{, }\StringTok{"Cambodia"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"Laos"}\NormalTok{, }\StringTok{"Philippines"}\NormalTok{, }\StringTok{"Vietnam"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{, }\StringTok{"Thailand"}\NormalTok{)}

\NormalTok{N.America }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"United{-}States"}\NormalTok{, }\StringTok{"Puerto{-}Rico"}\NormalTok{)}

\NormalTok{S.America }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Columbia"}\NormalTok{, }\StringTok{"Cuba"}\NormalTok{, }\StringTok{"Dominican{-}Republic"}\NormalTok{, }\StringTok{"Ecuador"}\NormalTok{, }\StringTok{"El{-}Salvador"}\NormalTok{, }\StringTok{"Guatemala"}\NormalTok{, }\StringTok{"Haiti"}\NormalTok{, }\StringTok{"Honduras"}\NormalTok{, }\StringTok{"Mexico"}\NormalTok{, }\StringTok{"Nicaragua"}\NormalTok{, }\StringTok{"Outlying{-}US(Guam{-}USVI{-}etc)"}\NormalTok{, }\StringTok{"Peru"}\NormalTok{, }\StringTok{"Jamaica"}\NormalTok{, }\StringTok{"Trinadad\&Tobago"}\NormalTok{)}

\CommentTok{\# Reclassify native.country into broader regions}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{=} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country, }
                                    \StringTok{"Europe"}    \OtherTok{=}\NormalTok{ Europe,}
                                    \StringTok{"Asia"}      \OtherTok{=}\NormalTok{ Asia,}
                                    \StringTok{"N.America"} \OtherTok{=}\NormalTok{ N.America,}
                                    \StringTok{"S.America"} \OtherTok{=}\NormalTok{ S.America,}
                                    \StringTok{"Other"}     \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"South"}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

To confirm the changes, we display the frequency distribution of \texttt{native.country}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country)}
   
\NormalTok{        Asia N.America S.America    Europe     Other }
         \DecValTok{993}     \DecValTok{44747}      \DecValTok{1946}       \DecValTok{797}       \DecValTok{115}
\end{Highlighting}
\end{Shaded}

By grouping the original 42 countries into 5 broader regions, we simplify the variable while maintaining its relevance for analysis.

\subsubsection*{\texorpdfstring{Simplifying \texttt{workclass}}{Simplifying workclass}}\label{simplifying-workclass}


The \texttt{workclass} variable originally contains several employment categories. Since ``Never-worked'' and ``Without-pay'' represent similar employment statuses, we merge them into a single category labeled ``Unemployed'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass }\OtherTok{=} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass, }\StringTok{"Unemployed"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Never{-}worked"}\NormalTok{, }\StringTok{"Without{-}pay"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To verify the updated categories, we check the frequency distribution:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass)}
   
\NormalTok{          Gov Unemployed    Private   Self}\SpecialCharTok{{-}}\NormalTok{emp }
         \DecValTok{6919}         \DecValTok{32}      \DecValTok{35851}       \DecValTok{5796}
\end{Highlighting}
\end{Shaded}

By reducing the number of unique categories in \texttt{workclass} and \texttt{native.country}, we improve model interpretability and reduce the risk of overfitting when applying machine learning algorithms.

\subsection{Outliers}\label{outliers}

Detecting and handling outliers is an essential step in data preprocessing, as extreme values can significantly impact statistical analysis and model performance. Here, we examine potential outliers in the \texttt{capital.loss} variable to determine whether adjustments are necessary.

\subsubsection*{Summary Statistics}\label{summary-statistics}


To gain an initial understanding of \texttt{capital.loss}, we compute its summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{capital.loss)}
\NormalTok{      Min. }\DecValTok{1}\NormalTok{st Qu.  Median    Mean }\DecValTok{3}\NormalTok{rd Qu.    Max. }
      \FloatTok{0.00}    \FloatTok{0.00}    \FloatTok{0.00}   \FloatTok{87.94}    \FloatTok{0.00} \FloatTok{4356.00}
\end{Highlighting}
\end{Shaded}

The summary output reveals the following insights:

\begin{itemize}
\tightlist
\item
  The minimum value is 0, while the maximum is 4356.\\
\item
  The \emph{median} is 0, which is significantly lower than the \emph{mean}, indicating a highly skewed distribution.\\
\item
  More than 75\% of the observations have a capital loss of 0, confirming a strong right-skew.\\
\item
  The mean capital loss is 87.94, which is influenced by a small number of extreme values.
\end{itemize}

\subsubsection*{Visualizing Outliers}\label{visualizing-outliers}


To further investigate the distribution of \texttt{capital.loss}, we use a boxplot and histogram:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ capital.loss)) }\SpecialCharTok{+}
     \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-26-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ capital.loss)) }\SpecialCharTok{+}
     \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{30}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-27-1} \end{center}

From these plots, we observe:

\begin{itemize}
\tightlist
\item
  The boxplot shows a strong \emph{positive skew}, with many extreme values above the upper whisker.\\
\item
  The histogram indicates that most observations have \emph{zero capital loss}, with a few cases around 2,000 and 4,000.
\end{itemize}

Since a large proportion of observations report no capital loss, we further examine the nonzero cases.

\subsubsection*{Zooming into the Nonzero Distribution}\label{zooming-into-the-nonzero-distribution}


To better visualize the spread of nonzero values, we focus on observations with \texttt{capital.loss\ \textgreater{}\ 0}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ capital.loss)) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{30}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{4000}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-28-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \FunctionTok{subset}\NormalTok{(adult, capital.loss }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
     \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ capital.loss)) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{3_Data_Preparation_files/figure-latex/unnamed-chunk-29-1} \end{center}

Key takeaways from these refined plots:

\begin{itemize}
\tightlist
\item
  The majority of nonzero values are below 500, with a small number extending beyond 4,000.\\
\item
  The distribution of nonzero values is approximately symmetric, suggesting that while there are extreme values, they follow a structured pattern rather than random anomalies.
\end{itemize}

\subsubsection*{Handling Outliers}\label{handling-outliers}


Although \texttt{capital.loss} contains many high values, these do not appear to be erroneous. Instead, they reflect genuine cases within the dataset. Since these values provide meaningful information about particular individuals, we retain them rather than applying transformations or removals.

However, if model performance is significantly affected by these extreme values, we might consider:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Winsorization}: Capping values at a reasonable percentile (e.g., the 95th percentile).\\
\item
  \emph{Log Transformation}: Applying a log transformation to reduce skewness.\\
\item
  \emph{Creating a Binary Indicator}: Introducing a new variable indicating whether a capital loss occurred (\texttt{capital.loss\ \textgreater{}\ 0}).
\end{enumerate}

Next, we perform a similar outlier analysis for the \texttt{capital.gain} variable. See the exercises below for a guided approach.

\section{Exercises}\label{exercises-1}

This section provides hands-on exercises to reinforce the key concepts covered in this chapter. These questions include theoretical, exploratory, and practical challenges related to data types, outliers, encoding techniques, and feature engineering.

\subsection*{Understanding Data Types}\label{understanding-data-types}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the difference between continuous and discrete numerical variables? Provide an example of each from real-world data.\\
\item
  How do ordinal categorical variables differ from nominal categorical variables? Give an example for both.
\end{enumerate}

\subsection*{Exploring the diamonds Dataset}\label{exploring-the-diamonds-dataset}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Report the summary statistics for the diamonds dataset using the \texttt{summary()} function. What insights can you derive from the output?\\
\item
  In the diamonds dataset, which variables are nominal, ordinal, and numerical? List them accordingly.
\end{enumerate}

\subsection*{Detecting and Handling Outliers}\label{detecting-and-handling-outliers}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Identify outliers in the variable \texttt{x}. If any exist, handle them appropriately. Follow the same approach as in Section \ref{Data-pre-outliers} for the \texttt{y} variable in the diamonds dataset.\\
\item
  Repeat the outlier detection process for the variable \texttt{z}. If necessary, apply transformations or filtering techniques.\\
\item
  Check for outliers in the \texttt{depth} variable. What method would you use to detect and handle them?
\end{enumerate}

\subsection*{Encoding Categorical Variables}\label{encoding-categorical-variables-1}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  The \texttt{cut} variable in the diamonds dataset is ordinal. How can we encode it properly using ordinal encoding?\\
\item
  The \texttt{color} variable in the diamonds dataset is nominal. How can we encode it using one-hot encoding?
\end{enumerate}

\subsection*{Analyzing the Adult Dataset}\label{analyzing-the-adult-dataset}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  Load the Adult dataset from the \textbf{liver} package and examine its structure. Identify the categorical variables and classify them as nominal or ordinal.\\
\item
  Compute the proportion of individuals who earn more than 50K (\texttt{\textgreater{}50K}). What does this distribution tell you about income levels in this dataset?\\
\item
  For the Adult dataset, generate the summary statistics, boxplot, and histogram for the variable \texttt{capital.gain}. What do you observe?\\
\item
  Based on the visualizations from the previous question, are there outliers in the \texttt{capital.gain} variable? If so, suggest a strategy to handle them.
\end{enumerate}

\subsection*{Feature Engineering Challenge}\label{feature-engineering-challenge}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{13}
\tightlist
\item
  Create a new categorical variable \texttt{Age\_Group} in the Adult dataset, grouping ages into:\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Young (â‰¤30 years old)\\
\item
  Middle-aged (31-50 years old)\\
\item
  Senior (\textgreater50 years old)\\
  Use the \texttt{cut()} function to implement this transformation.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\tightlist
\item
  Compute the mean \texttt{capital.gain} for each \texttt{Age\_Group}. What insights do you gain about income levels across different age groups?
\end{enumerate}

\subsection*{Advanced Data Preparation Challenges}\label{advanced-data-preparation-challenges}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  In the Adult dataset, the \texttt{education} variable contains 16 distinct levels. Reduce these categories into broader groups such as ``No Diploma,'' ``High School Graduate,'' ``Some College,'' and ``Postgraduate.'' Implement this transformation using the \texttt{fct\_collapse()} function.
\item
  The \texttt{capital.gain} and \texttt{capital.loss} variables represent financial assets. Create a new variable \texttt{net.capital} that computes the difference between \texttt{capital.gain} and \texttt{capital.loss}. Analyze its distribution.
\item
  Perform Min-Max scaling on the numerical variables in the Adult dataset (\texttt{age}, \texttt{capital.gain}, \texttt{capital.loss}, \texttt{hours.per.week}). Use the \texttt{mutate()} function to apply this transformation.
\item
  Perform Z-score normalization on the same set of numerical variables. Compare the results with Min-Max scaling. In what scenarios would one approach be preferable over the other?
\item
  Construct a logistic regression model to predict whether an individual earns more than 50K (\texttt{\textgreater{}50K}) based on selected numerical features (\texttt{age}, \texttt{education.num}, \texttt{hours.per.week}). Preprocess the data accordingly and interpret the coefficients of the model.
\end{enumerate}

\chapter{Exploratory Data Analysis}\label{chapter-EDA}

Exploratory Data Analysis (EDA) is a fundamental step in data science that helps uncover insights, detect patterns, and understand relationships within a dataset. Before applying statistical models or machine learning algorithms, it is essential to explore and visualize data to identify inconsistencies, anomalies, and potential predictors. EDA provides a structured yet flexible approach to data exploration, allowing data scientists to make informed decisions before proceeding with modeling.

EDA is an iterative process rather than a rigid sequence of steps. It encourages curiosity and adaptability, as different datasets may present unique challenges or insights. Some exploratory paths may lead to inconclusive findings, while others uncover valuable patterns that shape the direction of analysis. As familiarity with the data increases, analysts can refine their focus, identifying the most relevant features and variables for modeling.

The primary goal of EDA is to explore and summarize data rather than to perform hypothesis testing or confirm specific relationships. By using summary statistics, visualizations, and preliminary correlation analysis, EDA provides a foundational understanding of the dataset. However, these insights remain preliminary and require further validation through statistical testing or predictive modeling. Recognizing this distinction ensures that exploratory findings are interpreted cautiously and do not lead to premature conclusions.

Another key consideration in EDA is balancing statistical significance with practical relevance. Large datasets often reveal statistically significant relationships that may lack meaningful real-world implications. For example, a weak correlation between customer engagement and churn might be statistically significant yet offer little actionable insight for business decision-making. EDA encourages analysts to integrate domain expertise and practical considerations when interpreting patterns in data.

EDA also plays a crucial role in data cleaning and preparation. Missing values, inconsistencies, and outliers often become apparent during exploration, requiring careful handling to ensure data quality. While data cleaning is a distinct process, it is closely linked to EDA, as identifying and resolving data issues early helps establish a solid foundation for further analysis and modeling.

Selecting appropriate tools and techniques for EDA depends on the nature of the dataset and the specific analytical questions. Histograms and box plots help visualize distributions, while scatter plots and correlation matrices assess relationships between variables. The following sections will demonstrate these techniques through practical applications, including an in-depth analysis of the churn dataset to illustrate how EDA can uncover patterns relevant to customer retention.

The key objectives of EDA are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Understanding the structure of the data} -- Determine data types, variable ranges, the number of observations, and the presence of missing values or anomalies.\\
\item
  \textbf{Analyzing individual variable distributions} -- Examine numerical and categorical variables to assess their distribution, central tendency, and spread.\\
\item
  \textbf{Exploring relationships between variables} -- Identify correlations, dependencies, or interactions that may influence predictive modeling.\\
\item
  \textbf{Detecting patterns and outliers} -- Identify unusual data points and assess whether they result from data errors or represent meaningful trends.
\end{enumerate}

These objectives ensure a comprehensive understanding of the dataset, forming a strong foundation for subsequent modeling and analysis.

\section{Guiding Questions for EDA}\label{guiding-questions-for-eda}

Exploratory data analysis is most effective when structured around key questions that help uncover meaningful patterns and relationships. These questions generally fall into two broad categories: univariate and multivariate analysis.

Univariate analysis examines individual variables to assess their distributions, central tendencies, variability, and potential data quality issues such as missing values or outliers. Typical univariate questions include:

\begin{itemize}
\tightlist
\item
  What is the distribution of the target variable?\\
\item
  How are numerical variables, such as income or age, distributed?\\
\item
  Are there missing values, and how are they distributed across the dataset?
\end{itemize}

This analysis helps detect skewness, irregularities, and unexpected patterns that may impact later modeling. Common tools for univariate analysis include histograms, box plots, and summary statistics such as the mean, median, quartiles, and standard deviation.

Multivariate analysis explores relationships between two or more variables, identifying dependencies, interactions, or correlations that could influence predictive modeling. Key multivariate questions include:

\begin{itemize}
\tightlist
\item
  How does the target variable relate to predictor variables?\\
\item
  Are some predictors highly correlated, suggesting potential multicollinearity?\\
\item
  How do categorical and numerical variables interact?
\end{itemize}

To analyze these relationships, data scientists commonly use scatter plots, correlation matrices, and pairwise comparisons, which help visualize dependencies between variables and guide feature selection.

A common challenge in EDA is choosing the most appropriate visualization or statistical summary for a given analysis. The selection depends on both the type of data and the insight being sought. The table below provides a structured guide to selecting the most effective tools for various exploratory tasks:

\begin{table}

\caption{\label{tab:EDA-table-tools}EDA Tool Selection Guide.}
\centering
\begin{tabular}[t]{lll}
\toprule
EDA.Objective & Data.Type & Recommended.Tools\\
\midrule
Understand the distribution of a variable & Numerical (continuous/discrete) & Histogram, box plot, density plot, summary statistics\\
Examine a categorical variable's distribution & Categorical & Bar chart, frequency table\\
Identify outliers & Numerical & Box plot, histogram, Z-score analysis\\
Detect missing values & Any & Heatmap, summary statistics, missing data patterns\\
Analyze the relationship between two numerical variables & Numerical \& numerical & Scatter plot, correlation coefficient, regression line\\
\addlinespace
Compare a numerical variable across categories & Categorical \& numerical & Box plot, violin plot, grouped bar chart\\
Explore interactions between two categorical variables & Categorical \& categorical & Stacked bar chart, mosaic plot, contingency table\\
Assess correlation among multiple numerical variables & Multiple numerical & Correlation matrix, pair plot (scatterplot matrix)\\
Investigate complex multivariate relationships & Multiple variables & Facet grid, parallel coordinates, principal component analysis (PCA)\\
\bottomrule
\end{tabular}
\end{table}

By systematically addressing these univariate and multivariate questions with appropriate EDA techniques, we gain a deeper understanding of the dataset's structure and key patterns. This process not only improves data quality but also provides valuable insights that inform subsequent modeling and decision-making.

\section{EDA as Data Storytelling}\label{eda-as-data-storytelling}

Exploratory Data Analysis is more than just a technical step; it is a way to uncover and communicate meaningful insights from data. Beyond summarizing numbers and visualizing patterns, EDA helps shape the narrative hidden within the data. Effective data storytelling integrates data, visuals, and context, making findings more accessible and actionable. Whether communicating with data scientists, business professionals, or decision-makers, presenting insights clearly is essential.

While summary statistics provide an overview, visualizations reveal patterns, relationships, and anomalies that might otherwise go unnoticed. Different types of visualizations serve distinct purposes. Scatter plots and correlation matrices highlight relationships between numerical variables, while histograms and box plots illustrate distributions and potential skewness. Categorical data is best explored with bar charts or stacked visualizations, allowing comparisons across different groups. Choosing the right visualization ensures insights are both accurate and intuitive.

A strong narrative connects data insights with real-world significance. Rather than simply presenting a correlation coefficient or a distribution plot, a well-structured EDA report explains why a pattern matters and how it informs decision-making. Instead of stating that customers with high daytime phone usage have a higher churn rate, it is more impactful to provide context:

\emph{``Customers with extensive daytime usage are significantly more likely to churn, possibly due to pricing concerns or dissatisfaction with service quality. Targeted retention strategies, such as customized discounts or flexible pricing plans, may help mitigate this risk.''}

This approach goes beyond numerical reporting, framing insights as actionable strategies.

Data storytelling is widely used in business, scientific research, and journalism. Consider the following examples that illustrate how visual storytelling enhances understanding.

One example comes from climate science. Figure \ref{fig:EDA-fig-1} presents global mean surface temperature changes over the Common Era, highlighting long-term warming trends. Adapted from \citet{neukom2019no}, this visualization provides a historical perspective on climate change, illustrating temperature anomalies over time.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/ch4_EDA_fig_1} 

}

\caption{Global mean surface temperature history over the Common Era. Temperature anomalies with respect to 1961â€“1990 CE. The colored lines represent 30-year low-pass-filtered ensemble medians for different reconstruction methods.}\label{fig:EDA-fig-1}
\end{figure}

Another example focuses on global health and demographics. Figure \ref{fig:EDA-fig-2} illustrates the relationship between fertility rate and life expectancy across world regions from 1960 to 2015. Adapted from Hans Rosling's TED Talk \href{https://www.ted.com/talks/hans_rosling_new_insights_on_poverty}{``New insights on poverty''}, this visualization effectively conveys trends in population health and economic development over time.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/EDA-fig-2-1} 

}

\caption{Animated scatter plot of fertility rate and life expectancy at birth for different world regions from 1960 to 2015.}\label{fig:EDA-fig-2}
\end{figure}

These examples highlight how well-designed visualizations can make complex data more accessible and engaging.

When conducting EDA, it is essential to consider the broader narrative. Instead of just reporting statistical results, think about what a trend reveals, why it is relevant, and how it can inform decision-making. Integrating storytelling techniques ensures that EDA serves not only as a data exploration step but also as a communication tool that connects technical analysis with practical application.

\section{\texorpdfstring{EDA in Practice: The \emph{Churn} Dataset}{EDA in Practice: The Churn Dataset}}\label{EDA-sec-churn}

To illustrate the exploratory data analysis process, we will use the \emph{churn} dataset, which contains information about customer behavior, including whether a customer has churned (i.e., left the service) and various demographic and behavioral attributes.

EDA will help us understand this dataset by identifying patterns related to customer churn, determining which features influence retention, and establishing a structured approach for predictive modeling. By examining summary statistics and visualizations, we can extract meaningful insights before moving on to machine learning techniques in later chapters.

\subsection*{Problem Understanding}\label{problem-understanding-1}


Companies seek to minimize customer churn by identifying factors that influence customer decisions. Key business questions include:

\begin{itemize}
\tightlist
\item
  \textbf{Why} are customers leaving?\\
\item
  \textbf{What} are the primary factors contributing to churn?\\
\item
  \textbf{How} can we take action to improve retention?
\end{itemize}

EDA helps answer these questions by uncovering trends and patterns in customer behavior. In Chapter \ref{chapter-knn}, we will build a predictive model to identify customers likely to churn. Before doing so, it is essential to explore the data and understand its structure.

\subsection*{Data Understanding}\label{data-understanding}


The \emph{churn} dataset comes from IBM Sample Data Sets and contains 5000 customer records across 20 variables. This dataset is available in the \textbf{liver} package. The target variable, \texttt{churn}, indicates whether a customer has left the company. The dataset includes a mix of categorical and numerical variables:

\begin{itemize}
\tightlist
\item
  \texttt{state} : Categorical, state in the U.S. (51 states + D.C.).
\item
  \texttt{area.code} : Categorical, area code assigned to the customer.
\item
  \texttt{account.length} : Numerical (discrete), duration of account activity (days).
\item
  \texttt{voice.plan} : Categorical (binary), subscription to a voice mail plan (yes/no).
\item
  \texttt{voice.messages} : Numerical (discrete), number of voice mail messages.
\item
  \texttt{intl.plan} : Categorical (binary), subscription to an international calling plan (yes/no).
\item
  \texttt{intl.mins} : Numerical (continuous), total international call minutes.
\item
  \texttt{intl.calls} : Numerical (discrete), total international calls made.
\item
  \texttt{intl.charge} : Numerical (continuous), total international call charges.
\item
  \texttt{day.mins} : Numerical (continuous), total daytime call minutes.
\item
  \texttt{day.calls} : Numerical (discrete), total daytime calls made.
\item
  \texttt{day.charge} : Numerical (continuous), total daytime call charges.
\item
  \texttt{eve.mins} : Numerical (continuous), total evening call minutes.
\item
  \texttt{eve.calls} : Numerical (discrete), total evening calls made.
\item
  \texttt{eve.charge} : Numerical (continuous), total evening call charges.
\item
  \texttt{night.mins} : Numerical (continuous), total nighttime call minutes.
\item
  \texttt{night.calls} : Numerical (discrete), total nighttime calls made.
\item
  \texttt{night.charge} : Numerical (continuous), total nighttime call charges.
\item
  \texttt{customer.calls} : Numerical (discrete), number of customer service calls made.
\item
  \texttt{churn} : Categorical (binary), indicates whether the customer churned (yes/no).
\end{itemize}

To use this dataset, we first load the \textbf{liver} package and then import the dataset into \textbf{R}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{data}\NormalTok{(churn)  }
\end{Highlighting}
\end{Shaded}

To examine the structure of the dataset, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(churn)  }
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{5000}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{17} \DecValTok{36} \DecValTok{32} \DecValTok{36} \DecValTok{37} \DecValTok{2} \DecValTok{20} \DecValTok{25} \DecValTok{19} \DecValTok{50}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area.code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account.length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{128} \DecValTok{107} \DecValTok{137} \DecValTok{84} \DecValTok{75} \DecValTok{118} \DecValTok{121} \DecValTok{147} \DecValTok{117} \DecValTok{141}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.plan    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{26} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{24} \DecValTok{0} \DecValTok{0} \DecValTok{37}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.plan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.mins     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \FloatTok{13.7} \FloatTok{12.2} \FloatTok{6.6} \FloatTok{10.1} \FloatTok{6.3} \FloatTok{7.5} \FloatTok{7.1} \FloatTok{8.7} \FloatTok{11.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{5} \DecValTok{7} \DecValTok{3} \DecValTok{6} \DecValTok{7} \DecValTok{6} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{2.7} \FloatTok{3.7} \FloatTok{3.29} \FloatTok{1.78} \FloatTok{2.73} \FloatTok{1.7} \FloatTok{2.03} \FloatTok{1.92} \FloatTok{2.35} \FloatTok{3.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{265} \DecValTok{162} \DecValTok{243} \DecValTok{299} \DecValTok{167}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{110} \DecValTok{123} \DecValTok{114} \DecValTok{71} \DecValTok{113} \DecValTok{98} \DecValTok{88} \DecValTok{79} \DecValTok{97} \DecValTok{84}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{45.1} \FloatTok{27.5} \FloatTok{41.4} \FloatTok{50.9} \FloatTok{28.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.mins      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{197.4} \FloatTok{195.5} \FloatTok{121.2} \FloatTok{61.9} \FloatTok{148.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{99} \DecValTok{103} \DecValTok{110} \DecValTok{88} \DecValTok{122} \DecValTok{101} \DecValTok{108} \DecValTok{94} \DecValTok{80} \DecValTok{111}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{16.78} \FloatTok{16.62} \FloatTok{10.3} \FloatTok{5.26} \FloatTok{12.61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.mins    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{245} \DecValTok{254} \DecValTok{163} \DecValTok{197} \DecValTok{187}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{91} \DecValTok{103} \DecValTok{104} \DecValTok{89} \DecValTok{121} \DecValTok{118} \DecValTok{118} \DecValTok{96} \DecValTok{90} \DecValTok{97}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{11.01} \FloatTok{11.45} \FloatTok{7.32} \FloatTok{8.86} \FloatTok{8.41}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer.calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{0} \DecValTok{3} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

This command reveals that the dataset is stored as a \texttt{data.frame} with 5000 observations and 20 variables. The target variable, \texttt{churn}, categorizes whether a customer has left the service.

To summarize the dataset, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(churn)}
\NormalTok{        state              area.code    account.length  voice.plan}
\NormalTok{    WV     }\SpecialCharTok{:} \DecValTok{158}\NormalTok{   area\_code\_408}\SpecialCharTok{:}\DecValTok{1259}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{1.0}\NormalTok{   yes}\SpecialCharTok{:}\DecValTok{1323}  
\NormalTok{    MN     }\SpecialCharTok{:} \DecValTok{125}\NormalTok{   area\_code\_415}\SpecialCharTok{:}\DecValTok{2495}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{73.0}\NormalTok{   no }\SpecialCharTok{:}\DecValTok{3677}  
\NormalTok{    AL     }\SpecialCharTok{:} \DecValTok{124}\NormalTok{   area\_code\_510}\SpecialCharTok{:}\DecValTok{1246}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{100.0}             
\NormalTok{    ID     }\SpecialCharTok{:} \DecValTok{119}\NormalTok{                        Mean   }\SpecialCharTok{:}\FloatTok{100.3}             
\NormalTok{    VA     }\SpecialCharTok{:} \DecValTok{118}                        \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{127.0}             
\NormalTok{    OH     }\SpecialCharTok{:} \DecValTok{116}\NormalTok{                        Max.   }\SpecialCharTok{:}\FloatTok{243.0}             
\NormalTok{    (Other)}\SpecialCharTok{:}\DecValTok{4240}                                                  
\NormalTok{    voice.messages   intl.plan    intl.mins       intl.calls      intl.charge   }
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{0.000}\NormalTok{   yes}\SpecialCharTok{:} \DecValTok{473}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{0.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{0.000}\NormalTok{   no }\SpecialCharTok{:}\DecValTok{4527}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{8.50}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{3.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.300}  
\NormalTok{    Median }\SpecialCharTok{:} \FloatTok{0.000}\NormalTok{              Median }\SpecialCharTok{:}\FloatTok{10.30}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{4.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{2.780}  
\NormalTok{    Mean   }\SpecialCharTok{:} \FloatTok{7.755}\NormalTok{              Mean   }\SpecialCharTok{:}\FloatTok{10.26}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{4.435}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{2.771}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{17.000}              \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{12.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{6.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.240}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{52.000}\NormalTok{              Max.   }\SpecialCharTok{:}\FloatTok{20.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{20.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{5.400}  
                                                                                
\NormalTok{       day.mins       day.calls     day.charge       eve.mins       eve.calls    }
\NormalTok{    Min.   }\SpecialCharTok{:}  \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}  \DecValTok{0}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.0}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{143.7}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{87}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{24.43}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{166.4}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{87.0}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{180.1}\NormalTok{   Median }\SpecialCharTok{:}\DecValTok{100}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{30.62}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{201.0}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{100.0}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{180.3}\NormalTok{   Mean   }\SpecialCharTok{:}\DecValTok{100}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{30.65}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{200.6}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{100.2}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{216.2}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{113}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{36.75}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{234.1}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{114.0}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{351.5}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{165}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{59.76}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{363.7}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{170.0}  
                                                                                 
\NormalTok{      eve.charge      night.mins     night.calls      night.charge   }
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{14.14}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{166.9}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{87.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{7.510}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{17.09}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{200.4}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{100.00}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{9.020}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{17.05}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{200.4}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{99.92}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{9.018}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{19.90}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{234.7}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{113.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{10.560}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{30.91}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{395.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{175.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{17.770}  
                                                                     
\NormalTok{    customer.calls churn     }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{0.00}\NormalTok{   yes}\SpecialCharTok{:} \DecValTok{707}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.00}\NormalTok{   no }\SpecialCharTok{:}\DecValTok{4293}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{1.00}             
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{1.57}             
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{2.00}             
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{9.00}             
   
\end{Highlighting}
\end{Shaded}

This function provides a high-level overview of all variables, including their distributions and any potential missing values. The dataset is clean and ready for exploratory data analysis. In the next sections, we will explore its structure using visualizations and statistical summaries. This will help us identify key variables influencing churn and ensure the dataset is well-prepared for predictive modeling.

One notable observation is that there are 51 unique states represented in the dataset, yet only 3 unique area codes. This suggests that area codes do not correspond directly to state locations, which is worth investigating further.

\section{Investigating Categorical Variables}\label{chapter-EDA-categorical}

Categorical variables represent discrete values such as labels, names, or binary indicators. In the \emph{churn} dataset, key categorical features include \texttt{state}, \texttt{area.code}, \texttt{voice.plan}, and \texttt{intl.plan}. Understanding their distributions and relationships with the target variable helps uncover trends that may influence customer retention.

To begin, we examine the distribution of the target variable \texttt{churn} to determine whether the dataset is balanced:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{label =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{stat}\NormalTok{(count))))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{stat =} \StringTok{\textquotesingle{}count\textquotesingle{}}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-4-1} \end{center}

The bar plot reveals that the dataset is imbalanced, with more customers staying (\texttt{churn\ =\ "no"}) than leaving (\texttt{churn\ =\ "yes"}). The proportion of churners is approximately 1.4 percent, while the proportion of non-churners is 8.6 percent. Since imbalanced data can impact predictive modeling, understanding churn patterns is essential for improving retention strategies.

\subsection*{Relationship Between Churn and Subscription Plans}\label{relationship-between-churn-and-subscription-plans}


We first analyze \texttt{intl.plan}, which indicates whether a customer has an international calling plan. As a binary variable, it allows for a straightforward comparison of churn rates between subscribed and non-subscribed customers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ intl.plan, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ intl.plan, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-5-1} \includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-5-2}

The first plot (left) compares the raw counts of churners and non-churners among customers with and without an international plan. The second plot (right) normalizes the proportions, revealing that customers with an international plan have a significantly higher churn rate.

To quantify this relationship, we generate a contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{intl.plan, }
                 \AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"International Plan"}\NormalTok{)))}
\NormalTok{        International Plan}
\NormalTok{   Churn  yes   no  Sum}
\NormalTok{     yes  }\DecValTok{199}  \DecValTok{508}  \DecValTok{707}
\NormalTok{     no   }\DecValTok{274} \DecValTok{4019} \DecValTok{4293}
\NormalTok{     Sum  }\DecValTok{473} \DecValTok{4527} \DecValTok{5000}
\end{Highlighting}
\end{Shaded}

The results confirm that churn is more prevalent among customers subscribed to an international plan. This suggests that international service offerings may not be meeting customer expectations, leading to higher attrition. Companies may need to investigate whether pricing, service quality, or competition is influencing this trend.

\subsection*{Relationship Between Churn and Voice Mail Plan}\label{relationship-between-churn-and-voice-mail-plan}


Next, we examine \texttt{voice.plan}, which indicates whether a customer has subscribed to a voice mail plan.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ voice.plan, }\AttributeTok{fill =}\NormalTok{ churn)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ voice.plan, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-7-1} \includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-7-2}

Customers without a voice mail plan appear to churn at a slightly higher rate. This is confirmed using a contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{voice.plan, }\AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Churn"}\NormalTok{, }\StringTok{"Voice Mail Plan"}\NormalTok{)))}
\NormalTok{        Voice Mail Plan}
\NormalTok{   Churn  yes   no  Sum}
\NormalTok{     yes  }\DecValTok{102}  \DecValTok{605}  \DecValTok{707}
\NormalTok{     no  }\DecValTok{1221} \DecValTok{3072} \DecValTok{4293}
\NormalTok{     Sum }\DecValTok{1323} \DecValTok{3677} \DecValTok{5000}
\end{Highlighting}
\end{Shaded}

While the difference is less pronounced than for \texttt{intl.plan}, it suggests that customers who actively use voice mail services may be more engaged and therefore less likely to leave.

\subsubsection*{Key Insights}\label{key-insights}


\begin{itemize}
\tightlist
\item
  Customers subscribed to an international plan have a \textbf{significantly higher} churn rate, indicating a potential issue with service expectations, pricing, or customer satisfaction. This variable is likely to be an important predictor in churn models.
\item
  Customers with a voice mail plan have a \textbf{slightly lower} churn rate, suggesting that engagement with additional services may contribute to customer retention.
\item
  These insights highlight the importance of investigating product-specific factors when analyzing churn, as different subscription plans may have varying impacts on customer behavior.
\end{itemize}

By exploring categorical variables in this way, we uncover actionable insights that can inform both predictive modeling and business decisions aimed at reducing customer churn. In the next section, we will examine numerical variables to further refine our understanding of customer behavior.

\section{Investigating Numerical Variables}\label{EDA-sec-numeric}

We now turn to numerical variables in the churn dataset, examining their distributions and relationships with the target variable. Summary statistics provide an initial understanding, but visualizations such as histograms, box plots, and density plots help reveal patterns and potential predictors of churn.

\subsection*{Customer Service Calls and Churn}\label{customer-service-calls-and-churn}


The variable \texttt{customer.calls} represents the number of calls a customer makes to customer service. Since this is a discrete numerical variable, we use a histogram to examine its distribution:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ customer.calls), }
                 \AttributeTok{bins =} \DecValTok{10}\NormalTok{, }\AttributeTok{fill =} \StringTok{"skyblue"}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-9-1} \end{center}

The histogram shows that most customers make only a few service calls, while a smaller group contacts customer service frequently. The right-skewed distribution suggests that a few customers make an unusually high number of calls, potentially signaling dissatisfaction.

To further investigate, we overlay churn status:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ customer.calls, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"stack"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }
  
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ customer.calls, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-10-1} \includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-10-2}

The normalized histogram (right) reveals a striking trend: customers making \emph{four or more} service calls have a significantly higher churn rate. This suggests that frequent service interactions may indicate unresolved issues, leading to customer dissatisfaction.

Key Insights and Business Implications:

\begin{itemize}
\tightlist
\item
  Customers making frequent service calls are at higher risk of churning.\\
\item
  Companies could implement proactive retention strategies when a customer makes multiple calls, such as escalating issues or offering incentives after the third call.\\
\item
  This variable is likely to be a strong predictor in churn models and should be included in further analysis.
\end{itemize}

\subsection*{Daytime Minutes and Churn}\label{daytime-minutes-and-churn}


Next, we examine \texttt{day.mins}, which represents the number of minutes a customer spends on daytime calls. We use box plots and density plots to compare distributions between churners and non-churners.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ day.mins), }
                 \AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ day.mins, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-11-1} \includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-11-2}

The box plot (left) shows that customers who churn tend to have higher daytime call usage. The density plot (right) confirms this, with a noticeable peak in churners at higher \texttt{day.mins} values.

Key Insights and Business Implications:

\begin{itemize}
\tightlist
\item
  High \texttt{day.mins} usage is associated with increased churn.\\
\item
  Customers with extensive daytime usage may be dissatisfied with pricing or service quality.\\
\item
  Targeted retention offers, such as flexible rate plans for heavy users, could help mitigate churn.
\end{itemize}

\subsection*{Evening and Nighttime Minutes}\label{evening-and-nighttime-minutes}


To investigate whether evening and nighttime call patterns also relate to churn, we plot \texttt{eve.mins} and \texttt{night.mins}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ eve.mins), }\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ eve.mins, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-12-1} \includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-12-2}

While a slight trend suggests that churners have higher \texttt{eve.mins}, the effect is weaker than for \texttt{day.mins}. Similarly, analysis of \texttt{night.mins} does not reveal a clear distinction between churners and non-churners.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ night.mins), }\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ night.mins, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-13-1} \includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-13-2}

The similar distributions suggest that nighttime call usage is not a strong churn indicator.

Key Insights and Business Implications:

\begin{itemize}
\tightlist
\item
  Unlike daytime calls, evening and nighttime minutes do not strongly predict churn.\\
\item
  Focusing on daytime usage and service call patterns may yield better predictive power.\\
\item
  Further statistical testing (e.g., t-tests or logistic regression) could confirm whether subtle differences exist.
\end{itemize}

Your subsection is \textbf{well-structured} and provides a \textbf{clear} overview of the analysis. However, there are a few \textbf{areas for improvement} in terms of \textbf{clarity, conciseness, and flow}. Below is an improved version that maintains the \textbf{same structure and meaning} while enhancing \textbf{readability and coherence}.

\subsection*{International Calls and Churn}\label{international-calls-and-churn}


We now examine \texttt{intl.calls}, which represents the total number of international calls made by customers. To explore its relationship with churn, we visualize the distribution using box plots and density plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{y =}\NormalTok{ intl.calls), }
                 \AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ intl.calls, }\AttributeTok{fill =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-14-1} \includegraphics[width=0.5\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-14-2}

The box plot (left) indicates that churners (\texttt{churn=yes}) tend to make slightly fewer international calls than non-churners. The density plot (right) further supports this, showing a minor difference in distribution between the two groups.

Although there is a slight trend suggesting that churners make fewer international calls on average, the difference does not appear substantial. This suggests that \texttt{intl.calls} is not a strong predictor of churn. To confirm whether this relationship is statistically significant, further testing---such as a two-sample t-test or logistic regression---would be required. In Section \ref{two-sample-t-test} of the next chapter, we demonstrate how to formally test this relationship using statistical methods.

\subsubsection*{Final Takeaways}\label{final-takeaways}


\begin{itemize}
\tightlist
\item
  \texttt{customer.calls} and \texttt{day.mins} are \emph{strongly associated} with churn and should be key predictors in churn models.\\
\item
  \emph{Customers making four or more service calls} are at high risk of leaving.\\
\item
  \emph{High daytime minute usage} is another important churn indicator, possibly due to pricing concerns.\\
\item
  Evening and nighttime call usage shows \emph{no strong relationship} with churn, suggesting it may not be an essential predictive feature.
\end{itemize}

By focusing on service calls and daytime minutes, companies can take targeted action to reduce churn, such as optimizing customer support escalation processes and offering personalized rate plans. These findings also guide the feature selection process for future predictive modeling efforts.

\section{Investigating Multivariate Relationships}\label{EDA-sec-multivariate}

While univariate analysis provides insights into individual variables, multivariate analysis helps uncover interactions that may influence churn. Examining variable relationships can reveal behavioral patterns that might not be evident when analyzing each feature in isolation.

A useful example is the relationship between \texttt{day.mins} (Day Minutes) and \texttt{eve.mins} (Evening Minutes), visualized in the scatter plot below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ eve.mins, }\AttributeTok{y =}\NormalTok{ day.mins, }\AttributeTok{color =}\NormalTok{ churn), }\AttributeTok{size =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{400}\NormalTok{, }\AttributeTok{slope =} \SpecialCharTok{{-}}\FloatTok{0.6}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-15-1} \end{center}

The diagonal line, represented by the equation:

\[
\text{day.mins} = 400 - 0.6 \times \text{eve.mins}
\]

separates the dataset into two regions. Customers in the upper-right region, where both day and evening minutes are high, exhibit a noticeably higher churn rate. This pattern was not apparent in the univariate analysis of \texttt{eve.mins}, demonstrating how feature interactions can provide deeper insights.

To quantify this effect, we isolate the high-churn segment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub\_churn }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churn, (day.mins }\SpecialCharTok{\textgreater{}} \DecValTok{400} \SpecialCharTok{{-}} \FloatTok{0.6} \SpecialCharTok{*}\NormalTok{ eve.mins))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sub\_churn, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn, }\AttributeTok{label =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{stat}\NormalTok{(count))))) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{stat =} \StringTok{\textquotesingle{}count\textquotesingle{}}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{size =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-16-1} \end{center}

Within this subset, the churn rate is significantly higher than in the overall dataset, reinforcing the importance of considering variable interactions. The combination of high day and evening usage may indicate a specific customer behavior pattern that correlates with dissatisfaction.

Another key relationship exists between \texttt{customer.calls} and \texttt{day.mins}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ day.mins, }\AttributeTok{y =}\NormalTok{ customer.calls, }\AttributeTok{color =}\NormalTok{ churn), }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-17-1} \end{center}

This scatter plot reveals an interesting high-churn region in the upper left, where customers make frequent customer service calls but have low day-minute usage. This group may represent dissatisfied customers who are not heavy users but are still experiencing service-related frustrations. By contrast, high-minute users who also make frequent service calls show a lower churn rate, possibly indicating that engaged customers are more tolerant of service issues.

\subsubsection*{Key Takeaways}\label{key-takeaways}


\begin{itemize}
\tightlist
\item
  Multivariate analysis reveals that customers with both high day and evening call usage have a much higher churn rate.
\item
  Customers making frequent customer service calls but using few daytime minutes are also at a higher risk of leaving.
\item
  The interaction between frequent customer service calls and high call usage suggests that dissatisfaction alone does not always drive churn---usage patterns also play a role.
\item
  Identifying customers in high-churn regions of these scatter plots can help in targeted retention efforts.
\end{itemize}

In Chapter \ref{chapter-statistics}, we will move from exploratory insights to formal statistical analysis, applying techniques to quantify these relationships and assess their predictive value.

\subsection{Investigating Correlated Variables}\label{investigating-correlated-variables}

Correlation measures the degree to which two variables move together. A positive correlation means that as one variable increases, the other also increases. A negative correlation indicates that when one variable rises, the other decreases. If two variables have no correlation, changes in one provide no information about changes in the other.

A common misconception is that correlation implies causation. For example, if an analysis finds that customers who make more calls to customer service tend to have higher churn rates, it does not necessarily mean that calling customer service causes churn. It could be that dissatisfied customers are more likely to call for assistance before leaving, making the number of service calls a symptom rather than a cause.

The strength and direction of a correlation are measured by the correlation coefficient, denoted as \(r\), which ranges from -1 to 1. A value of 1 indicates a perfect positive relationship, while -1 represents a perfect negative correlation. A value near zero suggests no linear relationship. In large datasets, even small correlations may be statistically significant, but practical significance must also be considered. A correlation of 0.05 might be significant in a dataset with thousands of observations, but it is unlikely to provide meaningful predictive power.

Below, Figure \ref{fig:correlation} shows examples of different correlation coefficients.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/ch4_correlation} 

}

\caption{Example scatterplots showing different correlation coefficients.}\label{fig:correlation}
\end{figure}

When multiple variables are highly correlated, redundancy can become a problem. Including both variables in a model may not add much new information and can lead to instability, particularly in regression-based models where multicollinearity makes it difficult to determine the effect of individual predictors. Instead of automatically removing correlated variables, a more thoughtful approach involves assessing their practical relevance and whether they provide distinct information.

To examine correlations in the churn dataset, we compute the correlation matrix for the numerical variables and visualize it using a heatmap.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggcorrplot)  }
\NormalTok{variable\_list }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"intl.mins"}\NormalTok{,  }\StringTok{"intl.calls"}\NormalTok{,  }\StringTok{"intl.charge"}\NormalTok{, }
                  \StringTok{"day.mins"}\NormalTok{,   }\StringTok{"day.calls"}\NormalTok{,   }\StringTok{"day.charge"}\NormalTok{,}
                  \StringTok{"eve.mins"}\NormalTok{,   }\StringTok{"eve.calls"}\NormalTok{,   }\StringTok{"eve.charge"}\NormalTok{,}
                  \StringTok{"night.mins"}\NormalTok{, }\StringTok{"night.calls"}\NormalTok{, }\StringTok{"night.charge"}\NormalTok{)}

\NormalTok{cor\_matrix }\OtherTok{=} \FunctionTok{cor}\NormalTok{(churn[, variable\_list])}

\FunctionTok{ggcorrplot}\NormalTok{(cor\_matrix, }\AttributeTok{type =} \StringTok{"lower"}\NormalTok{, }\AttributeTok{lab =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lab\_size =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{4_Exploratory_Data_Analysis_files/figure-latex/unnamed-chunk-18-1} \end{center}

The correlation matrix highlights some key relationships. The charge variables are perfectly correlated with their corresponding minutes variables because charges are calculated directly from call duration. Including both would introduce redundancy. To avoid this, the charge variables should be removed, keeping only the minutes variables.

Another notable observation is that the number of calls within each time period is not strongly correlated with total minutes. One might expect that customers who make more calls would also spend more time on the phone, but the data does not support this assumption. This suggests that call frequency and call duration may capture different aspects of customer behavior, making it valuable to retain both types of variables for modeling.

By addressing correlations during exploratory data analysis, the dataset can be refined to ensure that only the most informative variables are used in predictive modeling. Removing redundant features reduces complexity while retaining meaningful signals, improving the interpretability and performance of models.

\section{Key Findings and Insights}\label{key-findings-and-insights}

The exploratory data analysis of the churn dataset has provided a deeper understanding of the factors influencing customer attrition. By examining individual variables and their interactions, we identified several key trends that will inform predictive modeling and business strategy.

One of the most striking findings is the role of customer service interactions in churn. Customers who have made four or more calls to customer service are significantly more likely to leave. This suggests that frequent interactions may indicate unresolved complaints, leading to dissatisfaction. Additionally, customers with both high daytime and evening call usage exhibit churn rates up to six times higher than average, indicating that high usage may correlate with dissatisfaction---perhaps due to service quality concerns or pricing issues.

The international calling plan also appears to be a strong predictor of churn. Customers who subscribe to this plan are leaving at a much higher rate, suggesting that the plan may not be delivering sufficient value. In contrast, customers with a voice mail plan show a lower churn rate, indicating that this feature may contribute to customer retention.

Several variables, while not directly linked to churn in univariate analysis, could still provide value when combined with other features in predictive modeling. For example, customers with relatively low daytime usage but frequent customer service calls show a higher likelihood of leaving, a pattern that suggests service dissatisfaction among lower-usage customers.

From a modeling perspective, some variables introduce redundancy. The charge variables (day, evening, night, and international) are perfectly correlated with their corresponding minute variables, as they are derived directly from them. Retaining only the minute variables will avoid multicollinearity while preserving all relevant information. Similarly, the area code and state fields may not contribute much to predictive power, as they do not show strong relationships with churn in this dataset.

\subsection*{Strategic Recommendations}\label{strategic-recommendations}


These findings present opportunities for targeted interventions to improve customer retention. Given that frequent customer service calls are a strong indicator of churn, companies should implement proactive escalation strategies. Customers making their third service call should receive priority attention, potentially with issue resolution specialists or targeted retention offers.

For high-usage customers, personalized plans and loyalty incentives could help reduce churn. Offering flexible pricing or additional benefits for high daytime and evening callers may address concerns that drive them to switch providers. Similarly, a review of the international plan is necessary to assess whether its pricing, service quality, or features are leading to dissatisfaction.

While some variables, such as night minutes and certain demographic features, do not show strong direct correlations with churn, they may still contribute when combined with other predictors in a machine learning model. Further analysis will determine their importance in a predictive framework.

By identifying these churn-related patterns early, businesses can take proactive steps to improve customer satisfaction and reduce attrition, strengthening overall retention efforts before relying on predictive modeling. These insights will serve as a foundation for the next stage of analysis, where machine learning models will be applied to quantify these relationships more precisely.

\section{Exercises}\label{exercises-2}

\subsection*{Conceptual Questions}\label{conceptual-questions}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is it important to perform exploratory data analysis before proceeding to the modeling phase? What are the potential risks of skipping EDA and directly applying data mining techniques?
\item
  If a predictor does not exhibit a clear relationship with the target variable during exploratory data analysis, should it be omitted from the modeling stage? Justify your answer by considering potential interactions, hidden patterns, and the role of feature selection.
\item
  What does it mean for two variables to be correlated? Explain the concept of correlation, including its direction and strength, and discuss how it differs from causation. Provide an example to illustrate your explanation.
\item
  How can you identify and address correlated variables during exploratory data analysis? Describe the steps you would take to manage correlated predictors effectively and explain the benefits of this approach for predictive modeling.
\item
  What are the consequences of including highly correlated variables in a predictive model? Discuss the impact of multicollinearity on model performance, interpretability, and stability, and explain how it can be detected and addressed.
\item
  Is it always advisable to remove one of two correlated predictors from a model? Discuss the advantages and drawbacks of this approach, and explain under what circumstances keeping both predictors might be beneficial.
\item
  For each of the following descriptive methods, determine whether it applies to categorical data, continuous numerical data, or both. Provide a brief explanation of how each method is used in exploratory data analysis.

  \begin{itemize}
  \tightlist
  \item
    Histograms\\
  \item
    Box plots\\
  \item
    Density plots\\
  \item
    Scatter plots\\
  \item
    Summary statistics\\
  \item
    Correlation analysis\\
  \item
    Contingency tables\\
  \item
    Bar plots\\
  \item
    Heatmaps
  \end{itemize}
\item
  A telecommunications company is analyzing customer data to identify factors influencing churn. During exploratory data analysis, they discover that customers with both high day minutes and high evening minutes have a significantly higher churn rate. What actionable insights could the company derive from this finding, and how might they use this information to reduce customer attrition?
\item
  Suppose you are conducting exploratory data analysis on a dataset with 20 predictor variables. After examining the correlation matrix, you find that several pairs of variables are highly correlated (r \textgreater{} 0.9). How would you address these correlations to ensure the reliability and interpretability of your predictive models? Describe the steps you would take to manage these correlated variables effectively.
\item
  Discuss the importance of considering both statistical and practical implications when evaluating correlations during exploratory data analysis. Provide an example of a statistically significant correlation that may not have real-world significance, and explain why it is essential to consider both aspects in data analysis.
\item
  Why is it important to investigate multivariate relationships rather than relying only on univariate analysis? Provide an example where an interaction between two variables reveals a pattern that would be missed in separate univariate analyses.
\item
  How does data visualization aid in the exploratory data analysis process? Discuss at least two specific examples where visualizations provide insights that summary statistics alone cannot reveal.
\item
  Suppose you discover that customers who have high daytime call usage and make frequent customer service calls are more likely to churn. What business actions could be taken based on this insight?
\item
  Outliers can be influential in statistical modeling. What are some possible causes of outliers in a dataset? How would you decide whether to keep, modify, or remove an outlier?
\item
  In the context of exploratory data analysis, explain why missing values are a critical issue. What are the different strategies for handling missing values, and under what circumstances would each be appropriate?
\end{enumerate}

\subsection*{Hands-On Practice: Exploring the Bank Dataset}\label{hands-on-practice-exploring-the-bank-dataset}


For hands-on practice, we will explore the \emph{bank} dataset available in the \textbf{R} package \textbf{liver}. The \emph{bank} dataset is related to direct marketing campaigns of a Portuguese banking institution. The campaigns were conducted via phone calls, with multiple contacts sometimes needed to determine whether a client would subscribe to a term deposit. The goal of this dataset is to predict whether a client will subscribe to a term deposit, which will be explored using classification techniques in Chapters \ref{chapter-knn} and \ref{chapter-nn}.

More details on this dataset can be found at: \url{https://rdrr.io/cran/liver/man/bank.html}.

We can import the dataset in \textbf{R} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{data}\NormalTok{(bank)      }
\end{Highlighting}
\end{Shaded}

To examine the dataset's structure, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(bank)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{17}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays    }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  \emph{Dataset Overview}: Report the summary statistics of the dataset, including the types of variables. What can you infer from the structure of the data?
\item
  \emph{Target Variable Analysis}: Generate a bar plot for the target variable \texttt{deposit} using the \texttt{ggplot()} function from \textbf{ggplot2}. What is the proportion of clients who subscribed to a term deposit?
\item
  \emph{Binary Variable Exploration}: Investigate the binary categorical variables \texttt{default}, \texttt{housing}, and \texttt{loan}. Create contingency tables and bar plots to visualize their distributions. What insights can you draw from these variables?
\item
  \emph{Exploring Numerical Variables}: Analyze the numerical variables in the dataset. Create histograms and box plots to visualize their distributions. Identify any patterns, skewness, or unusual observations.
\item
  \emph{Outlier Detection}: Identify whether any numerical variables contain outliers. How would you handle these outliers to maintain data integrity while ensuring robust analysis?
\item
  \emph{Correlation Analysis}: Compute the correlation matrix for the numerical variables. Identify pairs of highly correlated variables. What strategies would you use to handle these correlations to avoid redundancy and multicollinearity in predictive modeling?
\item
  \emph{Key EDA Findings}: Summarize the key findings from your exploratory data analysis based on the exercises above. If you were preparing a formal report, how would you highlight notable patterns, relationships, or anomalies?
\item
  \emph{Business Implications}: Based on your findings, what actionable insights could the bank derive from this exploratory analysis? How could these insights help in optimizing marketing strategies and improving customer targeting?
\item
  \emph{Multivariate Analysis}: Investigate the relationship between the number of previous marketing campaign contacts (\texttt{campaign}) and term deposit subscriptions (\texttt{deposit}). Does higher contact frequency correlate with increased subscriptions? Use a box plot or bar chart to support your findings.
\item
  \emph{Feature Engineering Insight}: Based on your EDA, propose at least one new feature that could improve the predictive power of a classification model for term deposit subscriptions. Justify your reasoning.
\item
  \emph{Seasonality Effects}: Investigate whether the time of year influences term deposit subscriptions by analyzing the \texttt{month} variable. Do certain months have a higher success rate? Visualize this pattern and discuss its potential business implications.
\item
  \emph{Effect of Employment Type}: Examine how the \texttt{job} variable relates to term deposit subscriptions. Which job categories have higher success rates? Present your findings using a suitable visualization and discuss how banks could use this insight for targeted marketing.
\item
  \emph{Interaction Effects}: Analyze whether interactions between different predictors, such as \texttt{education} and \texttt{job}, influence the likelihood of subscribing to a term deposit. Use appropriate visualizations or statistical summaries to support your findings.
\item
  \emph{Effect of Contact Duration}: Investigate whether the duration of the last contact (\texttt{duration} variable) has a strong relationship with term deposit subscription. Visualize the distribution and discuss whether longer calls are associated with higher success rates.
\item
  \emph{Comparison of Campaign Outcomes}: Compare the subscription rates (\texttt{deposit} variable) across different types of marketing campaigns (\texttt{campaign} variable). What trends emerge, and how could they inform future marketing strategies?
\end{enumerate}

\chapter{Statistical Inference and Hypothesis Testing}\label{chapter-statistics}

Statistical inference bridges the gap between \emph{what we observe in a sample} and \emph{what we want to understand about the population}. While exploratory data analysis (EDA) helps us identify patterns and relationships, statistical inference allows us to determine whether these patterns hold beyond our sample---or whether they could have arisen by chance. In this chapter, we transition from \emph{exploring} data to \emph{validating} insights through estimation, hypothesis testing, and quantifying uncertainty.

The goals of statistical inference can be summarized into three fundamental tasks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Estimating population characteristics}, such as averages or proportions, based on sample data.\\
\item
  \emph{Quantifying uncertainty} to measure how confident we can be in our results.\\
\item
  \emph{Testing hypotheses} to evaluate whether observed patterns are statistically meaningful or simply due to random variation.
\end{enumerate}

These tasks form the foundation of \emph{data-driven decision-making}, enabling us to distinguish meaningful insights from statistical noise. In this chapter, we will explore these three pillars---estimation, uncertainty, and hypothesis testing---using intuitive explanations and practical examples.

But statistical inference isn't just about applying formulas---it's also about \emph{critical thinking}. By the end of this chapter, you'll develop two essential skills:

\begin{itemize}
\tightlist
\item
  \emph{How to detect statistical misuses and misleading claims}, helping you critically evaluate data-driven arguments.\\
\item
  \emph{How to avoid common pitfalls in statistical analysis}, ensuring that your own conclusions are both sound and defensible.
\end{itemize}

For those interested in the art of identifying statistical manipulation, Darrell Huff's classic book, \href{https://www.goodreads.com/book/show/51291.How_to_Lie_with_Statistics}{\emph{How to Lie with Statistics}}, offers timeless lessons in statistical skepticism. Understanding these techniques is valuable not just for avoiding errors but also for recognizing when data is being used to mislead.

Let's dive in and learn how to make statistical inferences with confidence, curiosity, and a healthy dose of skepticism.

\section{Estimation: Using Data to Make Predictions}\label{estimation-using-data-to-make-predictions}

Estimation is a fundamental aspect of statistical inference that allows us to make informed guesses about a population based on a sample. Rather than relying on the entire population, which is often impractical, we use sample data to estimate key characteristics such as averages or proportions. For instance, in the churn dataset, we might want to estimate:

\begin{itemize}
\tightlist
\item
  The \emph{average number of customer service calls} among churners.\\
\item
  The \emph{proportion of customers} subscribed to the International Plan.
\end{itemize}

There are two main types of estimation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Point estimation} provides a single best guess for a population parameter, such as using the sample mean to estimate the population mean.\\
\item
  \emph{Interval estimation} gives a range of plausible values (a confidence interval) within which the true population parameter is likely to fall.
\end{enumerate}

Let's explore some examples:

\begin{example}
\protect\hypertarget{exm:ex-est-churn-proportion}{}\label{exm:ex-est-churn-proportion}To estimate the \emph{proportion of churners} in the dataset, we use the \emph{sample proportion} as a point estimate for the population proportion. Here's how to calculate it in R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{data}\NormalTok{(churn) }

\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn))[}\StringTok{"yes"}\NormalTok{]}
\NormalTok{      yes }
   \FloatTok{0.1414}
\end{Highlighting}
\end{Shaded}

The estimated proportion of churners in the dataset is 0.14, serving as our best guess for the proportion of churners in the population.
\end{example}

\begin{example}
\protect\hypertarget{exm:ex-est-service-call}{}\label{exm:ex-est-service-call}Now, let's estimate the \emph{average number of customer service calls} for customers who churned. The \emph{sample mean} serves as a point estimate for the population mean:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter churners}
\NormalTok{churned\_customers }\OtherTok{\textless{}{-}}\NormalTok{ churn[churn}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{, ]}

\CommentTok{\# Calculate the mean}
\NormalTok{mean\_calls }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{customer.calls)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Point Estimate: Average Customer Service Calls for Churners:"}\NormalTok{, mean\_calls)}
\NormalTok{   Point Estimate}\SpecialCharTok{:}\NormalTok{ Average Customer Service Calls }\ControlFlowTok{for}\NormalTok{ Churners}\SpecialCharTok{:} \FloatTok{2.254597}
\end{Highlighting}
\end{Shaded}

If the sample mean is \textbf{4 calls}, this would be our best estimate of the average number of customer service calls among all churners in the population.
\end{example}

\begin{quote}
\emph{While point estimates are useful, they provide no information about uncertainty. Confidence intervals help quantify the precision of an estimate, which we explore next.}
\end{quote}

\section{Quantifying Uncertainty: Confidence Intervals}\label{statistics-confidence-interval}

Confidence intervals help quantify uncertainty when estimating population parameters. Instead of simply stating that ``the average number of customer service calls is 4,'' a confidence interval provides a range, such as ``we are 95\% confident that the true average is between 3.8 and 4.2.'' This range accounts for sampling variability, offering a clearer picture of how reliable the estimate is.

A confidence interval consists of a point estimate, such as a sample mean or proportion, and a margin of error, which accounts for uncertainty. The general form of a confidence interval is:

\[
\text{Point Estimate}  \pm \text{Margin of Error}
\]

For a population mean, the confidence interval is calculated as:

\[
\bar{x} \pm z_{\frac{\alpha}{2}} \times \left( \frac{s}{\sqrt{n}} \right),
\]

where \(\bar{x}\) is the sample mean, \(z_{\frac{\alpha}{2}}\) is a critical value from the standard normal distribution (such as 1.96 for a 95\% confidence level), \(s\) is the sample standard deviation, and \(n\) is the sample size. This concept is illustrated in Figure \ref{fig:confidence-interval}, where the interval is centered around the point estimate and its width depends on the margin of error.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth]{images/ch5_confidence_interval} 

}

\caption{Confidence interval for the population mean. The interval is centered around the point estimate, with the width determined by the margin of error. The confidence level specifies the probability that the interval contains the true population parameter.}\label{fig:confidence-interval}
\end{figure}

Several factors influence the width of a confidence interval. Larger sample sizes generally yield narrower intervals, increasing precision, while higher variability in the data results in wider intervals. The choice of confidence level also affects the width; for example, a 99\% confidence level produces a wider interval than a 90\% confidence level because it must capture more possible values.

Imagine you want to estimate the average height of all students in a university. If you survey only 10 students, your confidence interval will be wide because you have little data. But if you survey 1,000 students, your estimate becomes much more precise, and the confidence interval shrinks. This illustrates why larger sample sizes lead to more reliable estimates---more data reduces uncertainty and results in tighter confidence intervals.

To illustrate, suppose we want to estimate the average number of customer service calls among churners with 95\% confidence:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate mean and standard error}
\NormalTok{mean\_calls }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{customer.calls)}
\NormalTok{se\_calls }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{customer.calls) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(churned\_customers))}

\CommentTok{\# Confidence Interval}
\NormalTok{z\_score }\OtherTok{\textless{}{-}} \FloatTok{1.96}  \CommentTok{\# For 95\% confidence}
\NormalTok{ci\_lower }\OtherTok{\textless{}{-}}\NormalTok{ mean\_calls }\SpecialCharTok{{-}}\NormalTok{ z\_score }\SpecialCharTok{*}\NormalTok{ se\_calls}
\NormalTok{ci\_upper }\OtherTok{\textless{}{-}}\NormalTok{ mean\_calls }\SpecialCharTok{+}\NormalTok{ z\_score }\SpecialCharTok{*}\NormalTok{ se\_calls}

\FunctionTok{cat}\NormalTok{(}\StringTok{"95\% Confidence Interval: ["}\NormalTok{, ci\_lower, }\StringTok{","}\NormalTok{, ci\_upper, }\StringTok{"]"}\NormalTok{)}
   \DecValTok{95}\NormalTok{\% Confidence Interval}\SpecialCharTok{:}\NormalTok{ [ }\FloatTok{2.120737}\NormalTok{ , }\FloatTok{2.388457}\NormalTok{ ]}
\end{Highlighting}
\end{Shaded}

If the computed interval is {[}2.12, 2.39{]}, we can say with 95\% confidence that the true average number of service calls for churners falls within this range.

For smaller sample sizes, it is better to use the t-distribution instead of the normal distribution, as it accounts for the added uncertainty when estimating the population standard deviation. This adjustment is applied automatically in R when using the \texttt{t.test()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{customer.calls, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}\SpecialCharTok{$}\NormalTok{conf.int}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{2.120509} \FloatTok{2.388685}
   \FunctionTok{attr}\NormalTok{(,}\StringTok{"conf.level"}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.95}
\end{Highlighting}
\end{Shaded}

Confidence intervals are particularly useful for comparing groups. If confidence intervals for two groups, such as churners and non-churners, do not overlap significantly, it suggests meaningful differences in behavior. By providing a range rather than a single estimate, confidence intervals help balance precision and uncertainty, making them a valuable tool in statistical inference.

\section{Hypothesis Testing}\label{hypothesis-testing}

Hypothesis testing provides a structured framework for evaluating claims about population parameters using sample data. It helps us assess whether patterns observed during exploratory analysis are statistically significant or simply the result of random variation. This method is fundamental to data-driven decision-making, enabling us to distinguish meaningful insights from noise.

At its core, hypothesis testing involves two competing statements about a population parameter:

\begin{itemize}
\tightlist
\item
  The \emph{null hypothesis} (\(H_0\)) represents the default assumption or status quo, often stating that there is no difference between groups, no effect of a treatment, or no relationship between variables.\\
\item
  The \emph{alternative hypothesis} (\(H_a\)) challenges \(H_0\), suggesting that a difference, effect, or relationship does exist.
\end{itemize}

Using sample evidence, we decide whether to:

\begin{itemize}
\tightlist
\item
  \emph{Reject \(H_0\)} and conclude that the data supports \(H_a\).\\
\item
  \emph{Fail to reject \(H_0\)}, meaning the evidence is insufficient to dismiss \(H_0\), though this does not prove it to be true.
\end{itemize}

The strength of the evidence against \(H_0\) is quantified using the \emph{p}-value, which represents the probability of obtaining the observed data---or something more extreme---if \(H_0\) were true. A smaller \emph{p}-value suggests stronger evidence against \(H_0\).

\begin{itemize}
\tightlist
\item
  If \(p < 0.05\), we reject \(H_0\) and conclude there is statistical evidence for \(H_a\).\\
\item
  If \(p > 0.05\), we fail to reject \(H_0\), meaning the evidence is not strong enough to support \(H_a\).
\end{itemize}

The threshold for decision-making is called the \emph{significance level} (\(\alpha\)), typically set at 0.05 (5\%). This value represents the maximum probability of making a \emph{Type I error}---incorrectly rejecting \(H_0\). In fields where errors have serious consequences, such as medicine or aerospace, stricter thresholds (e.g., \(\alpha = 0.01\)) are often used.

A simple takeaway, often emphasized in hypothesis testing, is:

Reject \(H_0\) if the \(p\)-value \textless{} \(\alpha\).

For example:\\
- If \(p = 0.03\) and \(\alpha = 0.05\), we reject \(H_0\) because \(p < \alpha\).\\
- If \(p = 0.12\), we fail to reject \(H_0\) because \(p > \alpha\).

Although \emph{p}-values provide a structured way to make decisions, they have limitations. A small \emph{p}-value does not necessarily mean a result is practically important---it only indicates statistical significance. Large datasets can generate small \emph{p}-values for trivial effects, while small datasets may fail to detect meaningful differences. Additionally, the binary reject/fail-to-reject approach can sometimes oversimplify interpretation.

\subsection{Types of Hypothesis Tests}\label{types-of-hypothesis-tests}

Depending on the research question, hypothesis tests can take different forms:

\begin{itemize}
\tightlist
\item
  \emph{Left-tailed test}: The alternative hypothesis states that the parameter is \emph{less than} a specified value (\(H_a: \theta < \theta_0\)). Example: Testing whether the average number of customer service calls is \emph{less than} 3.\\
\item
  \emph{Right-tailed test}: The alternative hypothesis states that the parameter is \emph{greater than} a specified value (\(H_a: \theta > \theta_0\)). Example: Testing whether the churn rate is \emph{greater than} 30\%.\\
\item
  \emph{Two-tailed test}: The alternative hypothesis states that the parameter is \emph{not equal to} a specified value (\(H_a: \theta \neq \theta_0\)), evaluating deviations in either direction. Example: Testing whether the mean monthly charges differ from \$50.
\end{itemize}

A useful analogy for hypothesis testing is a criminal trial. The \emph{null hypothesis} (\(H_0\)) represents the presumption of innocence, while the \emph{alternative hypothesis} (\(H_a\)) represents guilt. The jury weighs the evidence and either rejects \(H_0\) (convicts the defendant) or fails to reject \(H_0\) (acquits due to insufficient evidence). Just as juries can make errors, hypothesis tests also have two types of errors summarized in Table \ref{tab:hypothesis-errors}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2680}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3711}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3608}}@{}}
\caption{\label{tab:hypothesis-errors} Possible outcomes of hypothesis testing with two correct decisions and two types of errors.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\emph{Decision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Reality: \(H_0\) is True}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Reality: \(H_0\) is False}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\emph{Decision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Reality: \(H_0\) is True}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Reality: \(H_0\) is False}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Fail to Reject \(H_0\) & {\emph{Correct Decision}: Acquit an innocent person.} & {\emph{Type II Error (\(\beta\))}: Acquit a guilty person.} \\
Reject \(H_0\) & {\emph{Type I Error (\(\alpha\))}: Convict an innocent person.} & {\emph{Correct Decision}: Convict a guilty person.} \\
\end{longtable}

A \emph{Type I Error} (\(\alpha\)) occurs when \(H_0\) is rejected even though it is true---similar to convicting an innocent person. A \emph{Type II Error} (\(\beta\)) happens when \(H_0\) is not rejected even though it is false---similar to acquitting a guilty person. The probability of a Type I error is controlled by the chosen significance level (\(\alpha\)), while the probability of a Type II error depends on factors like sample size and test sensitivity.

\subsection*{Common Hypothesis Tests}\label{common-hypothesis-tests}


There are several widely used hypothesis tests, as listed in Table \ref{tab:hypothesis-errors}, each suited to different types of data.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2437}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3277}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}@{}}
\caption{\label{tab:hypothesis-test} Seven commonly used hypothesis tests, their null hypotheses (\(H_0\)), and the types of variables they apply to.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(H_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Can be used for
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(H_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Can be used for
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-sample t-test & \(H_0: \mu = \mu_0\) & A numerical variable \\
Test for Proportion & \(H_0: \pi = \pi_0\) & A categorical variable \\
Two-sample t-test & \(H_0: \mu_1 = \mu_2\) & A numerical and a binary variable \\
Two-sample Z-test & \(H_0: \pi_1 = \pi_2\) & Two binary variables \\
Chi-square Test & \(H_0: \pi_1 = \pi_2 = \pi_3\) & Two categorical variables (with \textgreater{} 2 categories) \\
Analysis of Variance (ANOVA) & \(H_0: \mu_1 = \mu_2 = \mu_3\) & A numerical and a categorical variable \\
Correlation Test & \(H_0: \rho = 0\) & Two numerical variables \\
\end{longtable}

Each test serves a specific purpose. The \emph{t-test} compares means, the \emph{Z-test} compares proportions, the \emph{Chi-square test} assesses categorical relationships, and \emph{ANOVA} compares means across multiple groups. These tests will be explored in the following sections with practical examples.

\section{One-sample t-test}\label{one-sample-t-test}

The one-sample t-test evaluates whether the mean of a numerical variable in a population is equal to a specified value. It is commonly used to compare a sample mean to a benchmark or theoretical expectation. The term ``one-sample'' reflects that only a single group is being tested against a fixed value, while ``t-test'' refers to the fact that the test statistic follows a t-distribution, which is used to compute the \emph{p}-value.

The hypotheses for a one-sample t-test depend on the research question and can be formulated in different ways. A two-tailed test assesses whether the mean differs from a specified value, regardless of direction. A left-tailed test evaluates whether the mean is lower than the specified value, while a right-tailed test examines whether the mean is greater. The mathematical formulation is:

\begin{itemize}
\tightlist
\item
  \emph{Two-Tailed Test}:
  \[
  \begin{cases}
  H_0:  \mu   =  \mu_0 \\
  H_a:  \mu \neq \mu_0
  \end{cases}
  \]
\item
  \emph{Left-Tailed Test}:
  \[
  \begin{cases}
  H_0:  \mu \geq \mu_0 \\
  H_a:  \mu  <   \mu_0
  \end{cases}
  \]
\item
  \emph{Right-Tailed Test}:
  \[
  \begin{cases}
  H_0:  \mu \leq \mu_0 \\
  H_a:  \mu >   \mu_0
  \end{cases}
  \]
\end{itemize}

The \emph{p}-value represents the probability of observing the sample mean, or a more extreme value, under the assumption that the null hypothesis is true. A smaller \emph{p}-value provides stronger evidence against \(H_0\). If the \emph{p}-value is less than the significance level (\(\alpha = 0.05\)), the null hypothesis is rejected, indicating that the sample mean differs significantly from the specified value.

The following example demonstrates how to apply a one-sample t-test in R using the \texttt{t.test()} function.

\begin{example}
\protect\hypertarget{exm:ex-one-sample-test}{}\label{exm:ex-one-sample-test}A company assumes that, on average, customers make two service calls before churning. To test whether the actual average number of customer service calls among churners differs from this assumed value, we conduct a one-sample t-test using the \emph{churn} dataset provided in the \textbf{liver} package.

To conduct the test, we set up the following hypotheses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Null Hypothesis (\(H_0\))}: \(H_0: \mu = 2\) (The average number of customer service calls is 2.)\\
\item
  \emph{Alternative Hypothesis (\(H_a\))}: \(H_a: \mu \neq 2\) (The average number of customer service calls is not 2.)
\end{enumerate}

We can present the hypotheses in mathematical form as:
\[
\begin{cases}
    H_0: \mu = 2   \\
    H_a: \mu \neq 2  
\end{cases}
\]

We begin by loading the \emph{churn} dataset and filtering the customers who have churned:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)  }\CommentTok{\# Load the liver package}
\FunctionTok{data}\NormalTok{(churn)     }\CommentTok{\# Load the churn dataset}

\CommentTok{\# Filter churned customers}
\NormalTok{churned\_customers }\OtherTok{\textless{}{-}}\NormalTok{ churn[churn}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

Now, we conduct a two-tailed one-sample t-test in R using the \texttt{t.test()} function; If you want to know more about the functionality of the \texttt{t.test()} function, you can find more by typing \texttt{?t.test} in the R console.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_test }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(churned\_customers}\SpecialCharTok{$}\NormalTok{customer.calls, }\AttributeTok{mu =} \DecValTok{2}\NormalTok{)}
\NormalTok{t\_test}
   
\NormalTok{    One Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  churned\_customers}\SpecialCharTok{$}\NormalTok{customer.calls}
\NormalTok{   t }\OtherTok{=} \FloatTok{3.7278}\NormalTok{, df }\OtherTok{=} \DecValTok{706}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.0002086}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true mean is not equal to }\DecValTok{2}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{2.120509} \FloatTok{2.388685}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean of x }
    \FloatTok{2.254597}
\end{Highlighting}
\end{Shaded}

The output includes the \emph{p}-value, test statistic, degrees of freedom, and confidence interval for the population mean. Since the \emph{p}-value = \ensuremath{2\times 10^{-4}} is less than the significance level (\(\alpha = 0.05\)), we reject the null hypothesis (\(H_0\)). This would indicate that there is sufficient evidence, at the 5\% significance level, to conclude that the true average number of customer service calls differs from 2.

The test also provides a 95\% confidence interval, {[}2.12, 2.39{]}, which represents the range of plausible values for the true population mean. Since 2 is outside this interval, we have further evidence that the true average number of service calls is different from the assumed value. Additionally, the sample mean, 2.25, is reported as the best estimate of the population mean.

Since the sample standard deviation is used in place of the population standard deviation, the test statistic follows a t-distribution with \(n - 1\) degrees of freedom. It measures how far the sample mean deviates from the hypothesized mean in terms of standard error. A larger absolute value indicates stronger evidence against \(H_0\).
\end{example}

The one-sample t-test provides a structured approach for comparing a sample mean to a predefined benchmark. It not only determines statistical significance but also offers additional insights through the confidence interval, sample mean, and test statistic. While statistical significance is important, practical relevance must also be evaluated to determine whether the observed difference has meaningful real-world implications. Even if a statistically significant difference is detected, the magnitude of the difference determines whether it has real-world implications. A deviation of 0.1 calls may be negligible, whereas a difference of two calls could impact customer service strategies.

By integrating statistical inference with domain knowledge, the one-sample t-test allows analysts to determine whether deviations from expectations are both statistically significant and practically meaningful.

\section{Hypothesis Testing for Proportion}\label{hypothesis-testing-for-proportion}

The \emph{test for proportion} evaluates whether the proportion (\(\pi\)) of a specific category in the population aligns with a hypothesized value (\(\pi_0\)). It is particularly useful for binary categorical variables, where observations fall into one of two groups, such as churned vs.~not churned. This test helps determine whether an observed sample proportion deviates significantly from a specified benchmark, making it valuable in business and scientific contexts.

For instance, a company might want to assess whether the proportion of churners in the population aligns with an expected value based on historical data or industry standards. The following example demonstrates how to apply the proportion test in R using the \texttt{prop.test()} function.

\begin{example}
\protect\hypertarget{exm:ex-test-proportion}{}\label{exm:ex-test-proportion}A company assumes that 15\% of its customers churn. To test whether the actual churn rate in the \emph{churn} dataset differs from this assumption, we conduct a proportion test. The hypotheses are:

\[
\begin{cases}
H_0: \pi  =   0.15 \\ 
H_a: \pi \neq 0.15 
\end{cases}
\]

The test is performed in R using the \texttt{prop.test()} function. If you would like to explore the details of this function, you can type \texttt{?prop.test} in the R console.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_test }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{sum}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{), }
                       \AttributeTok{n =} \FunctionTok{nrow}\NormalTok{(churn), }
                       \AttributeTok{p =} \FloatTok{0.15}\NormalTok{)}
\NormalTok{prop\_test}
   
    \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{sample proportions test with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{sum}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) out of }\FunctionTok{nrow}\NormalTok{(churn), null probability }\FloatTok{0.15}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{2.8333}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.09233}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true p is not equal to }\FloatTok{0.15}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.1319201} \FloatTok{0.1514362}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{        p }
   \FloatTok{0.1414}
\end{Highlighting}
\end{Shaded}

The output provides key results, including the \emph{p}-value, confidence interval, and sample proportion. These results are interpreted as follows:

The \emph{p}-value indicates the probability of obtaining the observed sample proportion if the assumed population proportion were true. Since the \emph{p}-value = 0.0923, and it is greater than \(\alpha = 0.05\), we do not reject the null hypothesis. This means there is insufficient evidence to conclude that the churn rate in the population differs from 15\%. In this case, we report:\\
\emph{``There is no statistically significant evidence to suggest that the population proportion of churners deviates from 15\%.''}\\
If the \emph{p}-value were smaller than 0.05, we would reject the null hypothesis, concluding that the churn rate is significantly different from 15\%.

The test also provides a \emph{95\% confidence interval}, {[}0.13, 0.15{]}, which represents the plausible range for the true population proportion (\(\pi\)). If 0.15 lies within this interval, it supports failing to reject \(H_0\). If 0.15 is outside the interval, it strengthens the evidence against \(H_0\).

Additionally, the test reports the \emph{sample proportion}, 0.14, which is the observed proportion of churners in the dataset. This value serves as an estimate of the true population proportion.
\end{example}

This test helps assess whether the observed churn rate aligns with the company's expectation. The \emph{p}-value determines statistical significance, while the confidence interval and sample proportion provide additional context for interpretation. By considering both statistical results and practical implications, businesses can evaluate whether their assumptions about churn are accurate or require adjustment.

\section{Two-sample T-test}\label{two-sample-t-test}

The \emph{two-sample t-test}, also known as Student's t-test, is a statistical method used to compare the means of a numerical variable between two independent groups. It assesses whether the observed difference between the group means is statistically significant or simply due to random variation. The test is named after \href{https://en.wikipedia.org/wiki/William_Sealy_Gosset}{William Sealy Gosset}, who worked at Guinness Brewery in Dublin and published under the pseudonym ``Student'' to maintain confidentiality regarding statistical quality control methods.

In Section \ref{EDA-sec-numeric} of the previous chapter, we explored the relationship between \emph{International Calls} (\texttt{intl.calls}) and churn status using visualizations like box plots and density plots. While visualizations help identify potential differences, statistical testing quantifies the likelihood that these differences are due to chance.

\includegraphics[width=0.5\linewidth]{5_Statistics_files/figure-latex/unnamed-chunk-8-1} \includegraphics[width=0.5\linewidth]{5_Statistics_files/figure-latex/unnamed-chunk-8-2}

The boxplot (left) and density plot (right) illustrate the distributions of \texttt{intl.calls} for churners and non-churners. While the visualizations suggest only minor differences, we perform a two-sample t-test to assess whether these differences are statistically significant.

To conduct the test, we first establish the hypotheses:

\begin{itemize}
\tightlist
\item
  \emph{Null Hypothesis (\(H_0\))}: The mean number of international calls is the same for churners and non-churners (\(\mu_1 = \mu_2\)).
\item
  \emph{Alternative Hypothesis (\(H_a\))}: The mean number of international calls differs between churners and non-churners (\(\mu_1 \neq \mu_2\)).
\end{itemize}

This can also be expressed mathematically as:
\[
\begin{cases}
    H_0: \mu_1 = \mu_2   \\
    H_a: \mu_1 \neq \mu_2 
\end{cases}
\]

We perform the test in R using the \texttt{t.test()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_test\_calls }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(intl.calls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn, }\AttributeTok{data =}\NormalTok{ churn)}
\NormalTok{t\_test\_calls}
   
\NormalTok{    Welch Two Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  intl.calls by churn}
\NormalTok{   t }\OtherTok{=} \SpecialCharTok{{-}}\FloatTok{3.2138}\NormalTok{, df }\OtherTok{=} \FloatTok{931.13}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.001355}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true difference }\ControlFlowTok{in}\NormalTok{ means between group yes and group no is not equal to }\DecValTok{0}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.5324872} \SpecialCharTok{{-}}\FloatTok{0.1287201}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean }\ControlFlowTok{in}\NormalTok{ group yes  mean }\ControlFlowTok{in}\NormalTok{ group no }
            \FloatTok{4.151344}          \FloatTok{4.481947}
\end{Highlighting}
\end{Shaded}

The function evaluates the difference in means between the two groups (\texttt{churn\ =\ "yes"} vs.~\texttt{churn\ =\ "no"}) and provides a \emph{p}-value, confidence interval, and descriptive statistics.

The \emph{p}-value = 0.0014. Since this value is less than the significance level (\(\alpha = 0.05\)), we reject the null hypothesis, concluding that the mean number of international calls differs significantly between churners and non-churners. The \emph{95\% confidence interval} = {[}-0.53, -0.13{]} provides a range of plausible values for the true difference in means. If the interval does not include zero, we have statistical evidence that the two groups differ significantly in their average number of international calls. The test output also provides the \emph{sample means}:\\
- Mean for churners = 4.15\\
- Mean for non-churners = 4.48

These values allow direct comparison of international call usage between churners and non-churners. For instance, if churners made an average of 1.5 international calls while non-churners made 2.3 calls, this suggests that churners tend to make fewer international calls.

The two-sample t-test assumes that the two groups are independent and that the variable of interest is normally distributed within each group when sample sizes are small. For larger samples, the Central Limit Theorem ensures the validity of the test even if normality is not strictly met. While minor deviations from normality are generally acceptable, large departures may require alternative tests, such as the Mann-Whitney U test.

From a business perspective, the test results suggest that international call frequency is a relevant factor in churn. Customers who churn tend to make fewer international calls. Companies may explore whether international call costs contribute to customer churn. If higher costs deter international calls, targeted discounts for low-usage customers could encourage engagement and improve retention. However, statistical significance does not always imply practical significance. Even if churners make fewer international calls on average, the actual impact on churn should be evaluated in conjunction with other variables.

Although this example uses a two-tailed test to detect any difference in means, a one-tailed test could be used if the research question specifies a directional hypothesis. For instance, if a company hypothesizes that churners make fewer international calls than non-churners, a one-tailed test could increase the test's sensitivity.

The two-sample t-test is a powerful and widely used method for comparing group means. It provides a statistical foundation for validating insights suggested by exploratory data analysis. By integrating graphical exploration with hypothesis testing, analysts can make well-informed inferences and derive actionable business insights.

\section{Two-Sample Z-Test}\label{two-sample-z-test}

In the previous section, we applied the \emph{two-sample t-test} to compare the mean number of international calls between churners and non-churners. While the t-test is useful for assessing differences in numerical variables, many business and scientific questions involve categorical variables, where proportions rather than means are of interest. The \emph{two-sample Z-test} is designed to compare the proportions of two independent groups, determining whether the observed difference in proportions is statistically significant. This test is particularly valuable when analyzing binary categorical variables, such as customer churn or subscription status.

In Section \ref{chapter-EDA-categorical} of the previous chapter, we examined the relationship between the \emph{Voice Mail Plan} (\texttt{voice.plan}) and churn status using bar plots. While visualizations suggest potential differences in churn rates between customers with and without a Voice Mail Plan, statistical testing quantifies whether these differences are statistically significant.

\includegraphics[width=0.5\linewidth]{5_Statistics_files/figure-latex/unnamed-chunk-10-1} \includegraphics[width=0.5\linewidth]{5_Statistics_files/figure-latex/unnamed-chunk-10-2}

The first bar plot (left) shows the raw counts of churners and non-churners across the two categories of \emph{Voice Mail Plan} (Yes or No), while the second plot (right) displays proportions, allowing direct comparison of churn rates. These visualizations suggest that customers without a Voice Mail Plan may have a higher churn rate, but hypothesis testing is needed to confirm whether this difference is statistically meaningful.

To formally test whether the proportion of churners with a Voice Mail Plan differs from the proportion of non-churners with the plan, we establish the following hypotheses:

\begin{itemize}
\item
  \emph{Null Hypothesis (\(H_0\))}: \(\pi_1 = \pi_2\)\\
  (The proportions of customers with a Voice Mail Plan are the same for churners and non-churners.)
\item
  \emph{Alternative Hypothesis (\(H_a\))}: \(\pi_1 \neq \pi_2\)\\
  (The proportions of customers with a Voice Mail Plan differ between churners and non-churners.)
\end{itemize}

These can also be expressed mathematically as:
\[
\begin{cases}
    H_0: \pi_1 = \pi_2   \\
    H_a: \pi_1 \neq \pi_2 
\end{cases}
\]

To perform the Z-test in R, we begin by creating a contingency table to summarize the counts of customers with and without a Voice Mail Plan in the churner and non-churner groups. This can be done using the \texttt{table()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_plan }\OtherTok{=} \FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn, churn}\SpecialCharTok{$}\NormalTok{voice.plan, }\AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"churn"}\NormalTok{, }\StringTok{"voice.plan"}\NormalTok{))}
\NormalTok{table\_plan}
\NormalTok{        voice.plan}
\NormalTok{   churn  yes   no}
\NormalTok{     yes  }\DecValTok{102}  \DecValTok{605}
\NormalTok{     no  }\DecValTok{1221} \DecValTok{3072}
\end{Highlighting}
\end{Shaded}

This table displays the count of churners and non-churners with and without a Voice Mail Plan. To conduct the Z-test, we use the \texttt{prop.test()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z\_test }\OtherTok{=} \FunctionTok{prop.test}\NormalTok{(table\_plan)}
\NormalTok{z\_test}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  table\_plan}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{60.552}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{7.165e{-}15}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.1701734} \SpecialCharTok{{-}}\FloatTok{0.1101165}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.1442716} \FloatTok{0.2844165}
\end{Highlighting}
\end{Shaded}

The output provides the \emph{p}-value, confidence interval, and sample proportions. Since the \emph{p}-value (0) is smaller than the significance level (\(\alpha = 0.05\)), we reject the null hypothesis. This result suggests that the proportion of customers with a Voice Mail Plan differs significantly between churners and non-churners.

The test also provides a \emph{95\% confidence interval} = {[}-0.1702, -0.1101{]} for the difference in proportions. Since this interval does not include zero, it reinforces the conclusion that the difference is statistically significant. Additionally, the sample proportions---0.1443 for churners and 0.2844 for non-churners---provide insight into the magnitude of this difference.

From a business perspective, this finding suggests that customers without a Voice Mail Plan may be more likely to churn. Companies could leverage this information by encouraging Voice Mail Plan subscriptions among at-risk customers or investigating whether the plan improves customer satisfaction and retention. Companies could leverage this information by encouraging Voice Mail Plan subscriptions among at-risk customers or investigating whether the plan improves customer satisfaction and retention. Although the Z-test shows a statistically significant difference, businesses should evaluate whether promoting the Voice Mail Plan meaningfully reduces churn rates and justifies marketing investment.

The two-sample Z-test provides a formal approach to comparing proportions between groups, complementing exploratory data analysis. By integrating statistical testing with business insights, companies can validate patterns and take targeted actions to reduce churn.

\section{Chi-square Test}\label{chi-square-test}

While the \emph{two-sample Z-test} is effective for comparing proportions between two groups, many real-world analyses involve categorical variables with more than two levels. The \emph{Chi-square test} allows us to assess whether multiple categorical groups are associated, providing a broader framework for understanding categorical relationships. This makes it particularly useful for understanding customer behaviors and business outcomes across multiple categories.

Unlike the Z-test, which focuses on comparing proportions between two groups, the Chi-square test evaluates whether distributions across multiple categories differ significantly from what would be expected under independence. It provides a formal way to test relationships between categorical variables and is widely used in marketing analysis, customer segmentation, and business decision-making.

To illustrate, we examine whether marital status is associated with purchasing a deposit in the \emph{bank} dataset (available in the \textbf{liver} package). This dataset will be revisited in Chapters \ref{chapter-knn} and \ref{chapter-nn} for classification modeling. The variable \texttt{marital} has three categories: ``divorced,'' ``married,'' and ``single,'' while the target variable \texttt{deposit} has two categories: ``yes'' (customers who purchased a deposit) and ``no'' (customers who did not). Our goal is to determine whether marital status influences deposit purchases.

We begin by visualizing the relationship between \texttt{marital} and \texttt{deposit} using bar plots:

\includegraphics[width=0.5\linewidth]{5_Statistics_files/figure-latex/unnamed-chunk-13-1} \includegraphics[width=0.5\linewidth]{5_Statistics_files/figure-latex/unnamed-chunk-13-2}

The first bar plot (left) displays the raw counts of deposit purchases across marital categories, while the second plot (right) presents the relative proportions. Visual inspection suggests differences in deposit purchase rates by marital status, but a statistical test is needed to confirm whether these differences are significant.

We summarize the observed counts in a contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_marital }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(bank}\SpecialCharTok{$}\NormalTok{deposit, bank}\SpecialCharTok{$}\NormalTok{marital, }\AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"deposit"}\NormalTok{, }\StringTok{"marital"}\NormalTok{))}
\NormalTok{table\_marital}
\NormalTok{          marital}
\NormalTok{   deposit divorced married single}
\NormalTok{       no       }\DecValTok{451}    \DecValTok{2520}   \DecValTok{1029}
\NormalTok{       yes       }\DecValTok{77}     \DecValTok{277}    \DecValTok{167}
\end{Highlighting}
\end{Shaded}

To formally test for independence, we define the hypotheses:
\[
\begin{cases}
    H_0: \pi_{divorced, \ yes} = \pi_{married, \ yes} = \pi_{single, \ yes}  \\
    H_a: At \ least \ one \ of \ the \ claims \ in \ H_0 \ is \ wrong.
\end{cases}
\]
The Chi-square test is applied using the \texttt{chisq.test()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chisq\_test }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(table\_marital)}
\NormalTok{chisq\_test}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s Chi{-}squared test}
\StringTok{   }
\StringTok{   data:  table\_marital}
\StringTok{   X{-}squared = 19.03, df = 2, p{-}value = 7.374e{-}05}
\end{Highlighting}
\end{Shaded}

The output includes the \emph{p}-value, Chi-square test statistic, degrees of freedom, and expected frequencies under \(H_0\). If the \emph{p}-value = \ensuremath{7.3735354\times 10^{-5}} is smaller than \(\alpha = 0.05\), we reject the null hypothesis, concluding that marital status and deposit purchases are not independent. This means that at least one marital group differs significantly from the others in deposit purchase rates.

Examining the expected frequencies can reveal which marital groups contribute most to the observed association. If a particular group has a much higher or lower deposit purchase rate than expected, marketing efforts can be tailored accordingly.

From a business perspective, these findings suggest that banks may benefit from personalizing their marketing strategies based on marital status. For example, if married customers are significantly more likely to purchase deposits, targeted promotional campaigns could emphasize financial planning for families. Conversely, if single customers exhibit lower deposit adoption rates, banks might develop incentive programs tailored to their financial goals.

The Chi-square test is a powerful tool for identifying relationships between categorical variables. By integrating visual analysis, contingency tables, and statistical hypothesis testing, businesses can make data-driven decisions to optimize customer engagement and product offerings.

\section{Analysis of Variance (ANOVA) Test}\label{analysis-of-variance-anova-test}

In the previous sections, we explored hypothesis tests that compare two groups, such as the \emph{two-sample t-test} and \emph{Z-test}. However, in many real-world scenarios, categorical variables have more than two levels. In such cases, the \emph{Analysis of Variance (ANOVA)} provides a systematic way to determine whether a numerical variable differs across multiple groups. It evaluates whether at least one group mean differs significantly from the others. ANOVA is especially useful when analyzing the relationship between a numerical variable and a categorical variable with multiple levels, providing a formal way to determine if the categorical variable impacts the numerical variable. The test relies on the F-distribution to assess whether the observed differences in means are statistically significant.

To illustrate, let's analyze the relationship between the variable \texttt{cut} and the target variable \texttt{price} in the popular \emph{diamonds} dataset (available in the \textbf{ggplot2} package). See \emph{Section X.X} for an overview of this dataset. The variable \texttt{cut} has five categories (``Fair,'' ``Good,'' ``Very Good,'' ``Premium,'' and ``Ideal''), while \texttt{price} is numerical. Our objective is to test whether the mean price of diamonds differs across the five cut categories.

We begin with a box plot to visualize the distribution of diamond prices for each category of \texttt{cut}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(diamonds)   }

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ cut, }\AttributeTok{y =}\NormalTok{ price, }\AttributeTok{fill =}\NormalTok{ cut)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"palevioletred1"}\NormalTok{, }\StringTok{"darkseagreen1"}\NormalTok{, }\StringTok{"skyblue1"}\NormalTok{, }\StringTok{"gold1"}\NormalTok{, }\StringTok{"lightcoral"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{5_Statistics_files/figure-latex/unnamed-chunk-16-1} \end{center}

The box plot displays the spread and median prices for diamonds in each cut category. While differences in medians and ranges suggest that cut quality might influence price, statistical testing is required to confirm whether these differences are significant. We apply an ANOVA test to formally assess this relationship.

To test whether the mean prices differ by cut type, we set up the following hypotheses:

\[
\begin{cases}
    H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu_5 \quad \text{(All group means are equal.)} \\
    H_a: \text{At least one group mean differs from the others.}  
\end{cases}
\]

To conduct the ANOVA test in R, we use the \texttt{aov()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anova\_test }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cut, }\AttributeTok{data =}\NormalTok{ diamonds)}
\FunctionTok{summary}\NormalTok{(anova\_test)}
\NormalTok{                  Df    Sum Sq   Mean Sq F value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\NormalTok{F)    }
\NormalTok{   cut             }\DecValTok{4} \FloatTok{1.104e+10} \FloatTok{2.760e+09}   \FloatTok{175.7} \SpecialCharTok{\textless{}}\FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   Residuals   }\DecValTok{53935} \FloatTok{8.474e+11} \FloatTok{1.571e+07}                   
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

The output provides the test statistic (F-value), degrees of freedom, and the \emph{p}-value. Since the \emph{p}-value is smaller than the significance level (\(\alpha = 0.05\)), we reject the null hypothesis. This indicates that the variable \texttt{cut} has a significant impact on the price of diamonds.

Rejecting \(H_0\) in ANOVA does not specify which groups differ. To identify these differences, \emph{post-hoc tests} such as Tukey's Honestly Significant Difference (Tukey HSD) test are necessary. These tests control for multiple comparisons while pinpointing significant pairwise differences. In this example, we could apply Tukey's test to determine which cut categories (e.g., ``Ideal'' vs.~``Good'') drive the observed differences.

Understanding the impact of diamond cut on price is crucial for pricing strategies and consumer insights. If higher-quality cuts command significantly higher prices, retailers may adjust marketing efforts accordingly. Conversely, if certain mid-tier cuts do not show meaningful price differences, companies might reconsider their pricing models to enhance competitiveness.

The ANOVA test provides a structured approach to evaluating whether a categorical variable with multiple levels influences a numerical variable. In this case, the relationship between \texttt{cut} and \texttt{price} suggests that diamond cut type is an important predictor of price, offering valuable insights into how quality impacts cost. By integrating statistical testing with business insights, analysts can determine whether categorical variables have meaningful effects and use this knowledge to inform data-driven decisions.

\section{Correlation Test}\label{correlation-test}

In the previous sections, we explored hypothesis tests for comparing means and proportions across groups. When analyzing relationships between two numerical variables, correlation testing provides a formal method to assess whether a significant linear association exists. The \emph{correlation test} evaluates both the strength and direction of the relationship by testing the null hypothesis that the population correlation coefficient (\(\rho\)) is equal to zero. This test is particularly useful in understanding how two continuous variables co-vary, which can inform business strategies, pricing models, and predictive analytics.

To illustrate, we examine whether a significant relationship exists between \texttt{carat} (diamond weight) and \texttt{price} in the \emph{diamonds} dataset (available in the \textbf{ggplot2} package). Since larger diamonds are generally more expensive, we expect a positive correlation between these variables. A scatter plot provides an initial visual assessment of the relationship:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carat, }\AttributeTok{y =}\NormalTok{ price), }\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Carat"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Price"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{5_Statistics_files/figure-latex/unnamed-chunk-18-1} \end{center}

The scatter plot shows a clear upward trend, suggesting that as \emph{carat} increases, so does \emph{price}. However, visualizations alone do not confirm statistical significance. To formally test this relationship, we establish the following hypotheses:

\[
\begin{cases}
    H_0: \rho   =  0 \quad \text{(There is no linear correlation between `carat` and `price`.)} \\
    H_a: \rho \neq 0 \quad \text{(There is a significant linear correlation between `carat` and `price`.)}
\end{cases}
\]

To conduct the correlation test in R, we use the \texttt{cor.test()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cor\_test }\OtherTok{\textless{}{-}} \FunctionTok{cor.test}\NormalTok{(diamonds}\SpecialCharTok{$}\NormalTok{carat, diamonds}\SpecialCharTok{$}\NormalTok{price)}
\NormalTok{cor\_test}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s product{-}moment correlation}
\StringTok{   }
\StringTok{   data:  diamonds$carat and diamonds$price}
\StringTok{   t = 551.41, df = 53938, p{-}value \textless{} 2.2e{-}16}
\StringTok{   alternative hypothesis: true correlation is not equal to 0}
\StringTok{   95 percent confidence interval:}
\StringTok{    0.9203098 0.9228530}
\StringTok{   sample estimates:}
\StringTok{         cor }
\StringTok{   0.9215913}
\end{Highlighting}
\end{Shaded}

The output provides key results, including the \emph{p}-value, correlation coefficient, and confidence interval:

\begin{itemize}
\tightlist
\item
  \textbf{p-value}: If the \emph{p}-value = 0 is smaller than the significance level (\(\alpha = 0.05\)), we reject \(H_0\), confirming that the correlation is statistically significant.
\item
  \textbf{Correlation Coefficient}: The correlation coefficient (\(r = 0.92\)) quantifies the strength and direction of the relationship. A value close to 1 indicates a strong positive correlation, while a value near 0 suggests no linear association.
\item
  \textbf{Confidence Interval}: The 95\% confidence interval {[}0.92, 0.92{]} provides a plausible range for the true population correlation (\(\rho\)). If this interval does not include 0, it further supports rejecting \(H_0\) and confirms a meaningful association.
\end{itemize}

The correlation coefficient of 0.92 suggests a strong positive relationship between \emph{carat} and \emph{price}, meaning that larger diamonds tend to be more expensive. The small \emph{p}-value confirms that this pattern is unlikely due to random variation, while the confidence interval provides an estimate of how precisely we can measure this correlation.

Beyond statistical significance, this relationship has practical implications for diamond pricing strategies. If the correlation is particularly strong, pricing models could rely more on \emph{carat} as a key determinant of value. However, if variability remains high despite a significant correlation, additional factors---such as diamond clarity, cut, or market conditions---may play an influential role. Further analysis could involve multivariate regression to assess how \emph{carat} interacts with other attributes in predicting price.

By integrating visualization, statistical inference, and business insights, the correlation test offers a robust framework for understanding numerical relationships. This approach ensures that observed trends are both statistically sound and practically meaningful, laying the foundation for more advanced modeling techniques.

\section{Wrapping Up}\label{wrapping-up}

This chapter provided a foundation for statistical inference, beginning with \emph{estimation}, where we explored how point estimates and confidence intervals help quantify population parameters while accounting for uncertainty. We then introduced \emph{hypothesis testing}, learning how to formulate null and alternative hypotheses, compute test statistics, and interpret \emph{p}-values to make informed decisions. Through practical examples, we applied various statistical tests, including \emph{t}-tests for comparing means, proportion tests for categorical data, ANOVA for assessing differences across multiple groups, and the Chi-square test and correlation analysis for uncovering relationships between variables. Together, these tools form a robust framework for extracting insights and answering key research questions.

Statistical inference plays a critical role in data-driven decision-making, helping analysts distinguish meaningful patterns from random variation. These methods are widely used in business and research, from evaluating marketing strategies to predicting customer behavior. However, reliable conclusions require more than statistical significance. It is essential to check assumptions, contextualize results, and integrate domain knowledge to ensure findings are both accurate and actionable.\\
While statistical inference and hypothesis testing are essential tools in data science, they fall outside the scope of machine learning. If you are interested in exploring these topics further, we recommend introductory statistics textbooks such as \emph{Intuitive Introductory Statistics} by Wolfe and Schneider \citep{wolfe2017intuitive}.

In the next chapter, we transition from statistical inference to predictive modeling, focusing on how to partition datasets effectively. Just as hypothesis testing helps determine whether patterns in data are real, proper data partitioning ensures that machine learning models generalize well to unseen data. As we move forward, ensuring data validity and model robustness will be key to building reliable predictive systems.

\section{Exercises}\label{exercises-3}

\subsection*{Conceptual Questions}\label{conceptual-questions-1}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is hypothesis testing important in data science? Explain its role in making data-driven decisions and how it complements exploratory data analysis.
\item
  What is the difference between a confidence interval and a hypothesis test? How do they provide different ways of drawing conclusions about population parameters?
\item
  The \emph{p}-value represents the probability of observing the sample data, or something more extreme, assuming the null hypothesis is true. How should \emph{p}-values be interpreted, and why is a \emph{p}-value of 0.001 in a two-sample \emph{t}-test not necessarily evidence of practical significance?
\item
  Explain the concepts of \emph{Type I} and \emph{Type II} errors in hypothesis testing. Why is it important to balance the risks of these errors when designing statistical tests?
\item
  In a hypothesis test, failing to reject the null hypothesis does not imply that the null hypothesis is true. Explain why this is the case and discuss the implications of this result in practice.
\item
  When working with small sample sizes, why is the \emph{t}-distribution used instead of the normal distribution? How does the shape of the \emph{t}-distribution change as the sample size increases?
\item
  One-tailed and two-tailed hypothesis tests serve different purposes. When would a one-tailed test be more appropriate than a two-tailed test? Provide an example where each type of test would be applicable.
\item
  Both the two-sample \emph{Z}-test and the Chi-square test analyze categorical data but serve different purposes. How do they differ, and when would one be preferred over the other?
\item
  The \emph{Analysis of Variance} (ANOVA) test is designed to compare means across multiple groups. Why can't multiple \emph{t}-tests be used instead? What is the advantage of using ANOVA in this context?
\end{enumerate}

\subsection*{Hands-On Practice: Hypothesis Testing in R}\label{hands-on-practice-hypothesis-testing-in-r}


For the following exercises, use the \emph{churn}, \emph{bank}, \emph{marketing}, and \emph{diamonds} datasets available in the \textbf{liver} and \textbf{ggplot2} packages. We have previously used the \emph{churn}, \emph{bank}, and \emph{diamonds} datasets in this and earlier chapters. In Chapter \ref{chapter-regression}, we will introduce the \emph{marketing} dataset for regression analysis.

To load the datasets, use the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{library}\NormalTok{(ggplot2)   }

\CommentTok{\# To import the datasets}
\FunctionTok{data}\NormalTok{(churn)  }
\FunctionTok{data}\NormalTok{(bank)  }
\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)  }
\FunctionTok{data}\NormalTok{(diamonds)  }
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  We are interested in knowing the 90\% confidence interval for the population mean of the variable ``\texttt{night.calls}'' in the \emph{churn} dataset. In \textbf{R}, we can obtain a confidence interval for the population mean using the \texttt{t.test()} function as follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn}\SpecialCharTok{$}\NormalTok{night.calls, }\AttributeTok{conf.level =} \FloatTok{0.90}\NormalTok{)}\SpecialCharTok{$}\StringTok{"conf.int"}
\NormalTok{   [}\DecValTok{1}\NormalTok{]  }\FloatTok{99.45484} \FloatTok{100.38356}
   \FunctionTok{attr}\NormalTok{(,}\StringTok{"conf.level"}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.9}
\end{Highlighting}
\end{Shaded}

Interpret the confidence interval in the context of customer service calls made at night. Report the 99\% confidence interval for the population mean of ``\texttt{night.calls}'' and compare it with the 90\% confidence interval. Which interval is wider, and what does this indicate about the precision of the estimates? Why does increasing the confidence level result in a wider interval, and how does this impact decision-making in a business context?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Subgroup analyses help identify behavioral patterns in specific customer segments. In the \emph{churn} dataset, we focus on customers with both an \emph{International Plan} and a \emph{Voice Mail Plan} who make more than 220 daytime minutes of calls. To create this subset, we use:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub\_churn }\OtherTok{=} \FunctionTok{subset}\NormalTok{(churn, (intl.plan }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (voice.plan }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (day.mins }\SpecialCharTok{\textgreater{}} \DecValTok{220}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

Next, we compute the 95\% confidence interval for the proportion of churners in this subset using \texttt{prop.test()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(sub\_churn}\SpecialCharTok{$}\NormalTok{churn), }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}\SpecialCharTok{$}\StringTok{"conf.int"}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.2595701} \FloatTok{0.5911490}
   \FunctionTok{attr}\NormalTok{(,}\StringTok{"conf.level"}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.95}
\end{Highlighting}
\end{Shaded}

Compare this confidence interval with the overall churn rate in the dataset (see Section \ref{statistics-confidence-interval}). What insights can be drawn about this customer segment, and how might they inform retention strategies?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  In the \emph{churn} dataset, we test whether the mean number of customer service calls (\texttt{customer.calls}) is greater than 1.5 at a significance level of 0.01. The right-tailed test is formulated as:
\end{enumerate}

\[
\begin{cases}
  H_0:  \mu \leq 1.5 \\
  H_a:  \mu > 1.5
\end{cases}
\]

Since the level of significance is \(\alpha = 0.01\), the confidence level is \(1-\alpha = 0.99\). We perform the test using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ churn}\SpecialCharTok{$}\NormalTok{customer.calls, }
        \AttributeTok{mu =} \FloatTok{1.5}\NormalTok{, }
        \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{, }
        \AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
   
\NormalTok{    One Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  churn}\SpecialCharTok{$}\NormalTok{customer.calls}
\NormalTok{   t }\OtherTok{=} \FloatTok{3.8106}\NormalTok{, df }\OtherTok{=} \DecValTok{4999}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{7.015e{-}05}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true mean is greater than }\FloatTok{1.5}
   \DecValTok{99}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{1.527407}      \ConstantTok{Inf}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean of x }
      \FloatTok{1.5704}
\end{Highlighting}
\end{Shaded}

Report the \emph{p}-value and determine whether to reject the null hypothesis at \(\alpha=0.01\). Explain your decision and discuss its implications in the context of customer service interactions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\tightlist
\item
  In the \emph{churn} dataset, we test whether the proportion of churners (\(\pi\)) is less than 0.14 at a significance level of \(\alpha=0.01\). The confidence level is \(99\%\), corresponding to \(1-\alpha = 0.99\). The test is conducted in \textbf{R} using:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn), }
           \AttributeTok{p =} \FloatTok{0.14}\NormalTok{, }
           \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{, }
           \AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
   
    \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{sample proportions test with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{table}\NormalTok{(churn}\SpecialCharTok{$}\NormalTok{churn), null probability }\FloatTok{0.14}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.070183}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.6045}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true p is less than }\FloatTok{0.14}
   \DecValTok{99}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.0000000} \FloatTok{0.1533547}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{        p }
   \FloatTok{0.1414}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Report the \emph{p}-value and determine whether to reject the null hypothesis at \(\alpha=0.01\). Explain your conclusion and its potential impact on customer retention strategies.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{13}
\tightlist
\item
  In the \emph{churn} dataset, we examine whether the number of customer service calls (\texttt{customer.calls}) differs between churners and non-churners. To test this, we perform a two-sample \emph{t}-test:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(customer.calls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn, }\AttributeTok{data =}\NormalTok{ churn)}
   
\NormalTok{    Welch Two Sample t}\SpecialCharTok{{-}}\NormalTok{test}
   
\NormalTok{   data}\SpecialCharTok{:}\NormalTok{  customer.calls by churn}
\NormalTok{   t }\OtherTok{=} \FloatTok{11.292}\NormalTok{, df }\OtherTok{=} \FloatTok{804.21}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\SpecialCharTok{\textless{}} \FloatTok{2.2e{-}16}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ true difference }\ControlFlowTok{in}\NormalTok{ means between group yes and group no is not equal to }\DecValTok{0}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \FloatTok{0.6583525} \FloatTok{0.9353976}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   mean }\ControlFlowTok{in}\NormalTok{ group yes  mean }\ControlFlowTok{in}\NormalTok{ group no }
            \FloatTok{2.254597}          \FloatTok{1.457722}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Determine whether to reject the null hypothesis at a significance level of \(\alpha=0.05\). Report the \emph{p}-value and interpret the results, explaining whether there is evidence of a relationship between churn status and customer service call frequency.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\tightlist
\item
  In the \emph{marketing} dataset, we test whether there is a \emph{positive} relationship between \texttt{revenue} and \texttt{spend} at a significance level of \(\alpha = 0.025\). We perform a one-tailed correlation test using:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marketing}\SpecialCharTok{$}\NormalTok{spend, }
         \AttributeTok{y =}\NormalTok{ marketing}\SpecialCharTok{$}\NormalTok{revenue, }
         \AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{, }
         \AttributeTok{conf.level =} \FloatTok{0.975}\NormalTok{)}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s product{-}moment correlation}
\StringTok{   }
\StringTok{   data:  marketing$spend and marketing$revenue}
\StringTok{   t = 7.9284, df = 38, p{-}value = 7.075e{-}10}
\StringTok{   alternative hypothesis: true correlation is greater than 0}
\StringTok{   97.5 percent confidence interval:}
\StringTok{    0.6338152 1.0000000}
\StringTok{   sample estimates:}
\StringTok{        cor }
\StringTok{   0.789455}
\end{Highlighting}
\end{Shaded}

State the null and alternative hypotheses. Report the \emph{p}-value and determine whether to reject the null hypothesis. Explain your decision and discuss its implications for understanding the relationship between marketing spend and revenue.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  In the \emph{churn} dataset, for the variable ``\texttt{day.mins}'', test whether the mean number of ``Day Minutes'' is greater than 180. Set the level of significance to be 0.05.
\item
  In the \emph{churn} dataset, for the variable ``\texttt{intl.plan}'' test at \(\alpha=0.05\) weather the proportion of customers who have international plan is less than 0.15.
\item
  In the \emph{churn} dataset, test whether there is a relationship between the target variable ``\texttt{churn}'' and the variable ``\texttt{intl.charge}'' with \(\alpha=0.05\).
\item
  In the \emph{bank} dataset, test whether there is a relationship between the target variable ``\texttt{deposit}'' and the variable ``\texttt{education}'' with \(\alpha=0.05\).
\item
  Compute the proportion of customers in the \emph{churn} dataset who have an International Plan (\texttt{intl.plan}). Construct a 95\% confidence interval for this proportion using R, and interpret the confidence interval in the context of customer subscriptions.
\item
  Using the \emph{churn} dataset, test whether the average number of daytime minutes (\texttt{day.mins}) for churners differs significantly from 200 minutes. Conduct a one-sample \emph{t}-test in R and interpret the results in relation to customer behavior.
\item
  Compare the average number of international calls (\texttt{intl.calls}) between churners and non-churners. Perform a two-sample \emph{t}-test and evaluate whether the observed differences in means are statistically significant.
\item
  Test whether the proportion of customers with a Voice Mail Plan (\texttt{voice.plan}) differs between churners and non-churners. Use a two-sample \emph{Z}-test in R and interpret the results, considering the implications for customer retention strategies.
\item
  Investigate whether marital status (\texttt{marital}) is associated with deposit subscription (\texttt{deposit}) in the \emph{bank} dataset. Construct a contingency table and perform a Chi-square test to assess whether marital status has a significant impact on deposit purchasing behavior.
\item
  Using the \emph{diamonds} dataset, test whether the mean price of diamonds differs across different diamond cuts (\texttt{cut}). Conduct an ANOVA test and interpret the results. If the test finds significant differences, discuss how post-hoc tests could be used to further explore the findings.
\item
  Assess the correlation between \texttt{carat} and \texttt{price} in the \emph{diamonds} dataset. Perform a correlation test in R and visualize the relationship using a scatter plot. Interpret the results in the context of diamond pricing.
\item
  Construct a 95\% confidence interval for the mean number of customer service calls (\texttt{customer.calls}) among churners. Explain how the confidence interval helps quantify uncertainty and how it might inform business decisions regarding customer support.
\item
  Take a random sample of 100 observations from the \emph{churn} dataset and test whether the average \texttt{eve.mins} differs from 200. Repeat the test using a sample of 1000 observations. Compare the results and discuss how sample size affects hypothesis testing and statistical power.
\item
  Suppose a hypothesis test indicates that customers with a Voice Mail Plan are significantly less likely to churn (\emph{p} \textless{} 0.01). What are some potential business strategies a company could implement based on this finding? Beyond statistical significance, what additional factors should be considered before making marketing decisions?
\end{enumerate}

\chapter{Preparing Data to Model}\label{chapter-modeling}

Before we can build reliable machine learning models, we must ensure our data is well-prepared. The previous chapters established a foundation by addressing key steps in the Data Science Workflow (Figure \ref{fig:CRISP-DM}). Now, we focus on transitioning from data exploration to model building.

In Section \ref{problem-understanding}, we discussed defining the problem and aligning objectives with data-driven strategies. Chapter \ref{chapter-data-prep} addressed handling missing values, outliers, and data transformations to create a clean dataset. In Chapter \ref{chapter-EDA}, we visualized data to uncover patterns, while Chapter \ref{chapter-statistics} introduced statistical inference, including hypothesis testing and feature selection---tools that will help us validate our data partitioning.

Before diving into machine learning, we must complete the \emph{Setup Phase}, which ensures that our dataset is structured for robust model development. This phase involves three essential steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Partitioning the Data}: Splitting the dataset into training and testing sets to create a clear separation between model learning and evaluation.\\
\item
  \textbf{Validating the Partition}: Ensuring that the split is representative and unbiased so that insights from training generalize to new data.\\
\item
  \textbf{Balancing the Training Dataset}: Addressing potential class imbalances in categorical targets to prevent biased models.
\end{enumerate}

Although often overlooked, these steps are critical in ensuring that the modeling process is rigorous, fair, and effective. Students often ask, \emph{``Why is it necessary to partition the data?''} or \emph{``Why do we need to follow these specific steps?''} These are important questions, and we will address them throughout this chapter. But before we do, it's useful to briefly examine how the data science process aligns with and diverges from statistical inference. Understanding these similarities and differences helps bridge traditional statistics with the practical demands of modern machine learning.

\section{Statistical Inference in the Context of Data Science}\label{statistical-inference-in-the-context-of-data-science}

Although statistical inference remains a fundamental tool in data science, its role shifts when preparing data for modeling, as the goals and applications differ. While traditional inference focuses on drawing conclusions about populations from sample data, machine learning prioritizes predictive accuracy and generalization.

Statistical inference and data science diverge in two key ways when applied to modeling tasks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{From Significance to Practicality}: With large datasets containing thousands or even millions of observations, nearly any detected relationship can become statistically significant. However, statistical significance does not always translate into practical importance. A machine learning model might identify a minute effect size that is statistically valid but has negligible impact on decision-making. In modeling, the focus shifts from statistical significance to assessing whether an effect is strong enough to meaningfully influence predictions.
\item
  \emph{Exploration vs.~Hypothesis Testing}: Traditional statistical inference begins with a predefined hypothesis, such as testing whether a new treatment improves outcomes compared to a control. In contrast, data science often adopts an exploratory approach, using data to uncover patterns, relationships, and predictive features without rigid hypotheses. Rather than testing predefined relationships, machine learning practitioners iteratively refine datasets and evaluate which features contribute most to predictive accuracy.
\end{enumerate}

Despite these differences, statistical inference remains crucial in key stages of data preparation, particularly in:

\begin{itemize}
\tightlist
\item
  \emph{Partition Validation}: When dividing data into training and testing sets, statistical tests help ensure the subsets are representative of the original dataset.
\item
  \emph{Feature Selection}: Hypothesis testing can aid in selecting features that have strong relationships with the target variable, enhancing model performance.
\end{itemize}

By understanding these differences and applying statistical inference strategically, we can ensure that data preparation supports building robust, interpretable, and generalizable models. Throughout this chapter, we will see how inference techniques continue to play a role in refining datasets for machine learning.

\section{Why Is It Necessary to Partition the Data?}\label{why-is-it-necessary-to-partition-the-data}

Partitioning the dataset is a crucial step in preparing data for modeling. A common question students ask is, \emph{Why do we need to partition the data?} The answer lies in \emph{generalization}---the ability of a model to perform well on unseen data. Without proper partitioning, models may fit the training data exceptionally well but fail to make accurate predictions in real-world scenarios. Partitioning ensures that performance is evaluated on data the model has not seen before, providing an unbiased measure of its ability to generalize effectively.

Partitioning involves dividing the dataset into two subsets: the \emph{training set}, used to build the model, and the \emph{testing set}, used to evaluate performance. This separation simulates real-world conditions, where the model must make predictions on new data. It helps detect and address two common pitfalls in machine learning: \emph{overfitting} and \emph{underfitting}. These trade-offs are illustrated in Figure \ref{fig:model-complexity}, which highlights the balance between model complexity and performance on training and testing datasets.

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth]{images/ch6_model_complexity} 

}

\caption{The trade-off between model complexity and accuracy on the training and test sets. It highlights the optimal model complexity (sweet spot), where the test set accuracy reaches its highest value for unseen data.}\label{fig:model-complexity}
\end{figure}

\textbf{Overfitting} occurs when a model memorizes the training data, including noise and random fluctuations, instead of capturing general patterns. Such models achieve high accuracy on the training set but perform poorly on unseen data. For instance, a churn prediction model might memorize specific customer IDs rather than recognizing broader behavioral trends, making it ineffective for new customers.

\textbf{Underfitting}, in contrast, occurs when a model is too simplistic to capture underlying patterns. This might happen if the model lacks complexity or if preprocessing removes too much useful information. An underfitted churn model, for example, might predict a constant churn rate for all customers without considering individual differences, leading to poor performance.

Partitioning mitigates these risks by allowing us to evaluate performance on unseen data. Comparing accuracy on the training and testing sets helps determine whether the model is overfitting (high training accuracy but low testing accuracy) or underfitting (low accuracy on both). This evaluation enables iterative refinements to strike the right balance between complexity and generalization.

Partitioning also prevents \emph{data leakage}, a critical issue where information from the testing set inadvertently influences training. Data leakage inflates performance metrics, creating a false sense of confidence in the model's ability to generalize. Strictly separating the testing set from the training process ensures a more realistic assessment of model performance.

Beyond a simple train-test split, \emph{cross-validation} further enhances robustness. In cross-validation, the dataset is divided into multiple subsets (\emph{folds}). The model is trained on one subset and tested on another, repeating this process across all folds. The results are averaged to provide a more reliable estimate of model performance. Cross-validation is particularly useful when working with small datasets or tuning hyperparameters, as it minimizes bias introduced by a single train-test split.

Partitioning isn't just a technical step---it's fundamental to building models that generalize well. By addressing overfitting, underfitting, and data leakage, and by leveraging techniques like cross-validation, we ensure that models are both accurate and reliable in real-world applications.

To summarize, the general strategy for supervised machine learning consists of three key steps, illustrated in Figure \ref{fig:modeling}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Partitioning} the dataset into training and testing sets, followed by validating the partition.
\item
  \textbf{Building} machine learning models on the training data.
\item
  \textbf{Evaluating} the performance of models on the testing data to select the most effective approach.
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth]{images/ch6_partitioning} 

}

\caption{A general predictive machine learning process for building and evaluating models. The 80-20 split ratio is an example and may vary based on the dataset and task.}\label{fig:modeling}
\end{figure}

By following this structured process, we build models that are both robust and capable of making accurate predictions on unseen data. This chapter focuses on the first step: partitioning the data effectively, validating the partition, and preparing a balanced training dataset---key steps for developing reliable and interpretable machine learning models.

\section{Partitioning the Data}\label{sec-partitioning}

Partitioning the dataset is a fundamental step in preparing data for machine learning. The most common approach is the \emph{train-test split}, also known as the holdout method, where the dataset is divided into two subsets: a \emph{training set} used to build the model and a \emph{testing set} reserved for evaluating its performance. This separation ensures that the model is assessed on unseen data, providing an unbiased estimate of how well it generalizes.

A typical train-test split ratio is 70-30, 80-20, or 90-10, depending on the dataset size and modeling needs. The training set contains all available features, including the target variable, which is used to teach the model patterns in the data. The testing set, however, has the target variable temporarily hidden to simulate real-world conditions. The trained model is then applied to the testing set to predict these hidden values, and its predictions are compared to the actual target values to assess performance.

\subsection*{Example: Train-Test Split in R}\label{example-train-test-split-in-r}


To illustrate the process of data partitioning, we revisit the \emph{churn} dataset from Section \ref{EDA-sec-churn}, where the goal is to predict customer churn. Since this dataset is included in the \textbf{liver} package, we begin by loading the package and dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(churn) }
\end{Highlighting}
\end{Shaded}

In R, there are multiple ways to split a dataset into training and test sets. A simple and efficient method is to use the \texttt{partition()} function from the \href{https://CRAN.R-project.org/package=liver}{\textbf{liver}} package. This function allows for random partitioning of a dataset according to a specified ratio. The following example demonstrates how to split the \emph{churn} dataset into training (80\%) and test (20\%) sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{43}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

The process begins with \texttt{set.seed(43)}, ensuring \textbf{reproducibility} by generating the same random partition every time the code is run. This is crucial for consistency in model evaluation and collaborative work. The \texttt{partition()} function then divides the dataset into two subsets:

\begin{itemize}
\tightlist
\item
  \texttt{train\_set} (80\% of the data) -- used for model training,\\
\item
  \texttt{test\_set} (20\% of the data) -- reserved for evaluating model performance.
\end{itemize}

Additionally, \texttt{test\_labels} stores the \emph{true labels} from the test set, which will later be used to assess the model's predictive accuracy.

Reproducibility is a fundamental principle in machine learning. By setting a seed, we ensure that the dataset split remains consistent across different runs, allowing for precise comparisons and validation of results. While the seed value itself is arbitrary, choosing one guarantees that the partitioning process remains stable.\\
\#\#\# Why Partitioning Matters \{-\}

The primary reason for partitioning is to prevent \emph{data leakage}---a situation where information from the testing set influences training, leading to overly optimistic performance estimates. By strictly separating these sets, we ensure that performance metrics reflect the model's ability to generalize to new data rather than just memorizing training patterns.

Beyond a simple train-test split, \emph{cross-validation} can further enhance robustness by training and testing the model on multiple subsets of the data. This method is particularly useful when working with small datasets or tuning hyperparameters.

Partitioning lays the groundwork for reliable machine learning models. However, a well-executed split alone does not guarantee that the training and testing sets are representative of the original dataset. In the next section, we will validate the partition to confirm that both subsets retain key statistical properties, ensuring fair and unbiased model evaluation.

\section{Validating the Partition}\label{sec-validate-partition}

The success of the entire modeling process depends on the quality of the data partition. Validating the partition ensures that both the training and testing sets are representative of the original dataset, enabling the model to learn from diverse examples and generalize effectively to unseen data. Without validation, the modeling process risks bias---either the model fails to generalize because the training set isn't representative, or the testing set doesn't provide an accurate evaluation of real-world performance.

Validation involves comparing the training and testing sets to confirm that their distributions are statistically similar, particularly for key variables. Since datasets often include many variables, this step typically focuses on a small set of randomly selected features or features of particular importance, such as the target variable. The choice of statistical test depends on the type of variable being compared, as shown in Table \ref{tab:partition-test}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4300}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5700}}@{}}
\caption{\label{tab:partition-test} Suggested hypothesis tests for validating partitions, based on the type of target variable.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Suggested Test (from Chapter \ref{chapter-statistics})
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Suggested Test (from Chapter \ref{chapter-statistics})
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Numerical variable & Two-sample t-test \\
Binary/Flag variable & Two-sample Z-test \\
Categorical variable (with \textgreater{} 2 categories) & Chi-square test \\
\end{longtable}

Validating the partition is more than a procedural step---it is a safeguard against biased modeling. If the training and testing sets differ significantly, the model's performance could be compromised. If the training set is not representative of the original dataset, the model may fail to generalize effectively. Conversely, if the testing set does not reflect the population, model evaluation could be misleading. Ensuring that the split retains the characteristics of the original dataset allows for fair and reliable model assessment.

\subsection*{\texorpdfstring{Example: Validating the Target Variable \emph{churn}}{Example: Validating the Target Variable churn}}\label{example-validating-the-target-variable-churn}


Let's consider the \emph{churn} dataset introduced in the previous section. The target variable, \emph{churn} (whether a customer has churned or not), is binary. According to Table \ref{tab:partition-test}, the appropriate statistical test to validate the partition for this variable is a \emph{Two-Sample Z-Test}, which compares the proportion of churned customers in the training and testing sets. Thus, the hypotheses for the test are:\\
\[
\begin{cases}
H_0:  \pi_{\text{churn, train}} = \pi_{\text{churn, test}} \quad \text{(Proportions are equal)} \\
H_a:  \pi_{\text{churn, train}} \neq \pi_{\text{churn, test}} \quad \text{(Proportions are not equal)}
\end{cases}
\]

Here's how it can be implemented in \textbf{R}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{churn }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\NormalTok{n1 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(train\_set)}
\NormalTok{n2 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(test\_set)}

\NormalTok{test\_churn }\OtherTok{\textless{}{-}} \FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(x1, x2), }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(n1, n2))}
\NormalTok{test\_churn}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{c}\NormalTok{(x1, x2) out of }\FunctionTok{c}\NormalTok{(n1, n2)}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.1566}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.6923}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.0190317}  \FloatTok{0.0300317}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{   prop }\DecValTok{1}\NormalTok{ prop }\DecValTok{2} 
   \FloatTok{0.1425} \FloatTok{0.1370}
\end{Highlighting}
\end{Shaded}

Here, \(x_1\) and \(x_2\) represent the number of churned customers in the training and testing sets, respectively, while \(n_1\) and \(n_2\) denote the total number of observations in each set. The \texttt{prop.test()} function is used to compare the proportions of churned customers between the two subsets.

The test result provides a \emph{p}-value = 0.69. Since the \emph{p}-value is greater than the significance level (\(\alpha = 0.05\)), we fail to reject the null hypothesis (\(H_0\)). This indicates no statistically significant difference in the proportions of churned customers between the training and testing sets. By failing to reject \(H_0\), we confirm that the partition is valid with respect to the target variable \emph{churn}. The proportions of churned customers are consistent across both subsets, ensuring that the model will be trained and tested on representative data.

While validating the target variable is crucial, extending this process to key predictors such as \texttt{customer.calls} or \texttt{day.mins} ensures that both subsets remain representative across all important features. For example, numerical features can be validated using a two-sample t-test, while categorical features with multiple levels can be assessed using a Chi-square test. This broader validation ensures that important variables retain their statistical properties across training and testing sets.

\subsection*{What If the Partition Is Invalid?}\label{what-if-the-partition-is-invalid}


If statistical tests reveal significant differences between the training and testing sets, adjustments are necessary to ensure the partition remains representative. Several strategies can be applied:

\begin{itemize}
\tightlist
\item
  \emph{Revisiting the partitioning process}: Changing the random seed or adjusting the split ratio can sometimes lead to a more balanced split.
\item
  \emph{Stratified sampling}: Ensuring that key categorical variables, such as the target variable, are proportionally represented in both subsets.
\item
  \emph{Cross-validation}: Using k-fold cross-validation instead of a single train-test split to provide a more robust evaluation of model performance.
\end{itemize}

Additionally, if the dataset is small or highly variable, minor differences between training and testing sets might be inevitable. In such cases, alternative approaches like bootstrapping can help validate model performance more effectively.

Validating the partition is a critical step in the data preparation process. It ensures that the modeling process is fair, reliable, and capable of producing generalizable results. By addressing potential discrepancies early, we set the stage for robust machine learning models that perform effectively on real-world, unseen data.

\section{Balancing the Training Dataset}\label{balancing-the-training-dataset}

In many real-world classification problems, one class of the target variable is significantly underrepresented. This imbalance can lead to biased models that perform well for the majority class but fail to predict the minority class accurately. For example, in fraud detection, fraudulent transactions are rare compared to legitimate ones, and in churn prediction, the majority of customers may not churn. Without addressing this issue, models may appear to perform well based on accuracy alone but fail to identify rare yet important events.

Imbalanced datasets pose a challenge because most machine learning algorithms optimize for overall accuracy, which can favor the majority class. A churn prediction model trained on an imbalanced dataset, for example, might classify nearly all customers as non-churners, leading to high accuracy but failing to detect actual churners. This is problematic when the minority class (e.g., fraud cases, churners, or patients with a rare disease) is the key focus of the analysis.

\subsection*{Techniques for Addressing Class Imbalance}\label{techniques-for-addressing-class-imbalance}


Balancing the training dataset ensures that both classes are adequately represented during model training, improving the model's ability to generalize. Several techniques can be used to address class imbalance:

\begin{itemize}
\tightlist
\item
  \emph{Oversampling}: Increasing the number of minority class examples by duplicating existing observations or generating synthetic samples. The \emph{Synthetic Minority Over-sampling Technique (SMOTE)} is a popular approach that generates synthetic examples instead of simple duplication.
\item
  \emph{Undersampling}: Reducing the number of majority class examples by randomly removing observations.
\item
  \emph{Hybrid Methods}: Combining oversampling and undersampling to achieve a balanced dataset.
\item
  \emph{Class Weights}: Modifying the algorithm to penalize misclassifications of the minority class more heavily.
\end{itemize}

The choice of technique depends on factors such as dataset size, the severity of imbalance, and the specific machine learning algorithm used.

\subsection*{\texorpdfstring{Example: Balancing the \emph{Churn} Dataset}{Example: Balancing the Churn Dataset}}\label{example-balancing-the-churn-dataset}


First, we examine the distribution of the target variable (\emph{churn}) in the training dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check the class distribution}
\FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn)}
   
\NormalTok{    yes   no }
    \DecValTok{570} \DecValTok{3430}
\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{churn))}
   
\NormalTok{      yes     no }
   \FloatTok{0.1425} \FloatTok{0.8575}
\end{Highlighting}
\end{Shaded}

Suppose the output shows that churners (\texttt{churn\ =\ "yes"}) constitute only 0.14, while non-churners (\texttt{churn\ =\ "no"}) make up 0.86. This significant imbalance suggests that balancing may be necessary, particularly if churn prediction is a business priority.

To address this, we use the \textbf{ROSE} package in R to oversample the minority class (\texttt{churn\ =\ "yes"}) so that it constitutes 30\% of the training dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the ROSE package}
\FunctionTok{library}\NormalTok{(ROSE)}

\CommentTok{\# Oversample the training set to balance the classes with 30\% churners}
\NormalTok{balanced\_train\_set }\OtherTok{\textless{}{-}} \FunctionTok{ovun.sample}\NormalTok{(churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{method =} \StringTok{"over"}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.3}\NormalTok{)}\SpecialCharTok{$}\NormalTok{data}

\CommentTok{\# Check the new class distribution}
\FunctionTok{table}\NormalTok{(balanced\_train\_set}\SpecialCharTok{$}\NormalTok{churn)}
   
\NormalTok{     no  yes }
   \DecValTok{3430} \DecValTok{1444}
\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(balanced\_train\_set}\SpecialCharTok{$}\NormalTok{churn))}
   
\NormalTok{          no       yes }
   \FloatTok{0.7037341} \FloatTok{0.2962659}
\end{Highlighting}
\end{Shaded}

In this example, the \texttt{ovun.sample()} function increases the proportion of churners to 30\% of the training dataset. The formula notation \texttt{churn\ \textasciitilde{}\ .} specifies that the balancing is applied based on the target variable (\emph{churn}). After oversampling, the new class distribution is checked to ensure the desired balance.

\subsection*{Key Considerations for Balancing}\label{key-considerations-for-balancing}


Balancing should be performed \emph{only on the training dataset}, not the test dataset. The test dataset should remain representative of the original class distribution to provide an unbiased evaluation of model performance. Modifying the test set would introduce bias and make the model's performance appear artificially better than it would be in real-world scenarios.

Furthermore, balancing must be applied \emph{after partitioning} the dataset. If balancing is done before splitting, information from the test set may influence the training process (\emph{data leakage}), leading to misleadingly high performance.

That said, balancing is \emph{not always necessary}. Many modern machine learning algorithms, such as random forests and gradient boosting, incorporate class weighting or ensemble learning to handle imbalanced datasets effectively. Additionally, alternative evaluation metrics such as \emph{precision, recall, F1-score, and AUC-ROC} can provide better insights into model performance when dealing with imbalanced classes.

In summary, balancing the training dataset can improve model performance, especially when the minority class is the primary focus. However, it is not always required and should be used selectively. If balancing is necessary, it must be applied \emph{only after partitioning} to maintain the validity of model evaluation. By ensuring that both classes are adequately represented during training, we help machine learning models make more accurate and fair predictions.

\section{Exercises}\label{exercises-4}

\subsection*{Conceptual Questions}\label{conceptual-questions-2}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is partitioning the dataset crucial before training a machine learning model? Explain its role in ensuring generalization.\\
\item
  What is the main risk of training a model without separating the dataset into training and testing subsets? Provide an example where this could lead to misleading results.
\item
  Explain the difference between \emph{overfitting} and \emph{underfitting}. How does proper partitioning help address these issues?
\item
  Describe the role of the \emph{training set} and the \emph{testing set} in machine learning. Why should the test set remain unseen during model training?
\item
  What is \emph{data leakage}, and how can it occur during data partitioning? Provide an example of a scenario where data leakage could lead to overly optimistic model performance.
\item
  Compare and contrast \emph{random partitioning} and \emph{stratified partitioning}. When would stratified partitioning be preferred?
\item
  Why is it necessary to validate the partition after splitting the dataset? What could go wrong if the training and test sets are significantly different?
\item
  How would you validate that numerical variables, such as \texttt{customer.calls} in the \emph{churn} dataset, have similar distributions in both the training and testing sets?
\item
  If a dataset is highly imbalanced, why might a model trained on it fail to generalize well? Provide an example from a real-world domain where class imbalance is a serious issue.
\item
  Compare \emph{oversampling}, \emph{undersampling}, and \emph{hybrid methods} for handling imbalanced datasets. What are the advantages and disadvantages of each?
\item
  Why should balancing techniques be applied \emph{only} to the training dataset and \emph{not} to the test dataset?
\item
  Some machine learning algorithms are robust to class imbalance, while others require explicit handling of imbalance. Which types of models typically require class balancing, and which can handle imbalance naturally?
\item
  When dealing with class imbalance, why is \emph{accuracy} not always the best metric to evaluate model performance? Which alternative metrics should be considered?
\item
  Suppose a dataset has a rare but critical class (e.g., fraud detection). What steps should be taken in the \emph{data partitioning and balancing phase} to ensure an effective model?
\end{enumerate}

\subsection*{Hands-On Practice}\label{hands-on-practice}


For the following exercises, use the \emph{churn}, \emph{bank}, and \emph{risk} datasets available in the \textbf{liver} package. We have previously used the \emph{churn} and \emph{bank} datasets in this and earlier chapters. In Chapter \ref{chapter-bayes}, we will introduce the \emph{risk} dataset. Load the datasets using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# Load datasets}
\FunctionTok{data}\NormalTok{(churn)}
\FunctionTok{data}\NormalTok{(bank)}
\FunctionTok{data}\NormalTok{(risk)}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Partitioning the Data}\label{partitioning-the-data}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\item
  Using the \texttt{partition()} function, split the \emph{churn} dataset into 75\% training and 25\% testing. Ensure reproducibility by setting a seed value before partitioning.
\item
  Perform a \emph{90-10 train-test split} on the \emph{bank} dataset. Report the number of observations in each subset.
\item
  Apply \emph{stratified sampling} to partition the \emph{churn} dataset, ensuring that the proportion of churners (\texttt{churn\ ==\ "yes"}) remains the same in both training and test sets.
\item
  In the \emph{risk} dataset, partition the data using a \emph{60-40} split and store the training and test sets as \texttt{train\_risk} and \texttt{test\_risk}.
\item
  Compare the distribution of \texttt{income} in the training and test sets of the \emph{bank} dataset using \emph{density plots}. Do they appear similar?
\end{enumerate}

\subsubsection*{Validating the Partition}\label{validating-the-partition}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\item
  In the \emph{churn} dataset, test whether the proportion of churners is \emph{statistically different} between the training and test sets. Use a \emph{two-sample Z-test}.
\item
  In the \emph{bank} dataset, test whether the \emph{average age} of customers differs significantly between the training and test sets using a \emph{two-sample t-test}.
\item
  Perform a \emph{Chi-square test} to validate whether the distribution of marital status (\texttt{marital}) in the \emph{bank} dataset is similar between the training and test sets.
\item
  Suppose the \emph{churn} dataset was partitioned incorrectly, resulting in the training set having \emph{30\% churners} and the test set having \emph{15\% churners}. What statistical test could confirm this issue, and how could it be corrected?
\item
  Select three numerical variables from the \emph{risk} dataset and validate whether their distributions differ between the training and test sets using appropriate statistical tests.
\end{enumerate}

\subsubsection*{Balancing the Training Dataset}\label{balancing-the-training-dataset-1}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{24}
\item
  In the \emph{churn} dataset, check whether churners (\texttt{churn\ =\ "yes"}) are underrepresented in the training dataset. Report the class proportions.
\item
  Use \emph{random oversampling} to increase the number of churners (\texttt{churn\ =\ "yes"}) in the training set to \emph{40\%} of the dataset using the \emph{ROSE} package.
\item
  Apply \emph{undersampling} in the \emph{bank} dataset so that the proportion of customers with \texttt{deposit\ =\ "yes"} and \texttt{deposit\ =\ "no"} is \emph{equal} in the training set.
\item
  Compare the class distributions \emph{before and after balancing} the \emph{churn} dataset. Use \emph{bar plots} to visualize the change.
\end{enumerate}

\chapter{Classification using k-Nearest Neighbors}\label{chapter-knn}

\emph{Classification} is one of the fundamental tasks in machine learning, enabling models to categorize data into predefined groups. From detecting spam emails to predicting customer churn, classification algorithms are widely used across various domains. In this chapter, we will first explore the concept of classification, discussing its applications, key principles, and commonly used algorithms.

Once we have a solid understanding of classification, we will introduce \emph{k-Nearest Neighbors (kNN)}, a simple yet effective algorithm based on the idea of similarity between data points. kNN is widely used for classification due to its intuitive approach and ease of implementation. We will delve into the details of how kNN works, demonstrate its implementation in R, and discuss its strengths, limitations, and real-world applications.

To illustrate kNN in practice, we will apply it to a real-world dataset: the \emph{churn} dataset. Our goal will be to build a classification model that predicts whether a customer will churn based on their service usage and account features. Through this hands-on example, we will demonstrate data preprocessing, selecting the optimal \(k\), evaluating model performance, and interpreting results.

\section{Classification}\label{classification}

Have you ever wondered how your email app effortlessly filters spam, how your streaming service seems to know exactly what you want to watch next, or how banks detect fraudulent credit card transactions in real-time? These seemingly magical predictions are made possible by \emph{classification}, a fundamental task in machine learning.

At its core, classification involves assigning a label or category to an observation based on its features. For example, given customer data, classification can predict whether they are likely to churn or stay loyal. Unlike regression, which predicts continuous numerical values (e.g., house prices), classification deals with discrete outcomes. The target variable, often called the \emph{class} or \emph{label}, can either be:

\begin{itemize}
\tightlist
\item
  \emph{Binary}: Two possible categories (e.g., spam vs.~not spam).\\
\item
  \emph{Multi-class}: More than two categories (e.g., car, bicycle, or pedestrian in image recognition).
\end{itemize}

From diagnosing diseases to identifying fraudulent activities, classification is a versatile tool used across countless domains to solve practical problems.

\subsection*{Where Is Classification Used?}\label{where-is-classification-used}


Classification algorithms power many everyday applications and cutting-edge technologies. Here are some examples:\\
- \emph{Email filtering}: Sorting spam from non-spam messages.\\
- \emph{Fraud detection}: Identifying suspicious credit card transactions.\\
- \emph{Customer retention}: Predicting whether a customer will churn.\\
- \emph{Medical diagnosis}: Diagnosing diseases based on patient records.\\
- \emph{Object recognition}: Detecting pedestrians and vehicles in self-driving cars.\\
- \emph{Recommendation systems}: Suggesting movies, songs, or products based on user preferences.

Every time you interact with technology that ``predicts'' something for you, chances are, a classification model is working behind the scenes.

\subsection*{How Does Classification Work?}\label{how-does-classification-work}


Classification involves two critical phases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Training Phase}: The algorithm learns patterns from a labeled dataset, which contains both predictor variables (features) and target class labels. For instance, in a fraud detection system, the algorithm might learn that transactions involving unusually high amounts and originating from foreign locations are more likely to be fraudulent.\\
\item
  \textbf{Prediction Phase}: Once the model is trained, it applies these learned patterns to classify new, unseen data. For example, given a new transaction, the model predicts whether it is fraudulent or legitimate.
\end{enumerate}

A good classification model does more than just memorize the training data---it \emph{generalizes} well, meaning it performs accurately on new, unseen data. For instance, a model trained on historical medical records should be able to diagnose a patient it has never encountered before, rather than simply repeating past diagnoses.

\subsection*{Which Classification Algorithm Should You Use?}\label{which-classification-algorithm-should-you-use}


Different classification algorithms are designed for different kinds of problems and datasets. Some commonly used algorithms include:\\
- \emph{k-Nearest Neighbors (kNN)}: A simple, distance-based algorithm (introduced in this chapter).\\
- \emph{Naive Bayes}: Particularly useful for text classification, like spam filtering (covered in Chapter \ref{chapter-bayes}).\\
- \emph{Logistic Regression}: A popular method for binary classification tasks, such as predicting customer churn (covered in Chapter \ref{chapter-regression}).\\
- \emph{Decision Trees and Random Forests}: Versatile, interpretable methods for handling complex problems (covered in Chapter \ref{chapter-tree}).\\
- \emph{Neural Networks}: Effective for handling high-dimensional and complex data, such as images or natural language (covered in Chapter \ref{chapter-nn}).

The choice of algorithm depends on factors such as dataset size, feature relationships, and the trade-off between interpretability and performance. For instance, if you're working with a small dataset and need an easy-to-interpret solution, kNN or Decision Trees might be ideal. Conversely, for high-dimensional data like images or speech recognition, Neural Networks could be more effective.

To illustrate classification in action, consider a \emph{bank} dataset where the goal is to predict whether a customer will make a deposit (\texttt{deposit\ =\ yes}) or not (\texttt{deposit\ =\ no}). The features might include customer details like \texttt{age}, \texttt{education}, \texttt{job}, and \texttt{marital\ status}. By training a classification model on this data, the bank can identify and target potential customers who are likely to invest, improving their marketing strategy.

\subsection*{Why Is Classification Important?}\label{why-is-classification-important}


Classification forms the backbone of countless machine learning applications that drive smarter decisions and actionable insights in industries like finance, healthcare, retail, and technology. Understanding how it works is a critical step in mastering machine learning and applying it to solve real-world problems.

Among the many classification techniques, \emph{k-Nearest Neighbors (kNN)} stands out for its simplicity and effectiveness. Because it is easy to understand and requires minimal assumptions about the data, kNN is often used as a baseline model before exploring more advanced techniques. In the rest of this chapter, we will explore how kNN works, why it is widely used, and how to implement it in R.

\section{How k-Nearest Neighbors Works}\label{how-k-nearest-neighbors-works}

Have you ever sought advice from a few trusted friends before making a decision? The \emph{k-Nearest Neighbors (kNN)} algorithm follows a similar principle---it ``consults'' the closest data points to determine the category of a new observation. This simple yet effective idea makes kNN one of the most intuitive classification methods in machine learning.

Unlike many machine learning algorithms that require an explicit training phase, kNN is a \emph{lazy learning} or \emph{instance-based} method. Instead of constructing a complex model, it stores the entire training dataset and makes predictions on demand. When given a new observation, kNN identifies the \emph{k} closest data points using a predefined distance metric. The class label is then assigned based on a \emph{majority vote} among these nearest neighbors. The choice of \(k\), the number of neighbors considered, plays a crucial role in balancing sensitivity to local patterns and generalization to broader trends.

\subsection*{How Does kNN Classify a New Observation?}\label{how-does-knn-classify-a-new-observation}


To classify a new observation, kNN calculates its \emph{distance} from every data point in the training set using a specified metric, such as \emph{Euclidean distance}, for instance. After identifying the \(k\)-nearest neighbors, the algorithm assigns the most frequent class among them as the predicted category.

Figure \ref{fig:knn-image} illustrates this concept with two classes: {Class A (red circles)} and {Class B (blue squares)}. A new data point, represented by a \emph{dark star}, needs to be classified. The figure compares predictions for two different values of \(k\):

\begin{itemize}
\tightlist
\item
  \emph{When \(k = 3\)}: The algorithm considers the 3 closest neighbors---two blue squares and one red circle. Since the majority class is \emph{Class B (blue squares)}, the new point is classified as Class B.\\
\item
  \emph{When \(k = 6\)}: The algorithm expands the neighborhood to include the 6 nearest neighbors. This larger set consists of four red circles and two blue squares, shifting the majority class to \emph{Class A (red circles)}. As a result, the new point is classified as Class A.
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\linewidth]{images/ch7_knn} 

}

\caption{A two-dimensional toy dataset with two classes (Class A and Class B) and a new data point (dark star), illustrating the k-Nearest Neighbors algorithm with k = 3 and k = 6.}\label{fig:knn-image}
\end{figure}

These examples illustrate how the choice of \(k\) affects classification. A smaller \(k\) (e.g., 3) makes predictions highly sensitive to local patterns, capturing finer details but also increasing the risk of misclassification due to noise. In contrast, a larger \(k\) (e.g., 6) smooths predictions by incorporating more neighbors, reducing sensitivity to individual data points but potentially overlooking localized structures in the data. Selecting an appropriate \(k\) ensures that kNN generalizes well without becoming overly complex or overly simplistic.

\subsection*{Strengths and Limitations of kNN}\label{strengths-and-limitations-of-knn}


The kNN algorithm is widely used due to its simplicity and intuitive nature, making it an excellent starting point for classification problems. By relying only on distance metrics and majority voting, it avoids the complexity of training explicit models. However, this simplicity comes with trade-offs, particularly in handling large datasets and noisy features.

One of kNN's key strengths is its ease of implementation and interpretability. Since it does not require model training, it can be applied directly to datasets with minimal preprocessing. It performs well on small datasets where patterns are well-defined and feature relationships are strong. However, kNN is highly sensitive to irrelevant or noisy features, as distance calculations may become less meaningful when unnecessary attributes are included. Additionally, it can be computationally expensive for large datasets, since it must calculate distances for every training point during prediction. The choice of \(k\) also plays a crucial role---too small a \(k\) makes the algorithm overly sensitive to noise, while too large a \(k\) may oversimplify patterns, leading to reduced accuracy.

\section*{kNN in Action: A Toy Example for Drug Classification}\label{knn-in-action-a-toy-example-for-drug-classification}


To further illustrate kNN, consider a real-world scenario involving drug prescription classification. A dataset of 200 patients includes their \emph{age}, \emph{sodium-to-potassium (Na/K) ratio}, and the drug type they were prescribed. This dataset is synthetically generated to reflect a real-world scenario. For details on how this dataset was generated, refer to Section \ref{intro-R-exercises}. Figure \ref{fig:scatter-plot-ex-drug} visualizes this dataset, where different drug types are represented by:

\begin{itemize}
\tightlist
\item
  \emph{Red circles} for Drug A,\\
\item
  \emph{Green triangles} for Drug B, and\\
\item
  \emph{Blue squares} for Drug C.
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.95\linewidth]{7_kNN_files/figure-latex/scatter-plot-ex-drug-1} 

}

\caption{Scatter plot of Age vs. Sodium/Potassium Ratio for 200 patients, with drug type indicated by color and shape.}\label{fig:scatter-plot-ex-drug}
\end{figure}

Suppose three new patients arrive at the clinic, and we need to determine which drug is most suitable for them based on their \emph{age} and \emph{sodium-to-potassium ratio}. Their details are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Patient 1}: 40 years old with a Na/K ratio of 30.5.\\
\item
  \emph{Patient 2}: 28 years old with a Na/K ratio of 9.6.\\
\item
  \emph{Patient 3}: 61 years old with a Na/K ratio of 10.5.
\end{enumerate}

These patients are represented as \emph{orange circles} in Figure \ref{fig:scatter-plot-ex-drug-2}. Using kNN, we will classify the drug type for each patient.

\begin{figure}[H]

{\centering \includegraphics[width=0.95\linewidth]{7_kNN_files/figure-latex/scatter-plot-ex-drug-2-1} 

}

\caption{Scatter plot of Age vs. Sodium/Potassium Ratio for 200 patients, with drug type indicated by color and shape. The three new patients are represented by large orange circles.}\label{fig:scatter-plot-ex-drug-2}
\end{figure}

For \emph{Patient 1}, who is located deep within a cluster of red-circle points (Drug A), the classification is straightforward: \emph{Drug A}. All the nearest neighbors belong to Drug A, making it an easy decision.

For \emph{Patient 2}, the situation is more nuanced. If \(k = 1\), the nearest neighbor is a blue square, resulting in the classification \emph{Drug C}. When \(k = 2\), there is a tie between Drug B and Drug C, leading to no clear majority. With \(k = 3\), two out of the three nearest neighbors are blue squares, so the classification remains \emph{Drug C}.

For \emph{Patient 3}, classification becomes even more ambiguous. With \(k = 1\), the closest neighbor is a blue square, classifying the patient as \emph{Drug C}. However, for \(k = 2\) or \(k = 3\), the nearest neighbors belong to multiple classes, creating uncertainty in classification.

\begin{figure}[H]
\includegraphics[width=0.33\linewidth]{7_kNN_files/figure-latex/scatter-plot-ex-drug-3-1} \includegraphics[width=0.33\linewidth]{7_kNN_files/figure-latex/scatter-plot-ex-drug-3-2} \includegraphics[width=0.33\linewidth]{7_kNN_files/figure-latex/scatter-plot-ex-drug-3-3} \caption{Zoom-in plots for the three new patients and their nearest neighbors. The left plot is for Patient 1, the middle plot is for Patient 2, and the right plot is for Patient 3.}\label{fig:scatter-plot-ex-drug-3}
\end{figure}

These examples highlight several key aspects of kNN. The choice of \(k\) significantly influences classification---small values of \(k\) make the algorithm highly sensitive to local patterns, while larger values introduce smoothing by considering broader neighborhoods. Additionally, the selection of distance metrics, such as Euclidean distance, affects how neighbors are determined. Finally, proper feature scaling ensures that all variables contribute fairly to distance calculations, preventing dominance by features with larger numeric ranges.

This example demonstrates how kNN assigns labels based on proximity, reinforcing the importance of thoughtful parameter selection and preprocessing techniques. Before applying kNN to real-world datasets, it is essential to understand \emph{how} similarity is measured---this leads to the next discussion on distance metrics.

\section{Distance Metrics}\label{distance-metrics}

In the kNN algorithm, the classification of a new data point is determined by identifying the most \emph{similar} records from the training dataset. But how do we define and measure \emph{similarity}? While similarity might seem intuitive, applying it in machine learning requires precise \emph{distance metrics}. These metrics quantify the ``closeness'' between two data points in a multidimensional space, directly influencing how neighbors are selected for classification.

Imagine you're shopping online and looking for recommendations. You're a 50-year-old married female---who's more similar to you: a 40-year-old single female or a 30-year-old married male? The answer depends on how we measure the distance between you and each person. In kNN, this distance is computed using numerical features such as age and categorical features such as marital status. The smaller the distance, the more ``similar'' two individuals are, and the more influence they have in determining predictions. Since kNN assumes that closer points (lower distance) belong to the same class, choosing the right distance metric is crucial for accurate classification.

\subsection*{Euclidean Distance}\label{euclidean-distance}


The most widely used distance metric in kNN is \emph{Euclidean distance}, which measures the straight-line distance between two points. Think of it as the ``as-the-crow-flies'' distance, similar to the shortest path between two locations on a map. This metric is intuitive and aligns with how we often perceive distance in the real world.

Mathematically, the Euclidean distance between two points, \(x\) and \(y\), in \(n\)-dimensional space is given by:

\[
\text{dist}(x, y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots + (x_n - y_n)^2}, 
\]

where \(x = (x_1, x_2, \ldots, x_n)\) and \(y = (y_1, y_2, \ldots, y_n)\) represent the feature vectors of the two points. The differences between corresponding features (\(x_i - y_i\)) are squared, summed, and then square-rooted to calculate the distance.

\begin{example}
\protect\hypertarget{exm:ex-knn-euclidean-distance}{}\label{exm:ex-knn-euclidean-distance}Let's calculate the Euclidean distance between two patients based on their \emph{age} and \emph{sodium-to-potassium (Na/K) ratio}:

\begin{itemize}
\tightlist
\item
  Patient 1: \(x = (40, 30.5)\)\\
\item
  Patient 2: \(y = (28, 9.6)\)
\end{itemize}

Using the formula:\\
\[
\text{dist}(x, y) = \sqrt{(40 - 28)^2 + (30.5 - 9.6)^2} = \sqrt{(12)^2 + (20.9)^2} = 24.11
\]

This result quantifies the dissimilarity between the two patients. In kNN, this distance helps determine how similar Patient 1 is to Patient 2 and whether they should be classified into the same drug class.
\end{example}

\subsection*{Choosing the Right Distance Metric}\label{choosing-the-right-distance-metric}


While Euclidean distance is widely used in kNN, it is not always the best choice. Other distance metrics can be more suitable depending on the dataset's characteristics:

\begin{itemize}
\tightlist
\item
  \emph{Manhattan Distance}: Measures distance by summing the absolute differences between coordinates. This is useful when movement is restricted to grid-like paths, such as city blocks.
\item
  \emph{Hamming Distance}: Used for categorical variables, where the distance is the number of positions at which two feature vectors differ.
\item
  \emph{Cosine Similarity}: Measures the angle between two vectors rather than their absolute distance. This is useful in high-dimensional spaces, such as text classification.
\end{itemize}

The choice of distance metric depends on the data type and problem domain. If your dataset contains categorical or high-dimensional features, exploring alternative metrics---such as Manhattan or Cosine Similarity---might be necessary. For further details, refer to the \texttt{dist()} function in R.

\section{\texorpdfstring{How to Choose an Optimal \(k\)}{How to Choose an Optimal k}}\label{how-to-choose-an-optimal-k}

How many opinions do you seek before making an important decision? Too few might lead to a biased perspective, while too many might dilute the relevance of the advice. Similarly, in the k-Nearest Neighbors (kNN) algorithm, the choice of \(k\)---the number of neighbors considered for classification---directly impacts the model's performance. But how do we determine the right \(k\)?

There is no universally ``correct'' value for \(k\). The optimal choice depends on the specific dataset and classification problem, requiring careful consideration of the trade-offs involved.

\subsection*{Balancing Overfitting and Underfitting}\label{balancing-overfitting-and-underfitting}


When \(k\) is too small, such as \(k = 1\), the algorithm becomes highly sensitive to individual training points. Each new observation is classified based on its single closest neighbor, making the model highly reactive to noise and outliers. This can lead to \emph{overfitting}, where the model memorizes the training data but fails to generalize to unseen data. For example, a small cluster of mislabeled data points could disproportionately influence predictions, reducing the model's reliability.

Conversely, as \(k\) increases, the algorithm incorporates more neighbors into the classification decision. Larger \(k\) values smooth the decision boundary, reducing the impact of noise and outliers. However, if \(k\) is too large, the model may oversimplify, averaging out meaningful patterns in the data. When \(k\) is comparable to the size of the training set, the majority class dominates predictions, leading to \emph{underfitting}, where the model fails to capture important distinctions.

Choosing an appropriate \(k\) requires balancing these extremes. Smaller values of \(k\) capture fine-grained local structures but risk overfitting, while larger values provide more stability at the expense of detail.

\subsection*{\texorpdfstring{Choosing \(k\) Through Validation}{Choosing k Through Validation}}\label{choosing-k-through-validation}


Since the optimal \(k\) depends on the dataset, a common approach is to evaluate multiple values of \(k\) using a \emph{validation set} or \emph{cross-validation}. Performance metrics such as accuracy, precision, recall, and F1-score help identify the best \(k\) for a given problem.

To illustrate, we use the \emph{churn} dataset and evaluate the accuracy of the kNN algorithm across different \(k\) values (ranging from 1 to 30). Figure \ref{fig:kNN-plot} shows how accuracy fluctuates as \(k\) increases. The plot is generated using the \texttt{kNN.plot()} function from the \textbf{liver} package in R.

\begin{figure}[H]

{\centering \includegraphics[width=0.85\linewidth]{7_kNN_files/figure-latex/kNN-plot-1} 

}

\caption{Accuracy of the k-Nearest Neighbors algorithm for different values of k in the range from 1 to 30.}\label{fig:kNN-plot}
\end{figure}

From the plot, we observe that kNN accuracy fluctuates as \(k\) increases. The highest accuracy is achieved when \(k = 5\), where the algorithm balances sensitivity to local patterns with robustness to noise. At this value, kNN delivers an accuracy of 0.932 and an error rate of 0.068.

Choosing the optimal \(k\) is as much an art as it is a science. While there's no universal rule for selecting \(k\), experimentation and validation are key. Start with a range of plausible \(k\) values, test the model's performance, and select the one that provides the best results based on your chosen metric.

Keep in mind that the optimal \(k\) may vary across datasets. Whenever applying kNN to a new problem, repeating this process ensures the model remains both accurate and generalizable. By carefully tuning \(k\), we strike the right balance between overfitting and underfitting, improving the model's predictive power.

\section{Preparing Data for kNN}\label{preparing-data-for-knn}

The effectiveness of the kNN algorithm relies heavily on how the dataset is prepared. Since kNN uses distance metrics to evaluate similarity between data points, proper preprocessing is crucial to ensure accurate and meaningful results. Two essential steps in this process are \emph{feature scaling} and \emph{one-hot encoding}, which enable the algorithm to handle numerical and categorical features effectively. These steps are part of the \emph{Preparing Data for Modeling} stage in the Data Science Workflow (Figure \ref{fig:CRISP-DM}).

\subsection{Feature Scaling}\label{feature-scaling-1}

In most datasets, numerical features often have vastly different ranges. For instance, \emph{age} may range from 20 to 70, while \emph{income} could range from 20,000 to 150,000. Without proper scaling, features with larger ranges, such as income, will dominate distance calculations, leading to biased predictions. To address this, all numerical features must be transformed to comparable scales. See Section \ref{feature-scaling} for more details on scaling methods.

A widely used method is \emph{min-max scaling}, which transforms each feature to a specified range, typically \([0, 1]\), using the formula:
\[
x_{\text{scaled}} = \frac{x - \min(x)}{\max(x) - \min(x)},
\]
where \(x\) represents the original feature value, and \(\min(x)\) and \(\max(x)\) are the minimum and maximum values of the feature, respectively. This formula rescales each feature to a \([0,1]\) range, ensuring that no single feature dominates the distance calculation.

Another common method is \emph{z-score standardization}, which rescales features so that they have a mean of 0 and a standard deviation of 1:
\[
x_{\text{scaled}} = \frac{x - \text{mean}(x)}{\text{sd}(x)}
\]
This method is particularly useful when features contain outliers or follow different distributions. Unlike min-max scaling, z-score standardization does not constrain values within a fixed range but ensures that they follow a standard normal distribution, making it more robust to extreme values.

\begin{quote}
\textbf{Choosing the Right Scaling Method:}\\
Min-max scaling is preferable when feature values are bounded within a known range, such as pixel values in images or percentages. This ensures that all features contribute equally to the distance metric while maintaining their relative proportions. On the other hand, \emph{z-score standardization} is more suitable when data contains extreme values or follows different distributions across features. It transforms values into a standard normal distribution, making it particularly effective for datasets with outliers or varying units of measurement.
\end{quote}

\begin{quote}
\textbf{Avoiding Data Leakage:}
Scaling must always be performed \emph{after partitioning} the dataset into training and test sets. The scaling parameters, such as the minimum and maximum for min-max scaling or the mean and standard deviation for z-score standardization, should be computed from the \emph{training set only} and then applied consistently to both the training and test sets. Performing scaling before partitioning can introduce \emph{data leakage}, where information from the test set inadvertently influences the training process. This can lead to misleadingly high accuracy during evaluation, as the model indirectly gains access to test data before making predictions.
\end{quote}

\subsection{Scaling Training and Test Data the Same Way}\label{scaling-training-and-test-data-the-same-way}

To illustrate the importance of consistent scaling, consider the \emph{patient drug classification problem}, which involves two features: \texttt{age} and \texttt{sodium/potassium\ (Na/K)\ ratio}. Figure \ref{fig:scatter-plot-ex-drug-2} shows a dataset of 200 patients as the training set, with three additional patients in the test set. Using the \texttt{minmax()} function from the \textbf{liver} package, we demonstrate both correct and incorrect ways to scale the data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the liver package}
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# A proper way to scale the data}
\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_data, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age"}\NormalTok{, }\StringTok{"Ratio"}\NormalTok{))}

\NormalTok{test\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_data, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age"}\NormalTok{, }\StringTok{"Ratio"}\NormalTok{), }\AttributeTok{min =} \FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{Age), }\FunctionTok{min}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{Ratio)), }\AttributeTok{max =} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{Age), }\FunctionTok{max}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{Ratio)))}

\CommentTok{\# An incorrect way to scale the data}
\NormalTok{train\_scaled\_wrongly }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_data, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age"}\NormalTok{, }\StringTok{"Ratio"}\NormalTok{))}
\NormalTok{test\_scaled\_wrongly  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_data , }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age"}\NormalTok{, }\StringTok{"Ratio"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The difference is illustrated in Figure \ref{fig:ex-proper-scaling}. The middle panel shows the results of proper scaling, where the test set is scaled using the same parameters derived from the training set. This ensures consistency in distance calculations across both datasets. In contrast, the right panel shows improper scaling, where the test set is scaled independently. This leads to distorted relationships between the training and test data, which can cause unreliable predictions.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth]{7_kNN_files/figure-latex/ex-proper-scaling-1} \includegraphics[width=0.5\linewidth]{7_kNN_files/figure-latex/ex-proper-scaling-2} \includegraphics[width=0.5\linewidth]{7_kNN_files/figure-latex/ex-proper-scaling-3} 

}

\caption{Visualization illustrating the difference between proper scaling and improper scaling. The left panel shows the original data without scaling. The middle panel shows the results of proper scaling. The right panel shows the results of improper scaling.}\label{fig:ex-proper-scaling}
\end{figure}

\begin{quote}
\textbf{Key Insight:} Proper scaling ensures that distance metrics remain valid, while improper scaling creates inconsistencies that undermine the kNN algorithm's performance. \emph{Scaling parameters should always be derived from the training set and applied consistently to the test set}. Neglecting this principle introduces data leakage, which distorts model evaluation and leads to overly optimistic performance estimates.
\end{quote}

\subsection{One-Hot Encoding}\label{one-hot-encoding-1}

Categorical features, such as \emph{marital status} or \emph{subscription type}, cannot be directly used in distance calculations because distance metrics like Euclidean distance only work with numerical data. To overcome this, we use \emph{one-hot encoding}, which converts categorical variables into binary (dummy) variables.

For example, the categorical variable \texttt{voice.plan}, with levels \texttt{yes} and \texttt{no}, can be encoded as:

\[
\text{voice.plan-yes} = 
\begin{cases}
1 \quad \text{if voice plan = yes}  \\
0 \quad \text{if voice plan = no} 
\end{cases}
\]

For categorical variables with more than two categories, one-hot encoding creates multiple binary columns---one for each category except one, to avoid redundancy. This approach ensures that the categorical variable is fully represented without introducing unnecessary correlations.

The \textbf{liver} package in R provides the \texttt{one.hot()} function to perform one-hot encoding automatically. It identifies categorical variables and encodes them into binary columns, leaving numerical features unchanged. Applying one-hot encoding to the \emph{marital} variable in the \emph{bank} dataset, for instance, adds binary columns for the encoded categories:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(bank)}

\CommentTok{\# To perform one{-}hot encoding on the "marital" variable}
\NormalTok{bank\_encoded }\OtherTok{\textless{}{-}} \FunctionTok{one.hot}\NormalTok{(bank, }\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"marital"}\NormalTok{), }\AttributeTok{dropCols =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{str}\NormalTok{(bank\_encoded)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age             }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job             }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_divorced}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_married }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital\_single  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education       }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan            }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day             }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month           }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays           }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

Setting \texttt{dropCols\ =\ FALSE} retains the original categorical column in the dataset, which may be useful for reference or debugging. However, in most cases, it is recommended to remove the original column after encoding to avoid redundancy.

\begin{quote}
\textbf{Note:} One-hot encoding is unnecessary for ordinal features, where the categories have a natural order (e.g., \texttt{low}, \texttt{medium}, \texttt{high}). Ordinal variables should instead be assigned numerical values that preserve their order (e.g., \texttt{low\ =\ 1}, \texttt{medium\ =\ 2}, \texttt{high\ =\ 3}), enabling the kNN algorithm to treat them as numerical features. For instance, if \texttt{education.level} has values \{\texttt{low}, \texttt{medium}, \texttt{high}\}, one-hot encoding would lose the natural progression between these categories. Instead, assigning numerical values (\texttt{low\ =\ 1}, \texttt{medium\ =\ 2}, \texttt{high\ =\ 3}) allows the algorithm to recognize the ordinal nature of the feature, preserving its relationship in distance calculations.
\end{quote}

\section{Applying kNN Algorithm in Practice}\label{sec-kNN-churn}

Applying the kNN algorithm involves several key steps, from preparing the data to training the model, making predictions, and evaluating its performance. In this section, we demonstrate the entire workflow using the \emph{churn} dataset from the \textbf{liver} package in R. The target variable, \texttt{churn}, indicates whether a customer has churned (\texttt{yes}) or not (\texttt{no}), while the predictors include customer characteristics such as account length, international plan status, and call details. For details on exploratory data analysis, problem understanding, and data preparation for this dataset, refer to Section \ref{EDA-sec-churn}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(churn)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{5000}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{17} \DecValTok{36} \DecValTok{32} \DecValTok{36} \DecValTok{37} \DecValTok{2} \DecValTok{20} \DecValTok{25} \DecValTok{19} \DecValTok{50}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area.code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account.length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{128} \DecValTok{107} \DecValTok{137} \DecValTok{84} \DecValTok{75} \DecValTok{118} \DecValTok{121} \DecValTok{147} \DecValTok{117} \DecValTok{141}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.plan    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{26} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{24} \DecValTok{0} \DecValTok{0} \DecValTok{37}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.plan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.mins     }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \FloatTok{13.7} \FloatTok{12.2} \FloatTok{6.6} \FloatTok{10.1} \FloatTok{6.3} \FloatTok{7.5} \FloatTok{7.1} \FloatTok{8.7} \FloatTok{11.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{5} \DecValTok{7} \DecValTok{3} \DecValTok{6} \DecValTok{7} \DecValTok{6} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{2.7} \FloatTok{3.7} \FloatTok{3.29} \FloatTok{1.78} \FloatTok{2.73} \FloatTok{1.7} \FloatTok{2.03} \FloatTok{1.92} \FloatTok{2.35} \FloatTok{3.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{265} \DecValTok{162} \DecValTok{243} \DecValTok{299} \DecValTok{167}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{110} \DecValTok{123} \DecValTok{114} \DecValTok{71} \DecValTok{113} \DecValTok{98} \DecValTok{88} \DecValTok{79} \DecValTok{97} \DecValTok{84}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{45.1} \FloatTok{27.5} \FloatTok{41.4} \FloatTok{50.9} \FloatTok{28.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.mins      }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{197.4} \FloatTok{195.5} \FloatTok{121.2} \FloatTok{61.9} \FloatTok{148.3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{99} \DecValTok{103} \DecValTok{110} \DecValTok{88} \DecValTok{122} \DecValTok{101} \DecValTok{108} \DecValTok{94} \DecValTok{80} \DecValTok{111}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{16.78} \FloatTok{16.62} \FloatTok{10.3} \FloatTok{5.26} \FloatTok{12.61}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.mins    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{245} \DecValTok{254} \DecValTok{163} \DecValTok{197} \DecValTok{187}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{91} \DecValTok{103} \DecValTok{104} \DecValTok{89} \DecValTok{121} \DecValTok{118} \DecValTok{118} \DecValTok{96} \DecValTok{90} \DecValTok{97}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{11.01} \FloatTok{11.45} \FloatTok{7.32} \FloatTok{8.86} \FloatTok{8.41}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer.calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{0} \DecValTok{3} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset is a \emph{data.frame} in \textbf{R} with 5000 observations and 19 predictor variables. The target variable, \emph{churn}, indicates whether a customer has churned (\texttt{yes}) or not (\texttt{no}).

Based on insights gained in Section \ref{EDA-sec-churn}, we select the following features for building the kNN model:

\texttt{account.length}, \texttt{voice.plan}, \texttt{voice.messages}, \texttt{intl.plan}, \texttt{intl.mins}, \texttt{day.mins}, \texttt{eve.mins}, \texttt{night.mins}, and \texttt{customer.calls}.

The next steps involve preparing the data through feature scaling and one-hot encoding, followed by selecting an optimal \(k\), training the kNN model, and evaluating its performance.

\subsection{Step 1: Preparing the Data}\label{step-1-preparing-the-data}

The first step in applying kNN is to partition the dataset into training and test sets, followed by preprocessing tasks like feature scaling and one-hot encoding. Since the dataset is already cleaned and free of missing values, we can proceed directly with partitioning before applying these transformations.

We split the dataset into an 80\% training set and a 20\% test set using the \texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{43}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

The \texttt{partition()} function randomly splits the dataset while maintaining the class distribution of the target variable, ensuring a representative training and test set. As we validated the partition in Section \ref{sec-validate-partition}, we can now proceed with feature scaling and one-hot encoding to ensure compatibility with the kNN algorithm.

\subsubsection*{One-Hot Encoding}\label{one-hot-encoding-2}


Since kNN relies on distance calculations, categorical variables like \texttt{voice.plan} and \texttt{intl.plan} must be converted into numerical representations. One-hot encoding achieves this by creating binary (dummy) variables for each category. We apply the \texttt{one.hot()} function from the \textbf{liver} package to transform categorical features into a numerical format suitable for kNN:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"voice.plan"}\NormalTok{, }\StringTok{"intl.plan"}\NormalTok{)}

\NormalTok{train\_onehot }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(train\_set, }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}
\NormalTok{test\_onehot  }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(test\_set,  }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}

\FunctionTok{str}\NormalTok{(test\_onehot)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{1000}\NormalTok{ obs. of  }\DecValTok{22}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ state         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{51}\NormalTok{ levels }\StringTok{"AK"}\NormalTok{,}\StringTok{"AL"}\NormalTok{,}\StringTok{"AR"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{50} \DecValTok{14} \DecValTok{46} \DecValTok{10} \DecValTok{4} \DecValTok{25} \DecValTok{15} \DecValTok{11} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ area.code     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"area\_code\_408"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ account.length}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{118} \DecValTok{141} \DecValTok{85} \DecValTok{76} \DecValTok{147} \DecValTok{130} \DecValTok{20} \DecValTok{142} \DecValTok{72} \DecValTok{149}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.plan\_yes}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.plan\_no }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ voice.messages}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{37} \DecValTok{27} \DecValTok{33} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{37} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.plan\_yes }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.plan\_no  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.mins     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{6.3} \FloatTok{11.2} \FloatTok{13.8} \DecValTok{10} \FloatTok{10.6} \FloatTok{9.5} \FloatTok{6.3} \FloatTok{14.2} \FloatTok{14.7} \FloatTok{11.1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.calls    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{6} \DecValTok{5} \DecValTok{4} \DecValTok{5} \DecValTok{4} \DecValTok{19} \DecValTok{6} \DecValTok{6} \DecValTok{6} \DecValTok{9}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ intl.charge   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.7} \FloatTok{3.02} \FloatTok{3.73} \FloatTok{2.7} \FloatTok{2.86} \FloatTok{2.57} \FloatTok{1.7} \FloatTok{3.83} \FloatTok{3.97} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{223} \DecValTok{259} \DecValTok{196} \DecValTok{190} \DecValTok{155}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{98} \DecValTok{84} \DecValTok{139} \DecValTok{66} \DecValTok{117} \DecValTok{112} \DecValTok{109} \DecValTok{95} \DecValTok{80} \DecValTok{94}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day.charge    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{38} \DecValTok{44} \FloatTok{33.4} \FloatTok{32.2} \FloatTok{26.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.mins      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{221} \DecValTok{222} \DecValTok{281} \DecValTok{213} \DecValTok{240}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.calls     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{101} \DecValTok{111} \DecValTok{90} \DecValTok{65} \DecValTok{93} \DecValTok{99} \DecValTok{84} \DecValTok{63} \DecValTok{102} \DecValTok{92}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ eve.charge    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{18.8} \FloatTok{18.9} \FloatTok{23.9} \FloatTok{18.1} \FloatTok{20.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.mins    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{203.9} \FloatTok{326.4} \FloatTok{89.3} \FloatTok{165.7} \FloatTok{208.8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.calls   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{118} \DecValTok{97} \DecValTok{75} \DecValTok{108} \DecValTok{133} \DecValTok{78} \DecValTok{102} \DecValTok{148} \DecValTok{71} \DecValTok{108}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ night.charge  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{9.18} \FloatTok{14.69} \FloatTok{4.02} \FloatTok{7.46} \FloatTok{9.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ customer.calls}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ churn         }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

For binary categorical variables, one-hot encoding produces two columns (e.g., \texttt{voice.plan\_yes} and \texttt{voice.plan\_no}). Since one variable is always the complement of the other, we retain only one (e.g., \texttt{voice.plan\_yes}) to avoid redundancy.

\subsubsection*{Feature Scaling}\label{feature-scaling-2}


Since kNN calculates distances between data points, features with larger numerical ranges can disproportionately influence the results. Scaling ensures that all features contribute equally to distance calculations, preventing dominance by high-magnitude features.

To standardize the numerical variables, we apply min-max scaling using the \texttt{minmax()} function from the \textbf{liver} package. Scaling parameters (minimum and maximum values) must be computed from the training set and then applied consistently to both the training and test sets. This prevents data leakage, which occurs when test data influences the training process, leading to misleadingly high performance estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numeric\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"account.length"}\NormalTok{, }\StringTok{"voice.messages"}\NormalTok{, }\StringTok{"intl.mins"}\NormalTok{, }
                 \StringTok{"intl.calls"}\NormalTok{, }\StringTok{"day.mins"}\NormalTok{, }\StringTok{"day.calls"}\NormalTok{, }\StringTok{"eve.mins"}\NormalTok{, }
                 \StringTok{"eve.calls"}\NormalTok{, }\StringTok{"night.mins"}\NormalTok{, }\StringTok{"night.calls"}\NormalTok{, }
                 \StringTok{"customer.calls"}\NormalTok{)}

\NormalTok{min\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_set[, numeric\_vars], min)}
\NormalTok{max\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_set[, numeric\_vars], max)}

\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_onehot, }\AttributeTok{col =}\NormalTok{ numeric\_vars, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\NormalTok{test\_scaled  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_onehot,  }\AttributeTok{col =}\NormalTok{ numeric\_vars, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\end{Highlighting}
\end{Shaded}

The \texttt{minmax()} function normalizes the numerical features to the range \([0, 1]\), ensuring they have comparable scales while preserving relative differences. This transformation prevents any single feature from dominating the kNN distance calculations, leading to more balanced and accurate predictions.

\subsection{\texorpdfstring{Step 2: Choosing an Optimal \(k\)}{Step 2: Choosing an Optimal k}}\label{step-2-choosing-an-optimal-k}

The selection of \(k\) plays a crucial role in balancing \emph{local pattern recognition} and \emph{generalization} to unseen data. A small \(k\) may lead to \emph{overfitting}, making the model highly sensitive to noise, while a large \(k\) can result in \emph{oversmoothing}, causing the model to overlook important details. To find an optimal \(k\), we evaluate the model's accuracy across different values of \(k\) and select the one that yields the best performance.

A practical way to achieve this is by plotting model accuracy against different values of \(k\). The \texttt{kNN.plot()} function from the \textbf{liver} package automates this process, providing a clear visualization of the relationship between \(k\) and accuracy:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.plan\_yes }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+} 
\NormalTok{                  intl.plan\_yes }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ intl.calls }\SpecialCharTok{+} 
\NormalTok{                  day.mins }\SpecialCharTok{+}\NormalTok{ day.calls }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+}\NormalTok{ eve.calls }\SpecialCharTok{+} 
\NormalTok{                  night.mins }\SpecialCharTok{+}\NormalTok{ night.calls }\SpecialCharTok{+}\NormalTok{ customer.calls}

\FunctionTok{kNN.plot}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{train =}\NormalTok{ train\_scaled, }\AttributeTok{test =}\NormalTok{ test\_scaled, }
         \AttributeTok{k.max =} \DecValTok{30}\NormalTok{, }\AttributeTok{set.seed =} \DecValTok{43}\NormalTok{)}
\NormalTok{   Setting levels}\SpecialCharTok{:}\NormalTok{ reference }\OtherTok{=} \StringTok{"yes"}\NormalTok{, case }\OtherTok{=} \StringTok{"no"}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{7_kNN_files/figure-latex/unnamed-chunk-7-1} \end{center}

The resulting plot helps identify the optimal \(k\), balancing \emph{model complexity} and \emph{generalization}. From the visualization, we observe that the highest accuracy is achieved when \(k = 5\), suggesting that this value effectively captures meaningful patterns while minimizing sensitivity to outliers.

\subsection{Step 3: Training the Model and Making Predictions}\label{step-3-training-the-model-and-making-predictions}

With \(k = 5\) identified as the optimal value, we are now ready to classify test records using the k-NN algorithm. Unlike many machine learning methods, k-NN is a \textbf{lazy learner}, meaning it does not build an explicit model during training. Instead, it simply stores the training data in a structured format and performs classification only when a new observation needs to be classified.

Several R packages provide implementations of the k-NN algorithm, including \href{https://CRAN.R-project.org/package=class}{\textbf{class}} and \href{https://CRAN.R-project.org/package=liver}{\textbf{liver}}. The \textbf{class} package offers basic k-NN functionality, while the \textbf{liver} package provides an enhanced \texttt{kNN()} function that supports a formula-based interface and built-in normalization.

To apply the k-NN algorithm, we use the \texttt{kNN()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kNN\_predict }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{train =}\NormalTok{ train\_scaled, }\AttributeTok{test =}\NormalTok{ test\_scaled, }\AttributeTok{k =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this function:

\begin{itemize}
\tightlist
\item
  \texttt{formula} specifies the relationship between the target variable and predictors.\\
\item
  \texttt{train} contains the scaled training data.\\
\item
  \texttt{test} contains the scaled test data.\\
\item
  \texttt{k} determines the number of nearest neighbors to consider, set here to \(k = 5\) based on the selection in the previous step.
\end{itemize}

The \texttt{kNN()} function classifies test observations by computing distances between each test record and all training data points. It then selects the five closest neighbors based on the chosen distance metric and assigns the most frequent class among them as the predicted label. Since test data is independent of training data, these predictions provide an unbiased estimate of the model's ability to generalize to new observations.

\subsection{Step 4: Evaluating the Model}\label{step-4-evaluating-the-model}

Evaluating model performance is crucial to ensure that the kNN algorithm generalizes well to unseen data and makes reliable predictions. A confusion matrix provides a summary of correct and incorrect predictions by comparing the predicted labels to the actual labels in the test set. We compute it using the \texttt{conf.mat()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(kNN\_predict, test\_labels, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\NormalTok{          Actual}
\NormalTok{   Predict yes  no}
\NormalTok{       yes  }\DecValTok{54}   \DecValTok{7}
\NormalTok{       no   }\DecValTok{83} \DecValTok{856}
\end{Highlighting}
\end{Shaded}

From the confusion matrix, we see that the model correctly classified 910 instances, while 90 instances were misclassified. This summary helps assess model performance and identify areas for improvement.

\subsection*{Final Remarks}\label{final-remarks}


This step-by-step implementation of kNN highlighted the crucial role of data preprocessing, parameter tuning, and model evaluation in achieving reliable predictions. Key factors such as the choice of \(k\), feature scaling, and encoding categorical data significantly influence the accuracy and generalization of kNN models.

While the confusion matrix provides an initial assessment of model performance, additional evaluation metrics such as accuracy, precision, recall, and F1-score offer deeper insights. These aspects will be explored in detail in the next chapter (Chapter \ref{chapter-evaluation}).

\section{Key Takeaways from kNN}\label{key-takeaways-from-knn}

In this chapter, we explored the k-Nearest Neighbors (kNN) algorithm, a simple yet effective method for solving classification problems. We began by revisiting the concept of classification and its real-world applications, highlighting the difference between binary and multi-class problems. We then examined the mechanics of kNN, emphasizing its reliance on distance metrics to identify the most similar data points. Essential preprocessing steps, such as feature scaling and one-hot encoding, were discussed to ensure accurate and meaningful distance calculations. We also covered the importance of selecting an optimal \(k\) value and demonstrated the implementation of kNN using the \textbf{liver} package in R with the \emph{churn} dataset. Through practical examples, we reinforced the significance of proper data preparation and parameter tuning for building reliable classification models.

The simplicity and interpretability of kNN make it an excellent starting point for understanding classification and exploring dataset structures. However, the algorithm has notable limitations, including sensitivity to noise, computational inefficiency with large datasets, and the necessity for proper scaling and feature selection. These challenges make kNN less practical for large-scale applications, but it remains a valuable tool for small to medium-sized datasets and serves as a benchmark for evaluating more advanced algorithms.

While kNN is intuitive and easy to implement, its prediction speed and scalability constraints often limit its use in modern, large-scale datasets. Nonetheless, it is a useful baseline method and a stepping stone to more sophisticated techniques. In the upcoming chapters, we will explore advanced classification algorithms, such as Decision Trees, Random Forests, and Logistic Regression, which address the limitations of kNN and provide enhanced performance and scalability for a wide range of applications.

\section{Exercises}\label{exercises-5}

\subsection*{Conceptual Questions}\label{conceptual-questions-3}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain the fundamental difference between classification and regression. Provide an example of each.\\
\item
  What are the key steps in applying the kNN algorithm?\\
\item
  Why is the choice of \(k\) important in kNN, and what happens when \(k\) is too small or too large?\\
\item
  Describe the role of distance metrics in kNN classification. Why is Euclidean distance commonly used?\\
\item
  What are the limitations of kNN compared to other classification algorithms?\\
\item
  How does feature scaling impact the performance of kNN? Why is it necessary?\\
\item
  Describe how one-hot encoding is used in kNN. Why is it necessary for categorical variables?\\
\item
  How does kNN handle missing values? What strategies can be used to deal with missing data?\\
\item
  Explain the difference between \emph{lazy learning} (such as kNN) and \emph{eager learning} (such as decision trees or logistic regression).\\
\item
  Why is kNN considered a non-parametric algorithm? What advantages and disadvantages does this bring?
\end{enumerate}

\subsection*{Hands-On Practice: Applying kNN to the Bank Dataset}\label{hands-on-practice-applying-knn-to-the-bank-dataset}


Here, we want to apply the concepts covered in this chapter using the \emph{bank} dataset from the \textbf{liver} package. The \emph{bank} dataset contains customer information, including demographics and financial details, and the target variable \emph{deposit} indicates whether a customer subscribed to a term deposit. This dataset is well-suited for classification problems and provides an opportunity to practice kNN in real-world scenarios.

To begin, load the necessary package and dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# Load the dataset}
\FunctionTok{data}\NormalTok{(bank)}

\CommentTok{\# View the structure of the dataset}
\FunctionTok{str}\NormalTok{(bank)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{17}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays    }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Data Exploration and Preparation}\label{data-exploration-and-preparation}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Load the \emph{bank} dataset and display its structure. Identify the target variable and the predictor variables.\\
\item
  Count the number of instances where a customer subscribed to a term deposit (\emph{deposit = ``yes''}) versus those who did not (\emph{deposit = ``no''}). What does this tell you about the dataset?\\
\item
  Identify nominal variables in the dataset. Convert them into numerical features using one-hot encoding with the \texttt{one.hot()} function.\\
\item
  Partition the dataset into 80\% training and 20\% testing sets using the \texttt{partition()} function. Ensure the target variable remains proportionally distributed in both sets.\\
\item
  Validate the partitioning by comparing the class distribution of the target variable in the training and test sets.\\
\item
  Apply min-max scaling to numerical variables in both training and test sets. Ensure that the scaling parameters are derived from the training set only.
\end{enumerate}

\subsubsection*{\texorpdfstring{Choosing the Optimal \(k\)}{Choosing the Optimal k}}\label{choosing-the-optimal-k}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\tightlist
\item
  Use the \texttt{kNN.plot()} function to determine the optimal \(k\) value for classifying \texttt{deposit} in the \emph{bank} dataset.\\
\item
  What is the best \(k\) value based on accuracy? How does accuracy change as \(k\) increases?
\item
  Interpret the meaning of the accuracy curve generated by \texttt{kNN.plot()}. What patterns do you observe?
\end{enumerate}

\subsubsection*{Building and Evaluating the kNN Model}\label{building-and-evaluating-the-knn-model}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\tightlist
\item
  Train a kNN model using the optimal \(k\) and make predictions on the test set.\\
\item
  Generate a confusion matrix for the kNN model predictions using the \texttt{conf.mat()} function. Interpret the results.\\
\item
  Calculate the accuracy of the kNN model. How well does it perform in predicting \emph{deposit}?\\
\item
  Besides accuracy, what other evaluation metrics (e.g., precision, recall, F1-score) would be useful for assessing kNN performance in the \emph{bank} dataset? Compute and interpret these metrics.\\
\item
  Compare the performance of kNN with different values of \(k\) (e.g., \(k = 1, 5, 15, 25\)). How does changing \(k\) affect the classification results?\\
\item
  Train a kNN model using only a subset of features: \texttt{age}, \texttt{balance}, \texttt{duration}, and \texttt{campaign}. Compare its accuracy with the full-feature model. What does this tell you about feature selection?\\
\item
  Compare the accuracy of kNN when using min-max scaling versus z-score standardization. How does the choice of scaling method impact model performance?
\end{enumerate}

\subsection*{Critical Thinking and Real-World Applications}\label{critical-thinking-and-real-world-applications}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\tightlist
\item
  Suppose you are building a fraud detection system for a bank. Would kNN be a suitable algorithm? What are its advantages and limitations in this context?\\
\item
  How would you handle imbalanced classes in the \emph{bank} dataset? What strategies could improve classification performance?\\
\item
  In a high-dimensional dataset with hundreds of features, would kNN still be an effective approach? Why or why not?\\
\item
  Imagine you are working with a dataset where new data points arrive in real-time. What challenges would kNN face, and how could they be addressed?\\
\item
  If a financial institution wants to classify customers into different risk categories for loan approval, what preprocessing steps would be essential before applying kNN?\\
\item
  In a dataset where some features are irrelevant or redundant, how could you improve kNN's performance? What feature selection methods would you use?\\
\item
  If computation time is a concern, what strategies could you apply to make kNN more efficient for large datasets?\\
\item
  Suppose kNN is performing poorly on the \emph{bank} dataset. What possible reasons could explain this, and how would you troubleshoot the issue?
\end{enumerate}

\chapter{Model Evaluation}\label{chapter-evaluation}

As we progress through the Data Science Workflow, introduced in Chapter \ref{chapter-intro-DS} and illustrated in Figure \ref{fig:CRISP-DM}, we have already completed the first five phases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Problem Understanding}: Defining the problem we aim to solve.\\
\item
  \textbf{Data Preparation}: Cleaning, transforming, and organizing the data for analysis.\\
\item
  \textbf{Exploratory Data Analysis (EDA)}: Gaining insights and uncovering patterns in the data.\\
\item
  \textbf{Preparing Data for Modeling}: Setting up the data for modeling by scaling, encoding, and partitioning.\\
\item
  \textbf{Modeling}: Applying algorithms to make predictions or extract insights---such as the kNN classification method we explored in the previous chapter.
\end{enumerate}

Now, we arrive at the \textbf{Model Evaluation} phase, a pivotal step in the Data Science Workflow. This phase answers the critical question: \emph{How well does our model perform?}

Building a model is just the beginning. Without evaluation, we have no way of knowing whether our model generalizes well to new data or if it is simply memorizing patterns from the training set. A model that performs well during training but fails in real-world applications is of little practical value. Model evaluation ensures that our predictions are reliable and that the model effectively captures underlying patterns rather than just noise.

This chapter will introduce key evaluation techniques and metrics to assess the performance of classification and regression models, helping us make informed decisions about model selection and improvement.

\subsection*{Why Is Model Evaluation Important?}\label{why-is-model-evaluation-important}


Building a model is just the beginning. The real test of its effectiveness lies in its ability to generalize to \emph{new, unseen data}. Without proper evaluation, a model may seem successful during development but fail when applied in real-world scenarios.

Consider this example:\\
You develop a model to detect fraudulent credit card transactions, and it achieves 95\% accuracy. Sounds impressive, right? But if only 1\% of the transactions are actually fraudulent, your model might simply classify every transaction as legitimate to achieve high accuracy---completely ignoring all fraud cases. This highlights a crucial point: \textbf{accuracy alone can be misleading, especially in imbalanced datasets}.

Model evaluation provides a more comprehensive understanding of a model's performance by assessing:

\begin{itemize}
\tightlist
\item
  \emph{Strengths}: What the model does well (e.g., correctly detecting fraud).\\
\item
  \emph{Weaknesses}: Where it falls short (e.g., missing fraudulent cases or flagging too many legitimate transactions as fraud).\\
\item
  \emph{Trade-offs}: The balance between competing priorities, such as sensitivity vs.~specificity or precision vs.~recall.
\end{itemize}

A well-evaluated model aligns with real-world objectives. It helps answer key questions such as:

\begin{itemize}
\tightlist
\item
  How well does the model handle imbalanced datasets?\\
\item
  Is it good at identifying true positives (e.g., detecting cancer in medical diagnoses)?\\
\item
  Does it minimize false positives (e.g., avoiding mistakenly flagging legitimate emails as spam)?
\end{itemize}

As \href{https://en.wikipedia.org/wiki/George_E._P._Box}{George Box}, a renowned statistician, famously said, \emph{``All models are wrong, but some are useful.''} A model is always a simplification of reality---it cannot capture every nuance or complexity. However, through proper evaluation, we can determine whether a model is \emph{useful enough} to make reliable predictions and guide decision-making.

In this chapter, we will explore the evaluation of classification models, starting with \emph{binary classification}, where the target variable has two categories (e.g., spam vs.~not spam). We will then discuss evaluation metrics for \emph{multi-class classification}, where there are more than two categories (e.g., types of vehicles: car, truck, bike). Finally, we will introduce evaluation metrics for \emph{regression models}, where the target variable is continuous (e.g., predicting house prices).

Our goal is to establish a strong foundation in model evaluation, enabling you to assess model performance effectively and make data-driven decisions. Let's begin with one of the most fundamental tools in classification evaluation: the \emph{Confusion Matrix}.

\section{Confusion Matrix}\label{confusion-matrix}

The \emph{confusion matrix} is a fundamental tool for evaluating classification models. It provides a detailed breakdown of a model's predictions by categorizing them into four distinct groups based on actual versus predicted values. For binary classification problems, the confusion matrix is structured as shown in Table \ref{tab:confusion-matrix}.

In classification tasks, one class is typically designated as the \emph{positive class} (the class of interest), while the other is the \emph{negative class}. For instance, in fraud detection, fraudulent transactions might be considered the positive class, while legitimate transactions are the negative class.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4545}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}@{}}
\caption{\label{tab:confusion-matrix} Confusion matrix summarizing correct and incorrect predictions for binary classification problems. The \emph{positive class} refers to the class of interest, while the \emph{negative class} represents the other category.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\emph{Predicted}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\emph{Predicted}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{Actual Positive} & { True Positive (TP) } & { False Negative (FN) } \\
\emph{Actual Negative} & { False Positive (FP) } & { True Negative (TN) } \\
\end{longtable}

Each element in the confusion matrix corresponds to one of four possible prediction outcomes:

\begin{itemize}
\tightlist
\item
  \textbf{True Positives (TP)}: The model correctly predicts the positive class (e.g., fraud detected as fraud).\\
\item
  \textbf{False Positives (FP)}: The model incorrectly predicts the positive class (e.g., legitimate transactions falsely flagged as fraud).\\
\item
  \textbf{True Negatives (TN)}: The model correctly predicts the negative class (e.g., legitimate transactions classified correctly).\\
\item
  \textbf{False Negatives (FN)}: The model incorrectly predicts the negative class (e.g., fraudulent transactions classified as legitimate).
\end{itemize}

If this structure feels familiar, it mirrors the concept of \emph{Type I and Type II errors} introduced in Chapter \ref{chapter-statistics} on hypothesis testing. The diagonal elements (TP and TN) represent correct predictions, while the off-diagonal elements (FP and FN) represent incorrect ones.

\subsection*{Calculating Key Metrics}\label{calculating-key-metrics}


Using the values from the confusion matrix, we can derive key performance metrics for the model, such as \emph{accuracy} (also known as \emph{success rate}) and \emph{error rate}:
\[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{Total Predictions}} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}
\]
\[
\text{Error Rate} = 1 - \text{Accuracy} = \frac{\text{FP} + \text{FN}}{\text{Total Predictions}}
\]

Accuracy represents the proportion of correct predictions (TP and TN) among all predictions, providing an overall assessment of model performance. Conversely, the \emph{error rate} measures the proportion of incorrect predictions (FP and FN) among all predictions.

While accuracy provides a high-level assessment of performance, it does not distinguish between different types of errors, such as false positives and false negatives. For example, in an imbalanced dataset where one class significantly outnumbers the other, accuracy may appear high even if the model performs poorly at detecting the minority class. This is why we need additional metrics, such as sensitivity, specificity, precision, and recall, which we will explore in later sections.

\begin{example}
\protect\hypertarget{exm:ex-confusion-matrix-kNN}{}\label{exm:ex-confusion-matrix-kNN}Let's revisit the \emph{k-Nearest Neighbors (kNN)} model from Chapter \ref{chapter-knn}, where we built a classifier to predict customer churn using the \texttt{churn} dataset. We will now evaluate its performance using the confusion matrix.

First, we apply the kNN model and generate predictions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)  }
\CommentTok{\# Load the churn dataset}
\FunctionTok{data}\NormalTok{(churn)}

\CommentTok{\# Partition the data into training and testing sets}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{43}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}
\NormalTok{actual\_test }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}

\CommentTok{\# Build and predict using the kNN model}
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.plan }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+} 
\NormalTok{                  intl.plan }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ intl.calls }\SpecialCharTok{+} 
\NormalTok{                  day.mins }\SpecialCharTok{+}\NormalTok{ day.calls }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+}\NormalTok{ eve.calls }\SpecialCharTok{+} 
\NormalTok{                  night.mins }\SpecialCharTok{+}\NormalTok{ night.calls }\SpecialCharTok{+}\NormalTok{ customer.calls}

\NormalTok{kNN\_predict }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{train =}\NormalTok{ train\_set, }
                  \AttributeTok{test =}\NormalTok{ test\_set, }\AttributeTok{k =} \DecValTok{5}\NormalTok{, }\AttributeTok{scaler =} \StringTok{"minmax"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For details on how this kNN model was built, refer to Section \ref{sec-kNN-churn}. Now, we generate the confusion matrix for the predictions using the \texttt{conf.mat()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(kNN\_predict, actual\_test, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\NormalTok{          Actual}
\NormalTok{   Predict yes  no}
\NormalTok{       yes  }\DecValTok{54}   \DecValTok{7}
\NormalTok{       no   }\DecValTok{83} \DecValTok{856}
\end{Highlighting}
\end{Shaded}

Here, we set \texttt{reference\ =\ "yes"} to specify that \texttt{churn\ =\ yes} is the positive class, aligning the confusion matrix with our problem focus---correctly identifying customers who actually churned. The confusion matrix summarizes the model's performance as follows:

\begin{itemize}
\tightlist
\item
  \emph{True Positives (TP)}: 54 correctly predicted churn cases.\\
\item
  \emph{True Negatives (TN)}: 856 correctly predicted non-churn cases.\\
\item
  \emph{False Positives (FP)}: 83 incorrectly predicted churn when customers did not churn.\\
\item
  \emph{False Negatives (FN)}: 7 missed churn cases, predicting them as non-churn.
\end{itemize}

We can also visualize the confusion matrix using the \texttt{conf.mat.plot()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(kNN\_predict, actual\_test)}
\NormalTok{   Setting levels}\SpecialCharTok{:}\NormalTok{ reference }\OtherTok{=} \StringTok{"yes"}\NormalTok{, case }\OtherTok{=} \StringTok{"no"}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{8_Model_Evaluation_files/figure-latex/unnamed-chunk-4-1} \end{center}

From the confusion matrix, we compute accuracy and error rate:

\[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{Total Predictions}} = \frac{54 + 856}{1000} = 0.91
\]

\[
\text{Error Rate} = \frac{\text{FP} + \text{FN}}{\text{Total Predictions}} = \frac{83 + 7}{1000} = 0.09
\]

The accuracy indicates that the model correctly classified 91\% of the test set, while 9\% of predictions were incorrect.
\end{example}

While accuracy provides a useful summary of overall performance, it does not account for \emph{imbalanced datasets} or \emph{misclassification costs}. For example, in customer churn prediction, false negatives (missed churners) might be more costly than false positives (incorrectly predicted churners). Therefore, additional evaluation metrics are necessary to provide a deeper understanding of model performance.

To gain deeper insights into model performance, we now turn to \emph{sensitivity}, \emph{specificity}, \emph{precision}, and \emph{recall}---metrics that provide a more detailed evaluation of classification outcomes.

\section{Sensitivity and Specificity}\label{sensitivity-and-specificity}

In classification, it's important to evaluate not just how many predictions are correct overall, but how well the model identifies specific classes. \emph{Sensitivity} and \emph{Specificity} are two complementary metrics that focus on the model's ability to distinguish between positive and negative classes.

These metrics are particularly valuable in cases where class distribution is imbalanced. For example, in fraud detection or rare disease diagnosis, the majority of cases belong to the negative class, which can lead to misleadingly high accuracy. Sensitivity and specificity allow us to separately assess how well the model detects each class.

\subsection*{Sensitivity}\label{sensitivity}


\emph{Sensitivity} (also called \emph{Recall} in some fields, like information retrieval) measures the model's ability to correctly identify positive cases. It answers the question:

\begin{quote}
\emph{``Out of all the actual positives, how many did the model correctly predict?''}
\end{quote}

Mathematically, sensitivity is defined as:\\
\[
\text{Sensitivity} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
\]

Let's compute sensitivity for the \emph{k-Nearest Neighbors (kNN)} model built in Chapter \ref{chapter-knn}, where we predicted whether customers churned (\texttt{churn\ =\ yes}). Sensitivity in this case reflects the percentage of churners correctly identified by the model. Using the confusion matrix from Example \ref{exm:ex-confusion-matrix-kNN}:\\
\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{54}{54 + 7} = 0.885
\]

This means that our model has correctly identified 88.5\% of actual churners.

A \emph{perfect model} would achieve a sensitivity of \emph{1.0 (100\%)}, meaning it correctly identifies all positive cases. However, it's important to note that even a naÃ¯ve model that classifies \emph{all} customers as churners would also achieve 100\% sensitivity. This illustrates that sensitivity alone isn't enough to evaluate a model's performance---it must be paired with other metrics to capture the full picture.

\subsection*{Specificity}\label{specificity}


While sensitivity focuses on the positive class, \emph{Specificity} measures the model's ability to correctly identify negative cases. It answers the question:

\begin{quote}
\emph{``Out of all the actual negatives, how many did the model correctly predict?''}
\end{quote}

Specificity is particularly important in situations where avoiding false positives is critical. For example, in spam detection, incorrectly marking a legitimate email as spam (a false positive) can have more severe consequences than missing a few spam messages. Mathematically, specificity is defined as:\\
\[
\text{Specificity} = \frac{\text{True Negatives (TN)}}{\text{True Negatives (TN)} + \text{False Positives (FP)}}
\]

Using the kNN model and the confusion matrix from Example \ref{exm:ex-confusion-matrix-kNN}, let's calculate the specificity for identifying non-churners (\texttt{churn\ =\ no}):\\
\[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}} = \frac{856}{856 + 83} = 0.912
\]

This means the model correctly classified 91.2\% of the actual non-churners as not leaving the company.

A good classification model should ideally achieve \emph{high sensitivity and high specificity}, but the relative importance of these metrics depends on the problem domain. For example, in medical diagnostics, sensitivity is often prioritized to ensure no disease cases are missed, while in credit scoring, specificity might take precedence to avoid mistakenly classifying reliable customers as risks.

For the kNN model in Example \ref{exm:ex-confusion-matrix-kNN}, sensitivity is 0.885, while specificity is 0.912. This trade-off may be acceptable in this instance, as identifying churners (sensitivity) might be more critical than avoiding false positives (specificity).

\subsection*{Sensitivity vs.~Specificity: A Balancing Act}\label{sensitivity-vs.-specificity-a-balancing-act}


The trade-off between sensitivity and specificity is often an essential consideration in model evaluation. In many cases, improving one comes at the cost of the other:

\begin{itemize}
\tightlist
\item
  Increasing \emph{sensitivity} (recall) often leads to more false positives, lowering specificity.
\item
  Increasing \emph{specificity} reduces false positives but can increase false negatives, lowering sensitivity.
\end{itemize}

For example, in \textbf{medical screening}, missing a serious disease (false negative) can have severe consequences, so a model with \textbf{high sensitivity} is preferred---even if it results in more false positives (low specificity). In contrast, in \textbf{email spam filtering}, a high false positive rate (flagging important emails as spam) can be frustrating for users. Therefore, a model with \textbf{high specificity} is preferable, even if it occasionally misses spam emails.

This balance is one of the core challenges in classification. The optimal trade-off depends on the business or domain priorities.

In the next section, we will refine this evaluation further by introducing \emph{precision} and \emph{recall}. These metrics extend sensitivity and specificity by focusing on the reliability of positive predictions and the ability to capture all relevant positive cases.

\section{Precision, Recall, and F1-Score}\label{precision-recall-and-f1-score}

In addition to sensitivity and specificity, precision, recall, and the F1-score offer deeper insights into a classification model's performance. These metrics are particularly valuable in scenarios with imbalanced datasets, where accuracy alone can be misleading.\\
Precision, also known as \emph{positive predictive value}, measures how many of the model's predicted positives are actually positive. It answers the question:

\begin{quote}
\emph{``When the model predicts positive, how often is it correct?''}\\
Mathematically, precision is defined as:\\
\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]
Precision is especially important in applications where false positives are costly. For example, in fraud detection, flagging legitimate transactions as fraudulent can lead to customer dissatisfaction and unnecessary investigations.
\end{quote}

Recall (equivalent to sensitivity) measures the model's ability to identify positive cases. It answers the question:

\begin{quote}
\emph{``Out of all the actual positives, how many did the model correctly predict?''}\\
Mathematically, recall is defined as:\\
\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]
Recall is particularly useful when missing positive cases (false negatives) has serious consequences, such as failing to diagnose a disease or missing fraudulent transactions. While recall is often used interchangeably with sensitivity in medical diagnostics, it is more commonly referred to as recall in areas like information retrieval, spam detection, and text classification.
\end{quote}

\subsection*{Precision vs.~Recall: A Trade-Off}\label{precision-vs.-recall-a-trade-off}


There is an inherent trade-off between precision and recall:

\begin{itemize}
\tightlist
\item
  Increasing precision makes the model more selective, reducing false positives but potentially missing true positives (lower recall).\\
\item
  Increasing recall allows the model to capture more positive cases, reducing false negatives but potentially misclassifying negatives as positives (lower precision).
\end{itemize}

For example, a medical test for cancer screening should prioritize high recall to ensure that no patient with cancer is missed. However, in email spam detection, precision might be more important to avoid mistakenly classifying important emails as spam.

\subsection*{The F1-Score: Balancing Precision and Recall}\label{the-f1-score-balancing-precision-and-recall}


To balance this trade-off, the F1-score combines precision and recall into a single metric. It is the harmonic mean of precision and recall, emphasizing their balance:
\[
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
   = \frac{2 \cdot \text{TP}}{2 \cdot \text{TP} + \text{FP} + \text{FN}}
\]
The F1-score is particularly useful in imbalanced datasets, where one class significantly outnumbers the other. Unlike accuracy, it considers both false positives and false negatives, providing a more informative evaluation of a model's predictive performance.

Now, let's apply these concepts to the k-Nearest Neighbors (kNN) model from Example \ref{exm:ex-confusion-matrix-kNN}, which predicts customer churn (\texttt{churn\ =\ yes}).

First, precision quantifies how often the model's predicted churners were actual churners:\\
\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{54}{54 + 83} = 0.394
\]
This means that when the model predicts churn, it is correct in 39.4\% of cases.

Next, recall measures how many of the actual churners were correctly identified by the model:
\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{54}{54 + 7} = 0.885
\]
This shows that the model successfully identifies 88.5\% of actual churners.

Finally, the F1-score provides a single measure that balances precision and recall:
\[
F1 = \frac{2 \cdot 54}{2 \cdot 54 + 83 + 7} = 0.545
\]
The F1-score provides a summary measure of a model's ability to correctly identify churners while minimizing false predictions.

\subsection*{Choosing the Right Metric}\label{choosing-the-right-metric}


While the F1-score is a valuable metric, it assumes that precision and recall are equally important, which may not always align with the priorities of a particular problem. In medical diagnostics, recall (ensuring no cases are missed) might be more critical than precision. In spam filtering, precision (avoiding false positives) might take precedence to prevent misclassifying important emails.

For a more comprehensive evaluation, we now turn to metrics that assess performance across different classification thresholds. Instead of relying on a fixed decision threshold, these metrics analyze how the model behaves when the classification cutoff changes. This leads us to the Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC), which provide insights into how well the model distinguishes between positive and negative cases.

\section{Taking Uncertainty into Account}\label{taking-uncertainty-into-account}

When evaluating a classification model, metrics such as precision, recall, and F1-score provide valuable insights into its performance. However, these metrics are based on discrete predictions, where each observation is classified as either positive or negative. This approach overlooks an important factor: \emph{uncertainty}. Many classification models, including k-Nearest Neighbors (kNN), can output probability scores instead of fixed labels, offering a measure of confidence for each prediction.

These probability scores allow us to fine-tune how decisions are made by adjusting the \emph{classification threshold}. By default, a threshold of 0.5 is commonly used, meaning that if a model assigns a probability of 50\% or higher to the positive class, the instance is classified as positive; otherwise, it is classified as negative. However, this default may not always be ideal. Adjusting the threshold can significantly impact model performance, allowing it to better align with \emph{business goals} or \emph{domain-specific needs}. For example, in some applications, missing true positives (false negatives) is far costlier than misclassifying negatives as positives---or vice versa. By experimenting with different thresholds, we can explore trade-offs between sensitivity, specificity, precision, and recall to optimize model decisions.

\begin{example}
\protect\hypertarget{exm:ex-confusion-matrix-kNN-prob}{}\label{exm:ex-confusion-matrix-kNN-prob}Let's revisit the kNN model from Example \ref{exm:ex-confusion-matrix-kNN}, which predicts customer churn (\texttt{churn\ =\ yes}). This time, instead of making discrete predictions, we will obtain probability scores for the positive class (\texttt{churn\ =\ yes}) by setting the \texttt{type} parameter to \texttt{"prob"} in the \texttt{kNN()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kNN\_prob }\OtherTok{=} \FunctionTok{kNN}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{train =}\NormalTok{ train\_set, }
               \AttributeTok{test =}\NormalTok{ test\_set, }\AttributeTok{k =} \DecValTok{5}\NormalTok{, }\AttributeTok{scaler =} \StringTok{"minmax"}\NormalTok{,}
               \AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\NormalTok{kNN\_prob[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, ]}
\NormalTok{      yes  no}
   \DecValTok{6}  \FloatTok{0.4} \FloatTok{0.6}
   \DecValTok{10} \FloatTok{0.2} \FloatTok{0.8}
   \DecValTok{17} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{19} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{21} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{23} \FloatTok{0.2} \FloatTok{0.8}
   \DecValTok{29} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{31} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{36} \FloatTok{0.0} \FloatTok{1.0}
   \DecValTok{40} \FloatTok{0.0} \FloatTok{1.0}
\end{Highlighting}
\end{Shaded}

The output displays the first 10 probability scores for each class: the first column corresponds to \texttt{churn\ =\ yes}, while the second column corresponds to \texttt{churn\ =\ no}. For instance, if the first row has a probability of 0.4, the model is 40\% confident that the customer will churn, while a probability of 0.6 suggests a 60\% confidence that the customer will not churn.

To demonstrate the impact of threshold selection, we compute confusion matrices at two different thresholds: the default 0.5 and a stricter 0.7 threshold.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat}\NormalTok{(kNN\_prob[, }\DecValTok{1}\NormalTok{], actual\_test, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{          Actual}
\NormalTok{   Predict yes  no}
\NormalTok{       yes  }\DecValTok{54}   \DecValTok{7}
\NormalTok{       no   }\DecValTok{83} \DecValTok{856}
\FunctionTok{conf.mat}\NormalTok{(kNN\_prob[, }\DecValTok{1}\NormalTok{], actual\_test, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{, }\AttributeTok{cutoff =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{          Actual}
\NormalTok{   Predict yes  no}
\NormalTok{       yes  }\DecValTok{22}   \DecValTok{1}
\NormalTok{       no  }\DecValTok{115} \DecValTok{862}
\end{Highlighting}
\end{Shaded}

At a threshold of 0.5, the model classifies a customer as a churner if the probability of churn is at least 50\%. This confusion matrix aligns with the one in Example \ref{exm:ex-confusion-matrix-kNN}. However, when we raise the threshold to 0.7, the model becomes more conservative, requiring at least 70\% confidence before classifying an instance as churn. This shifts the balance between true positives, false positives, and false negatives:

\begin{itemize}
\tightlist
\item
  Lowering the threshold increases sensitivity, catching more true positives but potentially leading to more false positives.\\
\item
  Raising the threshold increases specificity, reducing false positives but at the risk of missing more true positives.
\end{itemize}

Adjusting the threshold is particularly useful in cases where the cost of false positives and false negatives is not equal. For example, in \emph{fraud detection}, false negatives (missing fraudulent transactions) can be costly, so lowering the threshold to prioritize recall (sensitivity) might be preferable. Conversely, in \emph{spam detection}, false positives (flagging legitimate emails as spam) are undesirable, so a higher threshold might be used to prioritize precision.
\end{example}

\subsection*{Choosing an Optimal Threshold}\label{choosing-an-optimal-threshold}


Fine-tuning the threshold allows us to align model behavior with business or domain-specific requirements. Suppose we need a sensitivity of at least 90\% to ensure that most churners are detected. By iteratively adjusting the threshold and recalculating sensitivity, we can determine the cutoff that achieves this goal. This process is known as setting an \emph{operating point} for the model.

However, threshold adjustments always involve trade-offs. A lower threshold improves recall but may reduce precision by increasing false positives. Conversely, a higher threshold increases precision but may lower recall by missing true positives. For instance, setting a threshold of 0.9 might achieve near-perfect specificity but could miss most actual churners.

While manually tuning the threshold can be helpful, a more systematic approach is needed to evaluate model performance across all possible thresholds. This leads us to the \emph{Receiver Operating Characteristic (ROC) curve} and \emph{Area Under the Curve (AUC)}, which provide a comprehensive way to assess a model's ability to distinguish between classes.

\section{ROC Curve and AUC}\label{roc-curve-and-auc}

While adjusting classification thresholds provides valuable insights, it is often impractical for systematically comparing models. Additionally, sensitivity, specificity, precision, and recall evaluate a model at a fixed threshold, offering only a snapshot of performance. Instead, we need a way to assess performance across a range of thresholds, revealing broader trends in model behavior. Models with similar overall accuracy may perform differently---one might excel at detecting positive cases but misclassify many negatives, while another might do the opposite. To systematically evaluate a model's ability to distinguish between positive and negative cases across all thresholds, we use the \emph{Receiver Operating Characteristic (ROC) curve} and its associated metric, the \emph{Area Under the Curve (AUC)}.

The \emph{ROC curve} visually represents the trade-off between sensitivity (true positive rate) and specificity (true negative rate) at various classification thresholds. It plots the \emph{True Positive Rate (Sensitivity)} against the \emph{False Positive Rate (1 - Specificity)}. Originally developed for radar signal detection during World War II, the ROC curve is now widely used in machine learning to assess classifier effectiveness.

Figure \ref{fig:roc-curve} illustrates key ROC curve characteristics. The vertical axis represents \emph{True Positive Rate (Sensitivity)}, while the horizontal axis represents \emph{False Positive Rate (1 - Specificity)}. Three scenarios are highlighted:

\begin{itemize}
\tightlist
\item
  \emph{Optimal Performance (Green Curve)}: A model with near-perfect performance passes through the top-left corner, achieving both high sensitivity and high specificity.\\
\item
  \emph{Good Performance (Blue Curve)}: A well-performing but imperfect model remains closer to the top-left corner than to the diagonal line.\\
\item
  \emph{Random Classifier (Diagonal Line)}: The gray dashed diagonal represents a model with no predictive power, classifying randomly. A model close to this line provides little practical utility.
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth]{images/ch8_roc-curve} 

}

\caption{The ROC curve illustrates the trade-off between sensitivity and specificity at different thresholds. The diagonal line represents a classifier with no predictive value (gray dashed line), while the curves represent varying levels of performance: green for optimal and blue for good.}\label{fig:roc-curve}
\end{figure}

Each point on the ROC curve corresponds to a specific threshold. As the threshold varies, the \emph{True Positive Rate (Sensitivity)} and \emph{False Positive Rate (1 - Specificity)} change, tracing the curve. A curve that remains close to the top-left corner indicates better performance, as the model achieves high sensitivity while minimizing false positives. However, moving along the curve reflects different trade-offs between sensitivity and specificity. Choosing an optimal threshold depends on the application:

\begin{itemize}
\tightlist
\item
  In \emph{medical diagnostics}, maximizing sensitivity ensures no cases are missed, even if it results in some false positives.\\
\item
  In \emph{fraud detection}, prioritizing specificity prevents legitimate transactions from being falsely flagged.
\end{itemize}

To construct the ROC curve, a classifier's predictions are sorted by their estimated probabilities for the positive class. Starting from the origin, each prediction's impact on sensitivity and specificity is plotted. Correct predictions (true positives) result in vertical movements, while incorrect predictions (false positives) lead to horizontal shifts.

\begin{example}
\protect\hypertarget{exm:ex-roc-curve-kNN}{}\label{exm:ex-roc-curve-kNN}Let's apply this concept to the \emph{k-Nearest Neighbors (kNN)} model from Example \ref{exm:ex-confusion-matrix-kNN-prob}, where we obtained probabilities for the positive class (\texttt{churn\ =\ yes}). We'll use these probabilities to generate the ROC curve for the model. The \textbf{pROC} package in R simplifies this process. Ensure the package is installed using \texttt{install.packages("pROC")} before proceeding.

To create an ROC curve, two inputs are needed: the estimated probabilities for the positive class and the actual class labels. Using the \texttt{roc()} function from the \textbf{pROC} package, we generate the ROC curve object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}

\NormalTok{roc\_knn }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(}\AttributeTok{response =}\NormalTok{ actual\_test, }\AttributeTok{predictor =}\NormalTok{ kNN\_prob[, }\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

We can then visualize the ROC curve using the \texttt{ggroc()} function from the \textbf{pROC} package or the \texttt{plot()} function for a basic display. Here's the ROC curve for the kNN model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(roc\_knn, }\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC curve for KNN with k = 5, based on churn data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth]{8_Model_Evaluation_files/figure-latex/roc-knn-churn-1} 

}

\caption{ROC curve for KNN with k = 5, based on churn data.}\label{fig:roc-knn-churn}
\end{figure}

The ROC curve visually demonstrates the model's performance across different thresholds. A curve closer to the top-left corner indicates better performance, as it achieves high sensitivity and specificity. The diagonal line represents a random classifier, serving as a baseline for comparison. In this case, the kNN model's ROC curve is much closer to the top-left corner, suggesting strong performance in distinguishing between churners and non-churners.
\end{example}

\subsection*{Area Under the Curve (AUC)}\label{area-under-the-curve-auc}


Another critical metric derived from the ROC curve is the \emph{Area Under the Curve (AUC)}, which quantifies the overall performance of the model. AUC represents the probability that a randomly chosen positive instance will have a higher predicted score than a randomly chosen negative instance. Mathematically, AUC is computed as:

\[
\text{AUC} = \int_{0}^{1} \text{TPR}(t) \, d\text{FPR}(t)
\]

where \(t\) represents the threshold, reinforcing that AUC measures the model's ability to rank positive cases above negative ones across all possible thresholds.

\begin{figure}[H]

{\centering \includegraphics[width=0.45\linewidth]{images/ch8_auc} 

}

\caption{The AUC summarizes the ROC curve into a single number, representing the modelâ€™s ability to rank positive cases higher than negative ones. AUC = 1: Perfect model. AUC = 0.5: No better than random guessing.}\label{fig:auc}
\end{figure}

AUC values range from 0 to 1, where a value of 1 indicates a perfect classifier with ideal discrimination between classes, while a value of 0.5 suggests no better performance than random guessing. AUC values between 0.5 and 1 represent varying levels of model performance, with higher values reflecting better separation between positive and negative cases.

For the kNN model, we compute the AUC using the \texttt{auc()} function from the \textbf{pROC} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{auc}\NormalTok{(roc\_knn)}
\NormalTok{   Area under the curve}\SpecialCharTok{:} \FloatTok{0.8494}
\end{Highlighting}
\end{Shaded}

The AUC value for this model is 0.849, meaning the model ranks positive cases higher than negative ones with a probability of 0.849.

In summary, the ROC curve and AUC provide a comprehensive way to evaluate classification models, enabling comparisons across multiple models and identifying optimal thresholds for specific tasks. These tools are particularly valuable for \emph{imbalanced datasets}, as they capture the trade-offs between sensitivity and specificity across all classification thresholds. By combining these insights with metrics like precision, recall, and the F1-score, we can develop a deeper understanding of model performance and select the best approach for the given problem.

In the next section, we extend our discussion to \emph{multi-class classification}, where the target variable has more than two possible categories, requiring modifications to standard evaluation metrics.

\section{Metrics for Multi-Class Classification}\label{metrics-for-multi-class-classification}

So far, we have focused on binary classification, where the target variable has two categories. However, many real-world problems involve \emph{multi-class classification}, where the target variable can belong to three or more categories. Examples include classifying species in ecological studies or identifying different types of vehicles. Evaluating such models requires extending performance metrics to handle multiple categories effectively.

In multi-class classification, the confusion matrix expands to include all classes, with each row representing the actual class and each column representing the predicted class. Correct predictions appear along the diagonal, while off-diagonal elements indicate misclassifications. This structure highlights which classes the model struggles to distinguish.

Metrics such as accuracy, precision, recall, and F1-score can be adapted for multi-class problems. Instead of evaluating a single positive class, we assess each class as if it were the positive class while treating all other classes as negative. This one-vs-all (also known as one-vs-rest) approach allows the calculation of precision, recall, and F1-score for each class separately. To summarize overall performance, different averaging techniques are used:

\begin{itemize}
\tightlist
\item
  \emph{Macro-average}: Computes the unweighted mean of the metric across all classes, treating each class equally. This is useful when all classes are of equal importance, regardless of their frequency in the dataset.\\
\item
  \emph{Micro-average}: Aggregates predictions across all classes before computing the metric, giving more weight to larger classes. This is particularly useful when the dataset has an uneven class distribution, as it provides a more representative measure of overall model performance.\\
\item
  \emph{Weighted-average}: Similar to macro-averaging but weights each class's metric by its frequency in the dataset. This ensures that larger classes contribute proportionally while preventing minority classes from being overshadowed.
\end{itemize}

These averaging methods ensure a fair evaluation, particularly in imbalanced datasets where some classes may have significantly fewer samples than others.

While metrics such as the ROC curve and AUC are primarily designed for binary classification, they can be extended to multi-class problems using strategies like one-vs-all, where an ROC curve is generated for each class against the others. However, in most practical applications, macro-averaged or weighted-averaged F1-score provides a concise and meaningful summary of multi-class model performance.

By applying these metrics, we can assess how well the model performs across all categories, identify weaknesses in specific classes, and ensure that the evaluation aligns with the problem's objectives. The next section explores evaluation metrics for \emph{regression models}, where the target variable is continuous rather than categorical.

\section{Evaluation Metrics for Continuous Targets}\label{evaluation-metrics-for-continuous-targets}

So far, we have focused on evaluating classification models, which predict discrete categories. However, many real-world problems involve predicting continuous target variables, such as house prices, stock market trends, or weather forecasts. These tasks require \emph{regression models}, which are assessed using metrics specifically designed for continuous data.

A widely used evaluation metric for regression models is the \emph{Mean Squared Error (MSE)}:
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]
where \(y_i\) represents the actual value, \(\hat{y}_i\) is the predicted value, and \(n\) is the number of observations. MSE calculates the average squared difference between predicted and actual values, with larger errors contributing disproportionately due to squaring. As a result, MSE is particularly sensitive to outliers. Lower values indicate better model performance, with zero representing a perfect fit.

While MSE is useful, its sensitivity to large errors may not always be desirable. A more robust alternative is the \emph{Mean Absolute Error (MAE)}, which measures the average absolute difference between predicted and actual values:
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]
Unlike MSE, MAE treats all errors equally, making it less sensitive to extreme values and easier to interpret. It is particularly useful when the target variable has a skewed distribution or when outliers are present.

Another key metric for evaluating regression models is the \emph{\(R^2\) score}, or \emph{coefficient of determination}. The \(R^2\) score measures the proportion of variance in the target variable that the model explains. It is defined as:
\[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]
where \(\bar{y}\) is the mean of the actual values. The \(R^2\) score ranges from 0 to 1, where higher values indicate a better fit. An \(R^2\) value of 1 means the model perfectly predicts the target variable, while a value of 0 suggests the model performs no better than simply predicting the mean of the target variable.

These metrics provide different perspectives on model performance. MSE is useful when penalizing larger errors is important, MAE is preferable for interpretability and robustness to outliers, and \(R^2\) helps quantify how well the model explains variability in the data. The choice of metric depends on the specific problem and goals. In Chapter \ref{chapter-regression}, we will explore these evaluation metrics in greater depth, alongside various regression modeling techniques.

\section{Key Takeaways from Model Evaluation}\label{key-takeaways-from-model-evaluation}

In this chapter, we explored the critical step of model evaluation, which determines how well a model performs and whether it meets the requirements of the problem at hand. Starting with foundational concepts, we examined metrics for evaluating classification models, including binary, multi-class, and regression models.

\subsection*{Key Takeaways}\label{key-takeaways-1}


\begin{itemize}
\item
  \textbf{Binary Classification Metrics}:\\
  We began by understanding the confusion matrix, which categorizes predictions into true positives, true negatives, false positives, and false negatives. From this, we derived key metrics such as accuracy, sensitivity (recall), specificity, precision, and the F1-score, each offering different perspectives on model performance.
\item
  \textbf{Threshold Tuning}:\\
  Recognizing the impact of probability thresholds on model predictions, we discussed how adjusting thresholds can help align a model with specific goals, such as maximizing sensitivity for critical applications or prioritizing specificity to avoid false positives.
\item
  \textbf{ROC Curve and AUC}:\\
  To evaluate model performance across all possible thresholds, we introduced the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC). These tools provide a systematic and visual way to assess a model's ability to distinguish between classes, making them particularly useful for comparing multiple models.
\item
  \textbf{Multi-Class Classification}:\\
  For classification problems involving more than two classes, we extended metrics such as precision, recall, and the F1-score by calculating per-class metrics and aggregating them using methods such as macro-average, micro-average, and weighted-average. These approaches ensure a balanced evaluation, especially when dealing with imbalanced datasets.
\item
  \textbf{Regression Metrics}:\\
  For problems involving continuous target variables, we introduced evaluation metrics such as mean squared error (MSE), mean absolute error (MAE), and the \(R^2\) score. These metrics allow for assessing prediction accuracy while accounting for trade-offs between penalizing large errors (MSE) and ensuring interpretability (MAE).
\end{itemize}

\subsection*{Closing Thoughts}\label{closing-thoughts}


This chapter emphasized that no single metric can fully capture a model's performance. Instead, evaluation should be guided by the specific goals and constraints of the problem, balancing trade-offs such as accuracy versus interpretability and false positives versus false negatives. Proper evaluation ensures that a model is not only accurate but also actionable and reliable in real-world applications.

By mastering these evaluation techniques, you are now equipped to critically assess model performance, optimize thresholds, and select the right model for the task at hand. In the following chapters, we will build on this foundation to explore advanced modeling techniques and their evaluation in greater detail.

\section{Exercises}\label{exercises-6}

\subsection*{Conceptual Questions}\label{conceptual-questions-4}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Why is model evaluation important in machine learning?\\
\item
  Explain the difference between training accuracy and test accuracy.\\
\item
  What is a confusion matrix, and why is it useful?\\
\item
  How does the choice of the positive class impact evaluation metrics?\\
\item
  What is the difference between sensitivity and specificity?\\
\item
  When would you prioritize sensitivity over specificity? Provide an example.\\
\item
  What is precision, and how does it differ from recall?\\
\item
  Why do we use the F1-score instead of relying solely on accuracy?\\
\item
  Explain the trade-off between precision and recall. How does changing the classification threshold impact them?\\
\item
  What is an ROC curve, and how does it help compare different models?\\
\item
  What does the Area Under the Curve (AUC) represent? How do you interpret different AUC values?\\
\item
  How can adjusting classification thresholds optimize model performance for a specific business need?\\
\item
  Why is accuracy often misleading for imbalanced datasets? What alternative metrics can be used?\\
\item
  What are macro-average and micro-average F1-scores, and when should each be used?\\
\item
  Explain how multi-class classification evaluation differs from binary classification.\\
\item
  What is Mean Squared Error (MSE), and why is it used in regression models?\\
\item
  How does Mean Absolute Error (MAE) compare to MSE? When would you prefer one over the other?\\
\item
  What is the \(R^2\) score in regression, and what does it indicate?\\
\item
  Can an \(R^2\) score be negative? What does it mean if this happens?\\
\item
  Why is it important to evaluate models using multiple metrics instead of relying on a single one?
\end{enumerate}

\subsection*{\texorpdfstring{Hands-On Practice: Evaluating Models with the \emph{Bank} Dataset}{Hands-On Practice: Evaluating Models with the Bank Dataset}}\label{hands-on-practice-evaluating-models-with-the-bank-dataset}


For these exercises, we will use the \emph{bank} dataset from the \textbf{liver} package. The dataset contains information on customer demographics and financial details, with the target variable \emph{deposit} indicating whether a customer subscribed to a term deposit.

Load the necessary package and dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\CommentTok{\# Load the dataset}
\FunctionTok{data}\NormalTok{(bank)}

\CommentTok{\# View the structure of the dataset}
\FunctionTok{str}\NormalTok{(bank)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{17}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays    }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Data Preparation}\label{data-preparation-1}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Load the \emph{bank} dataset and identify the target variable and predictor variables.\\
\item
  Check for class imbalance in the target variable (\emph{deposit}). How many customers subscribed to a term deposit versus those who did not?\\
\item
  Apply one-hot encoding to categorical variables using \texttt{one.hot()}.\\
\item
  Partition the dataset into 80\% training and 20\% test sets using \texttt{partition()}.\\
\item
  Validate the partitioning by comparing the class distribution of \emph{deposit} in the training and test sets.\\
\item
  Apply min-max scaling to numerical variables to ensure fair distance calculations in kNN models.
\end{enumerate}

\subsection*{Model Training and Evaluation}\label{model-training-and-evaluation}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\tightlist
\item
  Train a kNN model using the training set and predict \emph{deposit} for the test set.\\
\item
  Generate a confusion matrix for the test set predictions using \texttt{conf.mat()}. Interpret the results.\\
\item
  Compute the accuracy, sensitivity, and specificity of the kNN model.\\
\item
  Calculate precision, recall, and the F1-score for the model.\\
\item
  Use \texttt{conf.mat.plot()} to visualize the confusion matrix.\\
\item
  Experiment with different values of \(k\) (e.g., 3, 7, 15) and compare the evaluation metrics.\\
\item
  Plot the ROC curve for the kNN model using the \textbf{pROC} package.\\
\item
  Compute the AUC for the model. What does the value indicate about performance?\\
\item
  Adjust the classification threshold (e.g., from 0.5 to 0.7) and analyze how it impacts sensitivity and specificity.
\end{enumerate}

\subsection*{Critical Thinking and Real-World Applications}\label{critical-thinking-and-real-world-applications-1}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{35}
\tightlist
\item
  Suppose a bank wants to minimize false positives (incorrectly predicting a customer will subscribe). How should the classification threshold be adjusted?\\
\item
  If detecting potential subscribers is the priority, should the model prioritize precision or recall? Why?\\
\item
  If the dataset were highly imbalanced, what strategies could be used to improve model evaluation?\\
\item
  Consider a fraud detection system where false negatives (missed fraud cases) are extremely costly. How would you adjust the evaluation approach?\\
\item
  Imagine you are comparing two models: one has high accuracy but low recall, and the other has slightly lower accuracy but high recall. How would you decide which to use?\\
\item
  If a new marketing campaign resulted in a large increase in term deposit subscriptions, how might that affect the evaluation metrics?\\
\item
  Given the evaluation results from your model, what business recommendations would you make to a financial institution?
\end{enumerate}

\chapter{Naive Bayes Classifier}\label{chapter-bayes}

How can we make highly accurate predictions with minimal data and computation? Imagine a bank deciding whether to approve a loan based on some factors---such as a customer's income, age, and mortgage status. The Naive Bayes classifier offers a remarkably simple yet effective approach to such problems, relying on probability theory to make rapid, informed decisions.

Naive Bayes is a probabilistic classification algorithm that balances simplicity with effectiveness, making it a widely used approach in machine learning. It belongs to a family of classifiers based on Bayes' theorem and operates under a key simplifying assumption: all features are conditionally independent given the target class. While this assumption is rarely true in real-world data, it allows for fast computation and efficient probability estimation, making the algorithm highly scalable and practical.

Despite its simplicity, Naive Bayes delivers strong performance in a variety of applications, particularly in text classification, spam detection, sentiment analysis, and financial risk assessment. In these domains, feature dependencies are often weak enough that the independence assumption does not significantly impact accuracy.

Beyond its theoretical foundations, Naive Bayes is computationally efficient, making it well-suited for large-scale datasets with high-dimensional feature spaces. For instance, in risk prediction, where multiple financial indicators must be analyzed, Naive Bayes can assess a customer's likelihood of default in milliseconds. Its intuitive probabilistic reasoning and ease of implementation make it a valuable tool for both beginners and experienced practitioners.

The power of Naive Bayes comes from its foundation in Bayesian probability theory, specifically \emph{Bayes' Theorem}, introduced by the 18th-century mathematician Thomas Bayes \citep{bayes1958essay}. This theorem provides a mathematical framework for updating probability estimates as new data becomes available. By combining prior knowledge with new evidence, Bayes' theorem serves as the basis for many \emph{Bayesian methods} in statistics and machine learning.

\subsection*{Strengths and Limitations}\label{strengths-and-limitations}


The Naive Bayes classifier is widely valued for its simplicity and efficiency. It offers several advantages:

\begin{itemize}
\tightlist
\item
  It performs well on high-dimensional datasets, such as text classification problems with thousands of features.\\
\item
  It is computationally efficient, making it ideal for real-time applications like spam filtering and risk prediction.\\
\item
  It remains effective even when the independence assumption is violated, as long as feature dependencies are not too strong.
\end{itemize}

However, Naive Bayes also has limitations:

\begin{itemize}
\tightlist
\item
  The assumption that features are conditionally independent is rarely true in real-world datasets, especially when features exhibit strong correlations.\\
\item
  It struggles with continuous data unless a Gaussian distribution is assumed, which may not always be appropriate.\\
\item
  More complex models, such as decision trees or gradient boosting, often outperform Naive Bayes on datasets with intricate relationships between features.
\end{itemize}

Despite these limitations, Naive Bayes remains an essential tool in machine learning. Its ease of implementation, interpretability, and strong baseline performance make it a valuable first-choice model for many classification tasks.

\subsection*{What This Chapter Covers}\label{what-this-chapter-covers}


This chapter provides a comprehensive exploration of the Naive Bayes classifier. Specifically, we will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain the mathematical foundations of Naive Bayes, focusing on Bayes' theorem and its role in probabilistic classification.\\
\item
  Walk through the mechanics of Naive Bayes with step-by-step examples.\\
\item
  Introduce different variants of the algorithm---Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes---and discuss their appropriate use cases.\\
\item
  Examine practical considerations, including strengths, limitations, and real-world applications.\\
\item
  Implement Naive Bayes in R using the \emph{risk} dataset from the \textbf{liver} package to demonstrate its effectiveness.
\end{enumerate}

By the end of this chapter, you will have a thorough understanding of the Naive Bayes classifier, equipping you to apply it confidently in real-world classification problems.

\section{Bayes' Theorem and Probabilistic Foundations}\label{bayes-theorem-and-probabilistic-foundations}

When evaluating financial risk, how do we update our beliefs about a borrower's likelihood of defaulting as new information---such as income, debt, or mortgage status---becomes available? The ability to quantify uncertainty and refine predictions as new evidence arises is essential in decision-making, and this is precisely what Bayes' Theorem provides.

This theorem forms the foundation of probabilistic learning, helping us make data-driven decisions across diverse fields, including finance, medicine, and machine learning. When determining whether a loan applicant poses a financial risk, we often start with general expectations based on population statistics (\emph{prior knowledge}). However, as additional details---such as mortgage status or outstanding loans---become available, this new evidence refines our estimate (\emph{posterior probability}), leading to more informed decisions.

The foundation for this method was laid by \emph{Thomas Bayes}, an 18th-century Presbyterian minister and self-taught mathematician. His pioneering work introduced a systematic approach to updating probabilities as new data emerges, forming the basis of what is now known as \emph{Bayesian inference}. Those interested in exploring this concept further may find the book \href{https://www.goodreads.com/book/show/199798096-everything-is-predictable}{``Everything Is Predictable: How Bayesian Statistics Explain Our World''} insightful. The author argues that Bayesian statistics not only help predict the future but also shape rational decision-making in everyday life.

\subsection*{The Essence of Bayes' Theorem}\label{the-essence-of-bayes-theorem}


Bayes' Theorem provides a systematic way to update probabilities in light of new evidence. It answers the question: \emph{Given what we already know, how should our belief in a hypothesis change when we observe new data?}

Mathematically, Bayes' Theorem is expressed as:

\begin{equation} 
P(A|B) = P(A) \cdot \frac{P(B|A)}{P(B)} 
\label{eq:bayes-theorem}
\end{equation}

Where:

\begin{itemize}
\tightlist
\item
  \(P(A|B)\) is the posterior probability, representing the probability of event \(A\) (hypothesis) given that event \(B\) (evidence) has occurred.\\
\item
  \(P(A)\) is the prior probability, which reflects our initial belief about \(A\) before considering \(B\).\\
\item
  \(P(B|A)\) is the likelihood, representing the probability of observing \(B\) assuming \(A\) is true.\\
\item
  \(P(B)\) is the evidence, which accounts for the total probability of observing \(B\).
\end{itemize}

Bayes' Theorem provides a structured way to refine our understanding of uncertainty by combining prior knowledge with new observations. This principle underpins many probabilistic learning techniques, including the Naive Bayes classifier.

To illustrate its application, consider a financial risk assessment scenario from the \texttt{risk} dataset in the \textbf{liver} package. Suppose we want to estimate the probability that a customer has a good risk profile (\(A\)) given that they have a mortgage (\(B\)). Financial institutions often use such risk models to assess creditworthiness based on various factors, including mortgage status.

\begin{example}
\protect\hypertarget{exm:ex-bayes-risk}{}\label{exm:ex-bayes-risk}Let's use the \texttt{risk} dataset to calculate the probability of a customer being classified as good risk, given that they have a mortgage. We start by loading the dataset and inspecting the relevant data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)         }

\FunctionTok{data}\NormalTok{(risk)}

\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ risk }\SpecialCharTok{+}\NormalTok{ mortgage, }\AttributeTok{data =}\NormalTok{ risk)}
\NormalTok{              mortgage}
\NormalTok{   risk        yes no}
\NormalTok{     good risk  }\DecValTok{81} \DecValTok{42}
\NormalTok{     bad risk   }\DecValTok{94} \DecValTok{29}
\end{Highlighting}
\end{Shaded}

To improve readability, we add row and column totals to the contingency table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addmargins}\NormalTok{(}\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ risk }\SpecialCharTok{+}\NormalTok{ mortgage, }\AttributeTok{data =}\NormalTok{ risk))}
\NormalTok{              mortgage}
\NormalTok{   risk        yes  no Sum}
\NormalTok{     good risk  }\DecValTok{81}  \DecValTok{42} \DecValTok{123}
\NormalTok{     bad risk   }\DecValTok{94}  \DecValTok{29} \DecValTok{123}
\NormalTok{     Sum       }\DecValTok{175}  \DecValTok{71} \DecValTok{246}
\end{Highlighting}
\end{Shaded}

Now, we define the relevant events:

\begin{itemize}
\tightlist
\item
  \(A\): The customer has a \emph{good risk} profile.\\
\item
  \(B\): The customer has a mortgage (\texttt{mortgage\ =\ yes}).
\end{itemize}

The prior probability of a customer having good risk is given by:

\[
P(A) = \frac{\text{Total Good Risk Cases}}{\text{Total Cases}} = \frac{123}{246} = 0.5
\]

Using Bayes' Theorem, we compute the probability of a customer being classified as good risk given that they have a mortgage:

\begin{equation} 
\label{eq1}
\begin{split}
P(\text{Good Risk} | \text{Mortgage = Yes}) & = \frac{P(\text{Good Risk} \cap \text{Mortgage = Yes})}{P(\text{Mortgage = Yes})} \\
 & = \frac{\text{Good Risk with Mortgage Cases}}{\text{Total Mortgage Cases}} \\
 & = \frac{81}{175} \\
 & = 0.463
\end{split}
\end{equation}

This result indicates that among customers with mortgages, the probability of having a good risk profile is lower than in the general population. Such insights help financial institutions refine credit risk models by incorporating new evidence systematically.
\end{example}

\subsection*{How Does Bayes' Theorem Work?}\label{how-does-bayes-theorem-work}


Bayes' Theorem provides a structured way to update our understanding of uncertainty based on new information. In many real-world scenarios, we start with an initial belief about an event's likelihood, and as we gather more data, we refine this belief to make better-informed decisions.

For instance, in financial risk assessment, banks initially estimate a borrower's risk level based on general population statistics. However, as they collect more details---such as income, credit history, and mortgage status---Bayes' Theorem allows them to update the probability of the borrower being classified as high or low risk. This enables more precise lending decisions.

Beyond finance, Bayes' Theorem is widely applied in other domains:

\begin{itemize}
\tightlist
\item
  In medical diagnostics, it helps estimate the probability of a disease (\(A\)) given a positive test result (\(B\)), incorporating both the test's reliability and the disease's prevalence.\\
\item
  In spam detection, it computes the probability that an email is spam (\(A\)) based on the presence of certain keywords (\(B\)), refining predictions as new messages are analyzed.
\end{itemize}

Probability theory provides a rigorous mathematical structure for reasoning under uncertainty. Bayes' Theorem extends this by enabling a systematic approach to \textbf{learning from data} and improving decision-making in fields ranging from healthcare to finance and beyond.

\subsection*{A Gateway to Naive Bayes}\label{a-gateway-to-naive-bayes}


Bayes' Theorem provides a mathematical foundation for updating probabilities as new evidence emerges. However, in practical classification tasks, computing these probabilities directly can be computationally expensive, particularly for datasets with many features. This is where the \emph{Naive Bayes Classifier} comes in.

Naive Bayes builds directly on Bayes' Theorem by introducing a key simplification: it assumes that features are \emph{conditionally independent} given the target class. While this assumption is rarely true in real-world data, it drastically reduces computational complexity, making the algorithm highly efficient for large-scale problems.

Despite this simplification, Naive Bayes performs remarkably well in many applications. For example, in financial risk prediction, a bank may assess a borrower's creditworthiness using features like income, loan history, and mortgage status. While these factors may be correlated, Naive Bayes assumes they are independent given the borrower's risk category, allowing for rapid probability estimation and classification.

This efficiency makes Naive Bayes particularly effective in domains such as text classification, spam filtering, and sentiment analysis, where feature independence is a reasonable approximation. In the following sections, we will explore how this assumption enables \emph{fast, interpretable, and scalable classification} while maintaining competitive performance.

\section{Why is it Called ``Naive''?}\label{why-is-it-called-naive}

Imagine assessing a borrower's financial risk based on their income, mortgage status, and number of loans. Intuitively, these factors are related---individuals with higher income may have better loan repayment histories, and those with more loans might have a higher probability of financial distress. However, Naive Bayes assumes that all these features are independent once we know the risk category (good risk or bad risk).

This assumption is what makes the algorithm ``naive.'' In reality, features are often correlated, such as income and age, but by treating them as independent, Naive Bayes significantly simplifies probability calculations, making it both efficient and scalable.

To illustrate, consider the \texttt{risk} dataset from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(risk)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{246}\NormalTok{ obs. of  }\DecValTok{6}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{34} \DecValTok{37} \DecValTok{29} \DecValTok{33} \DecValTok{39} \DecValTok{28} \DecValTok{28} \DecValTok{25} \DecValTok{41} \DecValTok{26}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"single"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income  }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{28061} \DecValTok{28009} \DecValTok{27615} \DecValTok{27287} \DecValTok{26954}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ mortgage}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ nr.loans}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ risk    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"good risk"}\NormalTok{,}\StringTok{"bad risk"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

This dataset includes financial indicators such as age, income, marital status, mortgage, and number of loans. Naive Bayes assumes that given a person's risk classification (\texttt{good\ risk} or \texttt{bad\ risk}), these features do not influence one another. Mathematically, the probability of a customer being in the \texttt{good\ risk} category given their attributes is expressed as:

\[
P(Y = y_1 | X_1, X_2, \dots, X_5) = \frac{P(Y = y_1) \cdot P(X_1, X_2, \dots, X_5 | Y = y_1)}{P(X_1, X_2, \dots, X_5)}
\]

However, directly computing \(P(X_1, X_2, \dots, X_5 | Y = y_1)\) is computationally expensive, especially as the number of features grows. For instance, in datasets with hundreds or thousands of features, storing and calculating joint probabilities for all possible feature combinations becomes impractical.

The naive assumption of conditional independence simplifies this problem by expressing the joint probability as the product of individual probabilities:

\[
P(X_1, X_2, \dots, X_5 | Y = y_1) = P(X_1 | Y = y_1) \cdot P(X_2 | Y = y_1) \cdots P(X_5 | Y = y_1)
\]

This transformation eliminates the need to compute complex joint probabilities, making the algorithm scalable even for high-dimensional data. Instead of handling an exponential number of feature combinations, Naive Bayes only requires computing simple conditional probabilities for each feature given the class label.

In practice, this independence assumption is rarely true---features often exhibit some degree of correlation. However, Naive Bayes frequently performs well despite this limitation. It remains widely used in domains where:

\begin{itemize}
\tightlist
\item
  Feature dependencies are weak enough that the assumption does not significantly impact accuracy.
\item
  The focus is on speed and interpretability rather than capturing complex relationships.
\item
  Slight violations of the independence assumption do not severely affect predictive performance.
\end{itemize}

For example, in risk prediction, while income and mortgage status are likely correlated, treating them as independent still allows Naive Bayes to classify borrowers effectively. Similarly, in spam detection or text classification, where features (such as words in an email) are often independent enough, the algorithm delivers fast and accurate predictions.

By balancing computational efficiency with predictive power, Naive Bayes remains a foundational algorithm in machine learning, particularly for applications that demand scalability and interpretability.

\section{The Laplace Smoothing Technique}\label{the-laplace-smoothing-technique}

One of the challenges in Naive Bayes classification is handling feature categories that appear in the test data but are absent in the training data. Suppose we train a model on a dataset where no borrowers classified as ``bad risk'' are married. If we later encounter a married borrower in the test set, Naive Bayes would compute \(P(\text{bad risk} | \text{married})\) as zero. Because the algorithm multiplies probabilities when making predictions, even a single zero probability results in an overall probability of zero for that class, making it impossible for the model to predict that class.

This issue arises because Naive Bayes estimates probabilities from frequency counts in the training data. If a feature value never appears in a given class, its estimated probability is zero, which can lead to misclassification errors. To address this, \emph{Laplace smoothing} (also known as \emph{add-one smoothing}) is used. Named after the mathematician \href{https://en.wikipedia.org/wiki/Pierre-Simon_Laplace}{Pierre-Simon Laplace}, this technique ensures that every feature-category combination has a small, non-zero probability, even if it is missing in the training data.

To illustrate, consider the \texttt{marital} variable in the \texttt{risk} dataset. Suppose the category \texttt{married} is entirely absent for customers labeled as \texttt{bad\ risk}. This scenario can be visualized as follows:

\begin{verbatim}
            risk
   marital   good risk bad risk
     single         21       11
     married        51        0
     other           8       10
\end{verbatim}

Without smoothing, the probability of \texttt{bad\ risk} given \texttt{married} is:

\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married})}{\text{count}(\text{married})} = \frac{0}{\text{count}(\text{married})} = 0
\]

This means that any married borrower will always be classified as \texttt{good\ risk}, regardless of their other characteristics.

Laplace smoothing resolves this by modifying the probability calculation. Instead of assigning a strict zero probability, a small constant \(k\) (usually \(k = 1\)) is added to each count in the frequency table. The adjusted probability is given by:

\[
P(\text{bad risk} | \text{married}) = \frac{\text{count}(\text{bad risk} \cap \text{married}) + k}{\text{count}(\text{bad risk}) + k \times \text{number of categories in } \text{marital}}
\]

This adjustment ensures that:
- Every category receives a small positive count, avoiding zero probabilities.
- The total probability distribution remains valid.

In R, Laplace smoothing can be applied using the \texttt{laplace} argument in the \textbf{naivebayes} package. By default, \texttt{laplace\ =\ 0}, meaning no smoothing is applied. To apply smoothing, simply set \texttt{laplace\ =\ 1}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\CommentTok{\# Fit Naive Bayes with Laplace smoothing}
\NormalTok{formula\_nb }\OtherTok{=}\NormalTok{ risk }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ marital }\SpecialCharTok{+}\NormalTok{ mortgage }\SpecialCharTok{+}\NormalTok{ nr.loans}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{naive\_bayes}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula\_nb, }\AttributeTok{data =}\NormalTok{ risk, }\AttributeTok{laplace =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This ensures that no category is assigned a probability of zero, improving the model's robustness---particularly in cases where the training data is limited or imbalanced.

Laplace smoothing is a simple yet effective technique that prevents Naive Bayes from being overly sensitive to missing categories in training data. While \(k = 1\) is the most common approach, the value of \(k\) can be adjusted based on specific domain knowledge. By ensuring that probabilities remain well-defined, Laplace smoothing enhances the reliability of Naive Bayes classifiers in real-world applications.

\section{Types of Naive Bayes Classifiers}\label{types-of-naive-bayes-classifiers}

Naive Bayes is a versatile algorithm with different variants designed for specific data types and distributions. The choice of which variant to use depends on the nature of the features and the assumptions made about their underlying distribution. The three most common types are:

\begin{itemize}
\item
  \textbf{Multinomial Naive Bayes}: Best suited for categorical or count-based features, such as word frequencies in text data. This variant is commonly used in text classification, where features represent discrete counts (e.g., the number of times a word appears in a document). In the \emph{risk} dataset, the \texttt{marital} variable, which takes categorical values such as \texttt{single}, \texttt{married}, and \texttt{other}, aligns well with this variant.
\item
  \textbf{Bernoulli Naive Bayes}: Designed for binary features, where each variable represents the presence or absence of a characteristic. This variant is particularly useful in applications where data is represented as a set of binary indicators, such as whether an email contains a specific keyword in spam detection. In the \emph{risk} dataset, the \texttt{mortgage} variable, which has two possible values (\texttt{yes} or \texttt{no}), is an example of a binary feature suitable for this approach.
\item
  \textbf{Gaussian Naive Bayes}: Applied to continuous data where features are assumed to follow a normal (Gaussian) distribution. This variant estimates the likelihood of each feature using a normal distribution, making it ideal for datasets with numerical attributes such as age, income, or credit scores. In the \emph{risk} dataset, variables like \texttt{age} and \texttt{income} are continuous and thus well suited for this variant.
\end{itemize}

Each of these Naive Bayes classifiers is optimized for different data types, making it essential to select the one that best fits the dataset's characteristics. Understanding these distinctions allows for better model selection and improved performance. In the following sections, we will explore each variant in greater detail, examining their assumptions, strengths, and use cases.

\section{Case Study: Predicting Financial Risk with Naive Bayes}\label{case-study-predicting-financial-risk-with-naive-bayes}

Financial institutions must assess loan applicants carefully to balance profitability with risk management. Lending decisions rely on estimating the likelihood of default, which depends on various financial and demographic factors. A robust risk classification model helps institutions make informed decisions, reducing financial losses while ensuring fair lending practices.

In this case study, we apply the Naive Bayes classifier to predict whether a customer is a \emph{good risk} or \emph{bad risk} based on financial and demographic attributes. Using the \texttt{risk} dataset from the \href{https://CRAN.R-project.org/package=liver}{\textbf{liver}} package in R, we train and evaluate a probabilistic classification model. This case study demonstrates how Naive Bayes can be leveraged in financial decision-making, providing insights into customer risk profiles and supporting more effective credit evaluation.

\subsection*{Problem Understanding}\label{problem-understanding-2}


A key challenge in financial risk assessment is distinguishing between customers who are likely to repay loans and those at higher risk of default. Predictive modeling enables financial institutions to anticipate risk, optimize credit policies, and reduce non-performing loans. Key business questions include:

\begin{itemize}
\tightlist
\item
  What financial and demographic factors contribute to a customer's risk profile?\\
\item
  How can we predict whether a customer is a good or bad risk before approving a loan?\\
\item
  What insights can be gained to refine lending policies and mitigate financial losses?
\end{itemize}

By analyzing the \texttt{risk} dataset, we aim to develop a model that classifies customers based on risk level. This will allow lenders to make \emph{data-driven} decisions, improve credit scoring, and enhance loan approval strategies.

\subsection*{Data Understanding}\label{data-understanding-1}


The \texttt{risk} dataset contains financial and demographic attributes that help assess a customer's likelihood of being classified as either a \emph{good risk} or \emph{bad risk}. This dataset, included in the \textbf{liver} package, consists of 246 observations and 6 variables. It provides a structured way to analyze customer characteristics and predict financial risk levels.

The dataset includes 5 predictors and a binary target variable, \texttt{risk}, which distinguishes between customers who are more or less likely to default. The key variables are:

\begin{itemize}
\tightlist
\item
  \texttt{age}: Customer's age in years.\\
\item
  \texttt{marital}: Marital status (\texttt{single}, \texttt{married}, \texttt{other}).\\
\item
  \texttt{income}: Annual income.\\
\item
  \texttt{mortgage}: Indicates whether the customer has a mortgage (\texttt{yes}, \texttt{no}).\\
\item
  \texttt{nr\_loans}: Number of loans held by the customer.\\
\item
  \texttt{risk}: The target variable (\texttt{good\ risk}, \texttt{bad\ risk}).
\end{itemize}

For additional details about the dataset, refer to its \href{https://search.r-project.org/CRAN/refmans/liver/html/risk.html}{documentation}.

To begin the analysis, we load the dataset and examine its structure to understand its variables and data types:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(risk)}

\FunctionTok{str}\NormalTok{(risk)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{246}\NormalTok{ obs. of  }\DecValTok{6}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{34} \DecValTok{37} \DecValTok{29} \DecValTok{33} \DecValTok{39} \DecValTok{28} \DecValTok{28} \DecValTok{25} \DecValTok{41} \DecValTok{26}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"single"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ income  }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{28061} \DecValTok{28009} \DecValTok{27615} \DecValTok{27287} \DecValTok{26954}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ mortgage}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ nr.loans}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ risk    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"good risk"}\NormalTok{,}\StringTok{"bad risk"}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

To gain further insights, we summarize the dataset's key statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(risk)}
\NormalTok{         age           marital        income      mortgage     nr.loans    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.00}\NormalTok{   single }\SpecialCharTok{:}\DecValTok{111}\NormalTok{   Min.   }\SpecialCharTok{:}\DecValTok{15301}\NormalTok{   yes}\SpecialCharTok{:}\DecValTok{175}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{32.00}\NormalTok{   married}\SpecialCharTok{:} \DecValTok{78}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\DecValTok{26882}\NormalTok{   no }\SpecialCharTok{:} \DecValTok{71}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{41.00}\NormalTok{   other  }\SpecialCharTok{:} \DecValTok{57}\NormalTok{   Median }\SpecialCharTok{:}\DecValTok{37662}\NormalTok{             Median }\SpecialCharTok{:}\FloatTok{1.000}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{40.64}\NormalTok{                 Mean   }\SpecialCharTok{:}\DecValTok{38790}\NormalTok{             Mean   }\SpecialCharTok{:}\FloatTok{1.309}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{50.00}                 \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\DecValTok{49398}             \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{2.000}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{66.00}\NormalTok{                 Max.   }\SpecialCharTok{:}\DecValTok{78399}\NormalTok{             Max.   }\SpecialCharTok{:}\FloatTok{3.000}  
\NormalTok{           risk    }
\NormalTok{    good risk}\SpecialCharTok{:}\DecValTok{123}  
\NormalTok{    bad risk }\SpecialCharTok{:}\DecValTok{123}  
                   
                   
                   
   
\end{Highlighting}
\end{Shaded}

This summary provides an overview of variable distributions and identifies any missing values or potential anomalies. Since the dataset appears clean and well-structured, we can proceed to data preparation before training the Naive Bayes classifier.

\subsection*{Preparing Data for Modeling}\label{preparing-data-for-modeling-1}


Before training the Naive Bayes classifier, we need to split the dataset into training and testing sets. This step ensures that we can evaluate how well the model generalizes to unseen data. We use an 80/20 split, allocating 80\% of the data for training and 20\% for testing. To maintain consistency with previous chapters, we apply the \texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ risk, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{risk}
\end{Highlighting}
\end{Shaded}

Setting \texttt{set.seed(5)} ensures reproducibility so that the same partitioning is achieved each time the code is run. The \texttt{train\_set} will be used to train the Naive Bayes classifier, while the \texttt{test\_set} will serve as unseen data to evaluate the model's predictions. The \texttt{test\_labels} vector contains the true class labels for the test set, which we will compare against the model's predictions.

To verify that the training and test sets are representative of the original dataset, we compare the proportions of the \texttt{marital} variable across both sets. A chi-squared test is used to check whether the distribution of marital statuses (\texttt{single}, \texttt{married}, and \texttt{other}) is statistically similar between the training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{table}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{marital), }\AttributeTok{y =} \FunctionTok{table}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{marital))}
   
\NormalTok{    Pearson}\StringTok{\textquotesingle{}s Chi{-}squared test}
\StringTok{   }
\StringTok{   data:  table(train\_set$marital) and table(test\_set$marital)}
\StringTok{   X{-}squared = 6, df = 4, p{-}value = 0.1991}
\end{Highlighting}
\end{Shaded}

This statistical test evaluates whether the proportions of marital categories differ significantly between the training and test sets. The hypotheses for the test are:\\
\[
\begin{cases}
H_0:  \text{The proportions of marital categories are the same in both sets.}\\
H_a:  \text{At least one of the proportions is different.}
\end{cases}
\]
Since the p-value exceeds \(\alpha = 0.05\), we fail to reject \(H_0\), meaning that the marital status distribution remains statistically similar between the training and test sets. This confirms that the partitioning process maintains the dataset's characteristics, allowing for reliable model evaluation.

With a well-structured dataset and a validated partitioning process, we are now ready to train the Naive Bayes classifier and assess its predictive capabilities.

\subsection*{Applying the Naive Bayes Classifier}\label{applying-the-naive-bayes-classifier}


With the dataset partitioned and validated, we now proceed to train and evaluate the Naive Bayes classifier. The objective is to build a model using the training set and assess its ability to classify customers as \emph{good risk} or \emph{bad risk} in the test set.

Several R packages provide implementations of Naive Bayes, with two commonly used options being \href{https://CRAN.R-project.org/package=naivebayes}{\textbf{naivebayes}} and \href{https://CRAN.R-project.org/package=e1071}{\textbf{e1071}}. In this case study, we use the \textbf{naivebayes} package, which offers an efficient implementation of the classifier. The \texttt{naive\_bayes()} function in this package supports various probability distributions depending on the nature of the features:

\begin{itemize}
\tightlist
\item
  \emph{Categorical distribution} for discrete variables such as \texttt{marital} and \texttt{mortgage}.\\
\item
  \emph{Bernoulli distribution} for binary features, a special case of the categorical distribution.\\
\item
  \emph{Poisson distribution} for count-based variables, such as the number of loans.\\
\item
  \emph{Gaussian distribution} for continuous features, such as \texttt{age} and \texttt{income}.\\
\item
  \emph{Non-parametric density estimation} for continuous features when no specific distribution is assumed.
\end{itemize}

Unlike the k-NN algorithm in the previous chapter, which classifies new data without an explicit training phase, Naive Bayes follows a two-step process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Training phase} -- The model learns probability distributions from the training data.\\
\item
  \emph{Prediction phase} -- The trained model is used to classify new data points based on the learned probabilities.
\end{enumerate}

To train the model, we define a formula where \texttt{risk} is the target variable, and all other features serve as predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ risk }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ mortgage }\SpecialCharTok{+}\NormalTok{ nr.loans }\SpecialCharTok{+}\NormalTok{ marital}
\end{Highlighting}
\end{Shaded}

We then apply the \texttt{naive\_bayes()} function from the \textbf{naivebayes} package to train the classifier on the training dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naivebayes)}

\NormalTok{naive\_bayes }\OtherTok{=} \FunctionTok{naive\_bayes}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set)}

\NormalTok{naive\_bayes}
   
   \SpecialCharTok{==}\ErrorTok{===============================}\NormalTok{ Naive Bayes }\SpecialCharTok{==}\ErrorTok{================================}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{naive\_bayes.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
    
\NormalTok{   Laplace smoothing}\SpecialCharTok{:} \DecValTok{0}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
    
\NormalTok{   A priori probabilities}\SpecialCharTok{:} 
   
\NormalTok{   good risk  bad risk }
   \FloatTok{0.4923858} \FloatTok{0.5076142} 
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
    
\NormalTok{   Tables}\SpecialCharTok{:} 
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{age}\NormalTok{ (Gaussian) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
         
\NormalTok{   age    good risk  bad risk}
\NormalTok{     mean }\FloatTok{46.453608} \FloatTok{35.470000}
\NormalTok{     sd    }\FloatTok{8.563513}  \FloatTok{9.542520}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{income}\NormalTok{ (Gaussian) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
         
\NormalTok{   income good risk  bad risk}
\NormalTok{     mean }\FloatTok{48888.987} \FloatTok{27309.560}
\NormalTok{     sd    }\FloatTok{9986.962}  \FloatTok{7534.639}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{mortgage}\NormalTok{ (Bernoulli) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
           
\NormalTok{   mortgage good risk  bad risk}
\NormalTok{        yes }\FloatTok{0.6804124} \FloatTok{0.7400000}
\NormalTok{        no  }\FloatTok{0.3195876} \FloatTok{0.2600000}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{nr.loans}\NormalTok{ (Gaussian) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
           
\NormalTok{   nr.loans good risk  bad risk}
\NormalTok{       mean }\FloatTok{1.0309278} \FloatTok{1.6600000}
\NormalTok{       sd   }\FloatTok{0.7282057} \FloatTok{0.7550503}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
   \ErrorTok{::} \FunctionTok{marital}\NormalTok{ (Categorical) }
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}} 
            
\NormalTok{   marital    good risk   bad risk}
\NormalTok{     single  }\FloatTok{0.38144330} \FloatTok{0.49000000}
\NormalTok{     married }\FloatTok{0.52577320} \FloatTok{0.11000000}
\NormalTok{     other   }\FloatTok{0.09278351} \FloatTok{0.40000000}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

The \texttt{naive\_bayes()} function estimates the probability distributions for each feature, conditioned on the target class. Specifically:

\begin{itemize}
\tightlist
\item
  \textbf{Categorical features} (e.g., \texttt{marital}, \texttt{mortgage}) -- The function computes class-conditional probabilities.\\
\item
  \textbf{Continuous features} (e.g., \texttt{age}, \texttt{income}, \texttt{nr.loans}) -- The function assumes a Gaussian distribution and calculates the mean and standard deviation for each class.
\end{itemize}

To inspect the model's learned probability distributions, we summarize the trained model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(naive\_bayes)}
   
   \SpecialCharTok{==}\ErrorTok{===============================}\NormalTok{ Naive Bayes }\SpecialCharTok{==}\ErrorTok{================================} 
    
   \SpecialCharTok{{-}}\NormalTok{ Call}\SpecialCharTok{:} \FunctionTok{naive\_bayes.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set) }
   \SpecialCharTok{{-}}\NormalTok{ Laplace}\SpecialCharTok{:} \DecValTok{0} 
   \SpecialCharTok{{-}}\NormalTok{ Classes}\SpecialCharTok{:} \DecValTok{2} 
   \SpecialCharTok{{-}}\NormalTok{ Samples}\SpecialCharTok{:} \DecValTok{197} 
   \SpecialCharTok{{-}}\NormalTok{ Features}\SpecialCharTok{:} \DecValTok{5} 
   \SpecialCharTok{{-}}\NormalTok{ Conditional distributions}\SpecialCharTok{:} 
       \SpecialCharTok{{-}}\NormalTok{ Bernoulli}\SpecialCharTok{:} \DecValTok{1}
       \SpecialCharTok{{-}}\NormalTok{ Categorical}\SpecialCharTok{:} \DecValTok{1}
       \SpecialCharTok{{-}}\NormalTok{ Gaussian}\SpecialCharTok{:} \DecValTok{3}
   \SpecialCharTok{{-}}\NormalTok{ Prior probabilities}\SpecialCharTok{:} 
       \SpecialCharTok{{-}}\NormalTok{ good risk}\SpecialCharTok{:} \FloatTok{0.4924}
       \SpecialCharTok{{-}}\NormalTok{ bad risk}\SpecialCharTok{:} \FloatTok{0.5076}
   
   \SpecialCharTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

The summary output provides useful insights into how the classifier models each feature's probability distribution. This forms the basis for making predictions on new data points, which we explore in the next section.

\subsection*{Prediction and Model Evaluation}\label{prediction-and-model-evaluation}


After training the Naive Bayes classifier, we evaluate its performance by applying it to the test set, which contains customers unseen during training. The goal is to compare the predicted probabilities with the actual class labels stored in \texttt{test\_labels}.

To obtain the predicted class probabilities, we use the \texttt{predict()} function from the \textbf{naivebayes} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob\_naive\_bayes }\OtherTok{=} \FunctionTok{predict}\NormalTok{(naive\_bayes, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By specifying \texttt{type\ =\ "prob"}, the function returns posterior probabilities for each class instead of discrete predictions.

To inspect the model's predictions, we display the first 10 probability estimates:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Display the first 10 predictions}
\FunctionTok{round}\NormalTok{(}\FunctionTok{head}\NormalTok{(prob\_naive\_bayes, }\AttributeTok{n =} \DecValTok{10}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\NormalTok{         good risk bad risk}
\NormalTok{    [}\DecValTok{1}\NormalTok{,]     }\FloatTok{0.001}    \FloatTok{0.999}
\NormalTok{    [}\DecValTok{2}\NormalTok{,]     }\FloatTok{0.013}    \FloatTok{0.987}
\NormalTok{    [}\DecValTok{3}\NormalTok{,]     }\FloatTok{0.000}    \FloatTok{1.000}
\NormalTok{    [}\DecValTok{4}\NormalTok{,]     }\FloatTok{0.184}    \FloatTok{0.816}
\NormalTok{    [}\DecValTok{5}\NormalTok{,]     }\FloatTok{0.614}    \FloatTok{0.386}
\NormalTok{    [}\DecValTok{6}\NormalTok{,]     }\FloatTok{0.193}    \FloatTok{0.807}
\NormalTok{    [}\DecValTok{7}\NormalTok{,]     }\FloatTok{0.002}    \FloatTok{0.998}
\NormalTok{    [}\DecValTok{8}\NormalTok{,]     }\FloatTok{0.002}    \FloatTok{0.998}
\NormalTok{    [}\DecValTok{9}\NormalTok{,]     }\FloatTok{0.378}    \FloatTok{0.622}
\NormalTok{   [}\DecValTok{10}\NormalTok{,]     }\FloatTok{0.283}    \FloatTok{0.717}
\end{Highlighting}
\end{Shaded}

The output contains two columns:

\begin{itemize}
\tightlist
\item
  The first column represents the probability that a customer is classified as ``\texttt{good\ risk}.''
\item
  The second column represents the probability that a customer is classified as ``\texttt{bad\ risk}.''\\
  For example, if the second row has a probability of 0.987 for ``\texttt{bad\ risk},'' it indicates that the second customer in the test set is predicted to belong to the ``\texttt{bad\ risk}'' category with a probability of 0.987.
\end{itemize}

This probability-based output provides flexibility in decision-making. Instead of using a fixed threshold of 0.5, financial institutions can adjust the cutoff based on business objectives. For instance, if minimizing loan defaults is the priority, a more conservative threshold may be set. In the next section, we convert these probabilities into class predictions and evaluate the model using a confusion matrix and other performance metrics.

\subsubsection*{Confusion Matrix}\label{confusion-matrix-1}


To assess the classification performance of our Naive Bayes model, we compute the confusion matrix using the \texttt{conf.mat()} and \texttt{conf.mat.plot()} functions from the \textbf{liver} package. The confusion matrix compares the predicted class probabilities with the actual class labels, allowing us to measure the model's accuracy and analyze different types of errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract probability of "good risk"}
\NormalTok{prob\_naive\_bayes }\OtherTok{=}\NormalTok{ prob\_naive\_bayes[, }\DecValTok{1}\NormalTok{] }

\FunctionTok{conf.mat}\NormalTok{(prob\_naive\_bayes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"good risk"}\NormalTok{)}
\NormalTok{              Actual}
\NormalTok{   Predict     good risk bad risk}
\NormalTok{     good risk        }\DecValTok{24}        \DecValTok{3}
\NormalTok{     bad risk          }\DecValTok{2}       \DecValTok{20}

\FunctionTok{conf.mat.plot}\NormalTok{(prob\_naive\_bayes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"good risk"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.65\linewidth]{9_Naive_Bayes_files/figure-latex/unnamed-chunk-16-1} \end{center}

In this evaluation, we apply a \textbf{classification threshold of 0.5}, meaning that if a customer's predicted probability of being a ``\texttt{good\ risk}'' is at least 50\%, the model classifies them as ``\texttt{good\ risk}''; otherwise, they are classified as ``\texttt{bad\ risk}.'' Additionally, we specify \textbf{``\texttt{good\ risk}'' as the reference class}, meaning that performance metrics such as sensitivity and precision will be calculated with respect to this category.

The confusion matrix provides the following breakdown of model predictions:

\begin{itemize}
\tightlist
\item
  \textbf{True Positives (TP)}: Customers correctly classified as ``\texttt{good\ risk}.''\\
\item
  \textbf{True Negatives (TN)}: Customers correctly classified as ``\texttt{bad\ risk}.''\\
\item
  \textbf{False Positives (FP)}: Customers incorrectly classified as ``\texttt{good\ risk}'' when they were actually ``\texttt{bad\ risk}.''\\
\item
  \textbf{False Negatives (FN)}: Customers incorrectly classified as ``\texttt{bad\ risk}'' when they were actually ``\texttt{good\ risk}.''
\end{itemize}

The values in the confusion matrix quantify the model's classification accuracy and error rates at a cutoff of 0.5. Specifically, the model correctly predicts ``24 + 20'' cases and misclassifies ``3 + 2'' cases.

This matrix offers a structured way to assess classification performance, helping us understand how well the model differentiates between high- and low-risk customers. In the next section, we further analyze performance using additional evaluation metrics.

\subsubsection*{ROC Curve and AUC}\label{roc-curve-and-auc-1}


To further evaluate the model, we compute the \emph{Receiver Operating Characteristic (ROC) curve} and the \emph{Area Under the Curve (AUC)} value. These metrics provide a comprehensive assessment of the model's ability to distinguish between ``\texttt{good\ risk}'' and ``\texttt{bad\ risk}'' customers across different classification thresholds. The \textbf{pROC} package in R facilitates both calculations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)          }

\NormalTok{roc\_naive\_bayes }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, prob\_naive\_bayes)}

\FunctionTok{ggroc}\NormalTok{(roc\_naive\_bayes)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{9_Naive_Bayes_files/figure-latex/unnamed-chunk-17-1} \end{center}

The ROC curve plots the \textbf{true positive rate (sensitivity)} against the \textbf{false positive rate (1 - specificity)} at various threshold values. A curve that remains closer to the top-left corner indicates a well-performing model, while a curve near the diagonal suggests performance close to random guessing.

Next, we compute the \emph{AUC} value, which summarizes the ROC curve into a single number:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_naive\_bayes), }\DecValTok{3}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.957}
\end{Highlighting}
\end{Shaded}

The AUC value, 0.957, represents the probability that a randomly selected ``\texttt{good\ risk}'' customer will receive a higher predicted probability than a randomly selected ``\texttt{bad\ risk}'' customer. An AUC closer to 1 indicates strong predictive performance, while an AUC of 0.5 suggests no better performance than random guessing.

By analyzing the ROC curve and AUC, financial institutions can adjust the decision threshold to align with business objectives. If minimizing false negatives (misclassifying high-risk customers as low-risk) is a priority, the threshold can be lowered to increase sensitivity. Conversely, if false positives (denying loans to eligible customers) are a concern, a higher threshold can be set to improve specificity.

Through this case study, we have demonstrated how Naive Bayes can be applied to financial risk assessment. By evaluating model performance using the confusion matrix, ROC curve, and AUC, we identified its strengths and limitations. This highlights the efficiency and interpretability of Naive Bayes, making it a valuable tool for probabilistic classification in financial decision-making.

\subsection*{Takeaways from the Case Study}\label{takeaways-from-the-case-study}


This case study demonstrated how Naive Bayes can be applied to financial risk assessment by classifying customers as either \emph{good risk} or \emph{bad risk} based on demographic and financial attributes. Through key evaluation metrics such as the confusion matrix, ROC curve, and AUC, we analyzed the model's predictive power and identified its strengths and limitations.

The results highlight the \textbf{efficiency, simplicity, and interpretability} of Naive Bayes, making it a valuable tool for probabilistic classification in financial decision-making. The model's ability to provide probability estimates allows institutions to adjust decision thresholds based on business priorities---whether prioritizing sensitivity to minimize high-risk approvals or improving specificity to reduce false rejections.

While Naive Bayes performs well in this scenario, it relies on the assumption of feature independence, which may not always hold in real-world financial data. Future improvements could include using ensemble models or integrating additional financial indicators to refine predictions further.

By applying Naive Bayes to financial risk assessment, we demonstrated how probabilistic classification methods can support data-driven lending decisions, helping financial institutions manage risk effectively while optimizing credit policies.

\section{Exercises}\label{exercises-7}

\subsection*{Conceptual questions}\label{conceptual-questions-5}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Why is Naive Bayes considered a probabilistic classification model?\\
\item
  What is the difference between prior probability, likelihood, and posterior probability in Bayes' theorem?\\
\item
  What does it mean when we say Naive Bayes assumes feature independence?\\
\item
  In which situations does the feature independence assumption become problematic? Provide an example.\\
\item
  What are the key strengths of Naive Bayes? Why is it widely used in text classification and spam filtering?\\
\item
  What are the major limitations of Naive Bayes, and how do they impact its performance?\\
\item
  How does Laplace smoothing help in handling missing feature values in Naive Bayes?\\
\item
  When should you use multinomial Naive Bayes, Bernoulli Naive Bayes, or Gaussian Naive Bayes?\\
\item
  Compare the Naive Bayes classifier to k-Nearest Neighbors algorithm (Chapter \ref{chapter-knn}). How do their assumptions and outputs differ?\\
\item
  How does the choice of probability threshold affect model predictions?\\
\item
  Why does Naive Bayes remain effective even when the independence assumption is violated?\\
\item
  What type of dataset characteristics make Naive Bayes perform poorly compared to other classifiers?\\
\item
  How does the Gaussian Naive Bayes classifier handle continuous data?\\
\item
  How can domain knowledge help improve Naive Bayes classification results?\\
\item
  How would Naive Bayes handle imbalanced datasets? What preprocessing techniques could help?\\
\item
  Explain how prior probabilities can be adjusted based on business objectives in a classification problem.
\end{enumerate}

\subsection*{Hands-on implementation with the churn dataset}\label{hands-on-implementation-with-the-churn-dataset}


For the following exercises, we will use the \emph{churn} dataset from the \textbf{liver} package. This dataset contains information about customer subscriptions, and our goal is to predict whether a customer will churn (\texttt{churn\ =\ yes/no}) using the Naive Bayes classifier. In Section \ref{EDA-sec-churn}, we performed exploratory data analysis on this dataset to understand its structure and key features.

\subsubsection*{Data preparation}\label{data-preparation-2}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\tightlist
\item
  Load the \textbf{liver} package and the \emph{churn} dataset:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{data}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{17}
\item
  Display the structure and summary statistics of the dataset to examine its variables and their distributions.
\item
  Split the dataset into an 80\% training set and a 20\% test set using the \texttt{partition()} function from the \textbf{liver} package.
\item
  Verify that the partitioning maintains the distribution of the \texttt{churn} variable by comparing its proportions in the training and test sets.
\end{enumerate}

\subsection*{Training and evaluating the Naive Bayes classifier}\label{training-and-evaluating-the-naive-bayes-classifier}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Based on the exploratory data analysis in Section \ref{EDA-sec-churn}, select the following predictors for the Naive Bayes model: \texttt{account.length}, \texttt{voice.plan}, \texttt{voice.messages}, \texttt{intl.plan}, \texttt{intl.mins}, \texttt{day.mins}, \texttt{eve.mins}, \texttt{night.mins}, and \texttt{customer.calls}. Define the model formula:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.plan }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+} 
\NormalTok{                 intl.plan }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ day.mins }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+} 
\NormalTok{                 night.mins }\SpecialCharTok{+}\NormalTok{ customer.calls}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{21}
\item
  Train a Naive Bayes classifier on the training set using the \textbf{naivebayes} package.
\item
  Summarize the trained model. What insights can you gain from the estimated class-conditional probabilities?
\item
  Use the trained model to predict class probabilities for the test set using the \texttt{predict()} function from the \textbf{naivebayes} package.
\item
  Extract and examine the first 10 probability predictions. Interpret what these values indicate about the likelihood of customer churn.
\item
  Compute the confusion matrix using the \texttt{conf.mat()} function from the \textbf{liver} package with a classification threshold of 0.5.
\item
  Visualize the confusion matrix using the \texttt{conf.mat.plot()} function from the \textbf{liver} package.
\item
  Compute key evaluation metrics, including accuracy, precision, recall, and F1-score, based on the confusion matrix.
\item
  Lower the classification threshold from 0.5 to 0.3 and recompute the confusion matrix. How does adjusting the threshold affect model performance?
\item
  Plot the ROC curve and compute the AUC value for the model. Interpret the results in terms of the model's ability to distinguish between churn and non-churn customers.
\item
  Interpret the AUC value. What does it indicate about the model's ability to distinguish between churn and non-churn customers?\\
\item
  Train a Naive Bayes model with Laplace smoothing (\texttt{laplace\ =\ 1}) and compare the results to the model without smoothing. How does smoothing affect predictions?
\item
  Compare the Naive Bayes classifier to the k-Nearest Neighbors algorithm (Chapter \ref{chapter-knn}) trained on the same dataset. Evaluate their performance using accuracy, precision, recall, F1-score, and AUC. Which model performs better, and what factors might explain the differences in performance?
\item
  Experiment by removing one predictor variable at a time and retraining the model. How does this impact accuracy and other evaluation metrics?
\end{enumerate}

\subsection*{Real-world application and critical thinking}\label{real-world-application-and-critical-thinking}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{35}
\item
  Suppose a telecommunications company wants to use this model to reduce customer churn. What business decisions could be made based on the model's predictions?
\item
  If incorrectly predicting a false negative (missed churner) is more costly than a false positive, how should the decision threshold be adjusted?
\item
  A marketing team wants to offer promotional discounts to customers predicted to churn. How would you use this model to target the right customers?
\item
  Suppose the dataset included a new feature: customer satisfaction score (on a scale from 1 to 10). How could this feature improve the model?
\item
  What steps would you take if the model performed poorly on new customer data?
\item
  Explain why feature independence may or may not hold in this dataset. How could feature correlation impact the model's reliability?
\item
  Would Naive Bayes be suitable for multi-class classification problems? If so, how would you extend this model to predict multiple churn reasons instead of just \texttt{yes/no}?
\item
  If given time-series data about customer interactions over months, would Naive Bayes still be appropriate? Why or why not?
\end{enumerate}

\chapter{Regression Analysis: Foundations and Applications}\label{chapter-regression}

\emph{Regression analysis} has been a fundamental tool in statistical modeling for centuries and remains one of the most versatile techniques in data science. Its mathematical foundations were established by early statisticians such as Legendre and Gauss, who developed the least squares method. Since then, regression analysis has evolved into a widely used framework for examining relationships between variables. With advancements in computing and programming languages such as R, it is now accessible and scalable for addressing complex real-world problems.

Regression models provide a systematic approach for quantifying relationships, uncovering patterns, and making predictions. These models are applied across diverse fields, including economics, medicine, and engineering, to estimate effects, forecast outcomes, and support data-driven decision-making. Whether predicting the impact of advertising expenditure on sales, modeling housing prices, or identifying risk factors for disease, regression analysis serves as a cornerstone of statistical modeling.\\
As Charles Wheelan describes in \href{https://www.goodreads.com/book/show/15786586-naked-statistics}{\emph{Naked Statistics}}\citep{wheelan2013naked}, \emph{``Regression modeling is the hydrogen bomb of the statistics arsenal.''} This analogy highlights the method's immense power---when used correctly, it provides a formidable tool for making informed decisions, but its misuse can lead to misleading conclusions.

This chapter provides a structured introduction to regression techniques, beginning with simple linear regression and extending to multiple regression, generalized linear models (GLMs), and non-linear regression approaches. Throughout, we will apply these techniques to real-world datasets, including an \emph{online marketing dataset} for modeling the impact of digital advertising on revenue and a \emph{housing price dataset} to explore the relationship between property attributes and market value. By the end, readers will have a solid foundation in both the theoretical principles and practical applications of regression modeling in R, enabling them to analyze and interpret real-world data effectively.

\section{Simple Linear Regression}\label{sec-simple-regression}

Simple linear regression is the most fundamental regression model, allowing us to quantify the relationship between a \emph{single predictor} and a \emph{response variable}. It provides a straightforward approach to estimating how changes in one variable influence another. By focusing on a single predictor, we establish a clear understanding of regression mechanics before extending the model to multiple predictors.

To illustrate simple linear regression, we use the \emph{marketing} dataset from the \textbf{liver} package. This dataset captures \emph{daily digital marketing activities} and their impact on \emph{revenue generation}, making it an ideal real-world example for regression analysis. It includes key performance indicators of \emph{online advertising campaigns}, such as expenditure, user engagement metrics, and daily revenue.

The dataset consists of 40 observations and 8 variables:

\begin{itemize}
\tightlist
\item
  \texttt{spend}: Daily expenditure on pay-per-click (PPC) advertising.\\
\item
  \texttt{clicks}: Number of clicks on advertisements.\\
\item
  \texttt{impressions}: Number of times ads were displayed to users.\\
\item
  \texttt{transactions}: Number of completed transactions per day.\\
\item
  \texttt{click.rate}: Click-through rate (CTR), calculated as the proportion of impressions resulting in clicks.\\
\item
  \texttt{conversion.rate}: Conversion rate, representing the proportion of clicks leading to transactions.\\
\item
  \texttt{display}: Whether a display campaign was active (\texttt{yes} or \texttt{no}).\\
\item
  \texttt{revenue}: Total daily revenue (response variable).
\end{itemize}

We begin by loading the dataset and examining its structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(marketing, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\FunctionTok{str}\NormalTok{(marketing)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{40}\NormalTok{ obs. of  }\DecValTok{8}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ spend          }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{22.6} \FloatTok{37.3} \FloatTok{55.6} \FloatTok{45.4} \FloatTok{50.2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ clicks         }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{165} \DecValTok{228} \DecValTok{291} \DecValTok{247} \DecValTok{290} \DecValTok{172} \DecValTok{68} \DecValTok{112} \DecValTok{306} \DecValTok{300}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ impressions    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{8672} \DecValTok{11875} \DecValTok{14631} \DecValTok{11709} \DecValTok{14768} \DecValTok{8698} \DecValTok{2924} \DecValTok{5919} \DecValTok{14789} \DecValTok{14818}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ display        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ transactions   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ click.rate     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.9} \FloatTok{1.92} \FloatTok{1.99} \FloatTok{2.11} \FloatTok{1.96} \FloatTok{1.98} \FloatTok{2.33} \FloatTok{1.89} \FloatTok{2.07} \FloatTok{2.02}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ conversion.rate}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{1.21} \FloatTok{0.88} \FloatTok{1.03} \FloatTok{0.81} \FloatTok{1.03} \FloatTok{1.16} \FloatTok{1.47} \FloatTok{0.89} \FloatTok{0.98} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ revenue        }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{58.9} \FloatTok{44.9} \FloatTok{141.6} \FloatTok{209.8} \FloatTok{197.7}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 8 variables and 40 observations. The response variable, \texttt{revenue}, is continuous, while the remaining 7 variables serve as potential predictors.

\subsection*{Exploring Relationships in the Data}\label{exploring-relationships-in-the-data}


Before constructing a regression model, we first explore the relationships between variables to ensure that our assumptions hold and to identify strong predictors. A useful tool for this is the \texttt{pairs.panels()} function from the \textbf{psych} package, which provides a comprehensive overview of pairwise relationships:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}

\FunctionTok{pairs.panels}\NormalTok{(marketing)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{10_Regression_files/figure-latex/unnamed-chunk-2-1} \end{center}

This visualization includes:

\begin{itemize}
\tightlist
\item
  \emph{Scatter plots} (lower triangle), showing how each predictor relates to the response variable.\\
\item
  \emph{Histograms} (diagonal), illustrating the distribution of each variable.\\
\item
  \emph{Correlation coefficients} (upper triangle), quantifying the strength and direction of linear associations.
\end{itemize}

From the correlation matrix, we observe that \texttt{spend} and \texttt{revenue} exhibit a \emph{strong positive correlation} of 0.79. This suggests that \emph{higher advertising expenditure is associated with higher revenue}, making \texttt{spend} a strong candidate for predicting \texttt{revenue}.

In the next section, we formalize this relationship using a \emph{simple linear regression model}.

\subsection*{Fitting a Simple Linear Regression Model}\label{fitting-a-simple-linear-regression-model}


A logical starting point in regression analysis is examining the relationship between a single predictor and the response variable. This allows for a clearer understanding of how one variable influences another before incorporating additional predictors into more complex models. Here, we investigate how advertising expenditure (\texttt{spend}) affects daily revenue (\texttt{revenue}) using a simple linear regression model.

Before fitting the model, it is essential to visualize the relationship between these variables to assess whether a linear assumption is reasonable. A scatter plot with a fitted least-squares regression line provides insight into the strength and direction of the relationship:

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth]{10_Regression_files/figure-latex/scoter-plot-simple-reg-1} 

}

\caption{Scatter plot of daily revenue (â‚¬) versus daily spend (â‚¬) for 40 observations, with the fitted least-squares regression line (blue) showing the linear relationship.}\label{fig:scoter-plot-simple-reg}
\end{figure}

Figure \ref{fig:scoter-plot-simple-reg} illustrates the relationship between \texttt{spend} and \texttt{revenue} in the \emph{marketing} dataset. The scatter plot suggests a positive association, indicating that increased advertising expenditure is generally linked to higher revenue.

A simple linear regression model is mathematically expressed as:

\[
\hat{y} = b_0 + b_1x
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{y}\) represents the predicted value of the response variable (\texttt{revenue}).\\
\item
  \(x\) denotes the predictor variable (\texttt{spend}).\\
\item
  \(b_0\) is the intercept, indicating the estimated revenue when no advertising expenditure is made.\\
\item
  \(b_1\) is the slope, representing the expected change in revenue for a one-unit increase in \texttt{spend}.
\end{itemize}

This formulation provides a framework for estimating the relationship between advertising expenditure and revenue, which we will now proceed to quantify.

\subsection*{Estimating the Model in R}\label{estimating-the-model-in-r}


To estimate the regression coefficients, we use the \texttt{lm()} function in R, which fits a linear model using the least squares method. The syntax follows the format:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variable, }\AttributeTok{data =}\NormalTok{ dataset)}
\end{Highlighting}
\end{Shaded}

For our analysis, we model \texttt{revenue} as a function of \texttt{spend}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_reg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
\end{Highlighting}
\end{Shaded}

After fitting the model, we summarize the results using the \texttt{summary()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{175.640}  \SpecialCharTok{{-}}\FloatTok{56.226}    \FloatTok{1.448}   \FloatTok{65.235}  \FloatTok{210.987} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)  }\FloatTok{15.7058}    \FloatTok{35.1727}   \FloatTok{0.447}    \FloatTok{0.658}    
\NormalTok{   spend         }\FloatTok{5.2517}     \FloatTok{0.6624}   \FloatTok{7.928} \FloatTok{1.42e{-}09} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{93.82}\NormalTok{ on }\DecValTok{38}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6232}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6133} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{62.86}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{38}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.415e{-}09}
\end{Highlighting}
\end{Shaded}

The \texttt{summary()} output provides key insights into the estimated model. The regression equation based on the estimated coefficients is:

\[
\hat{\text{revenue}} = 15.71 + 5.25 \cdot \text{spend}
\]

where:

\begin{itemize}
\tightlist
\item
  The \textbf{intercept} (\(b_0\)) is 15.71, representing the estimated daily revenue when no money is spent on advertising (\texttt{spend\ =\ 0}).\\
\item
  The \textbf{slope} (\(b_1\)) is 5.25, meaning that for each additional â‚¬1 spent on advertising, daily revenue is expected to increase by approximately â‚¬5.25.
\end{itemize}

Beyond the estimated coefficients, the \texttt{summary()} output provides several key metrics for evaluating the regression model:

\begin{itemize}
\tightlist
\item
  \textbf{Estimate}: The estimated values of the intercept and slope.\\
\item
  \textbf{Standard error}: Measures the variability of each coefficient estimate. Smaller standard errors indicate more precise estimates.\\
\item
  \textbf{t-value and p-value}: The t-value quantifies how many standard errors the coefficient is from zero, while the p-value assesses statistical significance. A small p-value (typically \textless{} 0.05) suggests that the predictor has a significant impact on the response variable.\\
\item
  \textbf{Multiple R-squared (\(R^2\))}: Measures the proportion of variance in \texttt{revenue} explained by \texttt{spend}. Here, \(R^2 = 0.623\), meaning that \textbf{62.3\% of the variation in revenue is explained by advertising spend}.\\
\item
  \textbf{Residual standard error (RSE)}: Provides an estimate of the typical prediction error. In this case, \(RSE = 93.82\), indicating that, on average, predictions deviate from actual revenue values by approximately â‚¬93.82.
\end{itemize}

The results confirm a statistically significant relationship between advertising spend and revenue, supporting the use of regression analysis for business decision-making. In the next section, we explore how this model can be applied for prediction and how residual analysis helps validate model assumptions.

\subsection*{Interpreting the Regression Line}\label{interpreting-the-regression-line}


The regression line provides a mathematical approximation of the relationship between advertising spend and revenue. Once the model is estimated, it can be used for prediction. Suppose a company wants to estimate the expected revenue for a day when â‚¬25 is spent on pay-per-click (PPC) advertising. Using the regression equation:

\begin{equation} 
\begin{split}
\hat{\text{revenue}} & = 15.71 + 5.25 \cdot 25 \\
 & = 147
\end{split}
\end{equation}
Thus, the predicted daily revenue is approximately â‚¬147.

This predictive capability is particularly valuable for marketing teams planning advertising budgets. For example, if the goal is to maximize returns while controlling costs, the model provides an evidence-based estimate of how revenue responds to different levels of spending. Decision-makers can use this information to determine optimal advertising expenditures, set performance targets, and allocate marketing resources efficiently.

\subsection*{Residuals and Model Fit}\label{residuals-and-model-fit}


Residuals measure the difference between observed and predicted values, providing insight into how well the regression model fits the data. The residual for an observation is calculated as:

\[
\text{Residual} = y - \hat{y}
\]

where \(y\) is the actual observed value, and \(\hat{y}\) is the predicted value from the regression model. For example, suppose a day in the dataset has a marketing spend of â‚¬25 and an actual revenue of 185.36. The residual for this observation is:

\begin{equation} 
\begin{split}
\text{Residual} & = 185.36 - 147 \\
 & = 38.36
\end{split}
\end{equation}

Residuals play a crucial role in assessing model adequacy. Ideally, they should be randomly distributed around zero, indicating that the model captures the relationship between variables well. However, if residuals exhibit systematic patterns---such as curvature or increasing variance---this suggests that the model does not fully capture the relationship and may require adjustments, such as incorporating additional predictors or using a non-linear model.

The regression line is estimated using the least squares method, which finds the line that minimizes the sum of squared residuals, also known as the sum of squared errors (SSE):

\begin{equation} 
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\label{eq:sse}
\end{equation}

where \(y_i\) represents the observed revenue, \(\hat{y}_i\) is the predicted revenue, and \(n\) is the number of observations. Minimizing SSE ensures that the estimated regression line optimally represents the relationship between the predictor and response variable, leading to more accurate predictions.

Monitoring residuals is an essential step in regression analysis. If residuals exhibit no discernible pattern and are evenly spread around zero, the linear model is likely appropriate. However, if residuals show trends or increasing variability, further refinement---such as adding interaction terms, transforming variables, or considering a different modeling approach---may be necessary.

In summary, simple linear regression provides an effective way to model and interpret the relationship between two variables. By analyzing the \emph{marketing} dataset, we demonstrated how to estimate, interpret, and apply a regression model to make predictions. This foundational understanding of simple linear regression sets the stage for evaluating model quality and extending the framework to multiple predictors in the following sections.

\section*{Hypothesis Testing in Simple Linear Regression}\label{hypothesis-testing-in-simple-linear-regression}


Hypothesis testing in regression analysis helps determine whether a predictor variable has a statistically significant relationship with the response variable. Specifically, we test whether the estimated slope \(b_1\) from the sample regression model provides evidence of a true linear relationship in the population, where the unknown slope is denoted as \(\beta_1\).

The population regression equation models the relationship between a predictor \(x\) and a response \(y\) for the entire population and is expressed as:

\[
y = \beta_0 + \beta_1x + \epsilon
\]

where:

\begin{itemize}
\tightlist
\item
  \(\beta_0\) represents the population intercept, which is the expected value of \(y\) when \(x = 0\).\\
\item
  \(\beta_1\) represents the population slope, indicating how \(y\) changes for a one-unit increase in \(x\).\\
\item
  \(\epsilon\) is a random error term accounting for variability in \(y\) not explained by the linear model.
\end{itemize}

The primary objective of hypothesis testing in regression is to determine whether the slope \(\beta_1\) is significantly different from zero. If \(\beta_1 = 0\), the regression equation simplifies to:

\[
y = \beta_0 + \epsilon
\]

This suggests that the predictor \(x\) has no linear relationship with the response variable \(y\). Conversely, if \(\beta_1 \neq 0\), there is statistical evidence of an association between \(x\) and \(y\). To formally test this, we set up the following hypotheses:

\[
\begin{cases}
  H_0: \beta_1 =  0, \quad \text{(no linear relationship between \( x \) and \( y \))}  \\
  H_a: \beta_1 \neq 0, \quad \text{(a linear relationship exists between \( x \) and \( y \))}
\end{cases}
\]

The estimated slope \(b_1\) from the sample data provides an approximation of \(\beta_1\). To assess its significance, we rely on the following key statistical measures:

\begin{itemize}
\tightlist
\item
  \textbf{Standard error of the slope}: Measures the variability in the estimate \(b_1\).\\
\item
  \textbf{t-statistic}: Determines how many standard errors the estimated slope is from zero. It is computed as:
\end{itemize}

\[
t = \frac{b_1}{SE(b_1)}
\]

\begin{itemize}
\tightlist
\item
  \textbf{p-value}: Represents the probability of observing a t-statistic as extreme as the one calculated, assuming the null hypothesis is true. A small p-value (typically less than 0.05) provides strong evidence to reject \(H_0\), indicating that the predictor is significantly associated with the response variable.
\end{itemize}

To illustrate hypothesis testing in simple linear regression, we examine the results of the model that predicts \texttt{revenue} (daily revenue) based on \texttt{spend} (advertising expenditure) using the \emph{marketing} dataset. The estimated slope \(b_1\) for \texttt{spend} is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{175.640}  \SpecialCharTok{{-}}\FloatTok{56.226}    \FloatTok{1.448}   \FloatTok{65.235}  \FloatTok{210.987} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)  }\FloatTok{15.7058}    \FloatTok{35.1727}   \FloatTok{0.447}    \FloatTok{0.658}    
\NormalTok{   spend         }\FloatTok{5.2517}     \FloatTok{0.6624}   \FloatTok{7.928} \FloatTok{1.42e{-}09} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{93.82}\NormalTok{ on }\DecValTok{38}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6232}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.6133} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{62.86}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{38}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.415e{-}09}
\end{Highlighting}
\end{Shaded}

From the output:

\begin{itemize}
\tightlist
\item
  The \textbf{t-statistic} for the slope is 7.93.\\
\item
  The \textbf{p-value} is \ensuremath{1.4150362\times 10^{-9}}, which is very close to zero.
\end{itemize}

Since the p-value is significantly smaller than the commonly used significance level (\(\alpha = 0.05\)), we reject the null hypothesis \(H_0\). This confirms that the predictor \texttt{spend} has a statistically significant effect on \texttt{revenue}. Specifically:

\begin{itemize}
\tightlist
\item
  The slope estimate \(b_1 = 5.25\) suggests that for each additional â‚¬1 spent on advertising, daily revenue is expected to increase by approximately 5.25.\\
\item
  The strong statistical significance of \texttt{spend} validates its role as an important predictor for \texttt{revenue}, supporting its inclusion in the model.
\end{itemize}

Hypothesis testing in simple linear regression provides a structured approach for determining whether a predictor variable has a meaningful impact on the response variable. A statistically significant slope (\(\beta_1 \neq 0\)) indicates that changes in the predictor \(x\) are associated with changes in the response \(y\), allowing for data-driven decision-making.

While statistical significance establishes the presence of a relationship, it does not imply causation. Additional factors, such as potential confounders, omitted variables, and model assumptions, should be considered when interpreting regression results.

In the next sections, we will explore further techniques for evaluating regression model quality, including measures of goodness-of-fit and model diagnostics. We will also extend these concepts to multiple predictors, enabling more comprehensive analyses and better predictions.

\section*{Measuring the Quality of a Regression Model}\label{measuring-the-quality-of-a-regression-model}


Evaluating the effectiveness of a regression model goes beyond determining whether a predictor is statistically significant. While hypothesis testing confirms whether a predictor has a meaningful relationship with the response variable, it does not assess how well the model fits the data. To measure model quality, we rely on additional metrics that quantify predictive accuracy and explanatory power. Two key statistics for this purpose are the \textbf{Residual Standard Error (RSE)} and the \textbf{\(R^2\) (R-squared) statistic}.

\subsection*{Residual Standard Error (RSE)}\label{residual-standard-error-rse}


Residual Standard Error (RSE) provides an estimate of the typical prediction error in the model. It measures how much the observed values deviate from the predicted values on average. The formula for RSE is:

\[
RSE = \sqrt{\frac{1}{n-p-1} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},
\]
where \(y_i\) represents the observed values of the response variable, \(\hat{y}_i\) represents the predicted values, \(n\) is the number of observations, and \(p\) is the number of predictors in the model.

A smaller RSE indicates a model with more precise predictions. For example, in the simple linear regression model for the \emph{marketing} dataset, the RSE is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rse\_value }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(simple\_reg}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{summary}\NormalTok{(simple\_reg)}\SpecialCharTok{$}\NormalTok{df[}\DecValTok{2}\NormalTok{])}
\FunctionTok{round}\NormalTok{(rse\_value, }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{93.82}
\end{Highlighting}
\end{Shaded}

This value represents the average deviation of predicted revenue from actual revenue. A lower RSE suggests a better-fitting model, though it should always be interpreted in the context of the response variable's scale.

\subsection*{\texorpdfstring{R-squared (\(R^2\))}{R-squared (R\^{}2)}}\label{r-squared-r2}


The \(R^2\) statistic measures how well the regression model explains the variability in the response variable. It is defined as:

\[
R^2 = 1 - \frac{SSE}{SST}
\]

where:

\begin{itemize}
\tightlist
\item
  \(SST\) (Total Sum of Squares) represents the total variability in the response variable before fitting the model.\\
\item
  \(SSE\) (Sum of Squared Errors) represents the variability that remains unexplained after fitting the model.
\end{itemize}

\(R^2\) ranges from 0 to 1, where higher values indicate that the model explains a greater proportion of variability in the response variable. For example, in the \emph{marketing} dataset, the \(R^2\) value is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(simple\_reg)}\SpecialCharTok{$}\NormalTok{r.squared, }\DecValTok{3}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.623}
\end{Highlighting}
\end{Shaded}

This means that 62.3\% of the variation in \texttt{revenue} is explained by \texttt{spend}. While higher \(R^2\) values suggest a better fit, they do not guarantee that the model generalizes well to new data. It is important to supplement \(R^2\) with additional model diagnostics.

\subsection*{\texorpdfstring{Relationship Between \(R^2\) and the Correlation Coefficient}{Relationship Between R\^{}2 and the Correlation Coefficient}}\label{relationship-between-r2-and-the-correlation-coefficient}


In simple linear regression, \(R^2\) is directly related to the correlation coefficient \(r\) between the predictor and response variable:

\[
R^2 = r^2
\]

For example, in the \emph{marketing} dataset, the correlation between \texttt{spend} and \texttt{revenue} is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(marketing}\SpecialCharTok{$}\NormalTok{spend, marketing}\SpecialCharTok{$}\NormalTok{revenue), }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.79}
\end{Highlighting}
\end{Shaded}

Squaring this value gives:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(marketing}\SpecialCharTok{$}\NormalTok{spend, marketing}\SpecialCharTok{$}\NormalTok{revenue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\FloatTok{0.62}
\end{Highlighting}
\end{Shaded}

which matches the \(R^2\) value, reinforcing how \(R^2\) quantifies the strength of the linear relationship.

\subsection*{Adjusted R-squared}\label{adjusted-r-squared}


While \(R^2\) measures the proportion of variance explained by the model, \textbf{Adjusted \(R^2\)} accounts for the number of predictors, ensuring that adding unnecessary variables does not artificially inflate the statistic. It is calculated as:

\[
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \cdot \frac{n-1}{n-p-1},
\]
where \(n\) is the number of observations and \(p\) is the number of predictors.

Adjusted \(R^2\) penalizes the inclusion of irrelevant predictors, making it particularly useful in multiple regression settings. In simple linear regression (where \(p = 1\)), \(R^2\) and Adjusted \(R^2\) are equal, but in multiple regression, Adjusted \(R^2\) is often lower and provides a better measure of model performance.

\subsection*{Interpreting Model Quality}\label{interpreting-model-quality}


A good regression model should have:

\begin{itemize}
\tightlist
\item
  A \textbf{low RSE}, indicating that predictions are close to observed values.\\
\item
  A \textbf{high \(R^2\)}, suggesting that the model explains most of the variability in the response variable.\\
\item
  A \textbf{high Adjusted \(R^2\)}, ensuring that additional predictors improve the model rather than introducing noise.
\end{itemize}

However, a model should not be judged by these metrics alone. Even a high \(R^2\) model may fail if it violates regression assumptions or overfits the data. Additional diagnostics, such as residual analysis and cross-validation, are essential to ensure model reliability.

By understanding these measures of model quality, we gain deeper insight into the effectiveness of regression models and prepare for extending these concepts to multiple predictors in the next sections.

\section{Multiple Linear Regression}\label{sec-multiple-regression}

Simple linear regression is useful for modeling relationships between two variables, but in many real-world applications, multiple factors influence the response variable. Multiple linear regression extends simple regression by incorporating multiple predictors, improving both estimation accuracy and predictive performance.

To illustrate, we expand the previous model, which included only \texttt{spend} as a predictor, by adding \texttt{display}, an indicator of whether a display advertising campaign was active. This additional predictor allows us to assess its impact on revenue. The general equation for a multiple regression model with \(p\) predictors is:

\[
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]

where \(\beta_0\) is the intercept, and \(\beta_1, \beta_2, \dots, \beta_p\) represent the estimated effects of each predictor on the response variable.

For our case, the equation with two predictors, \texttt{spend} and \texttt{display}, is:

\[
\hat{\text{revenue}} = \beta_0 + \beta_1 \cdot \text{spend} + \beta_2 \cdot \text{display}
\]

where \texttt{spend} represents daily advertising expenditure and \texttt{display} is a categorical variable (\texttt{yes/no}), which R automatically converts into a binary indicator. Here, \texttt{display\ =\ 1} indicates an active display campaign, while \texttt{display\ =\ 0} means no display campaign.

\subsection*{Fitting the Multiple Regression Model}\label{fitting-the-multiple-regression-model}


We fit the multiple regression model using the \texttt{lm()} function in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multiple\_reg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{summary}\NormalTok{(multiple\_reg)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{        Min       }\DecValTok{1}\NormalTok{Q   Median       }\DecValTok{3}\NormalTok{Q      Max }
   \SpecialCharTok{{-}}\FloatTok{189.420}  \SpecialCharTok{{-}}\FloatTok{45.527}    \FloatTok{5.566}   \FloatTok{54.943}  \FloatTok{154.340} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\SpecialCharTok{{-}}\FloatTok{41.4377}    \FloatTok{32.2789}  \SpecialCharTok{{-}}\FloatTok{1.284} \FloatTok{0.207214}    
\NormalTok{   spend         }\FloatTok{5.3556}     \FloatTok{0.5523}   \FloatTok{9.698} \FloatTok{1.05e{-}11} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   display     }\FloatTok{104.2878}    \FloatTok{24.7353}   \FloatTok{4.216} \FloatTok{0.000154} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{78.14}\NormalTok{ on }\DecValTok{37}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7455}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7317} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{54.19}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{37}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.012e{-}11}
\end{Highlighting}
\end{Shaded}

The estimated regression equation is:

\[
\hat{\text{revenue}} = -41.44 + 5.36 \cdot \text{spend} + 104.29 \cdot \text{display}
\]

where:
- The \textbf{intercept} (\(\beta_0\)) is -41.44, representing the estimated revenue when both \texttt{spend} is zero and no display campaign is running.
- The \textbf{coefficient for \texttt{spend}} (\(\beta_1\)) is 5.36, indicating that for each additional â‚¬1 spent, revenue increases by approximately 5.36, assuming \texttt{display} remains unchanged.
- The \textbf{coefficient for \texttt{display}} (\(\beta_2\)) is 104.29, meaning that when a display campaign is active (\texttt{display\ =\ 1}), revenue increases by approximately 104.29, holding \texttt{spend} constant.

\subsection*{Making Predictions}\label{making-predictions}


Consider a scenario where the company spends â‚¬25 on advertising while running a display campaign (\texttt{display\ =\ 1}). Using the regression equation, the predicted revenue is:

\[
\hat{\text{revenue}} = -41.44 + 5.36 \cdot 25 + 104.29 \cdot 1 = 196.74
\]

Thus, the predicted revenue for that day is approximately â‚¬196.74.

The residual (prediction error) for a specific observation is calculated as the difference between the actual and predicted revenue:

\[
\text{Residual} = y - \hat{y} = 185.36 - 196.74 = -11.49
\]

The prediction error is smaller than that of the simple regression model, confirming that including \texttt{display} improves predictive accuracy.

\subsection*{Evaluating Model Performance}\label{evaluating-model-performance}


Adding \texttt{display} enhances the regression model by reducing prediction errors and improving model fit. We compare key performance metrics between the simple and multiple regression models:

\begin{itemize}
\item
  \textbf{Residual Standard Error (RSE):} In the simple regression model, \(RSE = 93.82\), whereas in the multiple regression model, \(RSE = 78.14\). The reduction in RSE indicates improved prediction accuracy.
\item
  \textbf{\(R^2\) (R-squared):} The simple regression model had \(R^2 = 62\%\), whereas the multiple regression model increased to \(R^2 = 75\%\), demonstrating improved explanatory power.
\item
  \textbf{Adjusted \(R^2\):} Unlike \(R^2\), Adjusted \(R^2\) accounts for the number of predictors. In the simple regression model, Adjusted \(R^2 = 61\%\), while in the multiple regression model, Adjusted \(R^2 = 73\%\), confirming that the additional predictor contributes meaningfully to model performance.
\end{itemize}

\subsection*{Key Takeaways}\label{key-takeaways-2}


The multiple regression model improves upon simple regression by providing a better fit, reducing prediction errors, and enabling more accurate estimation of revenue drivers. Including \texttt{display} alongside \texttt{spend} strengthens the model's ability to explain revenue variation. However, as models grow more complex, careful evaluation is necessary to prevent issues such as \textbf{multicollinearity} (high correlation between predictors) and \textbf{overfitting} (adding unnecessary predictors that reduce generalizability).

In the next sections, we will examine model assumptions, conduct diagnostics, and refine regression models to ensure validity and reliability.

\section{Generalized Linear Models (GLMs)}\label{generalized-linear-models-glms}

Linear regression provides a useful framework for modeling continuous outcomes, but it is not suitable when the response variable is binary, count-based, or follows a distribution other than normal. \emph{Generalized Linear Models (GLMs)} extend traditional linear regression by introducing a \emph{link function}, which transforms the relationship between predictors and the response variable, and a \emph{variance function}, which accounts for non-constant variability in the response. These extensions allow GLMs to accommodate a broader range of response variable distributions, making them widely applicable in fields such as finance, healthcare, and marketing.

GLMs retain the fundamental principles of linear regression but introduce three key components:
1. \emph{Random component}: Specifies the probability distribution of the response variable, which can belong to the exponential family (e.g., normal, binomial, or Poisson distributions).
2. \emph{Systematic component}: Represents the linear combination of predictor variables.
3. \emph{Link function}: Transforms the expected value of the response variable so that it can be modeled as a linear function of the predictors.

In the following sections, we introduce two commonly used GLMs:

\begin{itemize}
\tightlist
\item
  \emph{Logistic regression}, which models binary outcomes.
\item
  \emph{Poisson regression}, which is suited for modeling count data.
\end{itemize}

By extending regression beyond continuous responses, these models provide a more flexible and interpretable framework for analyzing data in a variety of applications. The next sections discuss their theoretical foundations and implementation in R.

\section{Logistic Regression}\label{logistic-regression}

Logistic regression is a generalized linear model designed for binary classification, where the response variable takes two values, such as 0/1 or yes/no. Instead of predicting a continuous outcome, logistic regression estimates the probability that an observation belongs to a particular category. To ensure that predicted probabilities remain within the range \([0,1]\), the model applies the \emph{logit function}, which transforms the linear combination of predictors into a probability scale:

\[
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]

Here, \(p\) represents the probability that the outcome is 1, and the logit transformation ensures a linear relationship between the predictors and the log-odds of the response variable.

\subsection*{Logistic Regression in R}\label{logistic-regression-in-r}


To illustrate logistic regression, we use the \emph{churn} dataset, which contains information on customer behavior. The objective is to predict whether a customer will \emph{churn} (leave the service) based on customer characteristics and service usage patterns. The selected predictors include variables such as \texttt{account.length}, \texttt{voice.plan}, \texttt{voice.messages}, \texttt{intl.plan}, \texttt{intl.mins}, \texttt{day.mins}, \texttt{eve.mins}, \texttt{night.mins}, and \texttt{customer.calls}, which capture aspects of user engagement and service utilization.

In R, logistic regression is implemented using the \texttt{glm()} function, which fits generalized linear models. The function follows the syntax:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variables, }\AttributeTok{data =}\NormalTok{ dataset, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

where \texttt{response\_variable} is the binary outcome, \texttt{predictor\_variables} are the independent variables, and \texttt{family\ =\ binomial} specifies a logistic regression model.

For the \emph{churn} dataset, we fit a logistic regression model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(churn)}

\NormalTok{logreg\_1 }\OtherTok{=} \FunctionTok{glm}\NormalTok{(churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+}\NormalTok{ day.mins }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+} 
\NormalTok{                         night.mins }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ customer.calls }\SpecialCharTok{+}\NormalTok{ intl.plan }\SpecialCharTok{+}\NormalTok{ voice.plan, }
               \AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

The model estimates the relationship between the predictors and the probability of churn. To examine the model's coefficients and significance levels, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(logreg\_1)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ account.length }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+}\NormalTok{ day.mins }\SpecialCharTok{+} 
\NormalTok{       eve.mins }\SpecialCharTok{+}\NormalTok{ night.mins }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ customer.calls }\SpecialCharTok{+}\NormalTok{ intl.plan }\SpecialCharTok{+} 
\NormalTok{       voice.plan, }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ churn)}
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                    Estimate Std. Error z value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{z}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)     }\FloatTok{8.8917584}  \FloatTok{0.6582188}  \FloatTok{13.509}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   account.length }\SpecialCharTok{{-}}\FloatTok{0.0013811}  \FloatTok{0.0011453}  \SpecialCharTok{{-}}\FloatTok{1.206}   \FloatTok{0.2279}    
\NormalTok{   voice.messages }\SpecialCharTok{{-}}\FloatTok{0.0355317}  \FloatTok{0.0150397}  \SpecialCharTok{{-}}\FloatTok{2.363}   \FloatTok{0.0182} \SpecialCharTok{*}  
\NormalTok{   day.mins       }\SpecialCharTok{{-}}\FloatTok{0.0136547}  \FloatTok{0.0009103} \SpecialCharTok{{-}}\FloatTok{15.000}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   eve.mins       }\SpecialCharTok{{-}}\FloatTok{0.0071210}  \FloatTok{0.0009419}  \SpecialCharTok{{-}}\FloatTok{7.561} \FloatTok{4.02e{-}14} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   night.mins     }\SpecialCharTok{{-}}\FloatTok{0.0040518}  \FloatTok{0.0009048}  \SpecialCharTok{{-}}\FloatTok{4.478} \FloatTok{7.53e{-}06} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   intl.mins      }\SpecialCharTok{{-}}\FloatTok{0.0882514}  \FloatTok{0.0170578}  \SpecialCharTok{{-}}\FloatTok{5.174} \FloatTok{2.30e{-}07} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   customer.calls }\SpecialCharTok{{-}}\FloatTok{0.5183958}  \FloatTok{0.0328652} \SpecialCharTok{{-}}\FloatTok{15.773}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   intl.planno     }\FloatTok{2.0958198}  \FloatTok{0.1214476}  \FloatTok{17.257}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice.planno   }\SpecialCharTok{{-}}\FloatTok{2.1637477}  \FloatTok{0.4836735}  \SpecialCharTok{{-}}\FloatTok{4.474} \FloatTok{7.69e{-}06} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   (Dispersion parameter }\ControlFlowTok{for}\NormalTok{ binomial family taken to be }\DecValTok{1}\NormalTok{)}
   
\NormalTok{       Null deviance}\SpecialCharTok{:} \FloatTok{4075.0}\NormalTok{  on }\DecValTok{4999}\NormalTok{  degrees of freedom}
\NormalTok{   Residual deviance}\SpecialCharTok{:} \FloatTok{3174.3}\NormalTok{  on }\DecValTok{4990}\NormalTok{  degrees of freedom}
\NormalTok{   AIC}\SpecialCharTok{:} \FloatTok{3194.3}
   
\NormalTok{   Number of Fisher Scoring iterations}\SpecialCharTok{:} \DecValTok{6}
\end{Highlighting}
\end{Shaded}

The output provides key information, including estimated coefficients, standard errors, z-statistics, and p-values. A small p-value (typically less than 0.05) suggests that the corresponding predictor has a statistically significant effect on the probability of churn. If a variable such as \texttt{account.length} has a large p-value, it suggests that the predictor does not contribute significantly to explaining churn and may be removed from the model. Refining the model by removing non-significant predictors and re-evaluating improves both interpretability and predictive performance.

\section{Poisson Regression}\label{poisson-regression}

Poisson regression is a generalized linear model designed for modeling count data, where the response variable represents the number of occurrences of an event within a fixed interval. Examples include the number of customer service calls received daily, website visits per hour, or purchases made per customer. Unlike linear regression, which assumes normally distributed residuals, Poisson regression assumes that the response variable follows a \emph{Poisson distribution} and that its mean equals its variance. This assumption makes Poisson regression particularly useful for data with non-negative integer counts.

The model is formulated as:

\[
\ln(\lambda) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
\]

where \(\lambda\) represents the expected count (mean) of the response variable, and the predictors \(x_1, x_2, \dots, x_p\) influence the log of \(\lambda\). The logarithmic transformation ensures that predicted values remain positive, preventing the model from producing negative counts.

\subsection*{Poisson Regression in R}\label{poisson-regression-in-r}


To illustrate Poisson regression, we analyze customer service call frequency using the \emph{churn} dataset. The objective is to model the number of customer service calls (\texttt{customer.calls}) based on customer attributes and service usage. Since \texttt{customer.calls} is an integer-valued response variable, Poisson regression is more appropriate than linear regression.

In R, Poisson regression is implemented using the \texttt{glm()} function, similar to logistic regression. The syntax follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(response\_variable }\SpecialCharTok{\textasciitilde{}}\NormalTok{ predictor\_variables, }\AttributeTok{data =}\NormalTok{ dataset, }\AttributeTok{family =}\NormalTok{ poisson)}
\end{Highlighting}
\end{Shaded}

For our example, we fit a Poisson regression model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ customer.calls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ churn }\SpecialCharTok{+}\NormalTok{ voice.messages }\SpecialCharTok{+}\NormalTok{ day.mins }\SpecialCharTok{+}\NormalTok{ eve.mins }\SpecialCharTok{+} 
\NormalTok{                           night.mins }\SpecialCharTok{+}\NormalTok{ intl.mins }\SpecialCharTok{+}\NormalTok{ intl.plan }\SpecialCharTok{+}\NormalTok{ voice.plan}

\NormalTok{reg\_pois }\OtherTok{=} \FunctionTok{glm}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{family =}\NormalTok{ poisson)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{customer.calls} is the response variable, while predictors such as \texttt{churn}, \texttt{intl.plan}, and \texttt{day.mins} help explain variations in call frequency. The \texttt{family\ =\ poisson} argument specifies that the model follows a Poisson distribution.

Once the model is fitted, we examine the results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(reg\_pois)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ churn)}
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                    Estimate Std. Error z value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{z}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)     }\FloatTok{0.9957186}  \FloatTok{0.1323004}   \FloatTok{7.526} \FloatTok{5.22e{-}14} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   churnno        }\SpecialCharTok{{-}}\FloatTok{0.5160641}  \FloatTok{0.0304013} \SpecialCharTok{{-}}\FloatTok{16.975}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice.messages  }\FloatTok{0.0034062}  \FloatTok{0.0028294}   \FloatTok{1.204} \FloatTok{0.228646}    
\NormalTok{   day.mins       }\SpecialCharTok{{-}}\FloatTok{0.0006875}  \FloatTok{0.0002078}  \SpecialCharTok{{-}}\FloatTok{3.309} \FloatTok{0.000938} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   eve.mins       }\SpecialCharTok{{-}}\FloatTok{0.0005649}  \FloatTok{0.0002237}  \SpecialCharTok{{-}}\FloatTok{2.525} \FloatTok{0.011554} \SpecialCharTok{*}  
\NormalTok{   night.mins     }\SpecialCharTok{{-}}\FloatTok{0.0003602}  \FloatTok{0.0002245}  \SpecialCharTok{{-}}\FloatTok{1.604} \FloatTok{0.108704}    
\NormalTok{   intl.mins      }\SpecialCharTok{{-}}\FloatTok{0.0075034}  \FloatTok{0.0040886}  \SpecialCharTok{{-}}\FloatTok{1.835} \FloatTok{0.066475}\NormalTok{ .  }
\NormalTok{   intl.planno     }\FloatTok{0.2085330}  \FloatTok{0.0407760}   \FloatTok{5.114} \FloatTok{3.15e{-}07} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   voice.planno    }\FloatTok{0.0735515}  \FloatTok{0.0878175}   \FloatTok{0.838} \FloatTok{0.402284}    
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   (Dispersion parameter }\ControlFlowTok{for}\NormalTok{ poisson family taken to be }\DecValTok{1}\NormalTok{)}
   
\NormalTok{       Null deviance}\SpecialCharTok{:} \FloatTok{5991.1}\NormalTok{  on }\DecValTok{4999}\NormalTok{  degrees of freedom}
\NormalTok{   Residual deviance}\SpecialCharTok{:} \FloatTok{5719.5}\NormalTok{  on }\DecValTok{4991}\NormalTok{  degrees of freedom}
\NormalTok{   AIC}\SpecialCharTok{:} \DecValTok{15592}
   
\NormalTok{   Number of Fisher Scoring iterations}\SpecialCharTok{:} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

The summary output provides estimated coefficients, standard errors, z-statistics, and p-values. A small p-value (typically \textless{} 0.05) suggests that a predictor significantly influences the expected number of customer calls. If predictors such as \texttt{voice.messages} or \texttt{night.mins} have large p-values, they may not contribute meaningfully and can be removed in subsequent model refinements.

Interpreting the coefficients in a Poisson regression model differs from linear regression. A coefficient represents the expected percentage change in the response variable for a one-unit increase in the predictor. For instance, if the coefficient of \texttt{intl.plan} is 0.3, it implies that customers with an international plan make approximately \(e^{0.3} - 1 \approx 35\%\) more service calls than those without one, holding all other predictors constant.

In summary, Poisson regression extends the linear regression framework to count data, making it a valuable tool for event frequency modeling. Like logistic regression, it belongs to the broader family of generalized linear models, enabling flexible modeling beyond continuous response variables. By iteratively refining the model and excluding non-significant predictors, we ensure an interpretable and effective model for practical applications.

In the next sections, we will explore techniques for validating and improving regression models to enhance their predictive reliability.

\section{Model Selection Using Stepwise Regression}\label{sec-stepwise-regression}

Selecting the right predictors is essential for building a regression model that is both accurate and interpretable. This process, known as \emph{model specification}, helps the model retain essential relationships while preventing overfitting and excluding irrelevant predictors. Proper model specification enhances predictive accuracy and ensures that insights derived from the model remain meaningful.

In practice, datasets---especially in business and data science applications---often contain numerous potential predictors. Managing this complexity requires systematic methods for identifying the most relevant variables. One such approach is \emph{stepwise regression}, an iterative algorithm that evaluates predictors based on their statistical contribution to the model. Stepwise regression iteratively adds or removes predictors based on their statistical significance, ensuring that only the most relevant variables are retained.

Due to its structured approach, stepwise regression is particularly useful for small to medium-sized datasets where automated predictor selection improves model interpretability without excessive computational burden.

\subsection*{The Role of AIC in Model Selection}\label{the-role-of-aic-in-model-selection}


To evaluate model quality during the selection process, we use criteria such as the \emph{Akaike Information Criterion (AIC)}. AIC provides a trade-off between model complexity and goodness of fit, where lower values indicate a more optimal balance between explanatory power and parsimony. It is defined as:\\
\[
AIC = 2p + n \log\left(\frac{SSE}{n}\right),
\]
where \(p\) represents the number of estimated parameters in the model, \(n\) is the number of observations, and \(SSE\) is the sum of squared errors, representing the total unexplained variability in the response variable and measuring the extent to which the model fails to account for observed data.

Unlike \(R^2\), which always increases when additional predictors are included, AIC accounts for overfitting by introducing a penalty for model complexity. This prevents overly complex models that fit the training data well but fail to generalize to new observations. By prioritizing models with a lower AIC, we select those that achieve the best balance between simplicity and predictive accuracy.

\subsection*{Implementing Stepwise Regression in R}\label{implementing-stepwise-regression-in-r}


Stepwise regression is implemented in R using the \texttt{step()} function, which automates the selection of predictors to find an optimal model. The function iteratively evaluates variables and makes inclusion or exclusion decisions based on statistical criteria. Three approaches can be specified using the \texttt{direction} argument: \texttt{"forward"}, which starts with no predictors and adds them incrementally; \texttt{"backward"}, which begins with all predictors and removes the least significant ones; and \texttt{"both"}, which combines forward selection and backward elimination to refine the model in an iterative process.

\begin{example}
\protect\hypertarget{exm:ex-stepwise-regression}{}\label{exm:ex-stepwise-regression}To illustrate stepwise regression, we apply it to the \emph{marketing} dataset, which contains seven predictors. The objective is to identify the best regression model for predicting \texttt{revenue} while ensuring a balance between model complexity and interpretability.

We begin by fitting a regression model that includes all available predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ml\_all }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{summary}\NormalTok{(ml\_all)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{138.00}  \SpecialCharTok{{-}}\FloatTok{59.12}   \FloatTok{15.16}   \FloatTok{54.58}  \FloatTok{106.99} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                     Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)}
\NormalTok{   (Intercept)     }\SpecialCharTok{{-}}\FloatTok{25.260020} \FloatTok{246.988978}  \SpecialCharTok{{-}}\FloatTok{0.102}    \FloatTok{0.919}
\NormalTok{   spend            }\SpecialCharTok{{-}}\FloatTok{0.025807}   \FloatTok{2.605645}  \SpecialCharTok{{-}}\FloatTok{0.010}    \FloatTok{0.992}
\NormalTok{   clicks            }\FloatTok{1.211912}   \FloatTok{1.630953}   \FloatTok{0.743}    \FloatTok{0.463}
\NormalTok{   impressions      }\SpecialCharTok{{-}}\FloatTok{0.005308}   \FloatTok{0.021588}  \SpecialCharTok{{-}}\FloatTok{0.246}    \FloatTok{0.807}
\NormalTok{   display          }\FloatTok{79.835729} \FloatTok{117.558849}   \FloatTok{0.679}    \FloatTok{0.502}
\NormalTok{   transactions     }\SpecialCharTok{{-}}\FloatTok{7.012069}  \FloatTok{66.383251}  \SpecialCharTok{{-}}\FloatTok{0.106}    \FloatTok{0.917}
\NormalTok{   click.rate      }\SpecialCharTok{{-}}\FloatTok{10.951493} \FloatTok{106.833894}  \SpecialCharTok{{-}}\FloatTok{0.103}    \FloatTok{0.919}
\NormalTok{   conversion.rate  }\FloatTok{19.926588} \FloatTok{135.746632}   \FloatTok{0.147}    \FloatTok{0.884}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{77.61}\NormalTok{ on }\DecValTok{32}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7829}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7354} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{16.48}\NormalTok{ on }\DecValTok{7}\NormalTok{ and }\DecValTok{32}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{5.498e{-}09}
\end{Highlighting}
\end{Shaded}

The initial model includes all predictors, but some may not contribute meaningfully to explaining \texttt{revenue}. Evaluating model fit using the Akaike Information Criterion (AIC) helps balance predictive accuracy with model simplicity.

Next, we apply stepwise regression using the \texttt{step()} function, setting \texttt{direction\ =\ "both"} to allow for both forward selection and backward elimination:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ml\_stepwise }\OtherTok{=} \FunctionTok{step}\NormalTok{(ml\_all, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\NormalTok{   Start}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{355.21}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spend }\SpecialCharTok{+}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+} 
\NormalTok{       click.rate }\SpecialCharTok{+}\NormalTok{ conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{0.6} \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{{-}}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{63.3} \DecValTok{192822} \FloatTok{353.23}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{67.2} \DecValTok{192826} \FloatTok{353.23}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}     \FloatTok{129.8} \DecValTok{192889} \FloatTok{353.24}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{364.2} \DecValTok{193123} \FloatTok{353.29}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{2778.1} \DecValTok{195537} \FloatTok{353.79}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3326.0} \DecValTok{196085} \FloatTok{353.90}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192759} \FloatTok{355.21}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{353.21}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+}\NormalTok{ click.rate }\SpecialCharTok{+} 
\NormalTok{       conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{67.9} \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{75.1} \DecValTok{192835} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}     \FloatTok{151.5} \DecValTok{192911} \FloatTok{351.24}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{380.8} \DecValTok{193141} \FloatTok{351.29}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{2787.2} \DecValTok{195547} \FloatTok{351.79}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3325.6} \DecValTok{196085} \FloatTok{351.90}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{0.6} \DecValTok{192759} \FloatTok{355.21}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{351.23}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ transactions }\SpecialCharTok{+}\NormalTok{ conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{47.4} \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}     \FloatTok{129.0} \DecValTok{192957} \FloatTok{349.25}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{312.9} \DecValTok{193141} \FloatTok{349.29}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \FloatTok{3425.7} \DecValTok{196253} \FloatTok{349.93}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{3747.1} \DecValTok{196575} \FloatTok{350.00}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{67.9} \DecValTok{192760} \FloatTok{353.21}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}       \FloatTok{5.2} \DecValTok{192822} \FloatTok{353.23}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{349.24}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display }\SpecialCharTok{+}\NormalTok{ conversion.rate}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ conversion.rate  }\DecValTok{1}      \FloatTok{89.6} \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}     \FloatTok{480.9} \DecValTok{193356} \FloatTok{347.34}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}    \FloatTok{5437.2} \DecValTok{198312} \FloatTok{348.35}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}      \FloatTok{47.4} \DecValTok{192828} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}      \FloatTok{40.2} \DecValTok{192835} \FloatTok{351.23}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}      \FloatTok{13.6} \DecValTok{192861} \FloatTok{351.23}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}   \FloatTok{30863.2} \DecValTok{223738} \FloatTok{353.17}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{347.26}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ impressions }\SpecialCharTok{+}\NormalTok{ display}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{{-}}\NormalTok{ impressions      }\DecValTok{1}       \DecValTok{399} \DecValTok{193364} \FloatTok{345.34}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}     \DecValTok{14392} \DecValTok{207357} \FloatTok{348.13}
   \SpecialCharTok{+}\NormalTok{ conversion.rate  }\DecValTok{1}        \DecValTok{90} \DecValTok{192875} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}        \DecValTok{52} \DecValTok{192913} \FloatTok{349.24}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}        \DecValTok{33} \DecValTok{192932} \FloatTok{349.25}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}         \DecValTok{8} \DecValTok{192957} \FloatTok{349.25}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}     \DecValTok{35038} \DecValTok{228002} \FloatTok{351.93}
   
\NormalTok{   Step}\SpecialCharTok{:}\NormalTok{  AIC}\OtherTok{=}\FloatTok{345.34}
\NormalTok{   revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display}
   
\NormalTok{                     Df Sum of Sq    RSS    AIC}
   \SpecialCharTok{\textless{}}\NormalTok{none}\SpecialCharTok{\textgreater{}}                         \DecValTok{193364} \FloatTok{345.34}
   \SpecialCharTok{+}\NormalTok{ impressions      }\DecValTok{1}       \DecValTok{399} \DecValTok{192965} \FloatTok{347.26}
   \SpecialCharTok{+}\NormalTok{ transactions     }\DecValTok{1}       \DecValTok{215} \DecValTok{193149} \FloatTok{347.29}
   \SpecialCharTok{+}\NormalTok{ conversion.rate  }\DecValTok{1}         \DecValTok{8} \DecValTok{193356} \FloatTok{347.34}
   \SpecialCharTok{+}\NormalTok{ click.rate       }\DecValTok{1}         \DecValTok{6} \DecValTok{193358} \FloatTok{347.34}
   \SpecialCharTok{+}\NormalTok{ spend            }\DecValTok{1}         \DecValTok{2} \DecValTok{193362} \FloatTok{347.34}
   \SpecialCharTok{{-}}\NormalTok{ display          }\DecValTok{1}     \DecValTok{91225} \DecValTok{284589} \FloatTok{358.80}
   \SpecialCharTok{{-}}\NormalTok{ clicks           }\DecValTok{1}    \DecValTok{606800} \DecValTok{800164} \FloatTok{400.15}
\end{Highlighting}
\end{Shaded}

The algorithm iteratively assesses each predictor's contribution, removing those that do not improve model performance or adding those that enhance it, based on AIC. For example, \texttt{spend} is removed in the first iteration as it does not significantly enhance the model. The stepwise process continues until no further improvements can be made, terminating after 6 iterations.

Tracking AIC values throughout the selection process allows us to quantify model improvements. The initial full model, which includes all predictors, has an AIC value of 355.21. After multiple iterations, the final model achieves a lower AIC value of 345.34, indicating a more efficient model with improved fit.

To examine the final selected model, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(ml\_stepwise)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{141.89}  \SpecialCharTok{{-}}\FloatTok{55.92}   \FloatTok{16.44}   \FloatTok{52.70}  \FloatTok{115.46} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\SpecialCharTok{{-}}\FloatTok{33.63248}   \FloatTok{28.68893}  \SpecialCharTok{{-}}\FloatTok{1.172} \FloatTok{0.248564}    
\NormalTok{   clicks        }\FloatTok{0.89517}    \FloatTok{0.08308}  \FloatTok{10.775} \FloatTok{5.76e{-}13} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   display      }\FloatTok{95.51462}   \FloatTok{22.86126}   \FloatTok{4.178} \FloatTok{0.000172} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{72.29}\NormalTok{ on }\DecValTok{37}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7822}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.7704} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{66.44}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{37}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{5.682e{-}13}
\end{Highlighting}
\end{Shaded}

Stepwise regression results in a more parsimonious model with only two predictors: \texttt{clicks} and \texttt{display}. The refined regression equation is:

\[
\hat{\text{revenue}} = -33.63 + 0.9 \cdot \text{clicks} + 95.51 \cdot \text{display}
\]

The final model demonstrates an improved fit compared to the initial full model. The \textbf{Residual Standard Error (RSE)}, which measures typical prediction error, has decreased from approximately 93.82 to 72.29, indicating improved accuracy. The \textbf{R-squared (\(R^2\))} value has increased from 62\% to 77\%, suggesting that a greater proportion of the variability in \texttt{revenue} is now explained by the selected predictors.
\end{example}

\subsection*{Strengths, Limitations, and Considerations for Stepwise Regression}\label{strengths-limitations-and-considerations-for-stepwise-regression}


Stepwise regression offers a systematic approach to model selection, balancing interpretability and efficiency. By iteratively refining the set of predictors, it helps identify an optimal model without manually testing every possible combination. However, stepwise regression also has important limitations that should be considered.

One key limitation is that the algorithm evaluates predictors sequentially rather than exhaustively considering all possible subsets of variables. This can sometimes result in suboptimal models, especially when strong predictor interactions are ignored. Additionally, stepwise regression is prone to \textbf{overfitting}, particularly in small datasets with many predictors. Overfitting occurs when the model captures random noise rather than meaningful relationships, reducing its generalizability to new data. Furthermore, the presence of \textbf{multicollinearity} among predictors can distort coefficient estimates and p-values, leading to misleading conclusions.

For high-dimensional datasets or cases where predictor selection must be more robust, alternative methods such as \emph{LASSO} (Least Absolute Shrinkage and Selection Operator) and \emph{Ridge Regression} are often preferred. These techniques introduce regularization, which helps stabilize model estimates and improve predictive accuracy by penalizing overly complex models. For further exploration, refer to \href{https://www.statlearning.com}{An Introduction to Statistical Learning with Applications in R} \citep{gareth2013introduction}.

Careful model specification is a crucial step in regression analysis. By selecting predictors systematically and evaluating model performance with appropriate criteria, we can construct models that are both accurate and interpretable. While stepwise regression has its limitations, it remains a widely used method for predictor selection in datasets of moderate size. Its ability to enhance predictive performance while maintaining simplicity makes it a valuable tool in data-driven decision-making.

\section{Extending Linear Models to Capture Non-Linear Relationships}\label{extending-linear-models-to-capture-non-linear-relationships}

Thus far, we have focused on linear regression models, which are simple, interpretable, and easy to implement. While these models work well when relationships between predictors and response variables are approximately linear, their predictive power is limited when the relationships exhibit curvature or other forms of non-linearity. In such cases, assuming a strictly linear relationship can lead to poor model performance and inaccurate predictions.

Earlier, we explored techniques such as stepwise regression (Section \ref{sec-stepwise-regression}) to refine model selection by reducing complexity and addressing multicollinearity. However, these methods do not account for non-linearity in relationships between predictors and the response variable. To address this limitation while maintaining model interpretability, we turn to \emph{polynomial regression}, an extension of linear regression that introduces non-linear terms.

\subsection*{The Need for Non-Linear Regression}\label{the-need-for-non-linear-regression}


Linear regression assumes a constant rate of change between predictors and the response variable, resulting in a straight-line relationship. However, many real-world datasets exhibit more complex patterns. Consider the scatter plot in Figure \ref{fig:scoter-plot-non-reg}, which depicts the relationship between \texttt{unit.price} (house price per unit area) and \texttt{house.age} (age of the house) from the \emph{house} dataset. The orange line represents a simple linear regression fit, which does not adequately capture the curvature in the data.

To better model this relationship, we can introduce non-linear terms into the regression equation. If the data suggests a quadratic trend, the model can be expressed as:

\[
unit.price = b_0 + b_1 \cdot house.age + b_2 \cdot house.age^2
\]

This equation incorporates both \texttt{house.age} and its squared term (\texttt{house.age\^{}2}), allowing for a curved relationship between the predictor and response variable. Although polynomial regression introduces non-linear predictors, the model remains a \emph{linear regression model} because the coefficients (\(b_0, b_1, b_2\)) are estimated using standard least squares methods. The blue curve in Figure \ref{fig:scoter-plot-non-reg} illustrates the improved fit of a quadratic regression model, which captures the pattern in the data more effectively than the simple linear model.

\textbackslash begin\{figure\}{[}H{]}

\{\centering \includegraphics[width=1\linewidth]{10_Regression_files/figure-latex/scoter-plot-non-reg-1}

\}

\textbackslash caption\{Scatter plot of house price (\$) versus house age (years) for the house dataset, with the fitted simple linear regression line in orange and the quadratic regression curve in blue.\}\label{fig:scoter-plot-non-reg}
\textbackslash end\{figure\}

This example highlights the need for non-linear regression techniques when the assumption of linearity does not hold. By incorporating polynomial terms, we can improve model accuracy while retaining interpretability, ensuring that predictions align more closely with real-world data patterns.

\section{Polynomial Regression}\label{polynomial-regression}

Polynomial regression extends linear regression by incorporating higher-degree terms of the predictor variable, such as squared (\(x^2\)) or cubic (\(x^3\)) terms. This allows the model to capture non-linear relationships while remaining \emph{linear in the coefficients}, meaning it can still be estimated using least squares. The general polynomial regression model is given by:

\[
\hat{y} = b_0 + b_1 \cdot x + b_2 \cdot x^2 + \dots + b_d \cdot x^d
\]

where \(d\) represents the degree of the polynomial. While polynomial regression provides flexibility, higher-degree polynomials (\(d > 3\)) can lead to overfitting, capturing noise rather than meaningful patterns, particularly at the boundaries of the predictor range.

\begin{example}
\protect\hypertarget{exm:ex-polynomial-regression}{}\label{exm:ex-polynomial-regression}To illustrate polynomial regression, we use the \emph{house} dataset from the \textbf{liver} package. This dataset includes housing prices and features such as age, proximity to public transport, and local amenities. Our goal is to model \texttt{unit.price} (house price per unit area) as a function of \texttt{house.age} and compare the performance of simple linear regression to polynomial regression.

First, we load the dataset and examine its structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(house)}

\FunctionTok{str}\NormalTok{(house)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{414}\NormalTok{ obs. of  }\DecValTok{6}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ house.age      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{32} \FloatTok{19.5} \FloatTok{13.3} \FloatTok{13.3} \DecValTok{5} \FloatTok{7.1} \FloatTok{34.5} \FloatTok{20.3} \FloatTok{31.7} \FloatTok{17.9}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ distance.to.MRT}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{84.9} \FloatTok{306.6} \DecValTok{562} \DecValTok{562} \FloatTok{390.6}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ stores.number  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{10} \DecValTok{9} \DecValTok{5} \DecValTok{5} \DecValTok{5} \DecValTok{3} \DecValTok{7} \DecValTok{6} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ latitude       }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ longitude      }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{122} \DecValTok{122} \DecValTok{122} \DecValTok{122} \DecValTok{122}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ unit.price     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{37.9} \FloatTok{42.2} \FloatTok{47.3} \FloatTok{54.8} \FloatTok{43.1} \FloatTok{32.1} \FloatTok{40.3} \FloatTok{46.7} \FloatTok{18.8} \FloatTok{22.1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset consists of 414 observations and 6 variables. The target variable is \texttt{unit.price}, while predictors include \texttt{house.age} (years), \texttt{distance.to.MRT} (distance to the nearest MRT station), \texttt{stores.number} (number of nearby convenience stores), \texttt{latitude}, and \texttt{longitude}.

We begin by fitting a simple linear regression model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_reg\_house }\OtherTok{=} \FunctionTok{lm}\NormalTok{(unit.price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ house.age, }\AttributeTok{data =}\NormalTok{ house)}

\FunctionTok{summary}\NormalTok{(simple\_reg\_house)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ unit.price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ house.age, }\AttributeTok{data =}\NormalTok{ house)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{31.113} \SpecialCharTok{{-}}\FloatTok{10.738}   \FloatTok{1.626}   \FloatTok{8.199}  \FloatTok{77.781} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{               Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept) }\FloatTok{42.43470}    \FloatTok{1.21098}  \FloatTok{35.042}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
\NormalTok{   house.age   }\SpecialCharTok{{-}}\FloatTok{0.25149}    \FloatTok{0.05752}  \SpecialCharTok{{-}}\FloatTok{4.372} \FloatTok{1.56e{-}05} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{13.32}\NormalTok{ on }\DecValTok{412}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.04434}\NormalTok{,    Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.04202} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{19.11}\NormalTok{ on }\DecValTok{1}\NormalTok{ and }\DecValTok{412}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \FloatTok{1.56e{-}05}
\end{Highlighting}
\end{Shaded}

The \emph{R-squared (\(R^2\))} value for this model is 0.04, indicating that only 4.43\% of the variability in house prices is explained by \texttt{house.age}. This suggests that the linear model does not fully capture the relationship.

Next, we fit a quadratic polynomial regression model to introduce curvature:

\[
unit.price = b_0 + b_1 \cdot house.age + b_2 \cdot house.age^2
\]

This can be implemented in R using the \texttt{poly()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg\_nonlinear\_house }\OtherTok{=} \FunctionTok{lm}\NormalTok{(unit.price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ house)}

\FunctionTok{summary}\NormalTok{(reg\_nonlinear\_house)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ unit.price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ house)}
   
\NormalTok{   Residuals}\SpecialCharTok{:}
\NormalTok{       Min      }\DecValTok{1}\NormalTok{Q  Median      }\DecValTok{3}\NormalTok{Q     Max }
   \SpecialCharTok{{-}}\FloatTok{26.542}  \SpecialCharTok{{-}}\FloatTok{9.085}  \SpecialCharTok{{-}}\FloatTok{0.445}   \FloatTok{8.260}  \FloatTok{79.961} 
   
\NormalTok{   Coefficients}\SpecialCharTok{:}
\NormalTok{                       Estimate Std. Error t value }\FunctionTok{Pr}\NormalTok{(}\SpecialCharTok{\textgreater{}}\ErrorTok{|}\NormalTok{t}\SpecialCharTok{|}\NormalTok{)    }
\NormalTok{   (Intercept)           }\FloatTok{37.980}      \FloatTok{0.599}  \FloatTok{63.406}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
   \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{)}\DecValTok{1}  \SpecialCharTok{{-}}\FloatTok{58.225}     \FloatTok{12.188}  \SpecialCharTok{{-}}\FloatTok{4.777} \FloatTok{2.48e{-}06} \SpecialCharTok{**}\ErrorTok{*}
   \FunctionTok{poly}\NormalTok{(house.age, }\DecValTok{2}\NormalTok{)}\DecValTok{2}  \FloatTok{109.635}     \FloatTok{12.188}   \FloatTok{8.995}  \SpecialCharTok{\textless{}} \FloatTok{2e{-}16} \SpecialCharTok{**}\ErrorTok{*}
   \SpecialCharTok{{-}{-}{-}}
\NormalTok{   Signif. codes}\SpecialCharTok{:}  \DecValTok{0} \StringTok{\textquotesingle{}***\textquotesingle{}} \FloatTok{0.001} \StringTok{\textquotesingle{}**\textquotesingle{}} \FloatTok{0.01} \StringTok{\textquotesingle{}*\textquotesingle{}} \FloatTok{0.05} \StringTok{\textquotesingle{}.\textquotesingle{}} \FloatTok{0.1} \StringTok{\textquotesingle{} \textquotesingle{}} \DecValTok{1}
   
\NormalTok{   Residual standard error}\SpecialCharTok{:} \FloatTok{12.19}\NormalTok{ on }\DecValTok{411}\NormalTok{ degrees of freedom}
\NormalTok{   Multiple R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.2015}\NormalTok{, Adjusted R}\SpecialCharTok{{-}}\NormalTok{squared}\SpecialCharTok{:}  \FloatTok{0.1977} 
\NormalTok{   F}\SpecialCharTok{{-}}\NormalTok{statistic}\SpecialCharTok{:} \FloatTok{51.87}\NormalTok{ on }\DecValTok{2}\NormalTok{ and }\DecValTok{411}\NormalTok{ DF,  p}\SpecialCharTok{{-}}\NormalTok{value}\SpecialCharTok{:} \ErrorTok{\textless{}} \FloatTok{2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The quadratic model achieves a significantly higher \emph{R-squared (\(R^2\))} value of 0.2, compared to the simple regression model. Additionally, the \emph{Residual Standard Error (RSE)} is lower, indicating smaller prediction errors. These improvements confirm that incorporating a quadratic term better captures the non-linear relationship between house age and price.
\end{example}

Polynomial regression effectively extends linear regression by allowing for curvature in the data. However, selecting an appropriate polynomial degree is crucial to avoid overfitting. More advanced techniques, such as splines and generalized additive models, provide additional flexibility while addressing some of the limitations of polynomial regression. These techniques are discussed in Chapter 7 of \href{https://www.statlearning.com}{An Introduction to Statistical Learning with Applications in R} \citep{gareth2013introduction}.

\section{Diagnosing and Validating Regression Models}\label{diagnosing-and-validating-regression-models}

Before deploying a regression model, it is essential to validate its assumptions. Ignoring these assumptions is akin to constructing a house on an unstable foundation---predictions based on an invalid model can lead to misleading conclusions and costly mistakes. Model diagnostics ensure that the model is robust, reliable, and appropriate for making predictions.

Linear regression models rely on several key assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Independence}: Observations should be independent, meaning the response for one observation does not depend on another.\\
\item
  \textbf{Linearity}: The relationship between the predictor(s) and the response variable should be approximately linear. Scatter plots of predictors against the response variable help assess this assumption.\\
\item
  \textbf{Normality}: The residuals (errors) should follow a normal distribution, which can be assessed visually using a Q-Q plot.\\
\item
  \textbf{Constant Variance (Homoscedasticity)}: The residuals should exhibit constant variance across all levels of the predictor(s). A residuals vs.~fitted values plot is typically used to check this assumption.
\end{enumerate}

Violations of these assumptions can undermine the validity of statistical inferences, leading to unreliable predictions and inaccurate parameter estimates.

\begin{example}
\protect\hypertarget{exm:ex-diagnosing-regression}{}\label{exm:ex-diagnosing-regression}To demonstrate model diagnostics, we evaluate the assumptions of the multiple regression model constructed in Example \ref{exm:ex-stepwise-regression} using the \emph{marketing} dataset. The fitted model predicts daily revenue (\texttt{revenue}) based on \texttt{clicks} and \texttt{display}.

We generate diagnostic plots for the model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ml\_stepwise }\OtherTok{=} \FunctionTok{lm}\NormalTok{(revenue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ clicks }\SpecialCharTok{+}\NormalTok{ display, }\AttributeTok{data =}\NormalTok{ marketing)}

\FunctionTok{plot}\NormalTok{(ml\_stepwise)  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=0.5\linewidth]{10_Regression_files/figure-latex/model-diagnostics-1} \includegraphics[width=0.5\linewidth]{10_Regression_files/figure-latex/model-diagnostics-2} \includegraphics[width=0.5\linewidth]{10_Regression_files/figure-latex/model-diagnostics-3} \includegraphics[width=0.5\linewidth]{10_Regression_files/figure-latex/model-diagnostics-4} \caption{Diagnostic plots for assessing regression model assumptions.}\label{fig:model-diagnostics}
\end{figure}

These diagnostic plots provide insights into the validity of the model's assumptions.

\begin{itemize}
\tightlist
\item
  The \textbf{Normal Q-Q plot} (upper-right) assesses whether residuals follow a normal distribution. If the points lie approximately along a straight line, the assumption of normality is satisfied. In this case, the residuals closely follow the theoretical normal distribution, supporting the assumption.\\
\item
  The \textbf{Residuals vs.~Fitted plot} (upper-left) checks for both linearity and homoscedasticity. A random scatter pattern without discernible structure supports the assumption of linearity, while an even vertical spread across fitted values confirms constant variance. Here, the residuals appear randomly distributed, suggesting that these assumptions hold.\\
\item
  The \textbf{Independence assumption} is not explicitly tested with diagnostic plots but depends on the dataset structure. In the \emph{marketing} dataset, daily revenue is unlikely to be influenced by prior days' revenue, making the independence assumption reasonable.
\end{itemize}

Based on these diagnostics, the regression model satisfies the required assumptions, confirming its suitability for inference and prediction. Failing to check these assumptions could result in unreliable results, underscoring the importance of model validation.
\end{example}

When assumptions are violated, alternative approaches may be necessary. \textbf{Robust regression} methods can be employed when normality or homoscedasticity assumptions do not hold. \textbf{Non-linear regression} techniques, including polynomial regression and splines, can address cases where relationships deviate from linearity. \textbf{Transformations of variables}, such as logarithmic or square root transformations, can also help stabilize variance and improve model fit.

Beyond assumption checks, cross-validation and out-of-sample testing provide additional validation by assessing how well the model generalizes to new data. These techniques prevent overfitting and ensure that model performance is not driven by noise in the training data.

Validating regression models is fundamental to producing reliable, interpretable, and actionable results. By following best practices in model diagnostics, we strengthen the statistical foundation of our analyses and enhance the trustworthiness of predictions.

\section{Exercises}\label{regression-exercises}

The exercises are structured to test theoretical understanding, interpretation of regression outputs, and practical implementation in \textbf{R} using datasets from the \textbf{liver} package.

\subsection*{Simple and Multiple Linear Regression (House, Insurance, and Cereal Datasets)}\label{simple-and-multiple-linear-regression-house-insurance-and-cereal-datasets}


\subsubsection*{Conceptual Questions}\label{conceptual-questions-6}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain the difference between \emph{simple linear regression} and \emph{multiple linear regression}.\\
\item
  What are the key assumptions of linear regression? How do these assumptions impact model performance?\\
\item
  Define and interpret the \emph{R-squared (\(R^2\))} value in a regression model.\\
\item
  Explain the purpose of the \emph{Residual Standard Error (RSE)} and how it differs from \(R^2\).\\
\item
  How does \emph{multicollinearity} affect a multiple regression model? How can it be detected?\\
\item
  What is the difference between \emph{Adjusted \(R^2\)} and \emph{\(R^2\)}? Why is Adjusted \(R^2\) preferred in multiple regression?
\item
  What are the advantages of using \emph{categorical variables} in a regression model? How does R handle categorical variables?
\end{enumerate}

\subsubsection*{Practical Exercises Using the House Dataset}\label{practical-exercises-using-the-house-dataset}


Load the \emph{house} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(house, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Fit a \emph{simple linear regression} model to predict \texttt{unit.price} based on \texttt{house.age}. Display and interpret the summary of the model.\\
\item
  Extend the model by fitting a \emph{multiple linear regression} model using \texttt{house.age}, \texttt{distance.to.MRT}, and \texttt{stores.number} as predictors. Interpret the coefficient estimates.\\
\item
  Use the \texttt{predict()} function to estimate house prices for properties with an age of 10, 20, and 30 years.\\
\item
  Assess whether \texttt{latitude} and \texttt{longitude} improve the model's predictive ability.\\
\item
  Evaluate the \emph{Residual Standard Error (RSE)} and \emph{\(R^2\)} of the model. What do these values tell you about model performance?\\
\item
  Create a \emph{residual plot} for the model and analyze whether the residuals appear randomly distributed.\\
\item
  Generate a \emph{Q-Q plot} for the residuals. What does it reveal about the normality assumption?
\end{enumerate}

\subsubsection*{Practical Exercises Using the Insurance Dataset}\label{practical-exercises-using-the-insurance-dataset}


Load the \emph{insurance} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(insurance, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\tightlist
\item
  Fit a multiple linear regression model predicting \texttt{charges} based on \texttt{age}, \texttt{bmi}, \texttt{children}, and \texttt{smoker}.\\
\item
  Interpret the coefficient of \texttt{smoker}. What does it suggest about the impact of smoking on insurance charges?\\
\item
  Assess whether \emph{interaction effects} exist between \texttt{age} and \texttt{bmi}.\\
\item
  Evaluate the model's Adjusted \(R^2\). Does adding \texttt{region} as a predictor improve the model?\\
\item
  Perform a \emph{stepwise regression} to determine the best subset of predictors.
\end{enumerate}

\subsubsection*{Practical Exercises Using the Cereal Dataset}\label{practical-exercises-using-the-cereal-dataset}


Load the \emph{cereal} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cereal, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{19}
\tightlist
\item
  Fit a multiple linear regression model predicting \texttt{rating} based on \texttt{calories}, \texttt{protein}, \texttt{sugars}, and \texttt{fiber}.\\
\item
  Based on the model summary, which predictor has the strongest impact on \texttt{rating}?\\
\item
  Does \texttt{sodium} significantly affect \texttt{rating}? Should it be included in the model?\\
\item
  Compare the effects of \texttt{fiber} and \texttt{sugars}. Which has a larger impact on \texttt{rating}?\\
\item
  Apply \emph{stepwise regression} to refine the model and identify the most relevant predictors.
\end{enumerate}

\subsection*{Polynomial Regression (House Dataset)}\label{polynomial-regression-house-dataset}


\subsubsection*{Conceptual Questions}\label{conceptual-questions-7}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{24}
\tightlist
\item
  What is \emph{polynomial regression}, and how does it differ from multiple linear regression?\\
\item
  Why does polynomial regression remain a \emph{linear model} even though it includes non-linear terms?\\
\item
  What is the risk of using \emph{high-degree polynomial regression}?\\
\item
  How do you determine the optimal degree for a polynomial regression model?\\
\item
  How can overfitting be detected in polynomial regression?
\end{enumerate}

\subsubsection*{Practical Exercises Using the House Dataset}\label{practical-exercises-using-the-house-dataset-1}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{29}
\tightlist
\item
  Fit a \emph{quadratic polynomial regression} model predicting \texttt{unit.price} using \texttt{house.age}. Compare it to a simple linear regression model.\\
\item
  Fit a \emph{cubic polynomial regression} model. Does it perform better than the quadratic model?\\
\item
  Plot the simple, quadratic, and cubic regression fits on the same graph.\\
\item
  Use \emph{cross-validation} to determine the best polynomial degree.\\
\item
  Interpret the coefficients of the quadratic regression model.
\end{enumerate}

\subsection*{Logistic Regression (Bank Dataset)}\label{logistic-regression-bank-dataset}


\subsubsection*{Conceptual Questions}\label{conceptual-questions-8}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{34}
\tightlist
\item
  What is the difference between \emph{linear regression} and \emph{logistic regression}?\\
\item
  Why does logistic regression use the \emph{logit function} instead of fitting a linear model directly?\\
\item
  How do you interpret the \emph{odds ratio} in a logistic regression model?\\
\item
  What is the \emph{confusion matrix}, and how is it used to evaluate logistic regression?\\
\item
  What is the difference between \emph{precision} and \emph{recall} in classification models?
\end{enumerate}

\subsubsection*{Practical Exercises Using the Bank Dataset}\label{practical-exercises-using-the-bank-dataset}


Load the \emph{bank} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(bank, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{39}
\tightlist
\item
  Fit a \emph{logistic regression} model predicting whether a customer subscribed to a term deposit (\texttt{y}) based on \texttt{age}, \texttt{balance}, and \texttt{duration}.\\
\item
  Interpret the coefficients in terms of \emph{odds ratios}.\\
\item
  Use the \texttt{predict()} function to estimate the probability of subscription for a new customer.\\
\item
  Create a \emph{confusion matrix} to evaluate the model's performance.\\
\item
  Compute the \emph{accuracy, precision, recall, and F1-score} of the model.\\
\item
  Use \emph{stepwise regression} to refine the logistic model.\\
\item
  Evaluate the \emph{receiver operating characteristic (ROC) curve} for the model.
\end{enumerate}

\subsection*{Stepwise Regression (House Dataset)}\label{stepwise-regression-house-dataset}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{46}
\tightlist
\item
  Apply \emph{stepwise regression} to the \texttt{house} dataset to identify the most relevant predictors for \texttt{unit.price}.\\
\item
  Compare the stepwise regression model to the full multiple regression model. Does it perform better?\\
\item
  Assess whether \emph{interaction terms} improve the stepwise regression model.
\end{enumerate}

\subsection*{Model Diagnostics and Validation}\label{model-diagnostics-and-validation}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{49}
\tightlist
\item
  Check the \emph{assumptions of linear regression} for the multiple regression model on the \emph{house} dataset.\\
\item
  Generate \emph{diagnostic plots} (residuals vs.~fitted, Q-Q plot, scale-location plot).\\
\item
  Use \emph{cross-validation} to assess model performance.\\
\item
  Compare the \emph{mean squared error (MSE)} of different models.\\
\item
  Assess whether a \emph{log-transformation} improves model performance.
\end{enumerate}

\chapter{Decision Trees and Random Forests}\label{chapter-tree}

Imagine a bank evaluating loan applications. Given details such as income, age, credit history, and debt-to-income ratio, how does the bank decide whether to approve or reject a loan? Similarly, how do online retailers recommend products based on customer preferences? These decisions, which mimic human reasoning, are often powered by \emph{decision trees}---a simple yet powerful machine learning technique that classifies data by following a series of logical rules.

Decision trees are widely used in various domains, from medical diagnosis and fraud detection to customer segmentation and automated decision-making. Their intuitive nature makes them highly interpretable, enabling data-driven decision-making without requiring deep mathematical expertise. However, while individual trees are easy to understand, they are prone to overfitting, capturing noise in the data rather than general patterns. \emph{Random forests} address this limitation by combining multiple decision trees to produce a more accurate and stable model.

To see decision trees in action, consider the example in Figure \ref{fig:tree-0}, which predicts whether a customer's credit risk is classified as ``good'' or ``bad'' based on features such as \texttt{age} and \texttt{income}. This tree is trained on the \texttt{risk} dataset, introduced in Chapter \ref{chapter-bayes}, and consists of decision nodes representing yes/no questions, such as whether yearly income is below â‚¬36,000 (\texttt{income\ \textless{}\ 36e+3}) or whether age is greater than 29. The final classification is determined at the terminal nodes, also known as leaves.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\linewidth]{11_Decision_Tree_files/figure-latex/tree-0-1} 

}

\caption{Decision tree for predicting credit risk based on age and income.}\label{fig:tree-0}
\end{figure}

Decision trees are highly interpretable, making them especially valuable in domains such as finance, healthcare, and marketing, where understanding model decisions is as important as accuracy. Their structured form allows for easy visualization of decision pathways, helping businesses with customer segmentation, risk assessment, and process optimization.

In this chapter, we explore how decision trees and random forests work, their strengths and limitations, and how they can be applied to solve real-world problems. By the end of the chapter, you will learn:

\begin{itemize}
\tightlist
\item
  The mechanics behind \emph{decision trees} and \emph{random forests}.\\
\item
  How to build, evaluate, and fine-tune decision trees using algorithms such as \emph{CART} and \emph{C5.0}.\\
\item
  How random forests improve predictive accuracy and generalization through ensemble learning.
\end{itemize}

We begin by examining the core principles of decision trees, including how they make predictions and how their performance can be optimized.

\section{How Decision Trees Work}\label{how-decision-trees-work}

A decision tree classifies or predicts outcomes by systematically dividing a dataset into smaller, more uniform groups based on feature values. Each split refines the classification or prediction, creating a structured, tree-like model. This \emph{divide-and-conquer} approach is widely used in classification and regression due to its intuitive nature and ability to model complex decision-making processes.

At each step, the algorithm selects the feature and threshold that best separate the data. This decision is based on metrics such as the \emph{Gini Index}, \emph{Entropy}, or \emph{Variance Reduction}, depending on the problem type. The tree continues growing until it meets a stopping criterion, such as reaching a predefined maximum depth, forming perfectly homogeneous subsets, or when further splits no longer improve performance.

To see this process in action, consider a simple dataset with two features (\(x_1\) and \(x_2\)) and two classes (Class A and Class B), as shown in Figure \ref{fig:tree-1}. The dataset consists of 50 data points, and the goal is to classify them into their respective categories.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/ch11_ex_tree_1} 

}

\caption{A two-dimensional toy dataset (50 observations) with two classes (Class A and Class B), used to illustrate how to build Decision Trees.}\label{fig:tree-1}
\end{figure}

The process begins by identifying the feature and threshold that best separate the two classes. The algorithm evaluates all possible splits and selects the one that maximizes homogeneity in the resulting subsets. For this dataset, the optimal split occurs at \(x_1 = 10\), dividing the dataset into two regions:

\begin{itemize}
\tightlist
\item
  The left region contains data points where \(x_1 < 10\), with 80\% belonging to Class A and 20\% to Class B.
\item
  The right region contains data points where \(x_1 \geq 10\), with 28\% in Class A and 72\% in Class B.
\end{itemize}

This first split is illustrated in Figure \ref{fig:tree-2}, where the decision boundary is drawn at \(x_1 = 10\).

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/ch11_ex_tree_2} 

}

\caption{Left: Decision boundary for a tree with depth 1. Right: The corresponding Decision Tree.}\label{fig:tree-2}
\end{figure}

Although this split improves class separation, some overlap remains, suggesting that further refinement is needed. The tree-building process continues by introducing additional splits based on \(x_2\), creating smaller, more homogeneous groups.

In Figure \ref{fig:tree-3}, the algorithm identifies new thresholds: \(x_2 = 6\) for the left region and \(x_2 = 8\) for the right region. These additional splits refine the classification process, improving the model's ability to distinguish between the two classes.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/ch11_ex_tree_3} 

}

\caption{Left: Decision boundary for a tree with depth 2. Right: The corresponding Decision Tree.}\label{fig:tree-3}
\end{figure}

This recursive process continues until the tree reaches a stopping criterion. Figure \ref{fig:tree-4} shows a fully grown tree with a depth of 5, demonstrating how decision trees create increasingly refined decision boundaries.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/ch11_ex_tree_4} 

}

\caption{Left: Decision boundary for a tree with depth 5. Right: The corresponding Decision Tree.}\label{fig:tree-4}
\end{figure}

At this depth, the tree has created highly specific decision boundaries that closely match the training data. While this deep tree perfectly classifies the training data, it may not generalize well to new observations. The model has likely captured not just meaningful patterns but also noise, a problem known as \emph{overfitting}. Overfitted trees perform well on training data but struggle to make accurate predictions on unseen data.

\subsection*{Making Predictions with a Decision Tree}\label{making-predictions-with-a-decision-tree}


After a decision tree is built, making predictions involves following the decision rules from the root node down to a leaf. Each split refines the prediction until a final classification or numerical estimate is reached.

For classification tasks, the tree assigns a new observation to the most common class in the leaf where it ends up. For regression tasks, the predicted outcome is the average target value of the data points in that leaf.

To illustrate, consider a new data point with \(x_1 = 8\) and \(x_2 = 4\) in Figure \ref{fig:tree-3}. The tree classifies it by following these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Since \(x_1 = 8\), the point moves to the left branch (\(x_1 < 10\)).\\
\item
  Since \(x_2 = 4\), the point moves to the lower-left region (\(x_2 < 6\)).\\
\item
  The final leaf node assigns the point to Class A with 80\% confidence.
\end{enumerate}

This step-by-step traversal of the tree ensures that predictions remain interpretable, making decision trees particularly useful in applications where understanding how a prediction was made is as important as accuracy.

\subsection*{Controlling Tree Complexity}\label{controlling-tree-complexity}


While decision trees are powerful, they can easily grow too complex, capturing noise rather than meaningful patterns. To improve generalization, various techniques help regulate tree complexity and prevent overfitting.

One approach is \textbf{pre-pruning}, which restricts tree growth during training by enforcing stopping criteria. These may include setting a maximum tree depth, requiring a minimum number of samples per node, or enforcing a minimum improvement in information gain at each split. By stopping early, pre-pruning prevents the tree from fitting the data too closely, reducing the risk of overfitting.

Alternatively, \textbf{post-pruning} allows the tree to grow fully before simplifying it. Once the tree is built, unnecessary nodes that contribute little to predictive accuracy are removed or merged. This approach often improves interpretability while maintaining performance.

The choice between pre-pruning and post-pruning depends on the dataset and problem at hand. Additionally, the way splits are chosen---using criteria such as \emph{Gini Index}, \emph{Entropy}, or \emph{Variance Reduction}---plays a crucial role in determining tree performance. These will be explored in later sections.

\section{Classification and Regression Trees (CART)}\label{classification-and-regression-trees-cart}

The classification and regression trees (CART) algorithm, introduced by Breiman et al.~in 1984 \citep{breiman1984classification}, is one of the most widely used methods for constructing decision trees. CART generates binary trees, meaning that each decision node splits the data into exactly two branches. It recursively partitions the training dataset into subsets of records that share similar values for the target variable. This partitioning is guided by a splitting criterion designed to minimize impurity in the resulting subsets. For classification tasks, CART employs measures such as the Gini index or entropy to evaluate splits, while for regression tasks, it minimizes the variance of the target variable.

The Gini index is commonly used to measure impurity in classification tasks. It is calculated as:

\[
Gini = 1 - \sum_{i=1}^k p_i^2
\]

where \(p_i\) represents the proportion of samples in the node that belong to class \(i\), and \(k\) is the total number of classes. A node is considered pure when all data points in it belong to a single class, resulting in a Gini index of zero. During tree construction, CART selects the feature and threshold that result in the largest reduction in impurity, splitting the data to create two more homogeneous child nodes.

The recursive nature of CART can lead to highly detailed trees that fit the training data perfectly. While this minimizes the error rate on the training set, it often results in overfitting, where the tree becomes overly complex and fails to generalize to unseen data. To mitigate this, CART employs pruning techniques to simplify the tree.

Pruning involves trimming branches that do not contribute meaningfully to predictive accuracy on a validation set. This is achieved by finding an adjusted error rate that penalizes overly complex trees with too many leaf nodes. The goal of pruning is to balance accuracy and simplicity, enhancing the tree's ability to generalize to new data. The pruning process is discussed in detail by Breiman et al. \citep{breiman1984classification}.

Despite its simplicity, CART is widely used in practice due to its interpretability, versatility, and ability to handle both classification and regression tasks. The tree structure provides an intuitive way to visualize decision-making, making it highly explainable. Additionally, CART works well with both numerical and categorical data, making it applicable across a range of domains.

However, CART has limitations. The algorithm tends to produce deep trees that may overfit the training data, particularly when the dataset is small or noisy. Its reliance on greedy splitting can also result in suboptimal splits, as it evaluates one feature at a time rather than considering all possible combinations.

To address these shortcomings, more advanced algorithms have been developed, such as C5.0, which incorporates improvements in splitting and pruning techniques, and random forests, which combine multiple decision trees to create more robust models. These approaches build on the foundations of CART, improving performance and reducing susceptibility to overfitting. The following sections explore these methods in detail.

\section{The C5.0 Algorithm for Building Decision Trees}\label{the-c5.0-algorithm-for-building-decision-trees}

The C5.0 algorithm, developed by J. Ross Quinlan, is an advanced iteration of earlier decision tree models, including C4.5 and ID3 (Iterative Dichotomiser 3). It introduces improvements in efficiency, flexibility, and accuracy, making it a widely used approach in both academic research and practical applications. While a commercial version is available through \href{http://www.rulequest.com/}{RuleQuest}, an open-source implementation is integrated into R and other machine learning tools.

C5.0 differs from other decision tree algorithms, such as CART, in several ways. Unlike CART, which constructs strictly binary trees, C5.0 allows for multi-way splits, particularly for categorical attributes. This can lead to more compact and interpretable trees when dealing with variables that have many distinct categories. Another key distinction lies in how the algorithm evaluates node purity. While CART uses measures such as the Gini index or variance reduction, C5.0 relies on entropy and information gain, concepts derived from information theory.

Entropy measures the degree of disorder in a dataset. Higher entropy indicates greater diversity among classes, while lower entropy suggests more homogeneous groups. The goal of C5.0 is to identify feature splits that reduce entropy, leading to purer subsets at each step of tree construction. The entropy for a variable \(x\) with \(k\) classes is defined as:

\[
Entropy(x) = - \sum_{i=1}^k p_i \log_2(p_i)
\]

where \(p_i\) represents the proportion of samples in class \(i\). A dataset with equal distribution among classes has maximum entropy, whereas a dataset with all samples belonging to the same class has entropy equal to zero. Using this measure, the algorithm calculates information gain, which quantifies the reduction in entropy resulting from a particular split. Given a candidate split \(S\) that divides dataset \(T\) into subsets \(T_1, T_2, \dots, T_c\), the entropy after splitting is computed as:

\[
H_S(T) = \sum_{i=1}^c \frac{|T_i|}{|T|} \cdot Entropy(T_i)
\]

The information gain from the split is then:

\[
gain(S) = H(T) - H_S(T)
\]

where \(H(T)\) represents the entropy before the split. The algorithm evaluates all potential splits and selects the one that maximizes information gain, ensuring that each decision step results in purer subsets.

To illustrate how C5.0 constructs decision trees, consider its application to the \texttt{risk} dataset, which classifies a customer's credit risk as good or bad based on features such as \texttt{age} and \texttt{income}. Figure \ref{fig:tree-C50} shows a decision tree trained using the \texttt{C5.0} function from the \texttt{C50} package in R.

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth]{11_Decision_Tree_files/figure-latex/tree-C50-1} 

}

\caption{C5.0 Decision Tree for predicting credit risk based on age and income.}\label{fig:tree-C50}
\end{figure}

The tree demonstrates how C5.0 selects splits to separate classes. Unlike CART, which only allows binary splits, C5.0 enables multi-way splits when dealing with categorical features. This flexibility can lead to trees that are more concise and easier to interpret, particularly in datasets where categorical variables play a significant role.

C5.0 has several advantages over other decision tree algorithms. It is computationally efficient, making it well-suited for large datasets, and its ability to handle multi-way splits enables more nuanced decision-making. Additionally, it incorporates feature weighting, prioritizing the most informative predictors, which can improve model accuracy.

However, C5.0 is not without limitations. The trees it generates can become overly complex, particularly when categorical attributes contain many unique values, increasing the risk of overfitting. To mitigate this, pruning techniques can be applied to simplify the tree while preserving accuracy. Another challenge is the computational cost of evaluating multiple splits for categorical variables, which can increase processing time for large datasets, though C5.0's optimizations help reduce this impact.

In summary, C5.0 builds upon earlier decision tree models by leveraging entropy and information gain to construct accurate and interpretable decision rules. Its ability to create multi-way splits makes it particularly effective for categorical data, while its efficiency allows it to scale well. In the next section, we explore random forests, an ensemble learning technique that enhances decision tree models by combining multiple trees for improved accuracy and robustness.

\section{Random forests: an ensemble approach}\label{random-forests-an-ensemble-approach}

Decision trees are effective models, but they tend to overfit, particularly when grown to their full depth. Random forests address this limitation by combining multiple decision trees into an ensemble, producing a more accurate and robust model. Instead of relying on a single tree, random forests aggregate the predictions of many trees, reducing overfitting and improving performance on complex datasets.

The algorithm introduces two key elements of randomness to enhance model diversity:

\begin{itemize}
\tightlist
\item
  \textbf{Bootstrap aggregation (bagging):} Each tree is trained on a random subset of the training data, created by sampling with replacement. This means some observations appear multiple times in a tree's training data, while others may be excluded. This diversity ensures that each tree learns slightly different patterns.\\
\item
  \textbf{Random feature selection:} At each split, the algorithm considers a random subset of features instead of evaluating all features. This decorrelates the trees, forcing them to rely on different combinations of features.
\end{itemize}

Once the forest is built, the predictions from all trees are aggregated to produce the final output:

\begin{itemize}
\tightlist
\item
  For classification, the final prediction is determined by majority voting, where each tree votes for a class, and the most common class is selected.\\
\item
  For regression, the final output is the average of the predictions from all trees.
\end{itemize}

The strength of random forests lies in their ability to leverage diversity. Individually, each tree is trained on a limited subset of data and features, making it a weak learner. However, when combined, their collective predictions form a stronger model. By aggregating multiple trees, random forests reduce the risk of errors from any single tree dominating the overall prediction.

Additionally, the randomness introduced through feature selection ensures that no single feature dominates the model, making random forests particularly effective for datasets with correlated or redundant features. This feature-level decorrelation enhances their ability to generalize to unseen data.

\subsection*{Advantages and limitations of random forests}\label{advantages-and-limitations-of-random-forests}


\textbf{Advantages}

\begin{itemize}
\tightlist
\item
  Reduced overfitting: By averaging predictions from multiple trees, random forests smooth out noise and variance, leading to better generalization.\\
\item
  High accuracy: They perform well on both classification and regression tasks, particularly for datasets with non-linear relationships or high-dimensional feature spaces.\\
\item
  Feature importance ranking: The algorithm provides feature importance scores, helping to identify the most influential predictors.\\
\item
  Robustness: They are resilient to noise and outliers, as the ensemble effect reduces the impact of anomalies on the final prediction.\\
\item
  Flexibility: Random forests can handle both numerical and categorical data and adapt well to different types of problems.
\end{itemize}

\textbf{Limitations}

\begin{itemize}
\tightlist
\item
  Computational complexity: Training hundreds or thousands of trees can be computationally intensive, especially on large datasets. However, this can be mitigated through parallel processing, as each tree is built independently.\\
\item
  Reduced interpretability: While individual decision trees are easy to interpret, the ensemble nature of random forests makes it difficult to understand how individual features contribute to predictions.\\
\item
  Potential loss of fine details: Although random forests reduce variance, they may smooth over intricate relationships that a well-tuned single decision tree could capture.
\end{itemize}

Random forests balance accuracy and robustness, addressing many of the weaknesses of individual decision trees while retaining their strengths. They are particularly effective in scenarios with noisy or high-dimensional data. Their ability to compute feature importance scores also provides valuable insights into the drivers of model predictions, making them useful for both predictive modeling and exploratory data analysis.

Random forests have become one of the most widely used machine learning algorithms due to their versatility, reliability, and strong performance across a variety of applications. In the next section, we apply random forests, along with decision trees, to a case study predicting income levels. This practical example demonstrates how these models work and how they can be evaluated in real-world scenarios.

\section{Case Study: Who Can Earn More Than \$50K Per Year?}\label{tree-case-study}

Predicting income levels is an important task in fields such as finance, marketing, and public policy. Banks use income models to assess creditworthiness, employers analyze salary trends for compensation planning, and governments rely on income predictions for taxation and social welfare policies. In this case study, we explore how decision trees and random forests can be applied to classify individuals based on their likelihood of earning more than \$50,000 per year.

For this analysis, we use the \emph{adult} dataset, a well-known benchmark dataset sourced from the \href{https://www.census.gov}{US Census Bureau} and available in the \textbf{liver} package. This dataset was previously introduced in Section \ref{Data-pre-adult} as part of the data preparation chapter (\ref{chapter-data-prep}). It contains demographic and employment-related attributes, such as education, working hours, marital status, and occupation, all of which influence earning potential. The goal is to build a classification model that predicts whether an individual belongs to one of two income groups: \texttt{\textless{}=50K} or \texttt{\textgreater{}50K}, treating \texttt{income} as the target variable.

\subsection*{Overview of the Dataset}\label{overview-of-the-dataset-1}


The \emph{adult} dataset, available in the \textbf{liver} package, provides demographic and employment-related attributes to predict income levels. We can load it directly into R and examine its summary using the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}

\FunctionTok{data}\NormalTok{(adult)}

\FunctionTok{summary}\NormalTok{(adult)}
\NormalTok{         age              workclass      demogweight             education    }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{17.0}\NormalTok{   ?           }\SpecialCharTok{:} \DecValTok{2794}\NormalTok{   Min.   }\SpecialCharTok{:}  \DecValTok{12285}\NormalTok{   HS}\SpecialCharTok{{-}}\NormalTok{grad     }\SpecialCharTok{:}\DecValTok{15750}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{28.0}\NormalTok{   Gov         }\SpecialCharTok{:} \DecValTok{6536}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \DecValTok{117550}\NormalTok{   Some}\SpecialCharTok{{-}}\NormalTok{college}\SpecialCharTok{:}\DecValTok{10860}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{37.0}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{worked}\SpecialCharTok{:}   \DecValTok{10}\NormalTok{   Median }\SpecialCharTok{:} \DecValTok{178215}\NormalTok{   Bachelors   }\SpecialCharTok{:} \DecValTok{7962}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{38.6}\NormalTok{   Private     }\SpecialCharTok{:}\DecValTok{33780}\NormalTok{   Mean   }\SpecialCharTok{:} \DecValTok{189685}\NormalTok{   Masters     }\SpecialCharTok{:} \DecValTok{2627}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{48.0}\NormalTok{   Self}\SpecialCharTok{{-}}\NormalTok{emp    }\SpecialCharTok{:} \DecValTok{5457}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \DecValTok{237713}\NormalTok{   Assoc}\SpecialCharTok{{-}}\NormalTok{voc   }\SpecialCharTok{:} \DecValTok{2058}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{90.0}\NormalTok{   Without}\SpecialCharTok{{-}}\NormalTok{pay }\SpecialCharTok{:}   \DecValTok{21}\NormalTok{   Max.   }\SpecialCharTok{:}\DecValTok{1490400}   \DecValTok{11}\NormalTok{th        }\SpecialCharTok{:} \DecValTok{1812}  
\NormalTok{                                                          (Other)     }\SpecialCharTok{:} \DecValTok{7529}  
\NormalTok{    education.num         marital.status            occupation   }
\NormalTok{    Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   Divorced     }\SpecialCharTok{:} \DecValTok{6613}\NormalTok{   Craft}\SpecialCharTok{{-}}\NormalTok{repair   }\SpecialCharTok{:} \DecValTok{6096}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{9.00}\NormalTok{   Married      }\SpecialCharTok{:}\DecValTok{22847}\NormalTok{   Prof}\SpecialCharTok{{-}}\NormalTok{specialty }\SpecialCharTok{:} \DecValTok{6071}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{10.00}\NormalTok{   Never}\SpecialCharTok{{-}}\NormalTok{married}\SpecialCharTok{:}\DecValTok{16096}\NormalTok{   Exec}\SpecialCharTok{{-}}\NormalTok{managerial}\SpecialCharTok{:} \DecValTok{6019}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{10.06}\NormalTok{   Separated    }\SpecialCharTok{:} \DecValTok{1526}\NormalTok{   Adm}\SpecialCharTok{{-}}\NormalTok{clerical   }\SpecialCharTok{:} \DecValTok{5603}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{12.00}\NormalTok{   Widowed      }\SpecialCharTok{:} \DecValTok{1516}\NormalTok{   Sales          }\SpecialCharTok{:} \DecValTok{5470}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{16.00}\NormalTok{                         Other}\SpecialCharTok{{-}}\NormalTok{service  }\SpecialCharTok{:} \DecValTok{4920}  
\NormalTok{                                          (Other)        }\SpecialCharTok{:}\DecValTok{14419}  
\NormalTok{            relationship                   race          gender     }
\NormalTok{    Husband       }\SpecialCharTok{:}\DecValTok{19537}\NormalTok{   Amer}\SpecialCharTok{{-}}\NormalTok{Indian}\SpecialCharTok{{-}}\NormalTok{Eskimo}\SpecialCharTok{:}  \DecValTok{470}\NormalTok{   Female}\SpecialCharTok{:}\DecValTok{16156}  
\NormalTok{    Not}\SpecialCharTok{{-}}\ControlFlowTok{in}\SpecialCharTok{{-}}\NormalTok{family }\SpecialCharTok{:}\DecValTok{12546}\NormalTok{   Asian}\SpecialCharTok{{-}}\NormalTok{Pac}\SpecialCharTok{{-}}\NormalTok{Islander}\SpecialCharTok{:} \DecValTok{1504}\NormalTok{   Male  }\SpecialCharTok{:}\DecValTok{32442}  
\NormalTok{    Other}\SpecialCharTok{{-}}\NormalTok{relative}\SpecialCharTok{:} \DecValTok{1506}\NormalTok{   Black             }\SpecialCharTok{:} \DecValTok{4675}                 
\NormalTok{    Own}\SpecialCharTok{{-}}\NormalTok{child     }\SpecialCharTok{:} \DecValTok{7577}\NormalTok{   Other             }\SpecialCharTok{:}  \DecValTok{403}                 
\NormalTok{    Unmarried     }\SpecialCharTok{:} \DecValTok{5118}\NormalTok{   White             }\SpecialCharTok{:}\DecValTok{41546}                 
\NormalTok{    Wife          }\SpecialCharTok{:} \DecValTok{2314}                                            
                                                                    
\NormalTok{     capital.gain      capital.loss     hours.per.week        native.country }
\NormalTok{    Min.   }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{1.00}\NormalTok{   United}\SpecialCharTok{{-}}\NormalTok{States}\SpecialCharTok{:}\DecValTok{43613}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   Mexico       }\SpecialCharTok{:}  \DecValTok{949}  
\NormalTok{    Median }\SpecialCharTok{:}    \FloatTok{0.0}\NormalTok{   Median }\SpecialCharTok{:}   \FloatTok{0.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.00}\NormalTok{   ?            }\SpecialCharTok{:}  \DecValTok{847}  
\NormalTok{    Mean   }\SpecialCharTok{:}  \FloatTok{582.4}\NormalTok{   Mean   }\SpecialCharTok{:}  \FloatTok{87.94}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{40.37}\NormalTok{   Philippines  }\SpecialCharTok{:}  \DecValTok{292}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}    \FloatTok{0.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}   \FloatTok{0.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{45.00}\NormalTok{   Germany      }\SpecialCharTok{:}  \DecValTok{206}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{41310.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{4356.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{99.00}\NormalTok{   Puerto}\SpecialCharTok{{-}}\NormalTok{Rico  }\SpecialCharTok{:}  \DecValTok{184}  
\NormalTok{                                                        (Other)      }\SpecialCharTok{:} \DecValTok{2507}  
\NormalTok{      income     }
    \SpecialCharTok{\textless{}=}\DecValTok{50}\NormalTok{K}\SpecialCharTok{:}\DecValTok{37155}  
    \SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{K }\SpecialCharTok{:}\DecValTok{11443}  
                 
                 
                 
                 
   
\end{Highlighting}
\end{Shaded}

The dataset consists of 48598 records and 15 variables. The target variable, \texttt{income}, is binary, with two categories: \texttt{\textless{}=50K} and \texttt{\textgreater{}50K}. The remaining 14 variables serve as predictors, encompassing demographic, occupational, and financial characteristics.

The predictors can be categorized as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Demographic attributes}

  \begin{itemize}
  \tightlist
  \item
    \texttt{age}: Age in years (numerical).\\
  \item
    \texttt{gender}: Gender (categorical, Male/Female).\\
  \item
    \texttt{race}: Race (categorical, 5 levels).\\
  \item
    \texttt{native.country}: Country of origin (categorical, 42 levels).
  \end{itemize}
\item
  \textbf{Education and employment details}

  \begin{itemize}
  \tightlist
  \item
    \texttt{education}: Highest education level attained (categorical, 16 levels).\\
  \item
    \texttt{education.num}: Years of education (numerical).\\
  \item
    \texttt{workclass}: Type of employment (categorical, 6 levels).\\
  \item
    \texttt{occupation}: Job category (categorical, 15 levels).\\
  \item
    \texttt{hours.per.week}: Weekly hours worked (numerical).
  \end{itemize}
\item
  \textbf{Financial attributes}

  \begin{itemize}
  \tightlist
  \item
    \texttt{capital.gain}: Income from capital gains (numerical).\\
  \item
    \texttt{capital.loss}: Losses from investments (numerical).
  \end{itemize}
\item
  \textbf{Household and relationship details}

  \begin{itemize}
  \tightlist
  \item
    \texttt{marital.status}: Marital status (categorical, 5 levels).\\
  \item
    \texttt{relationship}: Family role (categorical, 6 levels).
  \end{itemize}
\end{itemize}

This dataset provides a diverse set of features that influence income levels, making it suitable for building predictive models. For further details, refer to the \href{https://www.rdocumentation.org/packages/liver/versions/1.3/topics/adult}{dataset documentation}.

\subsection*{Data Preparation}\label{data-preparation-3}


Before building models, we must clean and preprocess the dataset to handle missing values and transform categorical variables. The \emph{adult} dataset includes missing values represented by \texttt{"?"}, which need to be addressed. Additionally, categorical variables such as \texttt{native.country}, \texttt{workclass}, and \texttt{race} have multiple levels that can be grouped to improve interpretability.

Since data preparation is covered in detail in Section \ref{Data-pre-adult} as part of the data preparation chapter (\ref{chapter-data-prep}), we summarize the necessary preprocessing steps here.

\subsubsection*{Handling Missing Values}\label{handling-missing-values}


The dataset encodes missing values as \texttt{"?"}. We first replace them with \texttt{NA}, remove unused factor levels, and apply imputation for categorical variables using random sampling from existing categories.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)    }\CommentTok{\# For handling missing values}

\CommentTok{\# Replace "?" with NA}
\NormalTok{adult[adult }\SpecialCharTok{==} \StringTok{"?"}\NormalTok{] }\OtherTok{=} \ConstantTok{NA}

\CommentTok{\# Remove unused factor levels}
\NormalTok{adult }\OtherTok{=} \FunctionTok{droplevels}\NormalTok{(adult)}

\CommentTok{\# Impute missing values using random sampling from existing categories}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass      }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{occupation     }\OtherTok{=} \FunctionTok{impute}\NormalTok{(}\FunctionTok{factor}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{occupation), }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Transforming Categorical Variables}\label{transforming-categorical-variables}


Some categorical variables contain too many levels, making them difficult to interpret and model efficiently. To simplify these variables, we group related categories.

The dataset originally contains 42 unique country values in \texttt{native.country}. We reduce dimensionality by categorizing them into broader regions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forcats)  }\CommentTok{\# For categorical variable transformation}

\NormalTok{Europe }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"England"}\NormalTok{, }\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Greece"}\NormalTok{, }\StringTok{"Holand{-}Netherlands"}\NormalTok{, }\StringTok{"Hungary"}\NormalTok{, }
           \StringTok{"Ireland"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Poland"}\NormalTok{, }\StringTok{"Portugal"}\NormalTok{, }\StringTok{"Scotland"}\NormalTok{, }\StringTok{"Yugoslavia"}\NormalTok{)}

\NormalTok{Asia }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"China"}\NormalTok{, }\StringTok{"Hong"}\NormalTok{, }\StringTok{"India"}\NormalTok{, }\StringTok{"Iran"}\NormalTok{, }\StringTok{"Cambodia"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"Laos"}\NormalTok{, }
         \StringTok{"Philippines"}\NormalTok{, }\StringTok{"Vietnam"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{, }\StringTok{"Thailand"}\NormalTok{)}

\NormalTok{N.America }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"United{-}States"}\NormalTok{, }\StringTok{"Puerto{-}Rico"}\NormalTok{)}

\NormalTok{S.America }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Columbia"}\NormalTok{, }\StringTok{"Cuba"}\NormalTok{, }\StringTok{"Dominican{-}Republic"}\NormalTok{, }\StringTok{"Ecuador"}\NormalTok{, }\StringTok{"El{-}Salvador"}\NormalTok{, }
              \StringTok{"Guatemala"}\NormalTok{, }\StringTok{"Haiti"}\NormalTok{, }\StringTok{"Honduras"}\NormalTok{, }\StringTok{"Mexico"}\NormalTok{, }\StringTok{"Nicaragua"}\NormalTok{, }
              \StringTok{"Outlying{-}US(Guam{-}USVI{-}etc)"}\NormalTok{, }\StringTok{"Peru"}\NormalTok{, }\StringTok{"Jamaica"}\NormalTok{, }\StringTok{"Trinadad\&Tobago"}\NormalTok{)}

\CommentTok{\# Reclassify \textasciigrave{}native.country\textasciigrave{} into broader regions}
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{native.country }\OtherTok{=} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{native.country, }
                                    \StringTok{"Europe"}    \OtherTok{=}\NormalTok{ Europe,}
                                    \StringTok{"Asia"}      \OtherTok{=}\NormalTok{ Asia,}
                                    \StringTok{"N.America"} \OtherTok{=}\NormalTok{ N.America,}
                                    \StringTok{"S.America"} \OtherTok{=}\NormalTok{ S.America,}
                                    \StringTok{"Other"}     \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"South"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The \texttt{workclass} variable contains categories that indicate a lack of formal employment. We consolidate \texttt{"Never-worked"} and \texttt{"Without-pay"} into \texttt{"Unemployed"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{workclass }\OtherTok{=} \FunctionTok{fct\_collapse}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{workclass, }\StringTok{"Unemployed"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Never{-}worked"}\NormalTok{, }\StringTok{"Without{-}pay"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To maintain consistency, we simplify the \texttt{race} variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adult}\SpecialCharTok{$}\NormalTok{race }\OtherTok{=} \FunctionTok{fct\_recode}\NormalTok{(adult}\SpecialCharTok{$}\NormalTok{race, }\StringTok{"Amer{-}Indian"} \OtherTok{=} \StringTok{"Amer{-}Indian{-}Eskimo"}\NormalTok{, }
                                    \StringTok{"Asian"} \OtherTok{=} \StringTok{"Asian{-}Pac{-}Islander"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

These preprocessing steps ensure that the dataset is clean and ready for modeling. In the next section, we partition the dataset into training and testing sets for model evaluation.

\subsection*{Preparing Data for Modeling}\label{preparing-data-for-modeling-2}


Before training tree-based models, we need to split the dataset into training and testing sets. This step ensures that we can evaluate how well the models generalize to unseen data. We use an 80/20 split, allocating 80\% of the data for training and 20\% for testing. To maintain consistency with previous chapters, we apply the \texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ adult, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{income}
\end{Highlighting}
\end{Shaded}

The \texttt{set.seed()} function ensures reproducibility by fixing the random seed. We use \texttt{train\_set} to train the classification models, while \texttt{test\_set} serves as unseen data for evaluation. The \texttt{test\_labels} vector contains the true class labels for \texttt{test\_set}, which we will compare against the model's predictions. This allows us to assess the performance of the CART, C5.0, and Random Forest models.

In practice, it is important to verify that the training and test sets are representative of the original dataset. One way to do this is by examining the distribution of \texttt{income} in both sets. We performed this validation and found the partition to be valid. We do not report it here but refer to Section \ref{sec-validate-partition} for details on how to validate partitions.

To predict whether an individual's income exceeds \$50K, we use the following predictors:

\texttt{age}, \texttt{workclass}, \texttt{education.num}, \texttt{marital.status}, \texttt{occupation}, \texttt{race}, \texttt{gender}, \texttt{capital.gain}, \texttt{capital.loss}, \texttt{hours.per.week}, and \texttt{native.country}.

We exclude \texttt{demogweight}, \texttt{education}, and \texttt{relationship} for the following reasons:

\begin{itemize}
\tightlist
\item
  \texttt{demogweight} is treated as an ID variable and does not provide meaningful predictive information.\\
\item
  \texttt{education} is removed because \texttt{education.num} represents the same information in a numerical format.\\
\item
  \texttt{relationship} is highly correlated with \texttt{marital.status} and does not provide additional independent information.
\end{itemize}

These selected predictors are used in the following formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ workclass }\SpecialCharTok{+}\NormalTok{ education.num }\SpecialCharTok{+}\NormalTok{ marital.status }\SpecialCharTok{+}\NormalTok{ occupation }\SpecialCharTok{+}\NormalTok{ race }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ capital.gain }\SpecialCharTok{+}\NormalTok{ capital.loss }\SpecialCharTok{+}\NormalTok{ hours.per.week }\SpecialCharTok{+}\NormalTok{ native.country}
\end{Highlighting}
\end{Shaded}

This formula will be used to train decision tree models using the CART and C5.0 algorithms, as well as the Random Forest algorithm. In the next section, we demonstrate how to build, evaluate, and compare these models using the \emph{adult} dataset.

\subsection*{Decision Tree with CART}\label{decision-tree-with-cart}


To fit a decision tree using the CART algorithm in \textbf{R}, we use the \href{https://CRAN.R-project.org/package=rpart}{\textbf{rpart}} package (Recursive Partitioning and Regression Trees), which provides a widely used implementation of CART. This package includes functions for building, visualizing, and evaluating decision trees.

If the \textbf{rpart} package is not installed, you can install it using the \texttt{install.packages("rpart")} command. Then, you can load it into your R session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart)}
\end{Highlighting}
\end{Shaded}

The decision tree is built using the \texttt{rpart()} function, which constructs a classification tree when \texttt{method\ =\ "class"} is specified:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree\_cart }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The \texttt{formula} argument defines the relationship between the target variable (\texttt{income}) and the predictors.
\item
  The \texttt{data} argument specifies the training dataset.
\item
  The \texttt{method\ =\ "class"} argument ensures that the model performs classification rather than regression.
\end{itemize}

\subsubsection*{Visualizing the Decision Tree}\label{visualizing-the-decision-tree}


To visualize the tree, we use the \href{https://CRAN.R-project.org/package=rpart.plot}{\textbf{rpart.plot}} package, which provides tools for graphical representation of \textbf{rpart} models. If not installed, it can be added with the \texttt{install.packages("rpart.plot")} command. Then, it is loaded as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart.plot)}
\end{Highlighting}
\end{Shaded}

The tree is displayed using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpart.plot}\NormalTok{(tree\_cart, }\AttributeTok{type =} \DecValTok{4}\NormalTok{, }\AttributeTok{extra =} \DecValTok{104}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{11_Decision_Tree_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{itemize}
\tightlist
\item
  The \texttt{type\ =\ 4} argument specifies a split-labeling style where decision rules appear inside the nodes.
\item
  The \texttt{extra\ =\ 104} argument displays both the predicted class and the probability of the most probable class in each terminal node.
\end{itemize}

If the tree is too large to fit within a single plot, the structure can also be examined using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(tree\_cart)}
\NormalTok{   n}\OtherTok{=} \DecValTok{38878} 
   
\NormalTok{   node}\ErrorTok{)}\NormalTok{, split, n, loss, yval, (yprob)}
         \SpecialCharTok{*}\NormalTok{ denotes terminal node}
   
    \DecValTok{1}\ErrorTok{)}\NormalTok{ root }\DecValTok{38878} \DecValTok{9217} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.76292505} \FloatTok{0.23707495}\NormalTok{)  }
      \DecValTok{2}\ErrorTok{)}\NormalTok{ marital.status}\OtherTok{=}\NormalTok{Divorced,Never}\SpecialCharTok{{-}}\NormalTok{married,Separated,Widowed }\DecValTok{20580} \DecValTok{1282} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.93770651} \FloatTok{0.06229349}\NormalTok{)  }
        \DecValTok{4}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textless{}} \FloatTok{7055.5} \DecValTok{20261}  \DecValTok{978} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.95172992} \FloatTok{0.04827008}\NormalTok{) }\SpecialCharTok{*}
        \DecValTok{5}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textgreater{}=}\FloatTok{7055.5} \DecValTok{319}   \DecValTok{15} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.04702194} \FloatTok{0.95297806}\NormalTok{) }\SpecialCharTok{*}
      \DecValTok{3}\ErrorTok{)}\NormalTok{ marital.status}\OtherTok{=}\NormalTok{Married }\DecValTok{18298} \DecValTok{7935} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.56634605} \FloatTok{0.43365395}\NormalTok{)  }
        \DecValTok{6}\ErrorTok{)}\NormalTok{ education.num}\SpecialCharTok{\textless{}} \FloatTok{12.5} \DecValTok{12944} \DecValTok{4163} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.67838381} \FloatTok{0.32161619}\NormalTok{)  }
         \DecValTok{12}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textless{}} \FloatTok{5095.5} \DecValTok{12350} \DecValTok{3582} \SpecialCharTok{\textless{}=}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.70995951} \FloatTok{0.29004049}\NormalTok{) }\SpecialCharTok{*}
         \DecValTok{13}\ErrorTok{)}\NormalTok{ capital.gain}\SpecialCharTok{\textgreater{}=}\FloatTok{5095.5} \DecValTok{594}   \DecValTok{13} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.02188552} \FloatTok{0.97811448}\NormalTok{) }\SpecialCharTok{*}
        \DecValTok{7}\ErrorTok{)}\NormalTok{ education.num}\SpecialCharTok{\textgreater{}=}\FloatTok{12.5} \DecValTok{5354} \DecValTok{1582} \SpecialCharTok{\textgreater{}}\DecValTok{50}\FunctionTok{K}\NormalTok{ (}\FloatTok{0.29548001} \FloatTok{0.70451999}\NormalTok{) }\SpecialCharTok{*}
\end{Highlighting}
\end{Shaded}

This prints a text-based version of the tree, showing nodes, splits, and predictions in a scrollable format.

\subsubsection*{Interpreting the Decision Tree}\label{interpreting-the-decision-tree}


The CART model produces a binary tree with four decision nodes and five leaves. Among the 12 predictors, the algorithm selects three---\texttt{marital.status}, \texttt{capital.gain}, and \texttt{education.num}---as the most relevant for predicting income. The most influential predictor, \texttt{marital.status}, appears at the root node, meaning that marital status is the first split in the tree.

The model categorizes individuals into five distinct groups, each represented by a terminal leaf. The blue leaves indicate those predicted to earn less than \$50,000 (\texttt{income\ \textless{}=\ 50K}), while the green leaves represent those earning more than \$50,000 (\texttt{income\ \textgreater{}\ 50K}).

As an example, the rightmost leaf corresponds to individuals who are married and have at least 13 years of education (\texttt{education.num\ \textgreater{}=\ 13}). This group represents 14\% of the dataset, with 70\% of them earning more than \$50,000 annually. The error rate for this leaf is 0.30, calculated as \(1 - 0.70\).

\subsection*{Decision Tree with C5.0}\label{decision-tree-with-c5.0}


To fit a decision tree using the C5.0 algorithm in \textbf{R}, we use the \href{https://CRAN.R-project.org/package=C50}{\textbf{C50}} package. If the \textbf{C50} package is not installed, you can install it using the \texttt{install.packages("C50")} command. Then, you can load it into your R session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(C50)}
\end{Highlighting}
\end{Shaded}

The tree is constructed using the \texttt{C5.0()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree\_C50 }\OtherTok{=} \FunctionTok{C5.0}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train\_set) }
\end{Highlighting}
\end{Shaded}

The \texttt{plot()} function can be used to visualize the tree, while the \texttt{summary()} function provides a detailed description of the model. Since the tree output is too large to display here, we print a summary of the model using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(tree\_C50)}
   
\NormalTok{   Call}\SpecialCharTok{:}
   \FunctionTok{C5.0.formula}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set)}
   
\NormalTok{   Classification Tree}
\NormalTok{   Number of samples}\SpecialCharTok{:} \DecValTok{38878} 
\NormalTok{   Number of predictors}\SpecialCharTok{:} \DecValTok{11} 
   
\NormalTok{   Tree size}\SpecialCharTok{:} \DecValTok{120} 
   
\NormalTok{   Non}\SpecialCharTok{{-}}\NormalTok{standard options}\SpecialCharTok{:}\NormalTok{ attempt to group attributes}
\end{Highlighting}
\end{Shaded}

The output includes key information about the model, such as the function call used to generate the tree, the number of predictors, and the number of observations used for training. It also reports a tree size of 74, indicating that the tree consists of 74 decision nodes---substantially larger than the tree produced by CART in this case.

\subsection*{Random Forest}\label{random-forest}


Random forests are implemented in \textbf{R} using the \href{https://CRAN.R-project.org/package=randomForest}{\textbf{randomForest}} package, which builds classification and regression models based on an ensemble of decision trees with randomly selected inputs. While multiple packages in R support random forest modeling, \textbf{randomForest} is one of the most widely used implementations due to its reliability and compatibility with the \textbf{caret} package for automated tuning.

If the \textbf{randomForest} package is not installed, you can install it using the \texttt{install.packages("randomForest")} command. Then, load it into your R session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(randomForest)}
\end{Highlighting}
\end{Shaded}

Using the same predictors as in the previous models, we construct a random forest model with 100 decision trees:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random\_forest }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula, }\AttributeTok{data =}\NormalTok{ train\_set, }\AttributeTok{ntree =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The \texttt{formula} argument specifies the relationship between the target variable (\texttt{income}) and the predictors.\\
\item
  The \texttt{data} argument defines the training dataset.\\
\item
  The \texttt{ntree\ =\ 100} argument sets the number of decision trees in the forest. A higher number of trees generally improves accuracy but increases computation time.
\end{itemize}

We can evaluate the importance of predictors using the \texttt{varImpPlot()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{varImpPlot}\NormalTok{(random\_forest)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{11_Decision_Tree_files/figure-latex/unnamed-chunk-18-1} \end{center}

This plot ranks predictors based on their contribution to model accuracy. In this case, \texttt{marital.status} appears as the most important predictor, followed by \texttt{capital.gain} and \texttt{education.num}.

To assess how the error rate changes as the number of trees increases, we use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(random\_forest)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{11_Decision_Tree_files/figure-latex/unnamed-chunk-19-1} \end{center}

This plot shows classification error as a function of the number of trees. The error rate stabilizes after approximately 40 trees, indicating that adding more trees beyond this point does not significantly improve accuracy.

Random forests provide a robust alternative to single decision trees by reducing overfitting and improving predictive performance through aggregation. The next section compares the performance of the CART, C5.0, and Random Forest models using evaluation metrics.

\subsection*{Prediction and Model Evaluation}\label{prediction-and-model-evaluation-1}


After training the models (CART, C5.0, and Random Forest), we evaluate their performance on the test set, which contains individuals unseen during training. The objective is to compare the predicted probabilities with the actual class labels stored in \texttt{test\_labels}.

To obtain predicted class probabilities, we use the \texttt{predict()} function for each model. For all three algorithms, we specify \texttt{type\ =\ "prob"} to extract probabilities instead of discrete class labels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob\_cart }\OtherTok{=} \FunctionTok{predict}\NormalTok{(tree\_cart, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\DecValTok{1}\NormalTok{]}
\NormalTok{prob\_C50 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(tree\_C50, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\DecValTok{1}\NormalTok{]}
\NormalTok{prob\_random\_forest }\OtherTok{=} \FunctionTok{predict}\NormalTok{(random\_forest, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

The \texttt{{[}\ ,\ 1\ {]}} index extracts the probability of the ``\texttt{\textless{}=50K}'' class, as class labels are stored in alphabetical order.

\subsubsection*{Confusion Matrix and Classification Errors}\label{confusion-matrix-and-classification-errors}


A confusion matrix summarizes model performance by displaying the number of true positives, true negatives, false positives, and false negatives. We generate confusion matrices for each model using the \texttt{conf.mat.plot()} function from the \textbf{liver} package, which provides a graphical representation. The \texttt{conf.mat()} function can also be used to display numeric values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf.mat.plot}\NormalTok{(prob\_cart, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"CART Prediction"}\NormalTok{)}
\FunctionTok{conf.mat.plot}\NormalTok{(prob\_C50, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"C5.0 Prediction"}\NormalTok{)}
\FunctionTok{conf.mat.plot}\NormalTok{(prob\_random\_forest, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"\textless{}=50K"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Random Forest Prediction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.33\linewidth]{11_Decision_Tree_files/figure-latex/unnamed-chunk-21-1} \includegraphics[width=0.33\linewidth]{11_Decision_Tree_files/figure-latex/unnamed-chunk-21-2} \includegraphics[width=0.33\linewidth]{11_Decision_Tree_files/figure-latex/unnamed-chunk-21-3}

The confusion matrices indicate the number of correctly classified instances for each model:

\begin{itemize}
\tightlist
\item
  CART: ``7091 + 1111 = 8202'' correct classifications.\\
\item
  C5.0: ``7084 + 1360 = 8444'' correct classifications.\\
\item
  Random Forest: ``7053 + 1335 = 8388'' correct classifications.
\end{itemize}

Among the three models, C5.0 has the highest accuracy, making the fewest misclassifications.

\subsubsection*{ROC Curve and AUC}\label{roc-curve-and-auc-2}


The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) value provide a comprehensive assessment of the model's ability to distinguish between income classes across different classification thresholds. These metrics are computed using the \textbf{pROC} package. If the \textbf{pROC} package is not installed, it can be added using \texttt{install.packages("pROC")}. Then, it can be loaded into the R session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}
\end{Highlighting}
\end{Shaded}

To generate the ROC curve, we compute the true positive rate and false positive rate for different threshold values using the \texttt{roc()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{roc\_cart }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, prob\_cart)}
\NormalTok{roc\_C50 }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, prob\_C50)}
\NormalTok{roc\_random\_forest }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, prob\_random\_forest)}
\end{Highlighting}
\end{Shaded}

We then visualize the ROC curves for all three models using \texttt{ggroc()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggroc}\NormalTok{(}\FunctionTok{list}\NormalTok{(roc\_cart, roc\_C50, roc\_random\_forest), }\AttributeTok{size =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC Curves with AUC for Three Models"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }
    \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"CART; AUC="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_cart), }\DecValTok{3}\NormalTok{)), }
                \FunctionTok{paste}\NormalTok{(}\StringTok{"C5.0; AUC="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_C50), }\DecValTok{3}\NormalTok{)), }
                \FunctionTok{paste}\NormalTok{(}\StringTok{"Random Forest; AUC="}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(roc\_random\_forest), }\DecValTok{3}\NormalTok{)))) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(.}\DecValTok{7}\NormalTok{, .}\DecValTok{3}\NormalTok{), }\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{17}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11_Decision_Tree_files/figure-latex/unnamed-chunk-25-1} \end{center}

In the ROC plot, the \emph{black} curve represents CART, the {\emph{red}} curve represents C5.0, and the {\emph{green}} curve represents Random Forest. The ROC curves suggest that both C5.0 and Random Forest outperform CART. However, distinguishing between C5.0 and Random Forest based on the ROC curve alone is challenging. Note that in the above ROC plots we also report the AUC values for each model, by using the \texttt{auc()} function from the \textbf{pROC} package. The AUC values provide further insight:

\begin{itemize}
\tightlist
\item
  CART: AUC = 0.841,\\
\item
  C5.0: AUC = 0.903,
\item
  Random Forest: AUC = 0.899,
\end{itemize}

Based on AUC values, C5.0 performs slightly better than the other two models, but all three demonstrate comparable accuracy, making them reliable for this classification task.

\section{Exercises}\label{tree-exercises}

These exercises test theoretical understanding, interpretation of Decision Trees and Random Forests outputs, and practical implementation in \textbf{R} using datasets from the \textbf{liver} package.

\subsection*{Decision Trees: Conceptual Questions}\label{decision-trees-conceptual-questions}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain the basic structure of a Decision Tree and how it makes predictions.\\
\item
  What are the key differences between \emph{classification trees} and \emph{regression trees}?\\
\item
  What is the purpose of \emph{splitting criteria} in Decision Trees? Describe the \emph{Gini Index}, \emph{Entropy}, and \emph{Variance Reduction}.\\
\item
  Why are \emph{Decision Trees prone to overfitting}? What techniques can be used to prevent overfitting?\\
\item
  Define \emph{pre-pruning} and \emph{post-pruning} in Decision Trees. How do they differ?\\
\item
  Explain the \emph{bias-variance tradeoff} in Decision Trees.\\
\item
  What are the advantages and disadvantages of Decision Trees compared to \emph{logistic regression} for classification problems?\\
\item
  What is the role of the \emph{maximum depth} parameter in a Decision Tree? How does it affect model performance?\\
\item
  Why might a Decision Tree \emph{favor continuous variables} over categorical variables when constructing splits?\\
\item
  Explain the difference between \emph{CART (Classification and Regression Trees)} and \emph{C5.0 Decision Trees}.
\end{enumerate}

\subsection*{Practical Exercises Using the Churn Dataset (Classification Tasks)}\label{practical-exercises-using-the-churn-dataset-classification-tasks}


The \emph{churn} dataset contains information about customer churn behavior in a telecommunications company. The goal is to predict whether a customer will churn based on various attributes.

\subsubsection{Data Preparation and Partitioning}\label{data-preparation-and-partitioning}

Load the dataset and partition it into a \emph{training set} (80\%) and a \emph{test set} (20\%) using the \texttt{partition()} function from the \textbf{liver} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(churn, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ churn, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{churn}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Fit a \emph{Decision Tree} using \texttt{churn} as the response variable and \texttt{day.mins}, \texttt{eve.mins}, \texttt{intl.mins}, \texttt{customer.calls}, and \texttt{voice.plan} as predictors.\\
\item
  Visualize the fitted Decision Tree using \texttt{rpart.plot()}. Interpret the tree structure and identify the key decision rules.\\
\item
  Identify the \emph{most important predictors} in the Decision Tree.\\
\item
  Compute the \emph{confusion matrix} and evaluate model performance.\\
\item
  Generate the \emph{ROC curve} and compute the \emph{AUC} for the Decision Tree model.\\
\item
  Evaluate the effect of \emph{pruning} on the Decision Tree by adjusting the complexity parameter (\texttt{cp}).\\
\item
  Fit a \emph{C5.0 Decision Tree} to the same data and compare its performance with the CART model.\\
\item
  Use the \texttt{predict()} function to estimate the probability of churn for a \emph{new customer} with the following attributes:

  \begin{itemize}
  \tightlist
  \item
    \texttt{day.mins\ =\ 200}\strut \\
  \item
    \texttt{eve.mins\ =\ 150}\strut \\
  \item
    \texttt{intl.mins\ =\ 10}\strut \\
  \item
    \texttt{customer.calls\ =\ 3}\strut \\
  \item
    \texttt{voice.plan\ =\ "yes"}\strut \\
  \end{itemize}
\item
  Compare the \emph{confusion matrix and classification accuracy} between CART and C5.0 models.\\
\item
  Implement \emph{cross-validation} to assess model performance.
\end{enumerate}

\subsection*{Random Forests: Conceptual Questions}\label{random-forests-conceptual-questions}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  What is the fundamental difference between \emph{Decision Trees} and \emph{Random Forests}?\\
\item
  How does \emph{bagging (Bootstrap Aggregation)} improve Random Forest models?\\
\item
  Explain how \emph{majority voting} works in a Random Forest classification model.\\
\item
  Why does Random Forest tend to \emph{outperform a single Decision Tree}?\\
\item
  How can we determine \emph{feature importance} in a Random Forest model?\\
\item
  Explain the \emph{limitations of Random Forests}.\\
\item
  How does increasing the number of trees (\texttt{ntree}) in a Random Forest affect model performance?
\end{enumerate}

\subsection*{Practical Exercises Using the Churn Dataset (Random Forests for Classification Tasks)}\label{practical-exercises-using-the-churn-dataset-random-forests-for-classification-tasks}


\subsubsection*{Fitting and Evaluating a Random Forest Model}\label{fitting-and-evaluating-a-random-forest-model}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{27}
\tightlist
\item
  Fit a \emph{Random Forest model} using \texttt{churn} as the response and \texttt{day.mins}, \texttt{eve.mins}, \texttt{intl.mins}, \texttt{customer.calls}, and \texttt{voice.plan} as predictors.\\
\item
  Identify the \emph{most important variables} using \texttt{varImpPlot()}.\\
\item
  Compare the \emph{accuracy of Random Forest} with the \emph{Decision Tree models (CART and C5.0)}.\\
\item
  Compute the \emph{confusion matrix} for the Random Forest model.\\
\item
  Compute the \emph{ROC curve and AUC} for the Random Forest model.\\
\item
  Adjust the number of trees (\texttt{ntree\ =\ 200}) and evaluate whether increasing the number of trees improves model accuracy.\\
\item
  Use the \texttt{tuneRF()} function to find the optimal value for \texttt{mtry} (number of predictors to consider at each split).\\
\item
  Predict churn probabilities for a \emph{new customer} using the Random Forest model.\\
\item
  Perform \emph{feature selection} by training a Random Forest model with \emph{only the top 3 most important features}.\\
\item
  Evaluate whether the \emph{simplified model performs comparably} to the full model.
\end{enumerate}

\subsection*{Regression Trees and Random Forests (redWines Dataset)}\label{regression-trees-and-random-forests-redwines-dataset}


The \emph{redWines} dataset contains wine quality scores (\texttt{quantity}, a score between 0 and 10) and 11 chemical attributes. The goal is to predict \texttt{quantity} using regression trees and Random Forests.

\subsubsection*{Data Preparation and Partitioning}\label{data-preparation-and-partitioning-1}


Load the dataset and partition it into a \emph{training set} (80\%) and a \emph{test set} (20\%) using the \texttt{partition()} function from the \textbf{liver} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(redWines, }\AttributeTok{package =} \StringTok{"liver"}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ redWines, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{quantity}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Conceptual Questions}\label{conceptual-questions-9}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\tightlist
\item
  How does a \emph{regression tree} differ from a \emph{classification tree}?\\
\item
  How is the \emph{Mean Squared Error (MSE)} used to evaluate regression trees?\\
\item
  Explain why \emph{Random Forest regression is generally preferred over a single regression tree}.
\end{enumerate}

\subsubsection*{Practical Exercises Using the redWines Dataset}\label{practical-exercises-using-the-redwines-dataset}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{40}
\tightlist
\item
  Fit a \emph{regression tree} predicting \texttt{quantity} based on all 11 predictors.\\
\item
  Visualize the tree and interpret the splits. What are the most important variables?\\
\item
  Compute the \emph{Mean Squared Error (MSE)} of the regression tree.\\
\item
  Fit a \emph{Random Forest regression model} to predict \texttt{quantity} and compare its performance with the single regression tree.\\
\item
  Compare the \emph{MSE of the Random Forest model} with that of the regression tree.\\
\item
  Identify the \emph{top 3 most important features} in the Random Forest model.\\
\item
  Use the trained Random Forest model to predict wine quality for a \emph{new observation} with the following attributes:

  \begin{itemize}
  \tightlist
  \item
    \texttt{fixed.acidity\ =\ 8.5}\strut \\
  \item
    \texttt{volatile.acidity\ =\ 0.4}\strut \\
  \item
    \texttt{citric.acid\ =\ 0.3}\strut \\
  \item
    \texttt{residual.sugar\ =\ 2.0}\strut \\
  \item
    \texttt{chlorides\ =\ 0.08}\strut \\
  \item
    \texttt{free.sulfur.dioxide\ =\ 30}\strut \\
  \item
    \texttt{total.sulfur.dioxide\ =\ 100}\strut \\
  \item
    \texttt{density\ =\ 0.995}\strut \\
  \item
    \texttt{pH\ =\ 3.2}\strut \\
  \item
    \texttt{sulphates\ =\ 0.6}\strut \\
  \item
    \texttt{alcohol\ =\ 10.5}
  \end{itemize}
\item
  Use \emph{cross-validation} to compare the Random Forest model with the regression tree.\\
\item
  Interpret whether Random Forest significantly improves prediction accuracy compared to the single Decision Tree.
\end{enumerate}

\chapter{Neural Networks: The Building Blocks of Artificial Intelligence}\label{chapter-nn}

For centuries, humans have been captivated by the idea of creating machines that can think and learn like us. Philosophers, inventors, and storytellers have explored this vision, from the mechanical automata of ancient Greece to the artificial beings imagined in science fiction. Early inventors like Hero of Alexandria designed self-operating machines, and myths of artificial life---such as the golem or automatons---reflected humanity's enduring fascination with intelligent systems. What was once the realm of imagination has now become reality in the form of \emph{Artificial Intelligence (AI)}, a transformative force reshaping industries, societies, and daily life.

AI is no longer a futuristic concept---it powers everything from recommendation systems and fraud detection to autonomous vehicles and generative AI (GenAI) models capable of producing text, images, and music. These advancements have been fueled by exponential growth in computational power, the availability of vast datasets, and breakthroughs in machine learning algorithms. At the heart of this revolution lies a class of models known as \emph{neural networks}, the driving force behind deep learning.\\
Neural networks are computational models inspired by the structure and function of the human brain. Just as biological neurons connect to form intricate networks that process information, artificial neural networks consist of layers of interconnected nodes that learn from data. This design enables them to recognize patterns, extract meaningful insights, and make predictions, making them particularly suited for problems involving \emph{complex, high-dimensional, and unstructured data}---such as images, speech, and natural language. Unlike traditional machine learning models, which rely on handcrafted features, neural networks automatically discover representations in data, often outperforming classical approaches.

While deep learning---powered by advanced neural architectures---has made groundbreaking strides in areas like computer vision and language modeling, its foundation rests on simpler architectures. In this chapter, we focus on \emph{feed-forward neural networks}, also known as \emph{multilayer perceptrons (MLPs)}. These fundamental models serve as the building blocks for more sophisticated deep learning systems.

\subsection*{Why neural networks are powerful}\label{why-neural-networks-are-powerful}


Neural networks excel in tackling \emph{complex and nonlinear problems}, making them indispensable in modern AI applications. Their strengths stem from three key capabilities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Pattern recognition in complex data:} Neural networks are exceptionally effective at identifying patterns in unstructured data, such as detecting objects in images, recognizing speech, and generating human-like text. These tasks often pose significant challenges for traditional algorithms.
\item
  \textbf{Robustness to noise:} Due to their dense connectivity and adaptive learning, neural networks can extract meaningful information even from noisy or incomplete datasets, filtering out irrelevant variations.
\item
  \textbf{Scalability and adaptability:} Neural networks can handle massive datasets and model highly nonlinear relationships by adjusting their architecture---adding layers and neurons to capture increasing complexity.
\end{enumerate}

Despite these advantages, neural networks come with challenges. Unlike interpretable models such as decision trees, they are often seen as \emph{``black boxes''} because their decision-making process is distributed across many parameters, making it difficult to explain individual predictions. Additionally, training neural networks requires significant computational power, often necessitating specialized hardware like GPUs or TPUs to process large volumes of data efficiently.

The power of neural networks lies in their ability to emulate biological learning. Just as neurons in the brain collaborate to recognize patterns, artificial neurons in a network combine their outputs to solve problems that traditional algorithms struggle with. This ability to adaptively model nonlinear relationships has placed neural networks at the forefront of both research and real-world applications.

\subsection*{What's ahead}\label{whats-ahead}


In this chapter, we will explore the key principles of neural networks and their transformative applications through the following topics:

\begin{itemize}
\tightlist
\item
  Biological inspiration: How the structure and function of the human brain inspired artificial neural networks.\\
\item
  Core algorithmic principles: The fundamental mechanics of neural networks, including layers, nodes, and weights.\\
\item
  Activation functions: The role of non-linearity in enabling neural networks to learn complex patterns.\\
\item
  Training neural networks: The iterative optimization process used to adjust neural network parameters.\\
\item
  Case study: Applying neural networks to predict customer subscription behavior using the \emph{bank marketing dataset}.
\end{itemize}

Neural networks have revolutionized computing, enabling machines to tackle problems once considered unsolvable. From driving autonomous vehicles to powering medical diagnostics, these models are shaping the future of AI. To understand how they function, we begin by exploring their biological origins and the inspiration drawn from the human brain.

\section{Neural networks: inspired by biological neurons}\label{neural-networks-inspired-by-biological-neurons}

The foundation of neural networks is deeply rooted in the structure and function of biological neurons, which form the basis of learning and decision-making in animal brains. While individual neurons are relatively simple in structure, their true power lies in their dense and intricate connectivity. These networks of interconnected neurons enable the brain to perform highly complex tasks, such as pattern recognition, classification, reasoning, and decision-making. For example, the human brain contains approximately \(10^{11}\) neurons, with each neuron forming connections to an average of 10,000 others. This creates an astonishing \(10^{15}\) synaptic connections---a vast, dynamic network capable of extraordinary learning and adaptation.

Artificial neural networks (ANNs) are computational models inspired by these biological structures. While they do not replicate the full complexity of the brain, they abstract key principles of learning through interconnected units. By leveraging dense networks of artificial neurons, ANNs can model nonlinear and dynamic processes, enabling them to tackle complex problems in domains such as image recognition, speech processing, and decision-making. They are particularly adept at uncovering patterns and relationships in data, even in cases where traditional algorithms struggle.

As shown in Figure \ref{fig:net-brain}, a biological neuron is designed to process and transmit information. Dendrites act as input channels, collecting signals from other neurons. These signals are processed and integrated in the cell body, where a decision is made: if the combined input surpasses a certain threshold, the neuron ``fires'' and sends an output signal through its axon to other connected neurons. This nonlinear behavior---firing only when a certain input threshold is exceeded---plays a critical role in the brain's ability to process information efficiently.

Similarly, an artificial neuron (illustrated in Figure \ref{fig:net-1}) emulates this process using a mathematical model. It receives inputs (\(x_i\)) from either other artificial neurons or directly from a dataset. These inputs are combined using a weighted summation (\(\sum w_i x_i\)), where the weights (\(w_i\)) represent the strength of each input's influence. The combined signal is then passed through an activation function (\(f(.)\)) to introduce non-linearity, determining the final output (\(\hat{y}\)). This output is either transmitted to other artificial neurons or used as the final result of the model. The activation function is crucial, as it enables neural networks to learn and model complex, nonlinear relationships in data.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\linewidth]{images/ch12_net_brain} 

}

\caption{Visualization of a biological neuron, which processes input signals through dendrites and sends outputs through the axon.}\label{fig:net-brain}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth]{images/ch12_net_1} 

}

\caption{Illustration of an artificial neuron, designed to emulate the structure and function of a biological neuron in a simplified way.}\label{fig:net-1}
\end{figure}

One of the key advantages of artificial neural networks is their ability to generalize from data, even when faced with noise or incomplete information. Unlike traditional algorithms that rely on explicit rule-based decision-making, neural networks distribute learning across many interconnected neurons and weighted connections, allowing them to adapt and identify patterns even when individual data points are imperfect. This distributed learning process enables them to handle complex, high-dimensional data more effectively than many conventional models.

However, this flexibility comes at a cost. Neural networks often require large amounts of data and computational power to train effectively, and their decision-making process is less interpretable than traditional models such as decision trees. Unlike rule-based models that provide clear decision paths, neural networks embed their learned knowledge within millions of parameters, making it challenging to understand precisely how a prediction is made.

In the following sections, we will delve deeper into the mechanics of neural networks, starting with their core structure and the algorithms that enable them to learn from data.

\section{How neural networks work}\label{how-neural-networks-work}

Neural networks extend traditional linear models by incorporating multiple layers of processing to capture complex relationships in data. At their core, they build upon the fundamental concepts of linear regression. A linear regression model makes predictions using the following equation:

\[
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_p x_p
\]

where \(p\) represents the number of predictors, \(b_0\) is the intercept, and \(b_1\) to \(b_p\) are the learned coefficients. In this setup, \(\hat{y}\) is a weighted sum of the input features (\(x_1\) to \(x_p\)), where the weights (\(b_1\) to \(b_p\)) determine the relative influence of each feature on the prediction. This simple linear relationship can be visualized in Figure \ref{fig:net-reg}, where input features and the prediction are represented as nodes, with the coefficients acting as connecting weights.

\begin{figure}[H]

{\centering \includegraphics[width=0.4\linewidth]{images/ch12_net_reg} 

}

\caption{A graphical representation of a regression model: input features and predictions are shown as nodes, with the coefficients represented as connections between the nodes.}\label{fig:net-reg}
\end{figure}

While linear models are effective for capturing direct relationships between inputs and outputs, they struggle to represent more complex patterns, such as interactions between variables or hierarchical structures in data. Neural networks address this limitation by introducing multiple layers of artificial neurons between the input and output, allowing them to model intricate, nonlinear relationships. This structure is illustrated in Figure \ref{fig:net-large}.

The architecture of a neural network consists of the following key components:

\begin{itemize}
\item
  The \textbf{input layer} serves as the entry point for the data. Each node in this layer corresponds to an input feature, such as age, income, or pixel intensity in an image.
\item
  The \textbf{hidden layers} process the data by transforming inputs through multiple interconnected nodes (artificial neurons). Each hidden layer captures increasingly abstract features, allowing the network to learn complex patterns. Every neuron in a hidden layer is connected to neurons in both the preceding and succeeding layers, with each connection assigned a weight.
\item
  The \textbf{output layer} produces the final prediction. In classification tasks, it typically represents the probability of a given class, while in regression tasks, it outputs a continuous numerical value.
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth]{images/ch12_net_large} 

}

\caption{Visualization of a multilayer neural network model with two hidden layers.}\label{fig:net-large}
\end{figure}

In Figure \ref{fig:net-large}, the input layer passes the features into the network, where each hidden layer applies transformations before passing the processed information forward. The output layer aggregates this information to generate the final prediction. Each connection in the network is assigned a weight (\(w_i\)), which determines the strength of influence one neuron has on another. These weights are adjusted during training to optimize the model's accuracy.

The behavior of an artificial neuron can be mathematically expressed as:

\[
\hat{y} = f\left( \sum_{i=1}^{p} w_i x_i + b \right)
\]

where:\\
- \(x_i\) represents the input features,\\
- \(w_i\) represents the corresponding weights,\\
- \(b\) is a bias term that helps shift the activation threshold,\\
- \(\sum\) represents the summation of the weighted inputs,\\
- \(f(.)\) is the activation function, and\\
- \(\hat{y}\) is the output of the neuron.

A crucial component of this process is the activation function, which introduces non-linearity into the model. Without it, the neural network would be equivalent to a linear model, regardless of the number of layers. By applying a non-linear transformation to the combined input signals, activation functions allow neural networks to approximate complex relationships in data.

\subsection*{Key characteristics of neural networks}\label{key-characteristics-of-neural-networks}


Despite the diversity of neural network architectures, all neural networks share three fundamental characteristics that define their functionality:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Activation functions} The activation function transforms a neuron's net input into an output signal that is passed to the next layer. Activation functions introduce non-linearity, enabling the network to learn complex patterns. Common choices include the sigmoid function, ReLU (rectified linear unit), and hyperbolic tangent (tanh).
\item
  \textbf{Network architecture} The structure of a neural network defines its computational capacity. This includes the number of layers, the number of neurons in each layer, and how neurons are connected. For example, deep neural networks contain many hidden layers, enabling them to learn hierarchical and abstract representations of data.
\item
  \textbf{Training algorithm} Training a neural network involves adjusting the weights (\(w_i\)) and biases (\(b\)) in the model to minimize prediction errors. This is achieved by defining a loss function, which quantifies how far the network's predictions deviate from the true values. Optimization algorithms such as gradient descent iteratively update the weights to reduce this error, allowing the network to improve over time.
\end{enumerate}

In the following sections, we will explore these components in greater detail, beginning with activation functions and their role in enabling neural networks to model complex, non-linear relationships.

\section{Activation functions}\label{activation-functions}

The activation function is a fundamental component of a neural network, determining how an artificial neuron processes incoming signals and passes information forward. Much like biological neurons, which integrate input signals from dendrites and decide whether to ``fire,'' artificial neurons use activation functions to introduce non-linearity, allowing neural networks to model complex relationships that linear models cannot capture. Without activation functions, even deep neural networks would behave like simple linear models, limiting their ability to learn hierarchical and abstract features from data.

In mathematical terms, an artificial neuron computes a weighted sum of its inputs and applies an activation function \(f(x)\) to determine its output:

\[
\hat{y} = f\left( \sum_{i=1}^{p} w_i x_i + b \right)
\]

where \(x_i\) represents the input features, \(w_i\) are the corresponding weights, \(b\) is a bias term, and \(f(x)\) is the activation function. The choice of activation function significantly impacts a network's ability to learn and generalize.

\subsection*{The threshold activation function}\label{the-threshold-activation-function}


Early neural network models used a simple threshold activation function, mimicking the binary nature of biological neurons. The threshold function activates only when the input exceeds a certain value, producing an output of either 0 or 1. It is defined as:

\[
f(x) = 
\begin{cases} 
1 & \text{if } x \geq 0 \\ 
0 & \text{if } x < 0
\end{cases}
\]

This step-like behavior is visualized in Figure \ref{fig:active-fun-unit}. The function outputs 1 when the input is at least zero and 0 otherwise, making it useful for basic classification tasks.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth]{12_Neural_Network_files/figure-latex/active-fun-unit-1} 

}

\caption{Visualization of the threshold activation function (unit step).}\label{fig:active-fun-unit}
\end{figure}

While biologically intuitive, the threshold function has major drawbacks. It cannot model nuanced relationships between inputs and outputs, as it only provides binary decisions. More importantly, it is not differentiable, preventing its use in gradient-based optimization methods such as backpropagation. For these reasons, it has been largely replaced by continuous activation functions.

\subsection*{The sigmoid activation function}\label{the-sigmoid-activation-function}


A widely used alternative is the sigmoid activation function, also known as the logistic function. The sigmoid function maps any real-valued input to a smooth output between 0 and 1, making it suitable for probability-based interpretations. It is defined as:

\[
f(x) = \frac{1}{1 + e^{-x}}
\]

where \(e\) is the base of the natural logarithm (approximately 2.72). The sigmoid function has an S-shaped curve, as shown in Figure \ref{fig:active-fun-sigmoid}, and provides a differentiable alternative to the threshold function.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth]{12_Neural_Network_files/figure-latex/active-fun-sigmoid-1} 

}

\caption{Visualization of the sigmoid activation function.}\label{fig:active-fun-sigmoid}
\end{figure}

Although sigmoid activation is useful, it suffers from the \textbf{vanishing gradient problem}. When inputs are very large or very small, the function saturates, producing values close to 0 or 1. In these regions, the derivative is near zero, slowing down the learning process in deep networks. As a result, alternative activation functions are often preferred for hidden layers.

\subsection*{Other common activation functions}\label{other-common-activation-functions}


Several activation functions have been developed to address the limitations of the sigmoid function. Figure \ref{fig:active-fun-comparison} compares some of the most commonly used activation functions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hyperbolic Tangent (tanh):} Similar to the sigmoid function but with an output range of \((-1, 1)\). This symmetry around zero often leads to faster learning in practice.\\
\item
  \textbf{ReLU (Rectified Linear Unit):} Defined as \(f(x) = \max(0, x)\), ReLU is the most widely used activation function in deep neural networks due to its computational efficiency and ability to mitigate the vanishing gradient problem.\\
\item
  \textbf{Leaky ReLU:} A variation of ReLU that allows small negative values instead of zero for negative inputs, reducing the problem of inactive neurons.
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth]{12_Neural_Network_files/figure-latex/active-fun-comparison-1} 

}

\caption{Comparison of common activation functions: sigmoid, tanh, and ReLU.}\label{fig:active-fun-comparison}
\end{figure}

\subsection*{Choosing the right activation function}\label{choosing-the-right-activation-function}


The choice of activation function depends on the network architecture and the specific learning task:

\begin{itemize}
\tightlist
\item
  \textbf{Sigmoid} is commonly used in the output layer for binary classification problems, where the output represents a probability.
\item
  \textbf{Tanh} is often used in hidden layers because it is zero-centered, which can lead to faster convergence during training.\\
\item
  \textbf{ReLU} is the default choice for hidden layers in deep networks due to its efficiency and ability to mitigate the vanishing gradient problem.\\
\item
  \textbf{Leaky ReLU} is useful in cases where ReLU may cause neurons to become inactive, as it allows small negative outputs.\\
\item
  \textbf{Linear activation} is typically used in the final layer of regression models, where the output is a continuous value.
\end{itemize}

Activation functions such as sigmoid and tanh can suffer from \textbf{saturation}, where input values far from zero result in near-constant outputs. This leads to \textbf{vanishing gradients}, slowing down learning. One strategy to mitigate this issue is to preprocess input data through normalization or standardization, ensuring values remain within an optimal range.

ReLU and its variants have become the preferred choice for hidden layers in deep learning due to their computational efficiency and ability to propagate gradients effectively. However, selecting the right activation function depends on the specific problem and empirical evaluation.

\section{Network architecture}\label{network-architecture}

The ability of a neural network to learn and make predictions is fundamentally tied to its architecture, or topology. This refers to the arrangement of neurons and the connections between them, which define how data flows through the network. While neural networks can take on various forms, their architecture is primarily characterized by three factors:

\begin{itemize}
\tightlist
\item
  The number of layers in the network,\\
\item
  The number of neurons (or nodes) in each layer, and\\
\item
  The connectivity between neurons across layers.
\end{itemize}

The architecture of a neural network determines its capacity to model complex relationships. Larger networks with more layers and neurons can capture intricate patterns and decision boundaries. However, the effectiveness of a network is not solely determined by its size but also by how its components are organized and interconnected.

To understand network architecture, consider a simple example illustrated in Figure \ref{fig:net-reg}. This basic network consists of:

\begin{itemize}
\tightlist
\item
  Input nodes, which receive raw feature values from the dataset. Each input node corresponds to one feature and passes its value to the network.\\
\item
  Output nodes, which provide the network's final prediction, denoted as \(p\).
\end{itemize}

In this single-layer network, input nodes are directly connected to the output node through a set of weights (\(w_1, w_2, \dots, w_p\)), representing the influence of each input feature on the prediction. This simple architecture works well for basic classification or regression tasks but struggles with capturing complex patterns.

To address this limitation, additional layers can be introduced, as shown in Figure \ref{fig:net-large}. These intermediate layers, known as hidden layers, allow the network to model nonlinear relationships and discover hierarchical structures in data.

A multilayer network typically consists of three types of layers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The input layer, where raw features enter the network.\\
\item
  One or more hidden layers, which extract and refine patterns.\\
\item
  The output layer, which combines the processed information to generate the network's final prediction.
\end{enumerate}

In a fully connected network, each neuron in one layer transmits information to all neurons in the next layer, with an associated weight assigned to each connection. These weights determine how strongly neurons influence each other and are adjusted during training to optimize performance.

The introduction of hidden layers enables neural networks to process input data hierarchically. Early layers may learn basic features, such as edges in an image or simple word patterns in text, while deeper layers capture more abstract representations, such as object shapes or semantic meaning. Networks with multiple hidden layers are referred to as deep neural networks (DNNs), and training such models is known as deep learning. This approach has driven advancements in computer vision, speech recognition, and natural language processing.

The number of input and output nodes in a network is determined by the problem at hand.\\
- The input nodes match the number of features in the dataset. For example, a dataset with 20 features would have 20 input nodes.\\
- The output nodes depend on the task. In regression problems, there is typically one output node representing a continuous value. In classification tasks, the number of output nodes corresponds to the number of classes.

The number of hidden nodes in each layer is not predefined and must be determined based on the complexity of the problem. A larger number of hidden nodes increases the network's ability to capture intricate patterns but also raises the risk of overfitting---where the model performs well on training data but poorly on unseen data. Overly large networks can also be computationally expensive and slow to train.

Balancing network complexity and efficiency is essential for achieving optimal performance. This is often guided by the principle of Occam's Razor, which suggests that the simplest model that adequately explains the data is usually preferable. In practice, the optimal network architecture is determined through experimentation, incorporating techniques such as cross-validation and regularization methods like dropout and weight decay to mitigate overfitting.

While this section focuses on fully connected networks, specialized architectures such as convolutional neural networks (CNNs) for image processing and recurrent neural networks (RNNs) for sequential data provide additional flexibility for specific tasks. These architectures build upon the principles of deep learning and have enabled major breakthroughs in artificial intelligence.

In summary, the architecture of a neural network defines its capacity to solve problems. From simple single-layer networks to sophisticated deep neural networks, these architectures offer the flexibility to model a wide variety of tasks, ranging from basic regression to highly complex applications like autonomous driving and medical diagnosis. In the next section, we will explore how these architectures are trained to optimize their performance and learn from data.

\section{How neural networks learn}\label{how-neural-networks-learn}

A neural network, like an untrained system, starts with no knowledge of the task at hand. Through exposure to data, it gradually learns by adjusting internal connections, a process akin to how humans refine their understanding through experience. These connections, represented as weights, are updated as the network processes data, enabling it to discover patterns and relationships over time. Just as a child learns to recognize objects by repeatedly encountering them, a neural network improves its predictions through iterative refinements based on the data it encounters.

Training a neural network is a computationally intensive process that involves adjusting the weights connecting neurons. While neural networks have been studied since the mid-20th century, their real-world application remained limited until the 1980s, when a major breakthrough---the backpropagation algorithm---made it feasible to train multilayer networks efficiently. Backpropagation revolutionized neural network training by enabling networks to systematically learn from errors, making deep learning practical for real-world applications. Despite being computationally demanding compared to simpler machine learning algorithms, backpropagation remains the foundation of modern neural network training, powering advancements in fields such as computer vision and natural language processing.

At its core, backpropagation refines the network's weights through an iterative learning process consisting of two main phases: the forward phase and the backward phase. Each iteration, known as an epoch, begins with randomly initialized weights, as the network starts without prior knowledge. Over successive epochs, the network continuously updates its weights to minimize prediction errors.

In the forward phase, input data is passed through the network, layer by layer, from the input layer through any hidden layers before reaching the output layer. Each neuron processes its input by applying its associated weights, summing the weighted inputs, and transforming the result using an activation function. The output layer produces the network's final prediction, which is then compared to the actual target value from the training data. This comparison generates an error signal, quantifying how far the network's prediction deviates from the expected outcome.

In the backward phase, this error signal is propagated backward through the network to update the weights. The objective is to adjust the weights in a way that reduces the prediction error in subsequent forward passes. This process relies on gradient descent, an optimization technique that determines the optimal direction and magnitude of weight changes to minimize error. The gradient represents the rate of change of the error with respect to each weight, acting as a guide to indicate how adjustments should be made. This is analogous to descending a mountain: by always moving in the direction of the steepest downward slope, the network iteratively approaches the point of minimum error.

The magnitude of weight adjustments is controlled by a parameter known as the learning rate. A high learning rate enables the network to make large updates, which may speed up training but risks overshooting the optimal solution. Conversely, a low learning rate results in more gradual updates, ensuring precise refinements but potentially slowing convergence. Selecting an appropriate learning rate is crucial for efficient training, and modern optimization techniques, such as adaptive learning rate methods, help fine-tune this process dynamically.

To successfully apply gradient descent and backpropagation, the network's activation functions must be differentiable. This requirement ensures that the gradients can be computed efficiently, allowing meaningful weight updates. As a result, smooth, non-linear activation functions such as the sigmoid, hyperbolic tangent (tanh), and ReLU (Rectified Linear Unit) are widely used in neural networks. In modern deep learning, variations of gradient descent, such as stochastic gradient descent (SGD) and Adam, have been developed to improve efficiency and stability, particularly for large-scale datasets.

Through repeated cycles of forward and backward propagation, the network refines its weights, reducing overall error and improving its ability to generalize to unseen data. While this process may seem computationally complex, modern machine learning frameworks such as TensorFlow and PyTorch automate backpropagation, gradient descent, and weight updates, allowing practitioners to focus on designing the network architecture and preparing the data.

The development of backpropagation marked a turning point in neural network research, enabling these models to solve real-world problems with remarkable accuracy. Although training remains computationally demanding, advancements in hardware---such as GPUs and TPUs---have significantly accelerated the process, making it feasible to train large and complex networks efficiently. With a clear understanding of how neural networks learn, we now turn to their practical applications, examining how they uncover patterns in data to make accurate predictions in real-world settings.

\section{Case study: bank marketing}\label{case-study-bank-marketing}

How can a financial institution improve the effectiveness of its future marketing campaigns? To make a data-driven decision, we analyze the bank's previous marketing campaign to identify patterns that can inform future strategies. In this case study, we apply a neural network to predict whether a customer will subscribe to a term deposit based on demographic and campaign-related features. By analyzing this dataset, we aim to uncover key factors that influence customer subscription behavior, helping the bank design more effective marketing strategies.

For this analysis, we use the \emph{bank} dataset, a well-known benchmark dataset sourced from the \href{https://archive.ics.uci.edu/dataset/222/bank+marketing}{UC Irvine Machine Learning Repository} and available in the \textbf{liver} package. This dataset was used by \citet{moro2014data} for a data-driven approach to predicting the success of bank telemarketing campaigns. The goal is to build a classification model that predicts which customers are likely to subscribe to a term deposit.

\subsection*{Problem understanding}\label{problem-understanding-3}


Banks typically use two main strategies to market their financial products:

\begin{itemize}
\tightlist
\item
  \emph{Mass campaigns}: These campaigns reach a broad audience with minimal targeting, often resulting in low response rates (typically below 1\%).\\
\item
  \emph{Directed marketing}: This approach targets customers more likely to be interested in the product, improving conversion rates but raising potential privacy concerns.
\end{itemize}

In this case study, we aim to enhance the effectiveness of directed marketing by analyzing past campaign data to identify patterns in customer behavior. By predicting which customers are more likely to subscribe to a term deposit, the bank can optimize marketing efforts, reduce costs, and minimize intrusive communications while maintaining success rates.

A term deposit is a fixed-term savings product that offers customers higher interest rates than standard savings accounts. Banks use term deposits to secure long-term funds and strengthen their financial reserves. More details on term deposits can be found \href{https://www.investopedia.com/terms/t/termdeposit.asp}{here}.

\subsection*{Overview of the dataset}\label{overview-of-the-dataset-2}


The \emph{bank} dataset includes information on direct phone-based marketing campaigns conducted by a financial institution. Customers were contacted multiple times within the same campaign. The objective of this dataset is to predict whether a customer will subscribe to a term deposit (\texttt{deposit\ =\ "yes"} or \texttt{"no"}).

We load the \emph{bank} dataset directly into R and examine its structure using the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)   }\CommentTok{\# Load the liver package}

\FunctionTok{data}\NormalTok{(bank)   }\CommentTok{\# Load the bank marketing dataset }

\FunctionTok{str}\NormalTok{(bank)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{4521}\NormalTok{ obs. of  }\DecValTok{17}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{30} \DecValTok{33} \DecValTok{35} \DecValTok{30} \DecValTok{59} \DecValTok{35} \DecValTok{36} \DecValTok{39} \DecValTok{41} \DecValTok{43}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{8} \DecValTok{5} \DecValTok{5} \DecValTok{2} \DecValTok{5} \DecValTok{7} \DecValTok{10} \DecValTok{3} \DecValTok{8}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education}\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{2} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1787} \DecValTok{4789} \DecValTok{1350} \DecValTok{1476} \DecValTok{0} \DecValTok{747} \DecValTok{307} \DecValTok{147} \DecValTok{221} \SpecialCharTok{{-}}\DecValTok{88}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan     }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day      }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{19} \DecValTok{11} \DecValTok{16} \DecValTok{3} \DecValTok{5} \DecValTok{23} \DecValTok{14} \DecValTok{6} \DecValTok{14} \DecValTok{17}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{11} \DecValTok{9} \DecValTok{1} \DecValTok{7} \DecValTok{9} \DecValTok{4} \DecValTok{9} \DecValTok{9} \DecValTok{9} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{79} \DecValTok{220} \DecValTok{185} \DecValTok{199} \DecValTok{226} \DecValTok{141} \DecValTok{341} \DecValTok{151} \DecValTok{57} \DecValTok{313}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays    }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \DecValTok{339} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{176} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{147}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{4} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{3} \DecValTok{2} \DecValTok{0} \DecValTok{0} \DecValTok{2}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{1} \DecValTok{1} \DecValTok{4} \DecValTok{4} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains 4521 observations and 17 variables. The target variable, \texttt{deposit}, is binary, with two categories: \texttt{yes} and \texttt{no}. Below is a summary of all features:

\textbf{Demographic features:}\\
- \texttt{age}: Age of the customer (numeric).\\
- \texttt{job}: Type of job (e.g., ``admin.'', ``blue-collar'', ``management'').\\
- \texttt{marital}: Marital status (e.g., ``married'', ``single'').\\
- \texttt{education}: Level of education (e.g., ``secondary'', ``tertiary'').\\
- \texttt{default}: Whether the customer has credit in default (binary: ``yes'', ``no'').\\
- \texttt{balance}: Average yearly balance in euros (numeric).

\textbf{Loan information:}\\
- \texttt{housing}: Whether the customer has a housing loan (binary).\\
- \texttt{loan}: Whether the customer has a personal loan (binary).

\textbf{Campaign details:}\\
- \texttt{contact}: Type of communication used (e.g., ``telephone'', ``cellular'').\\
- \texttt{day}: Last contact day of the month (numeric).\\
- \texttt{month}: Last contact month of the year (categorical: ``jan'', ``feb'', ``mar'', \ldots, ``nov'', ``dec'').\\
- \texttt{duration}: Last contact duration in seconds (numeric).\\
- \texttt{campaign}: Total number of contacts made with the customer during the campaign (numeric).\\
- \texttt{pdays}: Days since the customer was last contacted (numeric).\\
- \texttt{previous}: Number of contacts before the current campaign (numeric).\\
- \texttt{poutcome}: Outcome of the previous campaign (e.g., ``success'', ``failure'').

\textbf{Target variable:}\\
- \texttt{deposit}: Indicates whether the customer subscribed to a term deposit (binary: ``yes'', ``no'').

This dataset contains a diverse set of features related to customer demographics and past interactions, making it well-suited for building predictive models to improve marketing strategies.

With the dataset ready, we now proceed to data preparation and model training using a neural network to predict customer subscription behavior.

\subsection*{Preparing data for modeling}\label{preparing-data-for-modeling-3}


Before training the neural network model, we need to prepare the dataset by splitting it into training and test sets. This ensures that we can assess how well the model generalizes to new, unseen data. We use an 80/20 split, allocating 80\% of the data for training and 20\% for testing. To maintain consistency with previous chapters, we apply the \texttt{partition()} function from the \textbf{liver} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{500}\NormalTok{)}

\NormalTok{data\_sets }\OtherTok{=} \FunctionTok{partition}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bank, }\AttributeTok{ratio =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{train\_set }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part1}
\NormalTok{test\_set  }\OtherTok{=}\NormalTok{ data\_sets}\SpecialCharTok{$}\NormalTok{part2}

\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{deposit}
\end{Highlighting}
\end{Shaded}

The \texttt{set.seed()} function ensures reproducibility by fixing the random seed. We use \texttt{train\_set} to train the classification models, while \texttt{test\_set} serves as unseen data for evaluation. The \texttt{test\_labels} vector contains the true class labels for \texttt{test\_set}, which we will compare against the model's predictions.

To validate this split, we compare the proportion of \texttt{deposit\ =\ "yes"} in the training and test sets using a two-sample Z-test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(train\_set}\SpecialCharTok{$}\NormalTok{deposit }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{x2 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(test\_set}\SpecialCharTok{$}\NormalTok{deposit }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}

\NormalTok{n1 }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(train\_set)}
\NormalTok{n2 }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(test\_set)}

\FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(x1, x2), }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(n1, n2))}
   
    \DecValTok{2}\SpecialCharTok{{-}}\NormalTok{sample test }\ControlFlowTok{for}\NormalTok{ equality of proportions with continuity correction}
   
\NormalTok{   data}\SpecialCharTok{:}  \FunctionTok{c}\NormalTok{(x1, x2) out of }\FunctionTok{c}\NormalTok{(n1, n2)}
\NormalTok{   X}\SpecialCharTok{{-}}\NormalTok{squared }\OtherTok{=} \FloatTok{0.0014152}\NormalTok{, df }\OtherTok{=} \DecValTok{1}\NormalTok{, p}\SpecialCharTok{{-}}\NormalTok{value }\OtherTok{=} \FloatTok{0.97}
\NormalTok{   alternative hypothesis}\SpecialCharTok{:}\NormalTok{ two.sided}
   \DecValTok{95}\NormalTok{ percent confidence interval}\SpecialCharTok{:}
    \SpecialCharTok{{-}}\FloatTok{0.02516048}  \FloatTok{0.02288448}
\NormalTok{   sample estimates}\SpecialCharTok{:}
\NormalTok{      prop }\DecValTok{1}\NormalTok{    prop }\DecValTok{2} 
   \FloatTok{0.1150124} \FloatTok{0.1161504}
\end{Highlighting}
\end{Shaded}

The test confirms that the proportions in both subsets are statistically similar (\emph{p}-value \textgreater{} 0.05), validating our split. This ensures that the training and test sets are representative of the original dataset, enabling us to evaluate the model's performance effectively.

Our objective is to classify customers as either likely (\texttt{deposit\ =\ "yes"}) or unlikely (\texttt{deposit\ =\ "no"}) to subscribe to a term deposit, based on the following predictors:

\texttt{age}, \texttt{default}, \texttt{balance}, \texttt{housing}, \texttt{loan}, \texttt{duration}, \texttt{campaign}, \texttt{pdays}, and \texttt{previous}.

\subsubsection*{One-hot encoding}\label{one-hot-encoding-3}


Since neural networks require numerical inputs, categorical variables must be converted into numeric representations. One-hot encoding achieves this by creating separate binary variables for each category. We apply the \texttt{one.hot()} function from the \textbf{liver} package to transform categorical features into a numerical format suitable for a neural network:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"default"}\NormalTok{, }\StringTok{"housing"}\NormalTok{, }\StringTok{"loan"}\NormalTok{)}

\NormalTok{train\_onehot }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(train\_set, }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}
\NormalTok{test\_onehot  }\OtherTok{=} \FunctionTok{one.hot}\NormalTok{(test\_set,  }\AttributeTok{cols =}\NormalTok{ categorical\_vars)}

\FunctionTok{str}\NormalTok{(test\_onehot)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{904}\NormalTok{ obs. of  }\DecValTok{20}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ age        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{43} \DecValTok{40} \DecValTok{56} \DecValTok{25} \DecValTok{31} \DecValTok{32} \DecValTok{23} \DecValTok{36} \DecValTok{32} \DecValTok{32}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ job        }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"admin."}\NormalTok{,}\StringTok{"blue{-}collar"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{5} \DecValTok{10} \DecValTok{2} \DecValTok{10} \DecValTok{2} \DecValTok{8} \DecValTok{5} \DecValTok{10} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ marital    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"divorced"}\NormalTok{,}\StringTok{"married"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ education  }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"primary"}\NormalTok{,}\StringTok{"secondary"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default\_no }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ default\_yes}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ balance    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{264} \DecValTok{194} \DecValTok{4073} \SpecialCharTok{{-}}\DecValTok{221} \DecValTok{171} \DecValTok{2089} \DecValTok{363} \DecValTok{553} \DecValTok{2204} \SpecialCharTok{{-}}\DecValTok{849}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing\_no }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ housing\_yes}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan\_no    }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{0} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ loan\_yes   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{1} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ contact    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{3}\NormalTok{ levels }\StringTok{"cellular"}\NormalTok{,}\StringTok{"telephone"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ day        }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{17} \DecValTok{29} \DecValTok{27} \DecValTok{23} \DecValTok{27} \DecValTok{14} \DecValTok{30} \DecValTok{11} \DecValTok{21} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ month      }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{12}\NormalTok{ levels }\StringTok{"apr"}\NormalTok{,}\StringTok{"aug"}\NormalTok{,}\StringTok{"dec"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{2} \DecValTok{9} \DecValTok{2} \DecValTok{10} \DecValTok{9} \DecValTok{2} \DecValTok{10} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ duration   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{113} \DecValTok{189} \DecValTok{239} \DecValTok{250} \DecValTok{81} \DecValTok{132} \DecValTok{16} \DecValTok{106} \DecValTok{11} \DecValTok{204}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ campaign   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{2} \DecValTok{2} \DecValTok{5} \DecValTok{1} \DecValTok{3} \DecValTok{1} \DecValTok{18} \DecValTok{2} \DecValTok{4} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ pdays      }\SpecialCharTok{:}\NormalTok{ int  }\SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ previous   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ poutcome   }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{4}\NormalTok{ levels }\StringTok{"failure"}\NormalTok{,}\StringTok{"other"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4} \DecValTok{4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ deposit    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{2} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The \texttt{one.hot()} function expands categorical variables into binary columns. For example, the binary variable \texttt{default} is transformed into two new variables, \texttt{default\_yes} and \texttt{default\_no}, indicating whether the customer has defaulted on credit. These selected predictors are used in the following formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{=}\NormalTok{ deposit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ default\_yes }\SpecialCharTok{+}\NormalTok{ housing\_yes }\SpecialCharTok{+}\NormalTok{ loan\_yes }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ balance }\SpecialCharTok{+}\NormalTok{ duration }\SpecialCharTok{+}\NormalTok{ campaign }\SpecialCharTok{+}\NormalTok{ pdays }\SpecialCharTok{+}\NormalTok{ previous}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Feature scaling}\label{feature-scaling-3}


Neural networks perform best when input features are on a similar scale. To achieve this, we normalize numerical variables using min-max scaling, transforming all inputs into a standardized range between 0 and 1. This prevents features with larger numerical ranges from dominating the learning process and helps improve model convergence.

To prevent data leakage, the scaling parameters (minimum and maximum values) are computed from the training set only and then applied consistently to the test set. This ensures that the test set remains an independent evaluation dataset, preventing it from influencing model training.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numeric\_vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"balance"}\NormalTok{, }\StringTok{"duration"}\NormalTok{, }\StringTok{"campaign"}\NormalTok{, }\StringTok{"pdays"}\NormalTok{, }\StringTok{"previous"}\NormalTok{)}

\NormalTok{min\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_onehot[, numeric\_vars], min)}
\NormalTok{max\_train }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(train\_onehot[, numeric\_vars], max)}

\NormalTok{train\_scaled }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(train\_onehot, }\AttributeTok{col =}\NormalTok{ numeric\_vars, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\NormalTok{test\_scaled  }\OtherTok{=} \FunctionTok{minmax}\NormalTok{(test\_onehot,  }\AttributeTok{col =}\NormalTok{ numeric\_vars, }\AttributeTok{min =}\NormalTok{ min\_train, }\AttributeTok{max =}\NormalTok{ max\_train)}
\end{Highlighting}
\end{Shaded}

To visualize the effect of scaling, we compare the distribution of \texttt{age} before and after applying min-max transformation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train\_set) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{colour =} \StringTok{"darkblue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Variable \textquotesingle{}age\textquotesingle{} before scaling"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train\_scaled) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age), }\AttributeTok{colour =} \StringTok{"darkblue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Variable \textquotesingle{}age\textquotesingle{} after scaling"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{12_Neural_Network_files/figure-latex/unnamed-chunk-7-1} \includegraphics[width=0.5\linewidth]{12_Neural_Network_files/figure-latex/unnamed-chunk-7-2}

The first histogram (left) displays the distribution of \texttt{age} in the training set before scaling, while the second histogram (right) shows the distribution after applying min-max scaling. The values are mapped to a range between 0 and 1 while preserving the original distribution.

\subsection*{Applying the neural network algorithm}\label{applying-the-neural-network-algorithm}


To implement a neural network, we use the \href{https://CRAN.R-project.org/package=neuralnet}{\textbf{neuralnet}} package in R. This package offers a straightforward and flexible way to build neural networks and provides functionality for visualizing the network topology. While \textbf{neuralnet} is a useful learning tool, it is also powerful enough for practical applications.

If the \textbf{neuralnet} package is not installed, it can be installed using \texttt{install.packages("neuralnet")}.Then, load it into the R session:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(neuralnet)}
\end{Highlighting}
\end{Shaded}

Next, we apply the \texttt{neuralnet()} function to the training dataset to build our model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralnet\_bank }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ formula,}
  \AttributeTok{data =}\NormalTok{ train\_scaled,}
  \AttributeTok{hidden =} \DecValTok{1}\NormalTok{,                }\CommentTok{\# Single hidden layer with 1 node}
  \AttributeTok{err.fct =} \StringTok{"sse"}\NormalTok{,           }\CommentTok{\# Loss function: Sum of Squared Errors}
  \AttributeTok{linear.output =} \ConstantTok{FALSE}      \CommentTok{\# Logistic activation function for classification}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here's what each argument in the function call does:

\begin{itemize}
\tightlist
\item
  \texttt{formula}: Specifies the relationship between the target variable (\texttt{deposit}) and the predictors.\\
\item
  \texttt{data}: Indicates the dataset used for training (\texttt{train\_scaled}).\\
\item
  \texttt{hidden\ =\ 1}: Defines the number of hidden layers and nodes (one hidden layer with a single node). For simplicity, we start with a minimal architecture, but more complex networks with additional nodes and layers may improve performance.\\
\item
  \texttt{err.fct\ =\ "sse"}: Specifies the sum of squared errors as the loss function. While SSE is commonly used, cross-entropy loss (\texttt{ce}) is often preferred for classification tasks.\\
\item
  \texttt{linear.output\ =\ FALSE}: Ensures that the output layer uses a logistic activation function, which is essential for classification tasks where the predicted values represent probabilities.
\end{itemize}

After training, we visualize the network to examine its topology:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(neuralnet\_bank, }\AttributeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{12_Neural_Network_files/figure-latex/unnamed-chunk-10-1} \end{center}

This visualization shows that the network consists of:

\begin{itemize}
\tightlist
\item
  9 input nodes, corresponding to the 9 predictors,\\
\item
  1 hidden layer containing a single node, and\\
\item
  2 output nodes representing the classification result (\texttt{yes} or \texttt{no}).
\end{itemize}

The training process converged after 1998 steps, with a final error rate of 284.9. An analysis of the trained network's weights indicates that \texttt{duration}---the length of the last phone call---has the strongest influence on the prediction. This finding aligns with prior studies, where call duration has been a key indicator of customer engagement in marketing campaigns.

This simple neural network model demonstrates how input features are processed through multiple layers to extract patterns and make predictions. In the next section, we will evaluate the model's performance and interpret its results.

\subsection*{Prediction and model evaluation}\label{prediction-and-model-evaluation-2}


After training the neural network, we evaluate its performance by applying it to the test set, which contains customers unseen during training. The goal is to compare the model's predictions with the actual class labels stored in \texttt{test\_labels}.

To obtain predictions from the neural network, we use the \texttt{predict()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob\_nn }\OtherTok{=} \FunctionTok{predict}\NormalTok{(neuralnet\_bank, test\_scaled)}
\end{Highlighting}
\end{Shaded}

To inspect the model's predictions, we display the first six output values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(prob\_nn)}
\NormalTok{             [,}\DecValTok{1}\NormalTok{]       [,}\DecValTok{2}\NormalTok{]}
\NormalTok{   [}\DecValTok{1}\NormalTok{,] }\FloatTok{0.9849665} \FloatTok{0.01506696}
\NormalTok{   [}\DecValTok{2}\NormalTok{,] }\FloatTok{0.9782933} \FloatTok{0.02174953}
\NormalTok{   [}\DecValTok{3}\NormalTok{,] }\FloatTok{0.8866629} \FloatTok{0.11342508}
\NormalTok{   [}\DecValTok{4}\NormalTok{,] }\FloatTok{0.9760059} \FloatTok{0.02403981}
\NormalTok{   [}\DecValTok{5}\NormalTok{,] }\FloatTok{0.9686046} \FloatTok{0.03144922}
\NormalTok{   [}\DecValTok{6}\NormalTok{,] }\FloatTok{0.9840902} \FloatTok{0.01594459}
\end{Highlighting}
\end{Shaded}

Each column represents one output unit. Since \texttt{neuralnet()} does not automatically apply a softmax function, the values should be interpreted as raw activations rather than probabilities. To classify a customer as likely to subscribe, we compare the output activations. If the activation corresponding to \texttt{deposit\ =\ "yes"} is greater than the activation for \texttt{deposit\ =\ "no"}, the customer is predicted as a subscriber.

We evaluate the predictions using a confusion matrix with a cutoff value of 0.5:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract the prediction for \textquotesingle{}deposit = "yes"\textquotesingle{}}
\NormalTok{prob\_nn\_yes }\OtherTok{=}\NormalTok{ prob\_nn[, }\DecValTok{2}\NormalTok{] }

\FunctionTok{conf.mat}\NormalTok{(prob\_nn\_yes, test\_labels, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{reference =} \StringTok{"yes"}\NormalTok{)}
\NormalTok{          Actual}
\NormalTok{   Predict yes  no}
\NormalTok{       yes  }\DecValTok{22}  \DecValTok{16}
\NormalTok{       no   }\DecValTok{83} \DecValTok{783}
\end{Highlighting}
\end{Shaded}

The confusion matrix summarizes the model's performance by comparing predicted versus actual labels. It provides insight into classification accuracy, as well as the number of true positives, false positives, true negatives, and false negatives.

Finally, we assess the model's performance by plotting the ROC curve and calculating the AUC:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)   }\CommentTok{\# For ROC curve}

\NormalTok{roc\_nn }\OtherTok{=} \FunctionTok{roc}\NormalTok{(test\_labels, prob\_nn\_yes)}

\FunctionTok{ggroc}\NormalTok{(roc\_nn, }\AttributeTok{size =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"ROC for Neural Network Algorithm"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(), }\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(.}\DecValTok{7}\NormalTok{, .}\DecValTok{3}\NormalTok{), }\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{17}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{12_Neural_Network_files/figure-latex/unnamed-chunk-14-1} \end{center}

The ROC curve evaluates how well the model differentiates between customers who subscribe (\texttt{deposit\ =\ "yes"}) and those who do not (\texttt{deposit\ =\ "no"}) at varying classification thresholds. A high AUC score indicates strong predictive performance, meaning the model is effective at distinguishing between the two classes.

\section*{Exercises}\label{exercises-8}


These exercises reinforce the concepts introduced in this chapter, focusing on understanding neural networks, tuning hyperparameters, and comparing their performance with tree-based models. They are divided into conceptual questions, practical tasks using the \emph{bank} dataset, and comparative exercises using the \emph{adult} dataset.

\subsubsection*{Conceptual questions}\label{conceptual-questions-10}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain the basic structure of a neural network. What role do the input, hidden, and output layers play?
\item
  What is the rule of activation functions in a neural network? Why are non-linear activation functions essential?
\item
  Compare and contrast ReLU, sigmoid, and tanh activation functions. In what scenarios would each be preferred?
\item
  Why are neural networks considered universal function approximators?
\item
  Explain how backpropagation updates the weights in a neural network.
\item
  Why do neural networks often require large datasets to perform well?
\item
  Define the bias-variance tradeoff in neural networks. How does model complexity affect bias and variance?
\item
  What is the purpose of dropout regularization, and how does it help prevent overfitting?
\item
  What is the role of the loss function in training a neural network?
\item
  What is weight initialization, and why is it important?
\item
  Compare shallow neural networks to deep neural networks. What advantages do deeper architectures provide?
\item
  How does the number of hidden layers affect a neural network's ability to model complex patterns?
\item
  What is the role of hyperparameter tuning in neural networks?
\item
  Compare the computational efficiency of decision trees and neural networks.
\end{enumerate}

\subsubsection*{\texorpdfstring{Practical exercises using the \emph{bank} dataset}{Practical exercises using the bank dataset}}\label{practical-exercises-using-the-bank-dataset-1}


\paragraph*{Data preparation and model training}\label{data-preparation-and-model-training}
\addcontentsline{toc}{paragraph}{Data preparation and model training}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\tightlist
\item
  Load the \emph{bank} dataset and examine its structure. What are the key predictors?
\item
  Split the dataset into training (80\%) and testing (20\%) sets, ensuring the split is statistically representative.
\item
  Apply one-hot encoding to categorical variables. What are the advantages of one-hot encoding over label encoding?
\item
  Normalize the numerical features using min-max scaling. Why is scaling important for neural networks?
\item
  Train a basic feed-forward neural network with one hidden layer containing five neurons. Evaluate its accuracy.
\item
  Increase the number of neurons in the hidden layer to ten. Does the accuracy improve?
\item
  Train a neural network with two hidden layers (five neurons in the first layer, three in the second). How does performance compare?
\item
  Change the activation function from ReLU to sigmoid. How does this affect the model's convergence and accuracy?
\item
  Train a model using cross-entropy loss instead of sum of squared errors. Which performs better?
\end{enumerate}

\paragraph*{Model evaluation and comparison with tree-based models}\label{model-evaluation-and-comparison-with-tree-based-models}
\addcontentsline{toc}{paragraph}{Model evaluation and comparison with tree-based models}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{23}
\tightlist
\item
  Compute the confusion matrix and interpret precision, recall, and F1-score for the neural network model.
\item
  Plot the ROC curve and calculate AUC. How well does the model differentiate between subscribed and non-subscribed customers?
\item
  Compare the neural network's performance with a decision tree classifier trained on the same dataset.
\item
  Compare the neural network's performance with a random forest model trained on the same dataset.
\item
  Which model (decision tree, random forest, or neural network) performs best in terms of accuracy, precision, and recall?
\end{enumerate}

\paragraph*{\texorpdfstring{Training neural networks on the \emph{adult} dataset}{Training neural networks on the adult dataset}}\label{training-neural-networks-on-the-adult-dataset}
\addcontentsline{toc}{paragraph}{Training neural networks on the \emph{adult} dataset}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{28}
\tightlist
\item
  Load the \emph{adult} dataset and examine its structure. What are the key differences between this dataset and the \emph{bank} dataset?
\item
  Preprocess the categorical features using one-hot encoding.
\item
  Normalize the numerical features using min-max scaling.
\item
  Split the dataset into training (80\%) and testing (20\%) sets.
\item
  Train a basic neural network with one hidden layer (five neurons) to predict income level (\texttt{\textless{}=50K} or \texttt{\textgreater{}50K}).
\item
  Increase the number of neurons in the hidden layer to ten. Does performance improve?
\item
  Train a deeper neural network with two hidden layers (ten and five neurons).
\item
  Compare ReLU, tanh, and sigmoid activation functions on model performance.
\item
  Train a decision tree on the \emph{adult} dataset and compare its accuracy with the neural network model.
\item
  Train a random forest on the \emph{adult} dataset and compare its performance to the neural network model.
\item
  Analyze the feature importance in the random forest model and compare it to the most influential features in the neural network model.
\item
  Compare the ROC curves of the neural network, decision tree, and random forest models. Which model has the highest AUC?
\item
  Which model performs better in predicting high-income individuals, and why?
\end{enumerate}

\chapter{Clustering}\label{chapter-cluster}

Every day, we interact with systems that organize vast amounts of data without explicit instructions. How does Netflix recommend movies tailored to your taste? How does Amazon categorize millions of products? These are real-world examples of \emph{clustering}, a machine learning technique that groups similar items based on shared characteristics---without any predefined labels.

In many real-world scenarios, we deal with large datasets where the structure is unknown. Unlike classification, which assigns predefined labels to data points (e.g., distinguishing between spam and non-spam emails), clustering is \emph{exploratory}---it helps uncover hidden patterns, making it a powerful tool for \emph{knowledge discovery}. By identifying \emph{meaningful groups}, clustering allows us to make sense of complex data and extract valuable insights.

Clustering is widely used across multiple domains, including:

\begin{itemize}
\tightlist
\item
  \emph{Customer segmentation} -- Identifying distinct customer groups to personalize marketing campaigns.\\
\item
  \emph{Market research} -- Understanding consumer behavior to enhance product recommendations.\\
\item
  \emph{Fraud detection} -- Detecting suspicious financial transactions that may indicate fraudulent activity.\\
\item
  \emph{Document organization} -- Automatically grouping large collections of text into meaningful categories.\\
\item
  \emph{Bioinformatics} -- Clustering genes with similar expression patterns to uncover biological insights.
\end{itemize}

This chapter provides a comprehensive introduction to clustering, covering:

\begin{itemize}
\tightlist
\item
  The fundamental principles of clustering and how it differs from classification.\\
\item
  The mechanics of clustering algorithms and how they define similarity.\\
\item
  \emph{K-means clustering}, one of the most widely used clustering techniques.\\
\item
  A practical case study: segmenting cereal brands based on their nutritional content.
\end{itemize}

By the end of this chapter, you will understand how clustering works, when to apply it, and how to implement it in real-world scenarios. Let's dive in!

\section{What is Cluster Analysis?}\label{cluster-what}

Clustering is an unsupervised machine learning technique that groups data points into \emph{clusters} based on their similarity. Unlike supervised learning, where models learn from labeled examples, clustering is \emph{exploratory}---it uncovers \emph{hidden structures} in data without predefined labels. The goal is to form groups where data points within the same cluster are highly similar, while those in different clusters are distinct.

But how does a computer determine which data points belong together? Clustering relies on \emph{similarity measures} that quantify how close or distant two points are. One of the most commonly used approaches is \emph{distance metrics}, such as \emph{Euclidean distance}, defined as:

\[
\text{dist}(x, y) = \sqrt{ \sum_{i=1}^n (x_i - y_i)^2}
\]

where \(x = (x_1, x_2, \ldots, x_n)\) and \(y = (y_1, y_2, \ldots, y_n)\) represent two data points with \(n\) features. The closer the two points, the more similar they are.

However, Euclidean distance is not always appropriate. For \emph{categorical variables}, alternative strategies such as \emph{one-hot encoding} transform categories into numerical values, enabling distance-based clustering. Additionally, features often require \emph{scaling} (e.g., min-max normalization) to ensure that no single variable dominates the clustering process.

Clustering is often compared to classification, but they serve different purposes. \emph{Classification assigns predefined labels to new data points based on past examples, whereas clustering discovers groupings from raw data.} Classification is typically used for prediction, while clustering is primarily for \emph{exploration and pattern discovery}. Because clustering generates labels rather than predicting existing ones, it is sometimes referred to as \emph{unsupervised classification}. These cluster assignments can then be used as inputs for further analysis, such as refining predictions in a neural network or decision tree model.

All clustering algorithms aim to achieve \emph{high intra-cluster similarity} (data points within a cluster are close together) and \emph{low inter-cluster similarity} (clusters are well separated). This concept is visually illustrated in Figure \ref{fig:cluster-1}, where effective clusters minimize internal variation while maximizing separation between groups.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\linewidth]{images/ch13_cluster_1} 

}

\caption{Clustering algorithms aim to minimize intra-cluster variation while maximizing inter-cluster separation.}\label{fig:cluster-1}
\end{figure}

Beyond its role in data exploration, clustering is widely used as a \emph{preprocessing step} in machine learning. Given the massive scale of modern datasets, clustering helps reduce complexity by identifying \emph{a smaller number of representative groups}, leading to several benefits:

\begin{itemize}
\tightlist
\item
  \emph{Reduced computation time} for downstream models.\\
\item
  \emph{Improved interpretability} by summarizing large datasets.\\
\item
  \emph{Enhanced predictive performance} by structuring inputs for supervised learning.
\end{itemize}

In the following sections, we will explore \emph{K-means clustering}, one of the most widely used clustering algorithms. We will also discuss methods for selecting the optimal number of clusters and apply clustering to a real-world dataset.

\section{K-means Clustering}\label{kmeans}

K-means clustering is one of the simplest and most widely used clustering algorithms. It aims to partition a dataset into \emph{\(k\)} clusters by iteratively refining cluster centers, ensuring that data points within each cluster are as similar as possible. The algorithm operates through an iterative process of assigning points to clusters and updating cluster centers based on those assignments. The process stops when the assignments stabilize, meaning no data points switch clusters.

The K-means algorithm requires the user to specify the number of clusters, \emph{\(k\)}, in advance. It follows these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Initialize:} Randomly select \emph{\(k\)} data points as the initial cluster centers.\\
\item
  \emph{Assignment:} Assign each data point to the nearest cluster center. This creates \emph{\(k\)} groups.\\
\item
  \emph{Update:} Compute the centroid (mean) of each cluster and move the cluster centers to these new locations.\\
\item
  \emph{Repeat:} Iterate steps 2 and 3 until convergence---when cluster assignments no longer change.
\end{enumerate}

Although K-means is simple and efficient, it has some limitations. The final clusters depend heavily on the \emph{initial choice of cluster centers}, meaning different runs of the algorithm may produce different results. Additionally, K-means is sensitive to outliers and assumes clusters are \emph{spherical and of similar size}, which may not always be the case in real-world data.

To illustrate how K-means works, consider a dataset with 50 records and two features, \emph{\(x_1\)} and \emph{\(x_2\)}, as shown in Figure \ref{fig:cluster-ex-1}. Our goal is to partition the data into \emph{three} clusters.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/ch13_cluster_ex_1} 

}

\caption{A simple dataset with 50 records and two features, ready for clustering.}\label{fig:cluster-ex-1}
\end{figure}

The first step is to randomly select three initial cluster centers (red stars), as shown in the left panel of Figure \ref{fig:cluster-ex-2}. Each data point is then assigned to the nearest cluster, forming three groups labeled in \emph{blue (Cluster A), green (Cluster B), and orange (Cluster C)}. The right panel of Figure \ref{fig:cluster-ex-2} displays these initial assignments. The dashed lines represent the \emph{Voronoi diagram}, which divides space into regions associated with each cluster center.

\begin{figure}[H]

{\centering \includegraphics[width=0.45\linewidth]{images/ch13_cluster_ex_2} \includegraphics[width=0.45\linewidth]{images/ch13_cluster_ex_3} 

}

\caption{Initial random cluster centers (left) and first cluster assignments (right).}\label{fig:cluster-ex-2}
\end{figure}

Since K-means is sensitive to \emph{initialization}, poor placement of initial cluster centers can lead to suboptimal clustering. To mitigate this issue, \emph{K-means++} \citep{arthur2006k} was introduced in 2007. This method strategically selects initial centers to improve convergence and reduce randomness.

Once the initial cluster assignments are made, K-means enters the \emph{update phase}. The first step is to recompute the centroid of each cluster, which is the \emph{mean position of all points assigned to that cluster}. The cluster centers are then moved to these new centroid locations, as shown in the left panel of Figure \ref{fig:cluster-ex-3}. The right panel illustrates how the \emph{Voronoi boundaries shift}, causing some data points to be reassigned to a different cluster.

\begin{figure}[H]

{\centering \includegraphics[width=0.45\linewidth]{images/ch13_cluster_ex_4} \includegraphics[width=0.45\linewidth]{images/ch13_cluster_ex_5} 

}

\caption{Updated cluster centers (left) and new assignments after centroid adjustment (right).}\label{fig:cluster-ex-3}
\end{figure}

This process---\emph{reassigning points and updating centroids}---continues iteratively. After another update, some points switch clusters again, leading to a refined Voronoi partition, as shown in Figure \ref{fig:cluster-ex-6}.

\begin{figure}[H]

{\centering \includegraphics[width=0.45\linewidth]{images/ch13_cluster_ex_6} \includegraphics[width=0.45\linewidth]{images/ch13_cluster_ex_7} 

}

\caption{Updated cluster centers and assignments after another iteration.}\label{fig:cluster-ex-6}
\end{figure}

The algorithm continues iterating until the cluster assignments stabilize---when \emph{no more points switch clusters}, as shown in Figure \ref{fig:cluster-ex-8}. At this point, the algorithm \emph{converges}, and the final clusters are established.

\begin{figure}[H]

{\centering \includegraphics[width=0.45\linewidth]{images/ch13_cluster_ex_8} 

}

\caption{Final cluster assignments after K-means convergence.}\label{fig:cluster-ex-8}
\end{figure}

Once clustering is complete, the results can be presented in two ways:

\begin{itemize}
\tightlist
\item
  \emph{Cluster Assignments:} Each data point is labeled as belonging to Cluster A, B, or C.\\
\item
  \emph{Centroid Coordinates:} The final positions of the cluster centers can be reported.
\end{itemize}

These final cluster centroids act as \emph{representative points}, summarizing the dataset and enabling further analysis. K-means clustering is widely used in applications such as customer segmentation, image compression, and document clustering. In the next section, we will explore methods for selecting the \emph{optimal number of clusters} to ensure meaningful partitions in real-world datasets.

\section{Choosing the Number of Clusters}\label{kmeans-choose}

One of the key challenges in \emph{K-means clustering} is selecting the appropriate number of clusters, \emph{\(k\)}. The choice of \(k\) significantly impacts the results---too few clusters may fail to capture meaningful structures, while too many clusters risk overfitting by creating overly fragmented groups. Unlike supervised learning, where evaluation metrics like accuracy guide model selection, clustering does not have an absolute ground truth, making the selection of \(k\) more subjective.

In some cases, domain knowledge can provide useful guidance. For example, when clustering movies, a reasonable starting point might be the number of well-known genres. In a business setting, marketing teams may set \(k = 3\) if they plan to design three distinct advertising campaigns. Similarly, seating arrangements at a conference might determine the number of groups based on the available tables. However, when no clear intuition exists, data-driven methods are needed to determine an optimal \(k\).

One widely used technique for choosing \(k\) is the \emph{elbow method}, which evaluates how the within-cluster variation changes as the number of clusters increases. As more clusters are added, the clusters become more \emph{homogeneous} (internal similarity increases), and overall \emph{heterogeneity} (difference between clusters) decreases. However, this improvement follows a diminishing returns pattern. The idea is to find the point at which adding another cluster \emph{no longer significantly reduces the within-cluster variance}.

This critical point, known as the \emph{elbow point}, represents the most natural number of clusters. The concept is illustrated in Figure \ref{fig:cluster-elbow}, where the curve shows the total within-cluster sum of squares (WCSS) as a function of \(k\). The ``elbow'' in the curve---where the rate of improvement slows---is a strong candidate for \(k\).

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth]{images/ch13_cluster_elbow} 

}

\caption{The elbow method helps determine the optimal number of clusters in K-means clustering.}\label{fig:cluster-elbow}
\end{figure}

While the elbow method provides a useful heuristic, it has limitations. In some datasets, the curve may not exhibit a clear elbow, making the choice of \(k\) more ambiguous. Additionally, evaluating many different values of \(k\) can be computationally expensive, especially for large datasets.

Other techniques can supplement or refine the selection of \(k\):

\begin{itemize}
\tightlist
\item
  \emph{Silhouette Score:} Measures how well each point fits within its assigned cluster compared to others. A higher silhouette score suggests a well-defined clustering structure.\\
\item
  \emph{Gap Statistic:} Compares the clustering result with a reference distribution to assess whether the structure is significant.\\
\item
  \emph{Cross-validation with clustering tasks:} In applications where clustering feeds into a downstream task (e.g., classification), the impact of different \(k\) values can be evaluated in that context.
\end{itemize}

Ultimately, the choice of \(k\) should be driven by both \emph{data characteristics} and \emph{practical considerations}. Clustering is often used for \emph{exploratory analysis}, meaning that the most useful \(k\) is not necessarily the mathematically ``optimal'' one but rather the one that yields meaningful, interpretable insights.

Observing how cluster characteristics evolve as \(k\) varies can itself be informative. Some groups may remain stable across different \(k\) values, indicating strong natural boundaries, while others may appear and disappear, suggesting more fluid structures in the data.

Rather than aiming for a perfect cluster count, it is often sufficient to find a \emph{reasonable and interpretable} clustering solution. In the next section, we will apply clustering to a real-world dataset, demonstrating how practical knowledge can guide the choice of \(k\) for actionable insights.

Now that we have explored K-means clustering and methods for selecting the optimal number of clusters, we apply these concepts to a real-world dataset.

\section{Case Study: Clustering Cereal Data}\label{kmeans-cereal}

In this case study, we apply \emph{K-means clustering} to the \emph{cereal} dataset from the \textbf{liver} package. This dataset contains nutritional information for 77 cereal brands, including calories, protein, fat, sodium, fiber, and sugar content. Understanding these nutritional profiles is valuable for marketing strategies, consumer targeting, and product positioning. Our goal is to segment cereals into distinct groups based on their nutritional similarities.

\subsection{Dataset Overview}\label{dataset-overview}

The \emph{cereal} dataset includes 77 observations and 16 variables, covering various nutritional attributes. It can be accessed through the \textbf{liver} package, as shown below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)  }\CommentTok{\# Load the liver package}

\FunctionTok{data}\NormalTok{(cereal)    }\CommentTok{\# Load the cereal dataset}
\end{Highlighting}
\end{Shaded}

We can examine its structure using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(cereal)}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{77}\NormalTok{ obs. of  }\DecValTok{16}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ name    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{77}\NormalTok{ levels }\StringTok{"100\% Bran"}\NormalTok{,}\StringTok{"100\% Natural Bran"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{4} \DecValTok{5} \DecValTok{6} \DecValTok{7} \DecValTok{8} \DecValTok{9} \DecValTok{10}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ manuf   }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{7}\NormalTok{ levels }\StringTok{"A"}\NormalTok{,}\StringTok{"G"}\NormalTok{,}\StringTok{"K"}\NormalTok{,}\StringTok{"N"}\NormalTok{,..}\SpecialCharTok{:} \DecValTok{4} \DecValTok{6} \DecValTok{3} \DecValTok{3} \DecValTok{7} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{7} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ type    }\SpecialCharTok{:}\NormalTok{ Factor w}\SpecialCharTok{/} \DecValTok{2}\NormalTok{ levels }\StringTok{"cold"}\NormalTok{,}\StringTok{"hot"}\SpecialCharTok{:} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ calories}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{70} \DecValTok{120} \DecValTok{70} \DecValTok{50} \DecValTok{110} \DecValTok{110} \DecValTok{110} \DecValTok{130} \DecValTok{90} \DecValTok{90}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ protein }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{4} \DecValTok{3} \DecValTok{4} \DecValTok{4} \DecValTok{2} \DecValTok{2} \DecValTok{2} \DecValTok{3} \DecValTok{2} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fat     }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{1} \DecValTok{5} \DecValTok{1} \DecValTok{0} \DecValTok{2} \DecValTok{2} \DecValTok{0} \DecValTok{2} \DecValTok{1} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sodium  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{130} \DecValTok{15} \DecValTok{260} \DecValTok{140} \DecValTok{200} \DecValTok{180} \DecValTok{125} \DecValTok{210} \DecValTok{200} \DecValTok{210}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fiber   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{10} \DecValTok{2} \DecValTok{9} \DecValTok{14} \DecValTok{1} \FloatTok{1.5} \DecValTok{1} \DecValTok{2} \DecValTok{4} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ carbo   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{5} \DecValTok{8} \DecValTok{7} \DecValTok{8} \DecValTok{14} \FloatTok{10.5} \DecValTok{11} \DecValTok{18} \DecValTok{15} \DecValTok{13}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sugars  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{6} \DecValTok{8} \DecValTok{5} \DecValTok{0} \DecValTok{8} \DecValTok{10} \DecValTok{14} \DecValTok{8} \DecValTok{6} \DecValTok{5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ potass  }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{280} \DecValTok{135} \DecValTok{320} \DecValTok{330} \SpecialCharTok{{-}}\DecValTok{1} \DecValTok{70} \DecValTok{30} \DecValTok{100} \DecValTok{125} \DecValTok{190}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ vitamins}\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{25} \DecValTok{0} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25} \DecValTok{25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ shelf   }\SpecialCharTok{:}\NormalTok{ int  }\DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{3} \DecValTok{1} \DecValTok{2} \DecValTok{3} \DecValTok{1} \DecValTok{3}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ weight  }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \FloatTok{1.33} \DecValTok{1} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cups    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.33} \DecValTok{1} \FloatTok{0.33} \FloatTok{0.5} \FloatTok{0.75} \FloatTok{0.75} \DecValTok{1} \FloatTok{0.75} \FloatTok{0.67} \FloatTok{0.67}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ rating  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{68.4} \DecValTok{34} \FloatTok{59.4} \FloatTok{93.7} \FloatTok{34.4}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

The dataset contains the following variables:

\begin{itemize}
\tightlist
\item
  \texttt{name}: Name of the cereal (categorical).\\
\item
  \texttt{manuf}: Manufacturer of the cereal (categorical).\\
\item
  \texttt{type}: Cereal type (hot or cold, categorical).\\
\item
  \texttt{calories}: Calories per serving (numerical).\\
\item
  \texttt{protein}: Grams of protein per serving (numerical).\\
\item
  \texttt{fat}: Grams of fat per serving (numerical).\\
\item
  \texttt{sodium}: Milligrams of sodium per serving (numerical).\\
\item
  \texttt{fiber}: Grams of dietary fiber per serving (numerical).\\
\item
  \texttt{carbo}: Grams of carbohydrates per serving (numerical).\\
\item
  \texttt{sugars}: Grams of sugar per serving (numerical).\\
\item
  \texttt{potass}: Milligrams of potassium per serving (numerical).\\
\item
  \texttt{vitamins}: Percentage of FDA-recommended vitamins (categorical: 0, 25, or 100).\\
\item
  \texttt{shelf}: Display shelf position (categorical: 1, 2, or 3).\\
\item
  \texttt{weight}: Weight of one serving in ounces (numerical).\\
\item
  \texttt{cups}: Number of cups per serving (numerical).\\
\item
  \texttt{rating}: Cereal rating score (numerical).
\end{itemize}

\subsection{Data Preprocessing}\label{data-preprocessing}

Before applying K-means clustering, we need to \emph{clean and preprocess} the data. We start by summarizing the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(cereal)}
\NormalTok{                           name    manuf    type       calories    }
    \DecValTok{100}\NormalTok{\% Bran                }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   A}\SpecialCharTok{:} \DecValTok{1}\NormalTok{   cold}\SpecialCharTok{:}\DecValTok{74}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{50.0}  
    \DecValTok{100}\NormalTok{\% Natural Bran        }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   G}\SpecialCharTok{:}\DecValTok{22}\NormalTok{   hot }\SpecialCharTok{:} \DecValTok{3}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{100.0}  
\NormalTok{    All}\SpecialCharTok{{-}}\NormalTok{Bran                 }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   K}\SpecialCharTok{:}\DecValTok{23}\NormalTok{             Median }\SpecialCharTok{:}\FloatTok{110.0}  
\NormalTok{    All}\SpecialCharTok{{-}}\NormalTok{Bran with Extra Fiber}\SpecialCharTok{:} \DecValTok{1}\NormalTok{   N}\SpecialCharTok{:} \DecValTok{6}\NormalTok{             Mean   }\SpecialCharTok{:}\FloatTok{106.9}  
\NormalTok{    Almond Delight           }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   P}\SpecialCharTok{:} \DecValTok{9}             \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{110.0}  
\NormalTok{    Apple Cinnamon Cheerios  }\SpecialCharTok{:} \DecValTok{1}\NormalTok{   Q}\SpecialCharTok{:} \DecValTok{8}\NormalTok{             Max.   }\SpecialCharTok{:}\FloatTok{160.0}  
\NormalTok{    (Other)                  }\SpecialCharTok{:}\DecValTok{71}\NormalTok{   R}\SpecialCharTok{:} \DecValTok{8}                            
\NormalTok{       protein           fat            sodium          fiber       }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.000}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.0}\NormalTok{   Min.   }\SpecialCharTok{:} \FloatTok{0.000}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{130.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{1.000}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{3.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{180.0}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{2.000}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{2.545}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{1.013}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{159.7}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{2.152}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{2.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{210.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{3.000}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{6.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{5.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{320.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{14.000}  
                                                                    
\NormalTok{        carbo          sugars           potass          vitamins     }
\NormalTok{    Min.   }\SpecialCharTok{:{-}}\FloatTok{1.0}\NormalTok{   Min.   }\SpecialCharTok{:{-}}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:} \SpecialCharTok{{-}}\FloatTok{1.00}\NormalTok{   Min.   }\SpecialCharTok{:}  \FloatTok{0.00}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{12.0}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{3.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{40.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{14.0}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{7.000}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{90.00}\NormalTok{   Median }\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{14.6}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{6.922}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{96.08}\NormalTok{   Mean   }\SpecialCharTok{:} \FloatTok{28.25}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{17.0}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{11.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{120.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:} \FloatTok{25.00}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{23.0}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{15.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{330.00}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{100.00}  
                                                                     
\NormalTok{        shelf           weight          cups           rating     }
\NormalTok{    Min.   }\SpecialCharTok{:}\FloatTok{1.000}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.50}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{0.250}\NormalTok{   Min.   }\SpecialCharTok{:}\FloatTok{18.04}  
    \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.000}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{1.00}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{0.670}   \DecValTok{1}\NormalTok{st Qu.}\SpecialCharTok{:}\FloatTok{33.17}  
\NormalTok{    Median }\SpecialCharTok{:}\FloatTok{2.000}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{1.00}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{0.750}\NormalTok{   Median }\SpecialCharTok{:}\FloatTok{40.40}  
\NormalTok{    Mean   }\SpecialCharTok{:}\FloatTok{2.208}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{1.03}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{0.821}\NormalTok{   Mean   }\SpecialCharTok{:}\FloatTok{42.67}  
    \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{3.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{1.00}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{1.000}   \DecValTok{3}\NormalTok{rd Qu.}\SpecialCharTok{:}\FloatTok{50.83}  
\NormalTok{    Max.   }\SpecialCharTok{:}\FloatTok{3.000}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{1.50}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{1.500}\NormalTok{   Max.   }\SpecialCharTok{:}\FloatTok{93.70}  
   
\end{Highlighting}
\end{Shaded}

Upon inspection, we notice \emph{unusual values} in the variables \texttt{sugars}, \texttt{carbo}, and \texttt{potass}, where some entries are set to \texttt{-1}. Since negative values are invalid for these nutritional attributes, we replace them with \texttt{NA}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal[cereal }\SpecialCharTok{==} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\FunctionTok{find.na}\NormalTok{(cereal)  }\CommentTok{\# Check missing values}
\NormalTok{        row col}
\NormalTok{   [}\DecValTok{1}\NormalTok{,]  }\DecValTok{58}   \DecValTok{9}
\NormalTok{   [}\DecValTok{2}\NormalTok{,]  }\DecValTok{58}  \DecValTok{10}
\NormalTok{   [}\DecValTok{3}\NormalTok{,]   }\DecValTok{5}  \DecValTok{11}
\NormalTok{   [}\DecValTok{4}\NormalTok{,]  }\DecValTok{21}  \DecValTok{11}
\end{Highlighting}
\end{Shaded}

Next, we handle missing values using \emph{K-nearest neighbors (KNN) imputation} with the \texttt{knnImputation()} function from the \textbf{DMwR2} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DMwR2)}
\NormalTok{cereal }\OtherTok{\textless{}{-}} \FunctionTok{knnImputation}\NormalTok{(cereal, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{find.na}\NormalTok{(cereal)  }\CommentTok{\# Verify missing values are filled}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\StringTok{" No missing values (NA) in the dataset."}
\end{Highlighting}
\end{Shaded}

For clustering, we exclude categorical and identifier variables (\texttt{name}, \texttt{manuf}, and \texttt{rating}), retaining only \emph{numerical features}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selected\_variables }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(cereal)[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{16}\NormalTok{)]}
\NormalTok{cereal\_subset }\OtherTok{\textless{}{-}}\NormalTok{ cereal[, selected\_variables]}
\end{Highlighting}
\end{Shaded}

Since the dataset includes features on \emph{different scales}, we apply \emph{min-max scaling} using the \texttt{minmax()} function from the \textbf{liver} package to ensure all variables contribute equally to the clustering process:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal\_mm }\OtherTok{\textless{}{-}} \FunctionTok{minmax}\NormalTok{(cereal\_subset, }\AttributeTok{col =} \StringTok{"all"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(cereal\_mm)  }\CommentTok{\# Check the transformed dataset}
   \StringTok{\textquotesingle{}data.frame\textquotesingle{}}\SpecialCharTok{:}    \DecValTok{77}\NormalTok{ obs. of  }\DecValTok{13}\NormalTok{ variables}\SpecialCharTok{:}
    \ErrorTok{$}\NormalTok{ type    }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ calories}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.182} \FloatTok{0.636} \FloatTok{0.182} \DecValTok{0} \FloatTok{0.545}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ protein }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.6} \FloatTok{0.4} \FloatTok{0.6} \FloatTok{0.6} \FloatTok{0.2} \FloatTok{0.2} \FloatTok{0.2} \FloatTok{0.4} \FloatTok{0.2} \FloatTok{0.4}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fat     }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.2} \DecValTok{1} \FloatTok{0.2} \DecValTok{0} \FloatTok{0.4} \FloatTok{0.4} \DecValTok{0} \FloatTok{0.4} \FloatTok{0.2} \DecValTok{0}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sodium  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.4062} \FloatTok{0.0469} \FloatTok{0.8125} \FloatTok{0.4375} \FloatTok{0.625}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ fiber   }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.7143} \FloatTok{0.1429} \FloatTok{0.6429} \DecValTok{1} \FloatTok{0.0714}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ carbo   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{0} \FloatTok{0.167} \FloatTok{0.111} \FloatTok{0.167} \FloatTok{0.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ sugars  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.4} \FloatTok{0.533} \FloatTok{0.333} \DecValTok{0} \FloatTok{0.533}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ potass  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.841} \FloatTok{0.381} \FloatTok{0.968} \DecValTok{1} \FloatTok{0.122}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ vitamins}\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.25} \DecValTok{0} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25} \FloatTok{0.25}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ shelf   }\SpecialCharTok{:}\NormalTok{ num  }\DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{1} \DecValTok{0} \FloatTok{0.5} \DecValTok{1} \DecValTok{0} \DecValTok{1}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ weight  }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.5} \FloatTok{0.83} \FloatTok{0.5} \FloatTok{0.5}\NormalTok{ ...}
    \SpecialCharTok{$}\NormalTok{ cups    }\SpecialCharTok{:}\NormalTok{ num  }\FloatTok{0.064} \FloatTok{0.6} \FloatTok{0.064} \FloatTok{0.2} \FloatTok{0.4} \FloatTok{0.4} \FloatTok{0.6} \FloatTok{0.4} \FloatTok{0.336} \FloatTok{0.336}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

To visualize the effect of normalization, we plot the \emph{sodium} distribution before and after scaling:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ cereal) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sodium), }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Before min{-}max normalization"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ cereal\_mm) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sodium), }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"After min{-}max normalization"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{13_Clustering_files/figure-latex/unnamed-chunk-8-1} \includegraphics[width=0.5\linewidth]{13_Clustering_files/figure-latex/unnamed-chunk-8-2}

After scaling, all values fall within the \emph{0--1 range}, making distance-based clustering more reliable.

\subsection{Applying K-means Clustering}\label{applying-k-means-clustering}

\subsubsection*{Choosing the Optimal Number of Clusters}\label{choosing-the-optimal-number-of-clusters}


Before clustering, we need to determine the \emph{optimal number of clusters}. We use the \emph{elbow method}, which plots the within-cluster sum of squares (WCSS) for different values of \(k\). The elbow point---where the improvement in WCSS slows---suggests an ideal \(k\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}

\FunctionTok{fviz\_nbclust}\NormalTok{(cereal\_mm, kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{, }\AttributeTok{k.max =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{4}\NormalTok{, }\AttributeTok{linetype =} \DecValTok{2}\NormalTok{, }\AttributeTok{color =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{13_Clustering_files/figure-latex/unnamed-chunk-9-1} \end{center}

From the plot, we observe that \emph{\(k = 4\) clusters} is a reasonable choice, as adding more clusters beyond this point yields diminishing improvements in WCSS.

\subsubsection*{Performing K-means Clustering}\label{performing-k-means-clustering}


We now apply the \emph{K-means algorithm} with \(k = 4\) clusters:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)  }\CommentTok{\# Ensure reproducibility}
\NormalTok{cereal\_kmeans }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(cereal\_mm, }\AttributeTok{centers =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To check cluster sizes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal\_kmeans}\SpecialCharTok{$}\NormalTok{size}
\NormalTok{   [}\DecValTok{1}\NormalTok{] }\DecValTok{36} \DecValTok{10} \DecValTok{13} \DecValTok{18}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Visualizing the Clusters}\label{visualizing-the-clusters}


To better understand the clustering results, we visualize the clusters using the \texttt{fviz\_cluster()} function from the \textbf{factoextra} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_cluster}\NormalTok{(cereal\_kmeans, cereal\_mm, }\AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }\AttributeTok{ellipse.type =} \StringTok{"norm"}\NormalTok{, }\AttributeTok{palette =} \StringTok{"custom\_palette"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{13_Clustering_files/figure-latex/unnamed-chunk-12-1} \end{center}

The scatter plot displays the \emph{four clusters}, with each point representing a cereal brand. Different colors indicate distinct clusters, and the ellipses represent the \emph{spread of each cluster} based on its standard deviation.

\subsubsection*{Interpreting the Results}\label{interpreting-the-results}


The clusters reveal natural groupings among cereals based on \emph{nutritional content}. For example:\\
- Some clusters may contain \emph{low-sugar, high-fiber cereals}, appealing to health-conscious consumers.\\
- Others may group \emph{high-calorie, high-sugar cereals}, often marketed to children.\\
- Another group may include \emph{balanced cereals}, offering a mix of moderate calories and nutrients.

To examine which cereals belong to a specific cluster (e.g., Cluster 1), we can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cereal}\SpecialCharTok{$}\NormalTok{name[cereal\_kmeans}\SpecialCharTok{$}\NormalTok{cluster }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

This command lists the names of cereals assigned to \emph{Cluster 1}, helping us interpret the characteristics of that group.

This case study demonstrated how \emph{K-means clustering} can segment cereals into meaningful groups based on \emph{nutritional content}. Through \emph{data preprocessing, feature scaling, and cluster visualization}, we successfully grouped cereals with similar characteristics. Such clustering techniques are widely applicable in \emph{marketing, consumer analytics, and product positioning}, providing actionable insights for businesses and researchers alike.

In this chapter, we explored the fundamentals of clustering, the mechanics of the K-means algorithm, and methods for choosing the optimal number of clusters. We then applied these concepts to a real-world dataset, demonstrating how K-means can extract meaningful insights. Clustering remains a powerful tool across various domains, from marketing to bioinformatics, making it an essential technique in the modern data science toolkit.

\section*{Exercises}\label{exercises-9}


These exercises reinforce the concepts introduced in this chapter, focusing on clustering fundamentals, hyperparameter tuning, and practical applications using the \emph{redWines} dataset. The exercises are divided into two categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Conceptual questions -- Understanding the theory behind clustering and K-means.\\
\item
  Practical exercises using the \emph{redWines} dataset -- Applying clustering techniques to real-world data.
\end{enumerate}

\subsection*{Conceptual questions}\label{conceptual-questions-11}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is clustering, and how does it differ from classification?\\
\item
  Explain the concept of similarity measures in clustering. What is the most commonly used distance metric for numerical data?\\
\item
  Why is clustering considered an unsupervised learning method?\\
\item
  What are some real-world applications of clustering? Name at least three.\\
\item
  Define the terms \emph{intra-cluster similarity} and \emph{inter-cluster separation}. Why are these important in clustering?\\
\item
  How does K-means clustering determine which data points belong to a cluster?\\
\item
  Explain the role of centroids in K-means clustering.\\
\item
  What happens if the number of clusters \(k\) in K-means is chosen too small? What if it is too large?\\
\item
  What is the elbow method, and how does it help determine the optimal number of clusters?\\
\item
  Why is K-means sensitive to the initial selection of cluster centers? How does K-means++ address this issue?\\
\item
  Describe a scenario where Euclidean distance might not be an appropriate similarity measure for clustering.\\
\item
  Why do we need to normalize or scale variables before applying K-means clustering?\\
\item
  How does clustering help in dimensionality reduction and preprocessing for supervised learning?\\
\item
  What are the key assumptions of K-means clustering?\\
\item
  How does the silhouette score help evaluate the quality of clustering?\\
\item
  Compare K-means with hierarchical clustering. What are the advantages and disadvantages of each?\\
\item
  Why is K-means not suitable for non-spherical clusters?\\
\item
  What is the difference between hard clustering (e.g., K-means) and soft clustering (e.g., Gaussian Mixture Models)?\\
\item
  What are outliers, and how do they affect K-means clustering?\\
\item
  What are alternative clustering methods that handle outliers better than K-means?
\end{enumerate}

\subsection*{\texorpdfstring{Practical exercises using the \emph{redWines} dataset}{Practical exercises using the redWines dataset}}\label{practical-exercises-using-the-redwines-dataset-1}


The \emph{redWines} dataset contains chemical properties of red wines and their quality scores. These exercises guide you through clustering analysis, from data preprocessing to model evaluation.

\subsubsection*{Data preparation and exploratory analysis}\label{data-preparation-and-exploratory-analysis}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Load the \emph{redWines} dataset from the \textbf{liver} package and inspect its structure.\\
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(liver)}
\FunctionTok{data}\NormalTok{(redWines)}
\FunctionTok{str}\NormalTok{(redWines)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{21}
\tightlist
\item
  Summarize the dataset using \texttt{summary()}. Identify any missing values.\\
\item
  Check the distribution of wine quality scores in the dataset. What is the most common wine quality score?\\
\item
  Since clustering requires numerical features, remove any non-numeric columns from the dataset.\\
\item
  Apply min-max scaling to normalize all numerical variables before clustering. Why is this step necessary?
\end{enumerate}

\subsubsection*{Applying K-means clustering}\label{applying-k-means-clustering-1}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{25}
\tightlist
\item
  Use the elbow method to determine the optimal number of clusters for the dataset.\\
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{fviz\_nbclust}\NormalTok{(redWines, kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\tightlist
\item
  Based on the elbow plot, choose an appropriate value of \(k\) and perform K-means clustering.\\
\item
  Visualize the clusters using a scatter plot of two numerical features.\\
\item
  Compute the silhouette score to evaluate cluster cohesion and separation.\\
\item
  Identify the centroids of the final clusters and interpret their meaning.
\end{enumerate}

\subsubsection*{Interpreting the clusters}\label{interpreting-the-clusters}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\tightlist
\item
  Assign the cluster labels to the original dataset and examine the average chemical composition of each cluster.\\
\item
  Compare the wine quality scores across clusters. Do some clusters contain higher-quality wines than others?\\
\item
  Identify which features contribute most to defining the clusters.\\
\item
  Are certain wine types (e.g., high acidity, high alcohol content) concentrated in specific clusters?\\
\item
  Experiment with different values of \(k\) and compare the clustering results. Does increasing or decreasing \(k\) improve the clustering?\\
\item
  Visualize how wine acidity and alcohol content influence cluster formation.
\end{enumerate}

  \bibliography{book.bib,packages.bib}

\end{document}
